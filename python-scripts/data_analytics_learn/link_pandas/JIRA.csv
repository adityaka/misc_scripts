Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocks),Outward issue link (Bonfire Testing),Outward issue link (Cloners),Outward issue link (Duplicate),Outward issue link (Problem/Incident),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affected Clouds),Custom field (Affects Version/s),Custom field (Approved By),Custom field (Approver),Custom field (Component),Custom field (Component),Custom field (Customers Affected),Custom field (Customers Affected),Custom field (Customers Affected),Custom field (Customers Affected),Custom field (Customers Affected),Custom field (Development),Custom field (Doc Approved),Custom field (Doc Impact),Custom field (Environments),Custom field (Epic Link),Custom field (HR INFO),Custom field (Incident Reported Time),Custom field (Incident Resolution Time),Custom field (Installation),Custom field (Internal Impact),Custom field (Internal Impact Reviewed),Custom field (Need Access To),Custom field (Need-Access-To),Custom field (PR Approval),Custom field (Phone Screen Feedback),Custom field (Phone Screen Score),Custom field (QA Approved),Custom field (QA Engineer),Custom field (QA Notes),Custom field (Raised During),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Notes),Custom field (Release Type),Custom field (Release Version),Custom field (Request Type),Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Custom field (Test Session(s)),Custom field (Testing Status),Custom field (Workaround/ Resolution),Custom field (Zendesk Ticket Count),Custom field (Zendesk Ticket IDs),Custom field ([CHART] Date of First Response),Custom field (ssh-pub-key),Custom field (unix-password-hash),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
"TtransportException in zeppelin logs, interpreter won't start",ZEP-1322,69581,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,namanm,megha,megha,08/Aug/17 5:08 PM,09/Aug/17 2:48 AM,09/Aug/17 6:01 AM,,,,,0,jira_escalated,,,,,,,,"This happened for expedia once, where the interpreter was not starting.
The interpreter logs were not present for about 7 hours i.e 27th july 11:50 UTC to 28th July 07 hours, which was about the time when interpreter wasnt starting up...Also there were a bunch of exceptions in zeppelin logs, the relevant logs are:

{code:java}
 INFO [2017-07-28 06:22:43,563] ({qtp1077325765-10473} InterpreterRestApi.java[restartSetting]:241) - Restart interpreterSetting :q

 INFO [2017-07-28 06:22:43,564] ({Thread-4012} RemoteInterpreter.java[close]:230) - Intp Process is running. Closing client.
 INFO [2017-07-28 06:22:43,565] ({Thread-4012} QuboleRemoteInterpreterHelper.java[closeWithTimeout]:34) - Will close interpreter with timeout
 INFO [2017-07-28 06:22:43,572] ({Thread-4012} RemoteInterpreter.java[close]:237) - Run interpreter process running on port: 40943 shutting down.
 INFO [2017-07-28 06:22:43,572] ({Thread-4012} RemoteInterpreterProcess.java[dereference]:194) - shutdown interpreter process
 INFO [2017-07-28 06:22:43,573] ({Thread-4012} QuboleRemoteInterpreterHelper.java[shutdownWithTimeout]:56) - Will shutdown interpreter with timeout
ERROR [2017-07-28 06:22:46,504] ({Thread-4012} QuboleRemoteInterpreterHelper.java[callWithTimeout]:79) - Error while trying to close/shutdown remote interpreter,will destroy the process
java.util.concurrent.ExecutionException: org.apache.thrift.transport.TTransportException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:202)
	at org.apache.zeppelin.interpreter.remote.QuboleRemoteInterpreterHelper.callWithTimeout(QuboleRemoteInterpreterHelper.java:77)
	at org.apache.zeppelin.interpreter.remote.QuboleRemoteInterpreterHelper.shutdownWithTimeout(QuboleRemoteInterpreterHelper.java:66)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.dereference(RemoteInterpreterProcess.java:201)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.close(RemoteInterpreter.java:248)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.close(LazyOpenInterpreter.java:79)
	at org.apache.zeppelin.interpreter.InterpreterGroup$1.run(InterpreterGroup.java:127)
Caused by: org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_shutdown(RemoteInterpreterService.java:358)
	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.shutdown(RemoteInterpreterService.java:346)
	at org.apache.zeppelin.interpreter.remote.QuboleRemoteInterpreterHelper$2.call(QuboleRemoteInterpreterHelper.java:61)
	at org.apache.zeppelin.interpreter.remote.QuboleRemoteInterpreterHelper$2.call(QuboleRemoteInterpreterHelper.java:58)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.lang.Thread.run(Thread.java:745)
 INFO [2017-07-28 06:22:46,505] ({Exec Default Executor} RemoteInterpreterProcess.java[onProcessComplete]:261) - Interpreter process exited 0
 INFO [2017-07-28 06:22:46,505] ({Thread-4012} RemoteInterpreterProcess.java[dereference]:205) - Exception in RemoteInterpreterProcess while synchronized dereference, can safely ignore exception while client.shutdown() may terminates remote process

{code}

Attaching some of the relevant logs.. There seemed to be a zeppelin restart at Jul 27 12:50:04, which was about 1 hour into the time when above logs are missing.

The cluster was recycled before i looked into this, hence I don't have any live stats.
",,megha,namanm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Aug/17 5:07 PM;megha;zeppelin--ip-10-23-4-61.txt;https://qubole.atlassian.net/secure/attachment/46617/zeppelin--ip-10-23-4-61.txt,08/Aug/17 5:07 PM;megha;zeppelin-interpreter-spark--user_vaksinghal_expedia-ip-10-23-4-61.txt;https://qubole.atlassian.net/secure/attachment/46616/zeppelin-interpreter-spark--user_vaksinghal_expedia-ip-10-23-4-61.txt,08/Aug/17 5:07 PM;megha;zeppelin-interpreter-spark-root-user_vaksinghal_expedia-ip-10-23-4-61.txt;https://qubole.atlassian.net/secure/attachment/46615/zeppelin-interpreter-spark-root-user_vaksinghal_expedia-ip-10-23-4-61.txt,08/Aug/17 5:07 PM;megha;zeppelin-root-ip-10-23-4-61.log.txt;https://qubole.atlassian.net/secure/attachment/46614/zeppelin-root-ip-10-23-4-61.log.txt,08/Aug/17 5:07 PM;megha;zeppelin_restart.txt;https://qubole.atlassian.net/secure/attachment/46618/zeppelin_restart.txt,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05kbr:,,,,,,,,,,,,,,,,,,,1.0,16134,2017-08-09 02:48:52.533,,,"09/Aug/17 2:48 AM;namanm;The TTransportException seen above is harmless and inspite of that the interpreter process was shutdown.
{noformat}
INFO [2017-07-28 06:22:46,505] ({Exec Default Executor} RemoteInterpreterProcess.java[onProcessComplete]:261) - Interpreter process exited 0
{noformat}

However, I see that pyspark interpreter was not found while trying to run the paragraphs after the interpreter was stopped:

{noformat}
INFO [2017-07-27 11:53:36,603] ({qtp1077325765-919} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Set interpreter bindings for note before para run: VFMQW6WA8V1501053395
ERROR [2017-07-27 11:53:36,676] ({qtp1077325765-919} NotebookServer.java[runParagraph]:1193) - Exception from run
org.apache.zeppelin.interpreter.InterpreterException: pyspark interpreter not found
	at org.apache.zeppelin.notebook.NoteInterpreterLoader.get(NoteInterpreterLoader.java:169)
	at org.apache.zeppelin.notebook.NoteInterpreterLoader.get(NoteInterpreterLoader.java:78)
	at org.apache.zeppelin.notebook.Note.run(Note.java:433)
	at org.apache.zeppelin.notebook.Note.run(Note.java:451)
	at org.apache.zeppelin.socket.NotebookServer.runParagraph(NotebookServer.java:1191)
	at org.apache.zeppelin.socket.NotebookServer.onMessage(NotebookServer.java:243)
	at org.apache.zeppelin.socket.NotebookSocket.onMessage(NotebookSocket.java:56)
	at org.eclipse.jetty.websocket.WebSocketConnectionRFC6455$WSFrameHandler.onFrame(WebSocketConnectionRFC6455.java:835)
	at org.eclipse.jetty.websocket.WebSocketParserRFC6455.parseNext(WebSocketParserRFC6455.java:349)
	at org.eclipse.jetty.websocket.WebSocketConnectionRFC6455.handle(WebSocketConnectionRFC6455.java:225)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

Did user remove the spark interpreter by any chance?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New User Invite,AD-149,69580,Bug,Open,AD,Admin,software,bmohanam,,,Major,,snehag,bmohanam,bmohanam,08/Aug/17 4:26 PM,09/Aug/17 12:17 AM,09/Aug/17 5:21 AM,,,,,0,jira_escalated,,,,,,,,"Customer Reported Issue:
Trying to add a new user  -- usually, the UI confirms that an invite email was sent. this time it just lands back on the screen with the send button. I've confirmed that the user has not received an email. any thoughts on what might be going wrong?",,bmohanam,mahuja,nimitk,snehag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05kbj:,,,,,,,,,,,,,,,,,,,1.0,16341,2017-08-08 16:33:20.406,,,08/Aug/17 4:28 PM;bmohanam;[~snehag] Can you reproduce this issue? If reproducible assign it to [~tabraiz]. If not let me know. ,"08/Aug/17 4:33 PM;mahuja;Attached Zendesk ticket. 

Slack conversation in #nextdoor-general at: https://qubole.slack.com/archives/C0DV5EM7T/p1502217264362735","08/Aug/17 10:53 PM;snehag;Seems to be working fine. Although, we have these issues where error is not surfaced to user: 

https://qubole.atlassian.net/browse/MW-987
https://qubole.atlassian.net/browse/AD-93

[~mahuja] Can you confirm if the user for which invite was sent is already part of that account.","08/Aug/17 10:57 PM;mahuja;Yes, the user eventually  became part of the account. I just grok the information walking through custome Slack channel","09/Aug/17 12:17 AM;nimitk;This issue is fixed as part of AD-76 and its already in master. As per labels in jira it will go out with RB47.

cc: [~mayurb]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stackoverflow error while performing union over Dataset type of object,SPAR-1859,69446,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,megha,megha,07/Aug/17 5:19 PM,07/Aug/17 5:19 PM,09/Aug/17 5:27 AM,,,,,0,jira_escalated,,,,,,,,"Command id: 87769202
Spark version 2.0.2

Error log snippet:

{code:java}
App > 17/08/07 16:24:42 Executor task launch worker-0 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
App > java.lang.StackOverflowError
App > 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4717)
App > 	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3777)
App > 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4658)
App > 	at java.util.regex.Pattern$Branch.match(Pattern.java:4602)
App > 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4568)
App > 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4717)
App > 	at java.util.regex.Pattern$Curly.match0(Pattern.java:4279)
App > 	at java.util.regex.Pattern$Curly.match(Pattern.java:4234)
App > 	at java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3798)
App > 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4658)
App > 	at java.util.regex.Pattern$Branch.match(Pattern.java:4604)
App > 	at java.util.regex.Pattern$Branch.match(Pattern.java:4602)
App > 	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4568)
App > 	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4717)
App > 	at java.util.regex.Pattern$Curly.match0(Pattern.java:4279)
App > 	at java.util.regex.Pattern$Curly.match(Pattern.java:4234)
App > 	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4658)
App > 	at java.util.regex.Pattern$Branch.match(Pattern.java:4604)
App > 	at java.util.regex.Pattern$Branch.match(Pattern.java:4602)
App > 	at java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3798)
App > 	at java.util.regex.Pattern$Start.match(Pattern.java:3461)
App > 	at java.util.regex.Matcher.search(Matcher.java:1248)
App > 	at java.util.regex.Matcher.find(Matcher.java:664)
App > 	at java.util.Formatter.parse(Formatter.java:2549)
App > 	at java.util.Formatter.format(Formatter.java:2501)
App > 	at java.util.Formatter.format(Formatter.java:2455)
App > 	at java.lang.String.format(String.java:2981)
App > 	at scala.collection.immutable.StringLike$class.formatLocal(StringLike.scala:335)
App > 	at scala.collection.immutable.StringOps.formatLocal(StringOps.scala:29)
App > 	at org.apache.spark.util.Utils$.bytesToString(Utils.scala:1152)
App > 	at org.apache.spark.util.Utils.bytesToString(Utils.scala)
App > 	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:185)
App > 	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:245)
App > 	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:272)

{code}
 

Attached is the code snippet within which a union is being performed on the dataset (object type is also Dataset).. 

The error seems to be coming from spark codebase and not from customer's code 

Customer is able to run the code on their local (will check for spark version)",,megha,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Aug/17 5:16 PM;megha;code.java;https://qubole.atlassian.net/secure/attachment/46548/code.java,,,,,,,,,,,,,AWS,,,,,None,oracle,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05jnz:,,,,,,,,,,,,,,,,,,,1.0,16265,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hive query fails due to ""Unable to read cluster information from JSON""",ACM-1435,69352,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,adityak,navdeepp,navdeepp,04/Aug/17 10:23 PM,09/Aug/17 4:54 AM,09/Aug/17 5:15 AM,,,,,0,jira_escalated,sustenance,,,,,,,"88198427
{code}
No config file specified - defaulting to hustler/configs/config.default for config file
Traceback (most recent call last):
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/bin/easy_hustler.py"", line 174, in
main()
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/bin/easy_hustler.py"", line 94, in main
overrides_str = "";"".join(qconf.get_cluster_config())
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/lib/py/hive_scripts/qbol_config.py"", line 3690, in get_cluster_config
cluster_config_from_tapp = curl.Curl.get(url, api_token)
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/lib/py/hive_scripts/utils/curl.py"", line 32, in get
api_result = json.loads(data.getvalue())
File ""/usr/lib64/python2.7/json/__init__.py"", line 339, in loads
return _default_decoder.decode(s)
File ""/usr/lib64/python2.7/json/decoder.py"", line 364, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
File ""/usr/lib64/python2.7/json/decoder.py"", line 382, in raw_decode
raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
FAILED: Hive Internal Error: java.lang.RuntimeException(java.io.IOException: Unable to read cluster information from JSON:
)
{code}
",,ajayb,navdeepp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05j3j:,,,,,,,,,,,,,,,,,,,2.0,"16292,16312",2017-08-08 08:35:29.444,,,"07/Aug/17 5:57 PM;navdeepp;Lyft is also facing this intermittently,
88626366

{code}
Getting Hadoop cluster information ...
No config file specified - defaulting to hustler/configs/config.default for config file
2017-08-07 18:26:30,001 ERROR tunnel_utils.py:56 - create_proxy - Cannot get local port for rds: hive-metastore-production.cu3yieho9vrv.us-east-1.rds.amazonaws.com. Exception: invalid literal for int() with base 10: ''.
Traceback (most recent call last):
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/bin/easy_hustler.py"", line 174, in
main()
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/bin/easy_hustler.py"", line 94, in main
overrides_str = "";"".join(qconf.get_cluster_config())
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/lib/py/hive_scripts/qbol_config.py"", line 3880, in get_cluster_config
ms = self.get_metastore_for_account()
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/lib/py/hive_scripts/qbol_config.py"", line 1838, in get_metastore_for_account
ms['id'], ms['dbmachine'], ms['dbport'], user, key_loc, private_key, bastion_node_port)
File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/lib/py/hive_scripts/utils/tunnel_utils.py"", line 57, in create_proxy
raise e
ValueError: invalid literal for int() with base 10: ''
FAILED: RuntimeException java.io.IOException: Unable to read cluster information from JSON:
java.lang.RuntimeException: java.io.IOException: Unable to read cluster information from JSON:
at org.apache.hadoop.hive.ql.exec.Utilities.getAuthorities(Utilities.java:4140)
at org.apache.hadoop.hive.ql.session.SessionState.populateHadoopAuthorities(SessionState.java:751)
at org.apache.hadoop.hive.ql.Context.populateHadoopAuthorities(Context.java:224)
at org.apache.hadoop.hive.ql.Context.getRelativizedScratchDir(Context.java:346)
at org.apache.hadoop.hive.ql.Context.getMRScratchDir(Context.java:374)
at org.apache.hadoop.hive.ql.Context.getMRTmpPath(Context.java:509)
at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.splitTasks(GenMapRedUtils.java:1025)
at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.splitPlan(GenMapRedUtils.java:417)
at org.apache.hadoop.hive.ql.optimizer.GenMRRedSink2.process(GenMRRedSink2.java:70)
at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:54)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.generateTaskTree(MapReduceCompiler.java:322)
at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:205)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10348)
at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:208)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:239)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:470)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:334)
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1196)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1250)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1133)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1123)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:250)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:197)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:353)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:497)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:853)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:808)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:715)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.io.IOException: Unable to read cluster information from JSON:
at com.qubole.hadoop.cluster.HadoopInitializer.parseJson(HadoopInitializer.java:35)
at com.qubole.hadoop.cluster.HadoopInitializer.getAuthorities(HadoopInitializer.java:66)
at org.apache.hadoop.hive.ql.exec.Utilities.getAuthorities(Utilities.java:4136)
... 43 more
Caused by: org.json.JSONException: A JSONObject text must begin with '{' at character 0
at org.json.JSONTokener.syntaxError(JSONTokener.java:410)
at org.json.JSONObject.(JSONObject.java:183)
at org.json.JSONObject.(JSONObject.java:431)
at com.qubole.hadoop.cluster.HadoopInitializer.parseJson(HadoopInitializer.java:25)
... 45 more
{code}

Hive-tier logs
{code}
...
ip-10-99-179-16 2017-08-07 18:25:26,590 [command:88626366] INFO  optimizer.ColumnPrunerProcFactory (ColumnPrunerProcFactory.java:pruneReduceSinkOperator(901)) - RS 115 newColExprMap: {KEY._col2=Column[_col2], KEY._col3=Column[_col3], KEY._col0=Column[_col0], KEY._col1=Column[_col1]}
ip-10-99-179-16 2017-08-07 18:25:26,590 [command:88626366] INFO  optimizer.ColumnPrunerProcFactory (ColumnPrunerProcFactory.java:pruneReduceSinkOperator(852)) - RS 28 oldColExprMap: {KEY._col2=Column[_col2], KEY._col3=Column[_col3], KEY._col0=Column[_col0], KEY._col1=Column[_col1], KEY._col4=Column[_col4]}
ip-10-99-179-16 2017-08-07 18:26:30,051 [command:88626366] ERROR SessionState (SessionState.java:printError(1352)) - FAILED: RuntimeException java.io.IOException: Unable to read cluster information from JSON:
ip-10-99-179-16 2017-08-07 18:26:30,055 [command:88626366] INFO  log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=releaseLocks start=1502130390055 end=1502130390055 duration=0 from=org.apache.hadoop.hive.ql.Driver>
ip-10-99-179-16 2017-08-07 18:26:30,063 [command:88626366] INFO  session.QuboleStatusMonitor (QuboleStatusMonitor.java:close(320)) - Closing Status Monitor
ip-10-99-179-16 2017-08-07 18:26:30,063 [command:88626366] INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(747)) - 0: Shutting down the object store...
ip-10-99-179-16 2017-08-07 18:26:30,218 [command:88626366] INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(747)) - 0: Metastore shutdown complete.
ip-10-99-179-16 2017-08-07 18:26:30,241 [command:88626366] INFO  mr.HadoopJobExecHelper (HadoopJobExecHelper.java:run(297)) - Killing all jobs
ip-10-99-179-16 2017-08-07 18:26:30,051 [command:88626366] INFO  log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=compile start=1502130310712 end=1502130390051 duration=79339 from=org.apache.hadoop.hive.ql.Driver>
ip-10-99-179-16 2017-08-07 18:26:30,051 [command:88626366] INFO  metadata.Hive (Hive.java:logDumpPhase(4353)) - Dumping metastore api call timing information for : compilation phase
ip-10-99-179-16 2017-08-07 18:26:30,051 [command:88626366] INFO  metadata.Hive (Hive.java:dumpAndClearMetaCallTiming(4344)) - Total time spent in this metastore function was greater than 1000ms : listPartitionsByExpr_(String, String, byte[], String, short, List, )=3651
ip-10-99-179-16 2017-08-07 18:26:30,051 [command:88626366] INFO  log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
ip-10-99-179-16 2017-08-07 18:26:30,051 [command:88626366] INFO  log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=releaseLocks start=1502130390051 end=1502130390051 duration=0 from=org.apache.hadoop.hive.ql.Driver>
ip-10-99-179-16 2017-08-07 18:26:30,055 [command:88626366] INFO  log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
{code}",08/Aug/17 8:35 AM;ajayb;[~navdeepp] you got some pointers from [~ajayaa] on the escalation-acm slack channel about next steps to investigate. See https://qubole.slack.com/archives/C44ATUP9V/p1502173310723316. Did you get chance to look at the logs as suggested there to debug this further?,"09/Aug/17 1:13 AM;navdeepp;[~ajayb] I had updated my last comment with hive-tier logs, the logs shown in UI are being written to hive logs. Also, I asked for further help from MW https://qubole.slack.com/archives/C43KWH50A/p1502197613717967
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Poor performance with spark.sql.qubole.split.computation=true,SPAR-1852,69349,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Critical,,mahuja,mpatel,mpatel,04/Aug/17 6:12 PM,06/Aug/17 10:50 PM,09/Aug/17 5:29 AM,,,,,0,jira_escalated,,,,,,,,"Expedia ran the same sparksql with spark.sql.qubole.split.computation=true (default), and false.

The execution of the query when spark.sql.qubole.split.computation=false, is ~3-4 mins, vs 2+hrs when it is true:

Fast: 88166567
Slow: 88145546

I'm seeing a ton of these messages in the slow logs:
{code}
17/08/04 20:38:28 main INFO S3AFileSystem: Using the credentials attached to the instance
17/08/04 20:38:28 main INFO S3AFileSystem: Initializing S3A FileSystem...
{code}

",,adas,mpatel,venkats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,Expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05j2v:,,,,,,,,,,,,,,,,,,,1.0,16003,2017-08-05 05:38:20.25,,,04/Aug/17 6:14 PM;mpatel;As a note: We're working with AWS because we found a big drop in performance for hive IOW + dynamic partition on July 14th and its been poor since. Our findings there are that it is related to s3 listings...,"05/Aug/17 5:38 AM;venkats;[~mpatel] I don't see this config set --conf spark.sql.hive.convertMetastoreOrc=true (this is by default is false I think) in the command where it ran faster with IPP disabled. Can we try disabling IPP with the spark.sql.hive.convertMetastoreOrc config set to true.

Below logs (excerpt) if you see IPP listing took very less time after which it seems like it stuck in Initializing S3AFileSystem for a while, similar logs getting printed for every table in the SQL. Another thing we can try here is try disabling S3AFileSystem and use NativeS3FileSystem to see if its an issue with S3AFileSystem or not.
  
{code:java}
App > 17/08/04 18:56:26 main WARN UserGroupInformation: No groups available for user vnistala
App > 17/08/04 18:56:26 main WARN UserGroupInformation: No groups available for user vnistala
App > 17/08/04 18:56:29 main INFO HiveMetastoreCatalog: No of partitions to be read: 9741
App > 17/08/04 18:56:29 main INFO HadoopFsRelation: Ignore File not found exceptions - true
App > 17/08/04 18:56:29 main INFO QuboleUtils: Using IPP optimization to get file statuses for the list of input file paths
App > 17/08/04 18:56:29 main INFO QuboleUtils: Input paths size - 9741
App > 17/08/04 18:56:29 main INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
App > 17/08/04 18:56:29 main INFO deprecation: mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.d
ir.recursive
App > 17/08/04 18:56:29 main INFO S3AFileSystem: Initializing S3A FileSystem...
App > 17/08/04 18:56:29 main INFO S3AFileSystem: Using the credentials attached to the instance
{code}
","05/Aug/17 5:56 AM;venkats;It seems like because of this config spark.hadoop.fs.s3a.impl.disable.cache=true, it is creating a new fileSystem object when we call getFileSystem on each of the paths discovered, which seems to be a lot of paths and thats why its taking a lot of time that explains why we are seeing the ""Initializing S3AFileSystem ... "" logs. 

Any reason to disable the cache? Btw, I don't see it being disabled in the Fast command (88166567) which is why its faster I think.

On a side note, I see spark.dynamicAllocation.enabled is set to false but --max-executors option is given, this won't scale up to max executors. I think we should tell them not to disable unless they are doing it for some specific reason.","06/Aug/17 9:22 PM;mpatel;Just noting down some test runs here:

||Command ID||IPP||*hive.convertMetastoreOrc||*fs.s3a.impl.disable.cache||Run Time||
|88501128|enabled (default)|true|false (default)|2hrs 9m|
|88145546|enabled (default)|true|true|2hrs 25m|
|88312052|enabled (default)|false|false (default)|8m 33s|
|88179943|disabled|false|false(default)|3m 33s|
|88166171|disabled|true|false (default)|error|

The thing which jumps out is that spark.sql.hive.convertMetastoreOrc seems to contribute to the most significant impact.

I also see that between 88312052 and 88179943, there's still a > 2x difference.

Can you check if there was any changes related to convertMetastoreOrc recently?

","06/Aug/17 10:50 PM;venkats;It seems like the last command error is fixed in this OS JIRA - https://issues.apache.org/jira/browse/SPARK-16948. Also we can try rerunning these commands on Spark-2.1.1, I know we have recently ported IPP optimizations to Spark-2.1.0 and 2.1.1 in R46. It seems like there are lot of fixes and new improvements introduced in that code path between 2.0.2 to 2.1.0. IMHO, we should give Spark-2.1.0 a try.

{code:java}
App > Exception in thread ""main"" java.util.NoSuchElementException: None.get
App > 	at scala.None$.get(Option.scala:347)
App > 	at scala.None$.get(Option.scala:345)
App > 	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$13.apply(HiveMetastoreCatalog.scala:301)
App > 	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$13.apply(HiveMetastoreCatalog.scala:288)
App > 	at scala.Option.getOrElse(Option.scala:121)
App > 	at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$convertToLogicalRelation(HiveMetastoreCatalog.scala:288)
App > 	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$.org$apache$spark$sql$hive$HiveMetastoreCatalog$OrcConversions$$convertToOrcRelation(HiveMetastoreCatalog.scala:419)
App > 	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$$anonfun$apply$2.applyOrElse(HiveMetastoreCatalog.scala:442)
App > 	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$$anonfun$apply$2.applyOrElse(HiveMetastoreCatalog.scala:427)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
App > 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
App > 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
App > 	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$.apply(HiveMetastoreCatalog.scala:427)
App > 	at org.apache.spark.sql.hive.HiveMetastoreCatalog$OrcConversions$.apply(HiveMetastoreCatalog.scala:408)
App > 	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
App > 	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
App > 	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
App > 	at scala.collection.immutable.List.foldLeft(List.scala:84)
App > 	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
App > 	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
App > 	at scala.collection.immutable.List.foreach(List.scala:381)
App > 	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
App > 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)
App > 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)
App > 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
App > 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
App > 	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
App > 	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)
App > 	at Matrix$.main(script.scala:176)
App > 	at Matrix.main(script.scala)
App > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
App > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
App > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
App > 	at java.lang.reflect.Method.invoke(Method.java:606)
App > 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:876)
App > 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
App > 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)
App > 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:130)
App > 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restart of presto in node bootstrap leads to failure of first query that starts the cluster,PRES-1199,69260,Bug,Open,PRES,Presto,software,stagra,,,Critical,,,biswajit,biswajit,03/Aug/17 10:56 PM,07/Aug/17 11:13 PM,09/Aug/17 3:53 AM,,,,,0,jira_escalated,,,,,,,,"Ankit as discussed over chat on the channel, raising a ticket for this. 

slack chat link of escalation channel:- 
https://qubole.slack.com/archives/C44R8EHAR/p1501752329419710

Issue:- 
The first query that starts the cluster fails, since cluster comes up and the query is spawned but the presto daemon is restarted from nodebootstrap and the query fails. The rerun of the query works fine after some time 

command id :- 
87748564

please do let me know if you need any logs",,biswajit,drose@qubole.com,rvenkatesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,tubemogul,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05in3:,,,,,,,,,,,,,,,,,,,1.0,16249,2017-08-07 22:43:29.744,,,"07/Aug/17 10:43 PM;drose@qubole.com;[~rvenkatesh][~stagra] This is affecting prod scheduled workloads for Adobe, can get this assigned and scope out some next steps to helping the customer.  This is the first group of reports from Vertica with hundreds to hopefully follow.","07/Aug/17 11:12 PM;rvenkatesh;cc [~ajayb] [~sriramg] Anyway we can control the status of the cluster in the node bootstrap ? The node bootstrap is restarting the server but the cluster is marked as up.

[~drose@qubole.com] what is the downside of running 1 node cluster always ?  ","07/Aug/17 11:13 PM;rvenkatesh;[~drose@qubole.com] Sorry - didnt complete. I understand it will cost more money. Is that something the customer will accept as a solution ?

Also what is the node bootstrap doing ? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lateral views on a table is taking long time to start ,HIVE-2431,69241,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Critical,,sbhatia,sbadam,sbadam,03/Aug/17 5:01 PM,07/Aug/17 5:12 PM,09/Aug/17 3:54 AM,,,,,0,jira_escalated,,,,,,,,"Command didn't start MR job for 2 hours. I login into hive tier node and took thread dump of hive CLI process. I have attached to this ticket. 

https://api.qubole.com/v2/analyze?command_id=87985929

Describe table - https://api.qubole.com/v2/analyze?command_id=88000627

",,asomani,sbadam,sbhatia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Aug/17 5:11 PM;sbadam;88633186_td2.txt;https://qubole.atlassian.net/secure/attachment/46546/88633186_td2.txt,07/Aug/17 5:11 PM;sbadam;88633186_thread_dump1;https://qubole.atlassian.net/secure/attachment/46547/88633186_thread_dump1,03/Aug/17 5:00 PM;sbadam;ps_1080_td1.log;https://qubole.atlassian.net/secure/attachment/46464/ps_1080_td1.log,03/Aug/17 5:00 PM;sbadam;ps_1080_td2.log;https://qubole.atlassian.net/secure/attachment/46465/ps_1080_td2.log,03/Aug/17 5:00 PM;sbadam;ps_1080_td3.log;https://qubole.atlassian.net/secure/attachment/46466/ps_1080_td3.log,03/Aug/17 5:00 PM;sbadam;ps_1080_td4.log;https://qubole.atlassian.net/secure/attachment/46467/ps_1080_td4.log,,,,,,,,AWS,,,,,None,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05ij3:,,,,,,,,,,,,,,,,,,,1.0,16247,2017-08-03 17:41:43.412,,,"03/Aug/17 5:41 PM;sbhatia;[~sbadam] Can you ask the customer to run this query with hive2.1.1. As Kulbir pointed it maybe because of this issue. https://issues.apache.org/jira/browse/HIVE-11141.
I will also look more into the patch and see what more optimizations they are doing.",07/Aug/17 2:54 PM;sbadam;[~sbhatia] - customer has run it on Hive2.1.1. But It still failed - https://api.qubole.com/v2/analyze?command_id=88633186 . I have attached couple of thread dumps of this command.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tag in Template API is considered as cluster label,MW-1249,69161,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,sbadam,sbadam,02/Aug/17 2:22 PM,03/Aug/17 8:55 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Tag in Template API is considered as cluster label

As per documentation(http://docs.qubole.com/en/latest/rest-api/command_template_api/run-command-template-api.html), tag means adding a tag to Template. But when we run, it is considered as cluster label.

{code:java}
sbadam-106-mbp:~ sbadam$ curl -i -X POST -H ""X-AUTH-TOKEN: $AUTH_TOKEN"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" -d '{""input_vars"":[{""year"":""2017""}, {""quarter"":""2""}], ""tag"":[""aws-test""]}' ""https://api.qubole.com/api/v1.2/command_templates/2340/run""
HTTP/1.1 422 Unprocessable Entity
Content-Type: application/json; charset=utf-8
Transfer-Encoding: chunked
Connection: keep-alive
Status: 422 Unprocessable Entity
X-UA-Compatible: IE=Edge,chrome=1
Cache-Control: no-cache
Set-Cookie: qbol_user_id=24866; path=/; secure
Set-Cookie: _tapp_session=f2c363e593314e3eaa0b7e0a4c2d5d0e; path=/; secure; HttpOnly
X-Request-Id: 7479f43c8a7941c5b976ea65886bf500
Date: Wed, 02 Aug 2017 20:56:33 GMT
Server: qubole
X-Frame-Options: SAMEORIGIN
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
X-XSS-Protection: 1; mode=block
X-Content-Type-Options: nosniff
X-Download-Options: noopen

{""error"":{""error_code"":422,""error_message"":""Incorrect parameters: There is no cluster with label '[\""aws-test\""]'.""}}
{code}


Nothing worked to an argument - ""tags"". 

{code:java}
curl -i -X POST -H ""X-AUTH-TOKEN: $AUTH_TOKEN"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" -d '{""input_vars"":[{""year"":""2017""}, {""quarter"":""2""}], ""tag"":""default"", ""tags"":""Testing1""}' ""https://api.qubole.com/api/v1.2/command_templates/2340/run""
{code}

-> Customer is expecting tag means adding a tag to the template/ command but not cluster label. 

",api.qubole.com,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05i1j:,,,,,,,,,,,,,,,,,,,1.0,16172,,,,"03/Aug/17 8:55 AM;sbadam;Customer question:

{code:java}
Is there a way to set the actual tags on the command (for searching purposes in command history) when invoking the template through the API (python sdk)?
{code}

Answer is, we have only REST API but not python SDK. Am I right, [~sumitm] ?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock was detected in Zep 0.6.2,ZEP-1277,69143,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Critical,,beria,sbadam,sbadam,02/Aug/17 9:28 AM,03/Aug/17 3:00 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Deadlock was detected in Zep interpreter(0.6.2)

Stack trace:


{code:java}
Found one Java-level deadlock:
=============================
""qtp1619236510-77795"":
  waiting to lock monitor 0x00007f9898171b78 (object 0x0000000580465b88, a java.util.HashMap),
  which is held by ""qtp1619236510-75330""
""qtp1619236510-75330"":
  waiting to lock monitor 0x00007f98bc034e08 (object 0x00000005ad119f70, a org.apache.zeppelin.interpreter.InterpreterGroup),
  which is held by ""pool-41-thread-33""
""pool-41-thread-33"":
  waiting to lock monitor 0x00007f98a40793b8 (object 0x00000005ad138010, a org.apache.zeppelin.interpreter.remote.RemoteInterpreter),
  which is held by ""Thread-21944""
""Thread-21944"":
  waiting to lock monitor 0x00007f98bc034e08 (object 0x00000005ad119f70, a org.apache.zeppelin.interpreter.InterpreterGroup),
  which is held by ""pool-41-thread-33""
Java stack information for the threads listed above:
===================================================
""qtp1619236510-77795"":
  at org.apache.zeppelin.interpreter.InterpreterFactory.getNoteInterpreterSettingBinding(InterpreterFactory.java:816)
  - waiting to lock <0x0000000580465b88> (a java.util.HashMap)
  at org.apache.zeppelin.notebook.NoteInterpreterLoader.getInterpreterSettings(NoteInterpreterLoader.java:62)
  at org.apache.zeppelin.notebook.Notebook.getBindedInterpreterSettings(Notebook.java:312)
  at org.apache.zeppelin.rest.NotebookRestApi.bind(NotebookRestApi.java:251)
  at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.apache.cxf.service.invoker.AbstractInvoker.performInvocation(AbstractInvoker.java:180)
  at org.apache.cxf.service.invoker.AbstractInvoker.invoke(AbstractInvoker.java:96)
  at org.apache.cxf.jaxrs.JAXRSInvoker.invoke(JAXRSInvoker.java:192)
  at org.apache.cxf.jaxrs.JAXRSInvoker.invoke(JAXRSInvoker.java:100)
  at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:57)
  at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:93)
  at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:272)
  - locked <0x0000000732a35d90> (a org.apache.cxf.phase.PhaseInterceptorChain)
  at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:121)
  at org.apache.cxf.transport.http.AbstractHTTPDestination.invoke(AbstractHTTPDestination.java:239)
  at org.apache.cxf.transport.servlet.ServletController.invokeDestination(ServletController.java:248)
  at org.apache.cxf.transport.servlet.ServletController.invoke(ServletController.java:222)
  at org.apache.cxf.transport.servlet.ServletController.invoke(ServletController.java:153)
  at org.apache.cxf.transport.servlet.CXFNonSpringServlet.invoke(CXFNonSpringServlet.java:167)
  at org.apache.cxf.transport.servlet.AbstractHTTPServlet.handleRequest(AbstractHTTPServlet.java:286)
  at org.apache.cxf.transport.servlet.AbstractHTTPServlet.doGet(AbstractHTTPServlet.java:211)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:575)
  at org.apache.cxf.transport.servlet.AbstractHTTPServlet.service(AbstractHTTPServlet.java:262)
  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:812)
  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1669)
  at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
  at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
  at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
  at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
  at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
  at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
  at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
  at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
  at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
  at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
  at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
  at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
  at org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:72)
  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)
  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577)
  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223)
  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)
  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)
  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)
  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
  at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)
  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
  at org.eclipse.jetty.server.Server.handle(Server.java:499)
  at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
  at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
  at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
  at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
  at java.lang.Thread.run(Thread.java:745)
""qtp1619236510-75330"":
  at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getInterpreterProcess(RemoteInterpreter.java:155)
  - waiting to lock <0x00000005ad119f70> (a org.apache.zeppelin.interpreter.InterpreterGroup)
  at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getScheduler(RemoteInterpreter.java:452)
  at org.apache.zeppelin.interpreter.LazyOpenInterpreter.getScheduler(LazyOpenInterpreter.java:137)
  at org.apache.zeppelin.interpreter.QuboleInterpreterFactoryHelper.stopSchedulerJobs(QuboleInterpreterFactoryHelper.java:28)
  at org.apache.zeppelin.interpreter.InterpreterFactory.restart(InterpreterFactory.java:874)
  - locked <0x0000000580465b88> (a java.util.HashMap)
  at org.apache.zeppelin.rest.InterpreterRestApi.stopSetting(InterpreterRestApi.java:244)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.apache.cxf.service.invoker.AbstractInvoker.performInvocation(AbstractInvoker.java:180)
  at org.apache.cxf.service.invoker.AbstractInvoker.invoke(AbstractInvoker.java:96)
  at org.apache.cxf.jaxrs.JAXRSInvoker.invoke(JAXRSInvoker.java:192)
  at org.apache.cxf.jaxrs.JAXRSInvoker.invoke(JAXRSInvoker.java:100)
  at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:57)
  at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:93)
  at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:272)
  - locked <0x00000005addf4b10> (a org.apache.cxf.phase.PhaseInterceptorChain)
  at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:121)
  at org.apache.cxf.transport.http.AbstractHTTPDestination.invoke(AbstractHTTPDestination.java:239)
  at org.apache.cxf.transport.servlet.ServletController.invokeDestination(ServletController.java:248)
  at org.apache.cxf.transport.servlet.ServletController.invoke(ServletController.java:222)
  at org.apache.cxf.transport.servlet.ServletController.invoke(ServletController.java:153)
  at org.apache.cxf.transport.servlet.CXFNonSpringServlet.invoke(CXFNonSpringServlet.java:167)
  at org.apache.cxf.transport.servlet.AbstractHTTPServlet.handleRequest(AbstractHTTPServlet.java:286)
  at org.apache.cxf.transport.servlet.AbstractHTTPServlet.doPut(AbstractHTTPServlet.java:223)
  at javax.servlet.http.HttpServlet.service(HttpServlet.java:598)
  at org.apache.cxf.transport.servlet.AbstractHTTPServlet.service(AbstractHTTPServlet.java:262)
  at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:812)
  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1669)
  at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
  at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
  at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
  at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
  at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
  at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
  at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
  at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
  at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
  at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
  at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
  at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
  at org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:72)
  at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)
  at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)
  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
  at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577)
  at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223)
  at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)
  at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)
  at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
  at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)
  at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
  at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)
  at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
  at org.eclipse.jetty.server.Server.handle(Server.java:499)
  at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
  at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
  at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
  at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
  at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
  at java.lang.Thread.run(Thread.java:745)
""pool-41-thread-33"":
  at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.init(RemoteInterpreter.java:181)
  - waiting to lock <0x00000005ad138010> (a org.apache.zeppelin.interpreter.remote.RemoteInterpreter)
  at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:247)
  - locked <0x00000005ad119f70> (a org.apache.zeppelin.interpreter.InterpreterGroup)
  at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
  - locked <0x00000005ad1171d8> (a org.apache.zeppelin.interpreter.remote.RemoteInterpreter)
  at org.apache.zeppelin.util.InterpreterStarter.run(InterpreterStarter.java:39)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
""Thread-21944"":
  at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:240)
  - waiting to lock <0x00000005ad119f70> (a org.apache.zeppelin.interpreter.InterpreterGroup)
  at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
  - locked <0x00000005ad138010> (a org.apache.zeppelin.interpreter.remote.RemoteInterpreter)
  at org.apache.zeppelin.interpreter.LazyOpenInterpreter.getProgress(LazyOpenInterpreter.java:127)
  at org.apache.zeppelin.notebook.Paragraph.progress(Paragraph.java:270)
  at org.apache.zeppelin.scheduler.JobProgressPoller.run(JobProgressPoller.java:51)
Found 1 deadlock.
...
{code}


Customer details:

{code:java}
Hi , I'm not being able to execute the paragraphs or visualize the interpreters on qubole. cluster id is : 11310, account: stg.ipg, notebook id 42635
{code}

Log location - s3://com.autodesk.edl.stg/apps/qubole/logs/hadoop/11310/559302/ec2-13-56-164-165.us-west-1.compute.amazonaws.com.master/zeppelin/logs



",api.qubole.com,beria,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,autodesk,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05hxj:,,,,,,,,,,,,,,,,,,,1.0,16223,2017-08-03 03:00:02.577,,,"03/Aug/17 3:00 AM;beria;This issue is already resolved ZEP-1098 , but was missed to be CP-ed in release-branch-45 . Will upload package for the same for now, and will be fixed in RB-46.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC results in Unexpected token END OF FILE at position 65535,MW-1247,69142,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sumitm,mpatel,mpatel,02/Aug/17 8:46 AM,02/Aug/17 8:46 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer is running 86907823 via JDBC driver. 

They are getting the following error on the client side. I think there's some issue with the log, in Analyze, they are unable to pull up the logs:

{code}
Unexpected token END OF FILE at position 65535.
at org.json.simple.shaded.qubole.parser.JSONParser.parse(Unknown Source)
at org.json.simple.shaded.qubole.parser.JSONParser.parse(Unknown Source)
at org.json.simple.shaded.qubole.parser.JSONParser.parse(Unknown Source)
at com.qubole.jdbc.common.qubole.QInterface.WaitForQueryToFinishAndFetchResults(QInterface.java:641)
at com.qubole.jdbc.common.qubole.QQuery.ExecuteQuery(QQuery.java:64)
at com.qubole.jdbc.common.dataengine.ULQueryExecutor.execute(ULQueryExecutor.java:228)
at com.simba.jdbc.common.SPreparedStatement.executeWithParams(Unknown Source)
at com.simba.jdbc.common.SPreparedStatement.executeQuery(Unknown Source)
at com.expedia.fraud.modeling.util.QuboleJdbcClient.queryExecute(QuboleJdbcClient.java:325)
at com.expedia.fraud.modeling.util.QuboleJdbcClient.addDatatoTransaction(QuboleJdbcClient.java:396)
at com.expedia.fraud.modeling.ScoreVisualizer.main(ScoreVisualizer.java:126)
java.sql.SQLException: [Qubole][QuboleDSII](100601) Exception: Unexpected token END OF FILE at position 65535.
at com.qubole.jdbc.common.dataengine.ULQueryExecutor.execute(ULQueryExecutor.java:234)
at com.simba.jdbc.common.SPreparedStatement.executeWithParams(Unknown Source)
at com.simba.jdbc.common.SPreparedStatement.executeQuery(Unknown Source)
at com.expedia.fraud.modeling.util.QuboleJdbcClient.queryExecute(QuboleJdbcClient.java:325)
at com.expedia.fraud.modeling.util.QuboleJdbcClient.addDatatoTransaction(QuboleJdbcClient.java:396)
Caused by: com.simba.support.exceptions.GeneralException: [Qubole][QuboleDSII](100601) Exception: Unexpected token END OF FILE at position 65535.
{code}

",,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,Expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05hxb:,,,,,,,,,,,,,,,,,,,1.0,16177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster Update Email list is not refreshed with User roles change,ACM-1418,69081,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,satyavathib,satyavathib,02/Aug/17 12:59 AM,02/Aug/17 1:01 AM,09/Aug/17 4:37 AM,,,,,0,jira_escalated,,,,,,,,"When ever we change the people in the list of system admin group , I think we are expected to refresh the list of people who are supposed to receive the cluster modification update emails. But this is not happening. I have tested this on my account as well, But it seems not updating the list as per the changes.

This issue is reported by a customer that the people who are removed from system admins are still receiving the emails for cluster modifications.

Please let me know If I am missing something here.",,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,adobe,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05hnb:,,,,,,,,,,,,,,,,,,,1.0,16066,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop fs related job failures for traveloka,HADTWO-1047,69023,Bug,Open,HADTWO,Hadoop2,software,ajayb,,,Major,,ayushia,navdeepp,navdeepp,01/Aug/17 7:25 AM,04/Aug/17 10:20 AM,09/Aug/17 6:03 AM,,,,,0,bootcamp,jira_escalated,,,,,,,"87561151
Caused by: java.io.IOException: Directory: /media/ebs2/s3ncache does not exist and cannot be made

{code}
2017-08-01 13:37:52,419 INFO [TezChild] s3a.S3AFileSystem: Initializing S3A FileSystem...
2017-08-01 13:37:52,423 INFO [TezChild] s3a.S3AFileSystem: Using the credentials attached to the instance
2017-08-01 13:37:53,940 ERROR [TezChild] tez.TezProcessor: java.lang.RuntimeException: java.io.IOException: java.io.IOException: org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Unable to read schema from given path: s3://tvlk-data-qubole-prod/avro_schema/edw.fact_flight_segment_booking.avsc
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:192)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:131)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:97)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:149)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:80)
	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:614)
	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:593)
	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:141)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:109)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:129)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:337)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: java.io.IOException: org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Unable to read schema from given path: s3://tvlk-data-qubole-prod/avro_schema/edw.fact_flight_segment_booking.avsc
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:250)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:189)
	... 25 more
Caused by: java.io.IOException: org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Unable to read schema from given path: s3://tvlk-data-qubole-prod/avro_schema/edw.fact_flight_segment_booking.avsc
	at org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.<init>(AvroGenericRecordReader.java:72)
	at org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat.getRecordReader(AvroContainerInputFormat.java:51)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:248)
	... 26 more
Caused by: org.apache.hadoop.hive.serde2.avro.AvroSerdeException: Unable to read schema from given path: s3://tvlk-data-qubole-prod/avro_schema/edw.fact_flight_segment_booking.avsc
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:146)
	at org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.getSchema(AvroGenericRecordReader.java:112)
	at org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.<init>(AvroGenericRecordReader.java:70)
	... 28 more
Caused by: java.io.IOException: Directory: /media/ebs2/s3ncache does not exist and cannot be made
	at org.apache.hadoop.fs.s3native.Cache.<init>(Cache.java:32)
	at org.apache.hadoop.fs.s3native.Cache.<init>(Cache.java:25)
	at org.apache.hadoop.fs.s3native.Cache.getLocalizedFile(Cache.java:169)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:132)
	... 30 more
{code}

87524327
Caused by: java.io.IOException: Directory: /media/ebs2/s3ncache does not exist and cannot be made
{code}
same as above
{code}

87364535
org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid directory for output-
{code}
2017-07-31 11:57:18,352 WARN [TezChild] fs.LocalDirAllocator$AllocatorPerContext$DirSelector: Failed to create /media/ebs2/s3a
2017-07-31 11:57:18,352 INFO [TezChild] fs.LocalDirAllocator$AllocatorPerContext$DirSelector: No valid dirs found for context: fs.s3a.buffer.dir
2017-07-31 11:57:18,371 FATAL [TezChild] tez.ReduceRecordSource: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 28
	at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:467)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:399)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:250)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:249)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
 at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
 at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:337)
 at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
 ]
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:472)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecordVector(ReduceRecordSource.java:399)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:250)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:249)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
 at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
 at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:337)
 at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid directory for output-
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:871)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:102)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:138)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.processVectorGroup(ReduceRecordSource.java:445)
	... 18 more
Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find any valid directory for output-
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$DirSelector.getPathForWrite(LocalDirAllocator.java:541)
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:627)
	at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:640)
	at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:221)
	at org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:93)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:607)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:914)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:895)
	at org.apache.hadoop.hive.ql.io.orc.WriterImpl.getStream(WriterImpl.java:2113)
	at org.apache.hadoop.hive.ql.io.orc.WriterImpl.flushStripe(WriterImpl.java:2130)
	at org.apache.hadoop.hive.ql.io.orc.WriterImpl.checkMemory(WriterImpl.java:353)
	at org.apache.hadoop.hive.ql.io.orc.MemoryManager.notifyWriters(MemoryManager.java:168)
	at org.apache.hadoop.hive.ql.io.orc.MemoryManager.addedRow(MemoryManager.java:157)
	at org.apache.hadoop.hive.ql.io.orc.WriterImpl.addRow(WriterImpl.java:2423)
	at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.write(OrcOutputFormat.java:86)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:837)
	... 22 more
{code}

For the first two commands (87561151, 87524327), verified from ganglia metrics that there are no disk space issues.

The cluster uses heterogeneous nodes, and all the reported failed tasks have run on ""m1.xlarge"" node types. The jobs are successful intermittently. Please help in determining root case for this.",,hiyer,navdeepp,sourabhg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2017-08-09T05:11:46.000-0700"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05hb3:,,,,,,,,,,,,,,,,,,,1.0,16165,2017-08-04 03:50:46.414,,,04/Aug/17 3:50 AM;sourabhg;[~abhishekmodi] [~hiyer]: Any findings you might want to share? Would help us in debugging this further. ,"04/Aug/17 10:20 AM;hiyer;What is happening here is:  
1. Master is an EBS-only type and does not have  any ephemeral drives. Hence the value of fs.s3a.buffer.dir is set to /media/ebs*  
2. Worker is an instance which does *not* support ebs volumes, e.g m1, m2, i3, d2, etc. Hence it does not have an ebs volumes and fs.s3a.buffer.dir is set to /media/ephemeral*  
3. fs.s3a.buffer.dir is a final property, so if the job starts on master, it picks up the value from there and it *cannot* be overridden by the value on worker. So the task tries to use /media/ebs* and fails.  

The suggested solution:  
1. Since on EBS-only types we symlink ebs* to ephemeral*, we should use the ephemeral* path instead when specifying the buffer dir.  
2. On instances that have instance store *and* support EBS, we should again symlink the ebs* to ephemeral*. eg, if an instance has 3 instance store volumes and 1 EBS, the EBS volume will be symlinked to /media/ephemeral3.  
3. One case to consider is that the master may have more volumes (IS + EBS) than the worker. Say the master has eph0-3 and the worker has only eph0-2 - in this case if we try to write to eph3 on the worker it may fail. Thus we need to ensure LocalDirAllocator ignores non-existant locations (it may already be doing so.)  

As such (1) and (2) above could be fixed by the ACM team, and only the 3rd part needs to be fixed/verified by us.  

Also see SPAR-1772 for some more context.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobProgressListener is logging wrong metric values ( possible overflow? ),SPAR-1831,68924,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,venkats,adubey,adubey,31/Jul/17 12:45 PM,02/Aug/17 9:28 PM,09/Aug/17 5:29 AM,,,,,1,jira_escalated,,,,,,,,"cc [~mahuja]


{code:java}
production-replica> select * from metrics where query_hists_id=86645915;
+------------+----------------+----------------------------------------+--------------+---------------------+---------------------+
| id         | query_hists_id | name                                   | value        | created_at          | updated_at          |
+------------+----------------+----------------------------------------+--------------+---------------------+---------------------+
| 1097613433 |       86645915 | FileSystemCounters.S3N_BYTES_READ      | -58956700000 | 2017-07-27 07:30:23 | 2017-07-27 07:30:23 |
| 1097613435 |       86645915 | FileSystemCounters.S3N_BYTES_WRITTEN   |            0 | 2017-07-27 07:30:23 | 2017-07-27 07:30:23 |
| 1097613437 |       86645915 | FileSystemCounters.S3N_RECORDS_READ    |    143390000 | 2017-07-27 07:30:23 | 2017-07-27 07:30:23 |
| 1097613439 |       86645915 | FileSystemCounters.S3N_RECORDS_WRITTEN |    142350000 | 2017-07-27 07:30:23 | 2017-07-27 07:30:23 |
| 1097613441 |       86645915 | Job Counters.SQL_EXECUTION_COUNT       |            0 | 2017-07-27 07:30:23 | 2017-07-27 07:30:23 |
+------------+----------------+----------------------------------------+--------------+---------------------+---------------------+

{code}


Can we investigate if this is some kind of overflow scenario ..
q1: bytes_read is negative 
q2: why dont we have other metrics related to CPU etc.",,adubey,bharatb,drose@qubole.com,karuppayyar,mahuja,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,oracle,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05h0n:,,,,,,,,,,,,,,,,,,,1.0,16173,2017-07-31 12:49:03.117,,,31/Jul/17 12:49 PM;mahuja;cc - [~bharatb],31/Jul/17 12:51 PM;adubey;not sure if this is an overflow case from a quick look - looks like we are using Long for capturing these numbers.,"31/Jul/17 4:54 PM;bharatb;1. spark does not allow easy capturing of metrics at app level. They have detailed info at stage and task level. We are adding metrics at app level now and some of them will go live with r46.

2. this looks like a genuine bug in the counting rather than overflow (like [~adubey] said above).","01/Aug/17 5:41 AM;karuppayyar;I see the following exception in driver logs
{code:java}
App > 17/07/27 01:14:47 Thread-3 WARN SdkMBeanRegistrySupport:
App > java.security.AccessControlException: access denied (""javax.management.MBeanTrustPermission"" ""register"")
App >   at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
App >   at java.lang.SecurityManager.checkPermission(SecurityManager.java:585)
App >   at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.checkMBeanTrustPermission(DefaultMBeanServerInterceptor.java:1848)
App >   at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:322)
App >   at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
App >   at com.qubole.com.amazonaws.jmx.MBeans.registerMBean(MBeans.java:52)
App >   at com.qubole.com.amazonaws.jmx.SdkMBeanRegistrySupport.registerMetricAdminMBean(SdkMBeanRegistrySupport.java:27)
App >   at com.qubole.com.amazonaws.metrics.AwsSdkMetrics.registerMetricAdminMBean(AwsSdkMetrics.java:390)
App >   at com.qubole.com.amazonaws.metrics.AwsSdkMetrics.<clinit>(AwsSdkMetrics.java:351)
App >   at com.qubole.com.amazonaws.services.s3.AmazonS3Client.<clinit>(AmazonS3Client.java:197)
App >   at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:371)
App >   at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2773)
App >   at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:92)
App >   at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2807)
App >   at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2789)
App >   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:376)
App >   at org.apache.hadoop.fs.Path.getFileSystem(Path.java:298)
App >   at org.apache.hadoop.mapred.InputPathProcessor.addAbsolutePath(InputPathProcessor.java:192)
App >   at org.apache.hadoop.mapred.InputPathProcessor.addInputPaths(InputPathProcessor.java:215)
App >   at org.apache.hadoop.mapred.InputPathProcessor.<init>(InputPathProcessor.java:141)
App >   at org.apache.hadoop.mapred.InputPathProcessor.<init>(InputPathProcessor.java:80)
App >   at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:226)
App >   at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:337)
App >   at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:333)
{code}
But dont have a concrete reason for the negative number.
Also [~bharatb]
I see this in driver log 

{code:java}
App > 17/07/27 01:30:12 main INFO JobProgressListener: Counters=FileSystemCounters.S3N_BYTES_READ:-58956671278,FileSystemCounters.S3N_BYTES_WRITTEN:0,FileSystemCounters.S3N_RECORDS_READ:143390261,FileSystemCounters.S3N_RECORDS_WRITTEN:142349672,Job Counters.SQL_EXECUTION_COUNT:0
{code}
The one on db(in description) seems to be different. Is this known issue?","02/Aug/17 6:32 AM;bharatb;The backtrace is a warning and not a problem.

The wrong metrics is a problem. We need to debug that. Can you please paste the actual code used and any other anomalies you find, here?","02/Aug/17 9:28 PM;karuppayyar;the query has 325 lines of code, you can access it using https://api.qubole.com/super_admin/query_hists/86645915/redirect",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Error ""Unable to fetch job stats"" using View Hadoop Jobs Spawned By a Command API",HADTWO-1042,68916,Bug,Open,HADTWO,Hadoop2,software,ajayb,,,Minor,,hiyer,wvaldez,wvaldez,31/Jul/17 9:12 AM,09/Aug/17 1:34 AM,09/Aug/17 6:03 AM,,,,,0,in-r47,jira_escalated,,,,,,,"Oracle user reported this issue and I was able to replicate with my account.

I run a Hive Query on Hadoop cluster with user willie@valdez.net which has super-admin access to wvaldez account (owned by wvaldez@qubole.com).

User wvaldez@qubole.com can use the View Hadoop Jobs Spawned By a Command API just fine but for willie@valdez.net it returns this:


{code:java}
[{""url"":""https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-54-82-72-41.compute-1.amazonaws.com%3A8088%2Fproxy%2Fapplication_1501516169382_0001%2F&clusterInst=564270"",""job_stats"":null,""error_message"":""*Unable to fetch job stats. Either the job doesn't exist or it has been retired by the JT*.""}]
{code}

This is command used:

{code:java}
curl  -i -X GET -H ""X-AUTH-TOKEN: $AUTH_TOKEN"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" ""https://api.qubole.com/api/v1.2/commands/87395622/jobs""
{code}

AUTH-TOKEN for willie@valdez.net
{code:java}
a2d5e7fdb95a494197a4208617c1eec2ca395c45cda54e7cb3585d4ef819bdbe
{code}

AUTH-TOKEN for wvaldez@qubole.com
{code:java}
xjezMQ6K6TLZYKPfAEoHyF8dyzpZ6cRYzbHQNqAUPJmDLgqudV68ffqgzarQQx7M
{code}

At first I thought it was a permissions issue but even with both being system-admin the problem still persists.",api.qubole.com,gayathrym,hiyer,sumitm,wvaldez,,,,,,,,,,,,,,,,,,,,,ROLL-102,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,Oracle,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2017-08-08T00:36:51.000-0700"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05gyv:,,,,,,,,,,,,,,,,,,,1.0,15618,2017-07-31 10:53:31.181,,,"31/Jul/17 10:53 AM;hiyer;The call to cluster proxy is failing with this error:
{code}
irb(main):007:0> response = QbolHelper::make_authenticated_https_request(cluster_app_info_url, auth_token)
=> #<Net::HTTPForbidden 403 encodedUrl doesn't belong to auth-token. readbody=true>
{code}

And that's because we're unable to find the account to which this auth token belongs.
{code}
production-replica> select account_id from qbol_users where authentication_token='a2d5e7fdb95a494197a4208617c1eec2ca395c45cda54e7cb3585d4ef819bdbe';
Empty set (0.00 sec)
{code}

[~wvaldez] I'm assuming the auth-token is valid. That being the case, [~sumitm] why would we not be able to find the user/account to which this auth token belongs?","01/Aug/17 12:38 AM;sumitm;[~hiyer] we keep auth_token in the encrypted format nowadays, so you can't search them directly. However, I was going through the code and found that we do send unencrypted keys only. 

[~hiyer] is cluster_proxy also talk to DB and tries to match the auth tokens?","01/Aug/17 12:42 AM;hiyer;> cluster_proxy also talk to DB and tries to match the auth tokens?  

Yes, cluster proxy also does that. Are you saying all such queries should now be rewritten to ""select blah from qbol_users where authentication_token='foo' or encrypt(authentication_token) = 'foo';""? ","01/Aug/17 1:03 AM;sumitm;[~hiyer] unfortunately yes. Just wondering how come we didn't catch this issue in past 2 months, time since the change went into prod. ","01/Aug/17 1:05 AM;hiyer;I guess that's because this has been selectively enabled? The other auth key (xjezMQ6K6TLZYKPfAEoHyF8dyzpZ6cRYzbHQNqAUPJmDLgqudV68ffqgzarQQx7M), for example, is not encrypted and that's why it works fine.","01/Aug/17 1:10 AM;sumitm;[~hiyer] on all dev/qa envs its encrypted.. On prod like envs we've not done the backfill, so only tokens created after the change are encrypted. Infact we wanted to encrypt tokens for all prod envs via a rake job, see ROLL-102.","01/Aug/17 1:19 AM;hiyer;> on all dev/qa envs its encrypted  

I guess nobody uses this API on dev/qa.  

> we wanted to encrypt tokens for all prod envs via a rake job  

Please hold off on that until this jira is fixed. I have updated the jira accordingly.",01/Aug/17 10:23 AM;wvaldez;[~hiyer] do we have an ETA on when this would be working?  Oracle is eager to be able to use this API.,02/Aug/17 7:28 PM;hiyer;[~wvaldez] the fix is ready. We can probably hotfix it next week if required.,03/Aug/17 7:38 AM;wvaldez;[~hiyer] it would be great to have the hotfix if possible.  if I understand the problem I think this might affect other APIs.,"07/Aug/17 11:14 AM;wvaldez;[~hiyer] Oracle is pushing for having this in sooner than later.  Can you provide an ETA for the hotfix please?

Thank you,
Willie","09/Aug/17 1:34 AM;gayathrym;Commit to MASTER :
*  1aff841a48abd51722e32708a6c94b26447ba2ab	Tue Aug 8 08:16:35 2017	cluster_proxy	master	HADTWO-1042	hiyer@qubole.com	fix: dev: HADTWO-1042: handle encrypted auth tokens		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Packaged dags not working in airflow 1.7.0,AIR-38,68829,Bug,Open,AIR,Airflow,software,sumitm,,,Major,,sumitm,navdeepp,navdeepp,28/Jul/17 8:39 AM,02/Aug/17 11:25 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"https://airflow.incubator.apache.org/concepts.html#packaged-dags

I created a zip file with some dags and copied to dags folder but airflow did not register those dags.
test cluster: 27860

Found OS bug for the version 1.7.1.2,
https://issues.apache.org/jira/browse/AIRFLOW-178

Please confirm if this is expected to break in 1.7.0",,navdeepp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05gfr:,,,,,,,,,,,,,,,,,,,1.0,16088,,,,02/Aug/17 11:25 PM;navdeepp;Will confirm if airflow update to version 1.8.1 solves this after it's out.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"There is no ""Remove"" option seen in last empty paragraph of a Notebook ",ZEP-1242,68735,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,mmajithia,p.vasa,p.vasa,27/Jul/17 4:29 PM,03/Aug/17 10:33 PM,09/Aug/17 6:03 AM,,,UI,,0,jira_escalated,,,,,,,,"Generally, under the Notebooks interface on UI, there is a gear icon to the right side corner of each paragraph.

When you click on the Gear icon, you can see the following options ->
a) Move Up
b) Insert New
c) Show Title 
d) Show Line Numbers etc. (Please refer the screen shot -> *""paragraph_options.png""*)

Out of these options, one such option is -> *""Remove""*
This is the last option which appears on the drop down list on clicking the gear icon.

Now, if there is an EMPTY paragraph and if it is also the last paragraph in a Notebook, then when you click on the ""Gear"" icon, the ""REMOVE"" option is hidden and never seen on the UI.
(A couple of customers have complained about this and also I was able to reproduce this on my own account)
(Please refer to the screen shot -> *""no_remove_option.png""*)

Please let me know if you need any additional information.",,p.vasa,rgupta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Jul/17 4:29 PM;p.vasa;no_remove_option.png.png;https://qubole.atlassian.net/secure/attachment/46156/no_remove_option.png.png,27/Jul/17 4:29 PM;p.vasa;paragraph_options.png.png;https://qubole.atlassian.net/secure/attachment/46155/paragraph_options.png.png,,,,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05g3j:,,,,,,nb-RB-47,,,,,,,,,,,,,2.0,"15975,16117",2017-07-27 20:42:54.672,,,27/Jul/17 8:42 PM;rgupta;I think this is supposed to be a feature. AFAIR ppl used to remove last para and then thr was no way to add new paras. [~mmajithia] can confirm?,"03/Aug/17 10:33 PM;rgupta;I digged in to this further. This was done as part of OS: https://qubole.atlassian.net/browse/ZEP-346 and we pulled it from thr.
I think its safe to add functionality to remove last para too....

[~mmajithia] lets target rb47 for this too?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notebook cursor issues,ZEP-1224,68601,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,rgupta,satyavathib,satyavathib,26/Jul/17 1:50 AM,27/Jul/17 9:42 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Issue is reported from reltio regarding notebook cursor stability. The summary of the issue is :

Some times when you click on a paragraph and try to input code, the cursor is in the paragraph but you cannot input anything, and some times your input character will replace the previous character. The problem will go away after you click somewhere else and come back to the paragraph again.

Not sure what data points we could get from the customer to prove this.",,rgupta,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,reltio,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05fa7:,,,,,,,,,,,,,,,,,,,1.0,15853,2017-07-27 21:42:09.87,,,27/Jul/17 9:42 PM;rgupta;editor in notebooks work in emacs mode. That cud be cause of trouble. [~mmajithia] is working on fixing it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SAML login is not working for Sovrn,MW-1230,68506,Bug,Open,MW,Middleware,software,sumitm,,,Blocker,,sumitm,sbadam,sbadam,25/Jul/17 10:53 PM,27/Jul/17 9:27 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"SAML login is not working for Sovrn. They have seen ""validation failed"" error in UI.

We haven't changed in UI but they are started seeing the problem So it must be problem on their SAML provider(BItium).

Errors we have seen in logs are:


{code:java}
[ERROR pid: 14009: 17-07-25 14:01:08 ] Digest mismatch
[ERROR pid: 14009: 17-07-25 14:01:08 ] /usr/lib/ruby/gems/2.1.0/gems/ruby-saml-0.8.1/lib/xml_security.rb:105:in `block in validate_signature'
/usr/lib/ruby/2.1.0/rexml/xpath.rb:67:in `each'
/usr/lib/ruby/2.1.0/rexml/xpath.rb:67:in `each'
/usr/lib/ruby/gems/2.1.0/gems/ruby-saml-0.8.1/lib/xml_security.rb:92:in `validate_signature'
/usr/lib/ruby/gems/2.1.0/gems/ruby-saml-0.8.1/lib/xml_security.rb:62:in `validate_document'
/usr/lib/ruby/gems/2.1.0/gems/ruby-saml-0.8.1/lib/onelogin/ruby-saml/response.rb:121:in `validate'
/usr/lib/ruby/gems/2.1.0/gems/ruby-saml-0.8.1/lib/onelogin/ruby-saml/response.rb:33:in `validate!'
/usr/lib/qubole/packages/tapp2-45.35.8/tapp/app/controllers/saml_controller.rb:50:in `callback'
{code}

-> We have asked Sovrn folks to share sha256 fingerprint.

",,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,sovrn,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05f6f:,,,,,,,,,,,,,,,,,,,1.0,16012,,,,"25/Jul/17 10:54 PM;sbadam;We are waiting on customer while we are debugging. They have option of resetting password and login via username-password but they are not happy with that solution. Marking it as Blocker as it needs to solved asap.

cc - [~drose@qubole.com]","27/Jul/17 9:27 AM;sbadam;[~sumitm], could you please let me know what we should we communicate to customer on ETA on the fix?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default Fair Scheduler pool specified in cluster config not being used,ACM-1386,68493,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Critical,,sankets,Kulbir,Kulbir,25/Jul/17 8:57 PM,07/Aug/17 10:12 AM,09/Aug/17 4:26 AM,,,,,1,jira_escalated,,,,,,,,"Environment details:
qbol_acc3321_cl34003

Modified cluster config to change default FSP from ""root.default"" to ""default"" and did ""Update and Push"", however still when hivecli.py is being launched we are specifying pool name with old value:

{code}
ec2-user  4273  4264  9 02:26 ?        00:00:05 python /usr/lib/hive_scripts/hivecli.py -f /tmp/qexec20170726-15458-1fq2cn0 --ugi syang@lyft.com,default_group *--pool root.default* --cmdid 65116694 --qid 86421595 --sid 0 -q -s -c hive.session.id=5205847 -c qubole.session.id=5205847 -i /tmp/mdhist20170726-15458-4r760b 3321 --cluster-id 34003 --qbol-user-id 27319 --retry 0 --hive-version=2.1.1 --use-hive-tier --is-adhoc-query --md-path /media/ephemeral0/tmp/tapp/tmp/2017-07-26/3321/86421595.md --slave-instance-type r4.8xlarge --master-instance-type r3.4xlarge
{code}

Why is middleware code still using old values, though the Fair Scheduler settings are pushable ?

Do we cache cluster config and reuse it on MW side until cluster is recycled ?

cc [~sumitm]",,ajayb,drose@qubole.com,Kulbir,sankets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Jul/17 3:16 AM;sankets;Screen Shot 2017-07-27 at 3.44.53 PM.png;https://qubole.atlassian.net/secure/attachment/46144/Screen+Shot+2017-07-27+at+3.44.53+PM.png,,,,,,,,,,,,,AWS,,,,,None,Lyft,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05f3j:,,,,,,,,,,,,,,,,,,,1.0,15824,2017-07-26 21:42:50.019,,,"26/Jul/17 6:46 PM;Kulbir;[~ajayb] [~sumitm] Lyft will likely escalate this soon, since due to this issue + current config, effectively jobs are not getting assigned to per user queues but instead to ""root.default"", which is not ideal.

Can we triage this quickly ?

cc [~adubey]",26/Jul/17 9:42 PM;drose@qubole.com;[~ajayb]Upgrading this to critical based on today's on-site visit.,26/Jul/17 10:50 PM;ajayb;Looks like the same as ACM-1023. [~sankets] can you go through the comments in MW-469 and prepare a fix?,"27/Jul/17 3:20 AM;sankets; !Screen Shot 2017-07-27 at 3.44.53 PM.png|thumbnail! 

They have overwritten FSP at query level that's why update of FSP value at cluster level is not having any effect on the query that is run. Just changing query level FSP value should fix the issue. Attached the screen shot as well.","31/Jul/17 7:25 PM;Kulbir;[~sankets] let's get on a call to discuss this , there is some weirdness in how FSP queue is being picked at command level. Seems like we pick this field based on value defined at cluster level, however if you rerun a old job(at which pt. may be cluster config was different) it can continue to use old setting instead of using current cluster config, plus sometimes the field is not even picked at all for default cluster.

Let me know what time works for you.","07/Aug/17 10:12 AM;Kulbir;[~sankets] any updates yet ?
I need to provide an update to customer so will appreciate your immediate response.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Redshift query fails with ""SQL exception executing statement: org.postgresql.util.PSQLException: Unable to interpret the update count in command completion tag""",SQOOP-120,68472,Bug,Open,SQOOP,SQOOP,software,sumitm,,,Major,,sumitm,navdeepp,navdeepp,25/Jul/17 12:35 PM,25/Jul/17 12:36 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Commands: 85245320, 82443093

The redshift query is failing with ""SQL exception executing statement: org.postgresql.util.PSQLException: Unable to interpret the update count in command completion tag: INSERT 0 5767815420"" Exception.

{code}
17/07/20 00:45:32 INFO -1 util.TunnelUtils: Tunnel Command output:20170
17/07/20 01:23:21 WARN -1 tool.EvalSqlTool: SQL exception executing statement: org.postgresql.util.PSQLException: Unable to interpret the update count in command completion tag: INSERT 0 5767815420.
at org.postgresql.core.v3.QueryExecutorImpl.interpretCommandStatus(QueryExecutorImpl.java:2138)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1797)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:257)
at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:510)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:386)
at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:379)
at org.apache.sqoop.tool.EvalSqlTool.run(EvalSqlTool.java:80)
at org.apache.sqoop.Sqoop.run(Sqoop.java:173)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:209)
at org.apache.sqoop.Sqoop.runTool(Sqoop.java:248)
at org.apache.sqoop.Sqoop.runTool(Sqoop.java:257)
at org.apache.sqoop.Sqoop.main(Sqoop.java:266)
2017-07-20 01:23:23,747 WARNING dbtapquerycli.py:130 - execute - sqoop retCode = 1
{code}

This seems relevant to the changes at :
https://github.com/pgjdbc/pgjdbc/blob/REL9_3_1102/org/postgresql/core/v3/QueryExecutorImpl.java#L2233
https://stackoverflow.com/questions/27043186/org-postgresql-util-psqlexception-unable-to-interpret-the-update-count-in-comma
",,navdeepp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05eyv:,,,,,,,,,,,,,,,,,,,1.0,15647,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connecting Tableau to Qubole using ODBC,ODBC-151,68467,Bug,Open,ODBC,odbc,software,stagra,,,Major,,sakshia,mayureshp,addon_zendesk_for_jira,25/Jul/17 11:15 AM,08/Aug/17 9:54 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Hi Team, 

We are trying to connect to Tableau over ODBC driver and when we try connecting we are getting following error.
Jul 23 12:29:47.694 INFO  5048 StatementState::InternalPrepare: Preparing query: CREATE LOCAL TEMPORARY TABLE `XT01_C5820E27_4718_4A55_8552_E8DFD54CA5B9_1_Connect` (
    `COL` INTEGER
    ) ON COMMIT PRESERVE ROWS
Jul 23 12:29:47.694 TRACE 5048 QConnection::ToNativeSQL: +++++ enter +++++
Jul 23 12:29:47.695 ERROR 5048 QConnection::ToNativeSQL: [Qubole][SQLEngine] (31480) syntax error near 'CREATE LOCAL TEMPORARY<<< ??? >>> TABLE `XT01_C5820E27_4718_4A55_8552_E8DFD54CA5B9_1_Connect` (
    `COL` INTEGER
    ) ON COMMIT PRESERVE ROWS'.
Jul 23 12:29:47.695 TRACE 5048 QStatement::CreateDataEngine: +++++ enter +++++
Jul 23 12:29:47.695 TRACE 5048 QDataEngine::QDataEngine: +++++ enter +++++
Jul 23 12:29:47.695 TRACE 5048 QDataEngine::Prepare: +++++ enter +++++
Jul 23 12:29:47.695 DEBUG 5048 QDataEngine::Prepare: Executing Query: CREATE LOCAL TEMPORARY TABLE `XT01_C5820E27_4718_4A55_8552_E8DFD54CA5B9_1_Connect` (
    `COL` INTEGER
    ) ON COMMIT PRESERVE ROWS
Jul 23 12:29:47.695 DEBUG 5048 QQuery::GetQueryMetadata: Starting to execute query: CREATE LOCAL TEMPORARY TABLE `XT01_C5820E27_4718_4A55_8552_E8DFD54CA5B9_1_Connect` (
    `COL` INTEGER
    ) ON COMMIT PRESERVE ROWS
Jul 23 12:29:48.275 FATAL 5048 HiveInterface::IssueCommandAndFetchQueryId: Query: CREATE LOCAL TEMPORARY TABLE `XT01_C5820E27_4718_4A55_8552_E8DFD54CA5B9_1_Connect` (
    `COL` INTEGER
    ) ON COMMIT PRESERVE ROWS Query Id: 85937607
Jul 23 12:29:48.275 DEBUG 5048 QQuery::GetQueryMetadata: Query id: 85937607








h3. Zendesk Support Attachments


[Attachment 1|https://qubole.zendesk.com/attachments/token/RjRRiLPOxKh3kSiqVoTbm0FYP/?name=Qubole]
",,addon_zendesk_for_jira,goden,mayureshp,sakshia,stagra,udayk,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jul/17 1:58 AM;udayk;Hive.tdc;https://qubole.atlassian.net/secure/attachment/46100/Hive.tdc,26/Jul/17 1:58 AM;udayk;Presto.tdc;https://qubole.atlassian.net/secure/attachment/46101/Presto.tdc,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05exr:,,,,,,,,,,,,,,,,,,,1.0,16002,2017-07-25 20:42:06.176,,,"25/Jul/17 8:42 PM;stagra;From the commands issued it looks like that you dont have tdc setup correctly.

Passing to [~sakshia] to take it forward.

C.C. [~udayk]",26/Jul/17 1:59 AM;udayk;[~mayureshp] Driver name has been changed recently due to which this issue might have happened. It will take some time to update tdc in paid-qubole. So share the following files with customer and check if the issue is happening. [^Hive.tdc]  [^Presto.tdc] ,"26/Jul/17 2:02 AM;mayureshp;Thanks for the update.

On Jul 26, 2017 1:59 AM, ""Uday Kiran (JIRA)"" <jira@qubole.atlassian.net>

","26/Jul/17 2:49 AM;stagra;[~udayk] this would mean we will have failures if any existing customer updates drivers?
This would be a cause of worry for some customers. Is it possible to revert the driver name?","26/Jul/17 3:06 AM;udayk;Since there will be only one dsn installed by the installer, driver name was changed. If we want to retain this single dsn, customers have to update their tdc else we need to create a new installer with multiple dsn pointing to older names.","26/Jul/17 3:09 AM;stagra;In case we cannot revert this, please send out a mail to customer success, solutions and support about this change. 
If possible, we should make sure that customer knows about updating tdc file while downloading the driver, maybe put up a note in bold red in release notes or some place else. 
I am worried that this is bite us back in future with failures in big accounts.
c.c. [~goden]",26/Jul/17 12:41 PM;goden;can we update the tdc file in paid-qubole?,26/Jul/17 12:41 PM;goden;also I believe bitquill told me they're making a change to eliminate downloading separate tdc file with the driver installer (it'll be included),"03/Aug/17 5:07 PM;goden;[~sakshia] can you take a look and verify the fix
===
We have committed the changes to the installer for the TDC files and pushed them to the bitquill branch in ODBC.

The commits are:
- 154d5b8
- 9d07393
- ae468cc

The Spark TDC is based on the Hive one.

The installer will install them to the directories as specified in the previous e-mail. The installer will make a best-effort to install the TDC files for every user, but it only works if the user has the My Tableau Repository directory under Documents, which gets created the first time the user runs Tableau Desktop.

We have tested that the TDC files get installed to the correct locations for Tableau Desktop, Tableau Server 32-bit, and Tableau Server 64-bit.","04/Aug/17 2:23 AM;sakshia;[~goden] There is a change in all the tdc files. The driver name in all the files, QuboleULInstaller/Build/TDC/Hive.tdc, QuboleULInstaller/Build/TDC/Presto.tdc and QuboleULInstaller/Build/TDC/Spark.tdc is now common. 'QuboleODBC'

Hence,  the property <driver name /> has to been changed to <driver name='QuboleODBC' /> for all the 3 tdc files. I will make those changes in paid-qubole as soon as possible. ","08/Aug/17 9:54 PM;sakshia;The tdc files in paid-qubole have been updated.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expedia-Hotels Cluster start issues for r3.4xlarge instance type,ACM-1375,68446,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,navdeepp,navdeepp,25/Jul/17 7:21 AM,26/Jul/17 6:48 AM,09/Aug/17 4:18 AM,,,,,0,jira_escalated,,,,,,,,"Account: 6656
Clusters: 34107, (34107 > clone: 34496 )

The master node seems to be unresponsive on the cluster start resulting in hadoop startup timeout before hustler scripts can finish.

Relevant notes shared by the customer for unresponsiveness:
[ec2-user@ip-10-27-143-250 ~]$ dmesg -T
{code}
[Tue Jul 25 10:19:01 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:02 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:02 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:02 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:02 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:03 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:03 2017] EXT4-fs (xvdaa): mounted filesystem with ordered data mode. Opts: nodelalloc
[Tue Jul 25 10:19:03 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:03 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:03 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:08 2017] serial8250_interrupt: 5 callbacks suppressed
[Tue Jul 25 10:19:08 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:10 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:12 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:14 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:16 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:18 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:20 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:22 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:24 2017] serial8250: too much work for irq4
[Tue Jul 25 10:19:26 2017] serial8250: too much work for irq4
[Tue Jul 25 10:22:56 2017] serial8250: too much work for irq4
[Tue Jul 25 10:22:56 2017] serial8250: too much work for irq4
[Tue Jul 25 10:22:57 2017] serial8250: too much work for irq4
[Tue Jul 25 10:22:57 2017] serial8250: too much work for irq4
[Tue Jul 25 10:22:57 2017] serial8250: too much work for irq4
[Tue Jul 25 10:23:03 2017] serial8250: too much work for irq4
[Tue Jul 25 10:23:03 2017] 8021q: 802.1Q VLAN Support v1.8
[Tue Jul 25 10:23:03 2017] 8021q: adding VLAN 0 to HW filter on device eth0
[Tue Jul 25 10:23:03 2017] VF could not set VLAN 0
[Tue Jul 25 10:23:07 2017] serial8250: too much work for irq4
[Tue Jul 25 10:23:07 2017] serial8250: too much work for irq4
[Tue Jul 25 10:23:07 2017] serial8250: too much work for irq4
[Tue Jul 25 10:23:08 2017] serial8250: too much work for irq4
[Tue Jul 25 10:23:25 2017] disable-oom.sh (5756): /proc/5679/oom_adj is deprecated, please use /proc/5679/oom_score_adj instead.
[Tue Jul 25 10:26:22 2017] serial8250: too much work for irq4
{code}

The cloned cluster(34496) was able to start with single node configuration but failed with more number of min nodes(10). This can just be intermittent, not sure on this data point.

Other findings:
After the cluster is started, some hive commands are failing with metastore connectivity errors:
86310531, 86314108
{code}
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: internal-shared-hive-metastore-elb-550897717.us-west-2.elb.amazonaws.com
{code}

Action points:

1. Need to update the customer about the cluster start-up issues and possible root cause.
2. To dig into the possibility of node unresponsiveness causing metastore issues.
3. Customer to update on any DNS issues reported when there are metastore connectivity issues.",,ajayaa,ajayb,navdeepp,prakharj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05etb:,,,,,,,,,,,,,,,,,,,1.0,16041,2017-07-26 05:57:12.029,,,"25/Jul/17 7:23 AM;navdeepp;Customer update on DNS issues:

 _I checked with Juuso and there was some kind of DNS resolution fluctuation from whitelisters. If we see second issue again, we will check whitelister logs to confirm that whitelister is causing the issue_",25/Jul/17 7:44 AM;navdeepp;The cloned cluster 34496 has hadoop_setup_timeout set to 7200. PCR-80 ,"26/Jul/17 5:57 AM;ajayaa;Upon investigating the cluster start issue of hotels.com reported yesterday (ACM-1375), we found out failures to reach to cloudfront urls in their cluster master (DNS resolution not working or outbound connection blocked). So we disabled the feature to send cloudfront events, which improved their cluster start time. The cluster start times are back to normal now. We suspect this is unique to hotels.com because they have restrictive network policies. We will continue our investigation of failure to reach cloudfront and slowness in UDF. Please remove hadoop_setup_timeout for their clusters if set.

P.S. We have disabled the feature `cluster.enable_udf_events` for account 6656.

cc [~prakharj]","26/Jul/17 6:29 AM;prakharj;For a given cluster start, can we detect that previous attempts to cloudfront failed and stop sending the new events?

Basically anyone with custom dns may face this issue. And if the cluster is not a spark cluster and not using EBS (=> it has less retries associated with it in wait_for_hadoop), then the cluster start will probably fail in all the cases.","26/Jul/17 6:37 AM;ajayb;[~prakharj] this was a hadoop2 cluster with zero EBS disks, so the retries were minimal.",26/Jul/17 6:43 AM;ajayaa;[~prakharj] We may lose events from a cluster because of genuine failures coming out of transient network connectivity. Are you saying it is okay to lose events from a single cluster among many?,"26/Jul/17 6:48 AM;prakharj;[~ajayb] [~ajayaa] 

These events are for analytics purpose and doesn't have any direct impact for the customer. So loosing should be fine.

Should we disable this feature for s4/s5 accounts until we fix this issue?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Airflow Pool does not limit running tasks,AIR-33,68383,Bug,Open,AIR,Airflow,software,sumitm,,,Major,,rupeshb,navdeepp,addon_zendesk_for_jira,24/Jul/17 12:07 PM,30/Jul/17 11:53 PM,09/Aug/17 5:22 AM,,,,,1,jira_escalated,,,,,,,,"Airflow Pool does not limit running tasks
Customer Dag: 84801785 

Seems related to https://issues.apache.org/jira/browse/AIRFLOW-584

h3. Zendesk Support Attachments


[Attachment 1|https://qubole.zendesk.com/attachments/token/hRWZDfiyqSA9RtI3TNZbciUPs/?name=Screen+Shot+2017-07-17+at+7.27.45+PM.png]
",,addon_zendesk_for_jira,drose@qubole.com,rupeshb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05efj:,,,,,,,,,,,,,,,,,,,1.0,15874,2017-07-30 23:53:21.205,,,"30/Jul/17 11:53 PM;rupeshb;This is not a bug from our end. Its an Airflow bug.
https://issues.apache.org/jira/browse/AIRFLOW-72
https://issues.apache.org/jira/browse/AIRFLOW-584
https://issues.apache.org/jira/browse/AIRFLOW-41

Some changes to overcome this issue are made in Airflow 1.8. Its Qubole compatible release should a out in a couple of weeks.

cc: [~sumitm]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMX AccessControlException using Java8,ACM-1370,68377,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Minor,,ajayb,wvaldez,wvaldez,24/Jul/17 9:06 AM,28/Jul/17 5:21 AM,09/Aug/17 5:21 AM,,,,,0,jira_escalated,,,,,,,,"We provided Oracle with node bootstrap code to run with Java8 and they are getting a java.security.AccessControlException.  It is just a warning so this is not critical.  It looks like there is permission missing in <JRE_HOME>/lib/security/java.policy for Java8.

Java8 bootstrap code:
{code:java}
export JAVA_HOME=/usr/lib/jvm/java-1.8.0_60
export PATH=$JAVA_HOME/bin:$PATH
sudo echo ""export JAVA_HOME=/usr/lib/jvm/java-1.8.0_60"" >> /etc/profile
sudo echo ""export PATH=$JAVA_HOME/bin:$PATH"" >> /etc/profile

sed -i 's/java-1.7.0/java-1.8.0_60/' /etc/hadoop/hadoop-env.sh
sed -i 's/java-1.7.0/java-1.8.0_60/' /etc/hadoop/mapred-env.sh
sed -i 's/java-1.7.0/java-1.8.0_60/' /etc/hadoop/yarn-env.sh

sudo echo ""export JAVA_HOME=/usr/lib/jvm/java-1.8.0_60"" >> /usr/lib/zeppelin/conf/zeppelin-env.sh

{code}

Exception:
{code:java}
App > java.security.AccessControlException: access denied (""javax.management.MBeanTrustPermission"" ""register"")
App > 	at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
App > 	at java.lang.SecurityManager.checkPermission(SecurityManager.java:585)
App > 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.checkMBeanTrustPermission(DefaultMBeanServerInterceptor.java:1848)
App > 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:322)
App > 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
App > 	at com.amazonaws.jmx.MBeans.registerMBean(MBeans.java:52)
App > 	at com.amazonaws.jmx.SdkMBeanRegistrySupport.registerMetricAdminMBean(SdkMBeanRegistrySupport.java:27)
App > 	at com.amazonaws.metrics.AwsSdkMetrics.registerMetricAdminMBean(AwsSdkMetrics.java:330)
App > 	at com.amazonaws.metrics.AwsSdkMetrics.<clinit>(AwsSdkMetrics.java:308)
App > 	at com.amazonaws.AmazonWebServiceClient.requestMetricCollector(AmazonWebServiceClient.java:619)
App > 	at com.amazonaws.AmazonWebServiceClient.isRMCEnabledAtClientOrSdkLevel(AmazonWebServiceClient.java:560)
App > 	at com.amazonaws.AmazonWebServiceClient.isRequestMetricsEnabled(AmazonWebServiceClient.java:552)
App > 	at com.amazonaws.AmazonWebServiceClient.createExecutionContext(AmazonWebServiceClient.java:513)
App > 	at com.amazonaws.services.sns.AmazonSNSClient.publish(AmazonSNSClient.java:1447)
App > 	at com.datalogix.datanado.orchestration.adapter.SNSPublisher.sendMessage(SNSPublisher.java:158)
App > 	at com.datalogix.ingest.aws.SnsPublisher.publish(SnsPublisher.java:35)
App > 	at com.datalogix.ingest.api.IngestResponseForwarder.forward(IngestResponseForwarder.java:37)
App > 	at com.datalogix.ingest.DatanadoIngestDriverBatch.publishJobCompleted(DatanadoIngestDriverBatch.scala:215)
App > 	at com.datalogix.ingest.DatanadoIngestDriverBatch.ingest(DatanadoIngestDriverBatch.scala:65)
App > 	at com.datalogix.ingest.IngestApp$delayedInit$body.apply(IngestApp.scala:91)
App > 	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)
App > 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
App > 	at scala.App$$anonfun$main$1.apply(App.scala:71)
App > 	at scala.App$$anonfun$main$1.apply(App.scala:71)
App > 	at scala.collection.immutable.List.foreach(List.scala:318)
App > 	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)
App > 	at scala.App$class.main(App.scala:71)
App > 	at com.datalogix.ingest.IngestApp$.main(IngestApp.scala:16)
App > 	at com.datalogix.ingest.IngestApp.main(IngestApp.scala)
App > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
App > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
App > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
App > 	at java.lang.reflect.Method.invoke(Method.java:497)
App > 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:856)
App > 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:186)
App > 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:211)
App > 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
App > 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

{code}

Need the following in java.policy file under <JRE_HOME>/lib/security:
{code:java}
grant {
// JMX Java Management eXtensions
permission javax.management.MBeanTrustPermission ""register"";
};
{code}",api,ajayb,anum,hiyer,jayapradag,prachim,shashankks,wvaldez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,Oracle,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05ee7:,,,,,,,,,,,,,,,,,,,1.0,15865,2017-07-25 01:31:03.032,,,"25/Jul/17 1:31 AM;ajayb;[~hiyer] any idea about this? In qweez/packer/cluster/scripts/commonscripts/java8_60.sh , I see us simply doing `yum -y install jdk1.8.0_60`. Not sure why permissions would be missing/incorrect.",25/Jul/17 1:47 AM;hiyer;Probably this permission has always been missing and never been noticed since it's not fatal as [~wvaldez] said. We should add it as part of base ami bake.,25/Jul/17 1:55 AM;ajayb;[~shashankks] can you pick up this change in base ami bake script?,"25/Jul/17 1:57 AM;shashankks;I can pick up the change, but are we targeting this for rb46?
cc [~karthikk]","25/Jul/17 1:59 AM;shashankks;Also, cluster base image has seen some changes in java installations as part of TOOLS-91 and we have baked a fresh base and release images for rb46. We can check if the issue is solved there?
","25/Jul/17 2:03 AM;ajayb;This fix is certainly not targeted for R46. And, sure it can be checked in R46 images before proceeding. [~anum] can someone try above steps in R46?",25/Jul/17 4:46 AM;anum;[~wvaldez] Can you give us more information on what should we try out to throw this exception? Did this show up in the query logs while cluster was starting up using a spark command on a spark cluster? The nodebootstrap logs during cluster start does not show this exception.,"25/Jul/17 6:48 AM;wvaldez;[~anum] This happened running a spark command on a spark cluster via command line (command Id: 85018518).  The error showed up in the main log when the job ran.  This is the command that was run:

{code:java}
/usr/lib/spark/bin/spark-submit --master yarn-client --class com.datalogix.ingest.IngestApp --conf spark.qubole.max.executors=20 --conf spark.executor.cores=2 --conf spark.executor.memory=12g --conf spark.yarn.executor.memoryOverhead=5128 --conf spark.executor.extraJavaOptions=""-XX:MaxPermSize=512m -Djava.net.preferIPv4Stack=true"" --conf spark.driver.extraJavaOptions=""-XX:MaxPermSize=512m -Djava.net.preferIPv4Stack=true"" --conf spark.executor.extraJavaOptions=""-XX:PermSize=512m -XX:MaxPermSize=512m -Djava.net.preferIPv4Stack=true"" s3n://d8-dataingest/user/zach.pember/ingest/datanado-ingest-spark-1.2.1-4.jar -ingestConfig s3ingest://dlx-dev-core-consumer/dev/etl/dlx/ingest/datanado/config/regression/email/vdbs-b2c/executions/ingest-config-7a060261-0baf-4102-be1d-bf520d5dd70d.json -configProvider s3ingest://dlx-dev-core-consumer/dev/etl/dlx/ingest/datanado/config/regression/email/vdbs-b2c/executions/ingest-config-source-7a060261-0baf-4102-be1d-bf520d5dd70d.json -eventStreamUri arn:aws:sns:us-east-1:289647624397:d8-ingest-events-ingest-service -orchestrationTopic arn:aws:sns:us-east-1:289647624397:d8-ingest-orchestration-email-job-topic -workDir s3n://dlx-dev-core-consumer/dev/etl/dlx/ingest/datanado/work -messageMetadata s3ingest://dlx-dev-core-consumer/dev/etl/dlx/ingest/datanado/config/regression/email/vdbs-b2c/executions/orchestration-config-7a060261-0baf-4102-be1d-bf520d5dd70d.json
{code}
",26/Jul/17 7:15 AM;anum;I was not able to reproduce it on a spark shell command that I tried running on a cluster with the nodebootstrap specified as given above.[~hiyer] [~jayapradag] Can you help me to understand what needs to be exactly tried out to reproduce this error. https://api.qubole.com/v2/analyze?command_id=86336832. I have added both of you to the account where I ran the query.,"26/Jul/17 11:06 PM;jayapradag;[~anum] We can reproduce this when some s3 files are accessed with spark command line
https://api.qubole.com/v2/analyze?command_id=86631788

{code}
/usr/lib/spark/bin/spark-submit s3://qubole-karma/testdata/spark/read_json.R
{code}

","26/Jul/17 11:08 PM;jayapradag;This warning is seen when any s3 files are accessed in spark (with command line, scale, R etc).","26/Jul/17 11:39 PM;jayapradag;Verified this with spark 1.6.1 version with the bootstrap given above with R46 builds in QA. 
We are not seeing the below warnings in R46.
""App > java.security.AccessControlException: access denied (""javax.management.MBeanTrustPermission"" ""register"")""

https://qa.qubole.net/v2/analyze?command_id=1760193
https://qa.qubole.net/v2/analyze?command_id=1760191",26/Jul/17 11:59 PM;anum;Thanks [~jayapradag]. [~ajayb] Then we can mark this jira as resolved right.,28/Jul/17 2:45 AM;prachim;I have seen this warning in Indix commands too while running on 2.1.latest spark cluster and came while accessing s3 location. They too are using a bootstrap script for java8.,"28/Jul/17 4:39 AM;ajayb;[~jayapradag] and [~prachim], I think you both are saying the issue is seen with R45 but no longer repros on R46? Or should [~jayapradag] try with spark 2.1 in R46 to confirm it before resolving the jira?","28/Jul/17 4:52 AM;prachim;yes, I was just bringing to notice that Indix also faced same warning. If it is fixed with r46 then its good. They have not raised this as an issue.","28/Jul/17 5:21 AM;jayapradag;Verified with spark 2.1.0 version [used the bootstrap script given in the jira for using java8 ] as well in QA with R46 , we don't see the warning.
https://qa.qubole.net/v2/analyze?command_id=1769209

So we are good with R46. we can resolve the jira.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Presto Cluster going down due to ""INACTIVITY"" during command execution",QBOL-6183,68342,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,sumitm,navdeepp,navdeepp,24/Jul/17 1:11 AM,27/Jul/17 12:03 AM,09/Aug/17 5:21 AM,,,,,0,jira_escalated,,,,,,,,"Command:86049916 (submit time: 24 July 2017 04:57:43)
This command got cancelled during execution with cluster going down due to ""INACTIVITY""

Last Presto server.log
2017-07-24T04:57:54.496Z        DEBUG  DecisionService RUNNING com.qubole.service.DecisionService      optimal = 2 running = 2 unknown = 0

Last Hustler.log
PID: 6095  2017-07-24 04:57:33,891 DEBUG  cluster.py:1387 - get_nodes - returning self._nodes = <Node: master (i-09e999cb0379b485c)>, <Node: node0004 (i-0acf6eebc803c106e)>, <Node: node0005 (i-09bc37f4c100a92fd)>

Query Logs:
{code}
Query: 20170724_045745_00003_x8ar5 , Progress: 64%
Connection refused - connect(2) for ""localhost"" port 20711
Time taken: 161.866681061 seconds
{code}",,navdeepp,stagra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05e6f:,,,,,,,,,,,,,,,,,,,1.0,16005,2017-07-27 00:02:56.677,,,"27/Jul/17 12:02 AM;stagra;Moving this to middleware, i think that is the better starting place.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyze option to overwrite Workspace Query from History Edit,AN-130,68205,Bug,Reopened,AN,Analyst,software,aswina,,,Major,,bhargavis,aabbas,aabbas,20/Jul/17 9:47 AM,09/Aug/17 5:42 AM,09/Aug/17 5:21 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Feature/improvement request from Sharon Horswood at Scripps:

When selecting a ""saved"" query (after it's run) from the ""History"" tab of Analyze and making an edit, and selecting ""Save"", a popup informs that it is saving the command to (Analyze) ""Workspace"" but that it cannot with the same name (Error = ""Name has been used for another saved query"").

The only option is to give the query a new name (i.e. there is ONLY a Cancel and Save option).

We are requesting one of 2 things: 
1) Another button be available like ""Save Over Existing"" 
2) or a Warning/Error remains, but states something to the order of ""A query with this name already exists. Any attempt to save will prompt for permission to overwrite the existing query""... and then a secondary ""Are you sure"" prompt after Save.

We understand that really any work that can be done in History can also be done in Workspace... and that Workspace is likely the most ideal place to make any change. However, given that you give the ability to edit in History, we believe this change will cause way less confusion to endusers.

Right now, our endusers keep saving multiple versions of the same query because the current error seems to indicate this is their only option (Name has been used for another saved query). 

Not giving a specific CID because this is easily reproducible with ANY saved workspace query that has been executed with results available in History. Happening in ALL our accounts.

Please reach out if you need more specific details.

cc: [~ankitag] [~rangasayeec]",,aabbas,ankitag,aswina,nimitk,rangasayeec,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Jul/17 1:16 PM;ankitag;Screen Shot 2017-07-28 at 13.14.31.png;https://qubole.atlassian.net/secure/attachment/46244/Screen+Shot+2017-07-28+at+13.14.31.png,,,,,,,,,,,,,AWS,,,,,None,scripps,ScrippsNetwork,,,,"{branch={count=1, dataType=branch}, json={""cachedValue"":{""errors"":[],""summary"":{""branch"":{""overall"":{""count"":1,""lastUpdated"":null},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":false}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04p85:,,,,,,AN Sprint 36 (24-Jul ~ 4-Aug),AN Sprint 37 (7-Aug ~ 15-Aug),,,,,,,,,,,,1.0,15914,2017-07-28 13:18:14.282,,,"28/Jul/17 1:18 PM;ankitag;Ideally on clicking save, it should just save a new version in existing command.
Can we prioritise this? [~rangasayeec][~aswina]

Adding screenshot of the current behaviour. Error should not be shown and Save should save a new version of the query. !Screen Shot 2017-07-28 at 13.14.31.png|thumbnail! ","28/Jul/17 1:22 PM;rangasayeec;Makes sense, I've prioritized it.

We'll get the restriction from History removed for now and in the long run make History read-only for queries. Edit will open query in workspace and work like a Save & Run (new version of existing command) from workspace.",28/Jul/17 1:28 PM;rangasayeec;[~aswina] I put this into the current sprint because this actually affects all customers and Scripps is actively complaining about it. Seemed like a minor fix. Let me know if you can pick this up.,"31/Jul/17 8:26 AM;aswina;[~rangasayeec] - It may not be possible in the current sprint because we are already working on AN-131, UI-3969, AN-154, and UI-6258.  All of these are hotfix candidates.

Assigning this jira to [~bhargavis].","31/Jul/17 8:27 AM;aswina;The expected behaviour is that, if a command is edited and run from History, and if it had a saved query associated with it, the functionality should be the same as 'Save and Run' from Workspace.  cc: [~rangasayeec] [~aabbas]

Please let me know if the above summary is fine.","31/Jul/17 8:31 AM;rangasayeec;[~aswina] Yes, precisely. Save & Run as new version of existing command.",03/Aug/17 12:33 AM;aswina;Updates are available [in this PR|https://bitbucket.org/qubole/qbol/pull-requests/5034/ui-6258-save-the-entire-query-even-if/diff] and UI-6258.,03/Aug/17 4:21 AM;aswina;Marked it duplicate by mistake.  Reopening this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Analyze UI is considering ""\t"" as a delimiter. Generating corrupted query results.",MW-1220,68112,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,tusharn,tusharn,19/Jul/17 4:28 AM,03/Aug/17 4:20 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,," In Analyze, when the user is running a select query in hive, and if data in one of the column is having “\t”, UI is considering ""\t"" as a delimiter(default delimiter for this table is "","") and moving the data after “\t” in next column and the following data is also moving to their next columns. This makes the whole data corrupted. Even if we download the query result, it is showing corrupted data.


I ran a query
“select street, state, city from test_with_serde_file where id=42061;”

PFA screenshot from UI and CLI for your reference.

To replicate the issue, PFB information.

Email Id : tusharn@qubole.com
Command Id to create table: 84941004 
Command Id for query: 85131811
File location: s3://tusharn/test1/csvfile/




",,aswina,sumitm,tusharn,,,,,,,,,,,,,,,,,,,,,,,,,,,JDBC-73,,,,19/Jul/17 4:07 AM;tusharn;Screen Shot 2017-07-19 at 16.37.16.png;https://qubole.atlassian.net/secure/attachment/45902/Screen+Shot+2017-07-19+at+16.37.16.png,19/Jul/17 4:11 AM;tusharn;Screen Shot 2017-07-19 at 16.40.11.png;https://qubole.atlassian.net/secure/attachment/45901/Screen+Shot+2017-07-19+at+16.40.11.png,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05d33:,,,,,,,,,,,,,,,,,,,1.0,15882,2017-07-20 01:57:12.172,,,"19/Jul/17 11:46 PM;tusharn;Hi [~aswina]

Please treat this issue with highest priority as user is not able to generate reports and we are not able to give any workaround for this issue.

Regards,
Tushar",20/Jul/17 1:57 AM;aswina;[~tusharn] - Can you share the command id that customer has tried executing?  ,"20/Jul/17 2:21 AM;tusharn;PFB details.

User Email id : sourabh.surana@insideview.com
Command id : 84888209

Regards,
Tushar","20/Jul/17 2:55 AM;aswina;I looked at the code that's used to retrieve results to display them in UI.

We have the following in `lib/download_handler/result_handler.rb`:

{code}
  def download_result_file
    ...
      if status && data.present?
        if !@raw
          data = data.gsub(""\001"", ""\t"")
          data = data.gsub(""\n"", ""\r\n"")
        end
    ...
  end

  ...

  def get_formatted_results
    ...
    if @formatted
      oplist.each{|row| results << row.split(""\t"")}
    else
      ...
    ...
  end
{code}

[~sumitm] - Should we retain the `Ctrl+A` delimiter instead of replacing them with `\t` always?  That may solve the issue that these guys are hitting.

I have given a temporary workaround to download raw results and replace `Ctrl + A` with tab or comma.","20/Jul/17 3:04 AM;aswina;Another bug that is that even while downloading raw results, the column header line is delimited by tab character.  Instead it should be delimited by `Ctrl + A` character.  cc: [~mukundag]","21/Jul/17 12:07 AM;sumitm;[~aswina] we may solve the issue with 1 customer, but a lot of others would be affected severely if we just go and change things in result_handler. We've to think through this first. ",21/Jul/17 12:12 AM;aswina;[~sumitm] - Is there any specific reason for replacing all Ctrl+A characters with tabs?  cc: [~hiyer] [~jssarma] for historical perspective.  ,21/Jul/17 12:13 AM;aswina;Reducing the priority as we have given a workaround to the customer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster UI option - Idle Cluster Timeout (in hours) is not working,ACM-1344,67940,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,sbadam,sbadam,17/Jul/17 5:53 PM,20/Jul/17 9:41 AM,09/Aug/17 4:20 AM,,,,,0,jira_escalated,,,,,,,,"Idle Cluster Timeout (in hours) in Cluster_configs is not working.

I increased timeout to 3 and 4 hours for a cluster in my account but it is auto-terminated within 1 hour(release interval). Please see below entries and attached cluster down details

{code:java}
production-replica> select id, idle_cluster_timeout, created_at from cluster_configs where cluster_id=28082 order by id desc limit 2;
+--------+----------------------+---------------------+
| id     | idle_cluster_timeout | created_at          |
+--------+----------------------+---------------------+
| 114389 |                    4 | 2017-07-17 22:57:19 |
| 114353 |                    3 | 2017-07-17 21:26:30 |
+--------+----------------------+---------------------+
2 rows in set (0.00 sec)

production-replica> select id, start_at, down_at from cluster_insts where cluster_id=28082 order by id desc limit 2\G
*************************** 1. row ***************************
      id: 546834
start_at: 2017-07-17 23:01:29
 down_at: 2017-07-17 23:52:05
*************************** 2. row ***************************
      id: 546740
start_at: 2017-07-17 21:30:26
 down_at: 2017-07-17 22:18:39
2 rows in set (0.00 sec)

production-replica>

{code}
",api.qubole.com,ajayaa,ajayb,mstolpner,sbadam,tanishg,zendesk_integ,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Jul/17 5:46 PM;sbadam;Screen Shot 2017-07-17 at 5.44.45 PM.png;https://qubole.atlassian.net/secure/attachment/45736/Screen+Shot+2017-07-17+at+5.44.45+PM.png,,,,,,,,,,,,,AWS,,,,,None,zvelo,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05cbz:,,,,,,,,,,,,,,,,,,,1.0,15830,2017-07-19 03:24:24.204,,,19/Jul/17 3:24 AM;ajayb;[~ajayaa] this seems related to what you were coincidentally looking at yesterday. Can you describe the expected behaviour with idle_cluster_timeout? Or should we ask [~tanishg] to explain it?,19/Jul/17 4:52 AM;ajayaa;[~sbadam] Did you run any command on these clusters during this time?,"19/Jul/17 9:56 AM;sbadam;No [~ajayaa], I didn't run any commands after updating Idle Cluster timeout to 3/ 4 hours and restarted cluster. I was just expecting to cluster to be online for configured duration but it was turned-off within 1 hour(release interval). ","20/Jul/17 4:18 AM;tanishg;Current behaviour is like this :
1. If the cluster is started manually and no query ran on that cluster then it will be terminated within 1 hour.
2. If some query ran on that cluster then it will be timeout accordingly w.r.t Idle Cluster Timeout

I think in your case 1 case has happened. 

May be we should use idle_session_timeout as default for everything and remove the check of 1 hour. Depends on product decision. But since by default it is 2 hour then customer might ask that why there cluster was up for 2 hour even when there is no query running. 
Let me know if this is the case. cc [~sbadam]

cc [~sumitm]","20/Jul/17 5:04 AM;ajayaa;IMO, the correct behaviour is, we should honour this variable irrespective of the commands run on the cluster. The default for this variable should be something like 45 minutes and if the cluster is idle for 45 minutes we should run can_terminate which then checks for hour boundary and apps directly submitted to master.","20/Jul/17 9:41 AM;zendesk_integ;Yeah [~tanishg], we should honor Idle Cluster Timeout irrespective of the commands run. I concur Ajaya's comments. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues due to dynamic allocation and spark shuffle service,SPAR-1787,67873,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,venkatak,addon_zendesk_for_jira,16/Jul/17 2:36 AM,16/Jul/17 2:36 AM,09/Aug/17 5:29 AM,,,,,0,jira_escalated,,,,,,,,"We have enabled both dynamic allocation and spark shuffle service by default and customer hit this issue because of that:

https://issues.apache.org/jira/browse/SPARK-13532

Can we have this fixed?",,addon_zendesk_for_jira,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05bxb:,,,,,,,,,,,,,,,,,,,1.0,15833,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC driver returning null response for successful queries,JDBC-73,67859,Bug,Open,JDBC,JDBC,software,stagra,,,Major,,sakshia,ekang,ekang,14/Jul/17 12:18 PM,09/Aug/17 12:48 AM,09/Aug/17 6:03 AM,,,,,0,solsup,,,,,,,,"Account: Revenue Analytics
User: virginia.giacomini@turner.com
Cluster: presto-looker-dev

Client used multiple versions of the Qubole jdbc driver (including the latest driver) and ran the query below through Looker and henplus and it doesn't work. But, when you change end = 'Adult Swim' to end = 'CNN' the query works... The response for Adult Swim is [QuboleJDBC]Null 

SELECT 
    pdt_fw_operative.sitesectionname  AS ""pdt_fw_operative.sitesectionname"",
    COALESCE(SUM(pdt_fw_operative.total_historical_constrained_ad_avails ), 0) AS ""pdt_fw_operative.capacity""
FROM hive.prod_looker_pdts.qubole_looker_test_jdbc AS pdt_fw_operative
WHERE ((((pdt_fw_operative.activity_date) >= (DATE_TRUNC('DAY', TIMESTAMP '2017-06-01')) AND (pdt_fw_operative.activity_date) < (DATE_TRUNC('DAY', TIMESTAMP '2017-07-10'))))) AND ((case
            when lower(pdt_fw_operative.targetedassetgroupname) like '%cnnmoney%' OR
                 lower(pdt_fw_operative.sitesectionname) like '%cnnmoney%' OR
                 lower(pdt_fw_operative.sitename) like '%cnnmoney%'
                then 'Money'
            else pdt_fw_operative.site
          end = 'Adult Swim'))
GROUP BY 1
ORDER BY 2 DESC
LIMIT 500",,aswina,ekang,sakshia,stagra,sumitm,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,turner,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05bu7:,,,,,,,,,,,,,,,,,,,,,2017-07-16 21:02:02.54,,,16/Jul/17 9:02 PM;stagra;[~sakshia] assigning this to you. ,"17/Jul/17 10:22 AM;sakshia;Can I get the command id of these commands?
[~ekang]","20/Jul/17 2:24 PM;ekang;https://api.qubole.com/v2/analyze?command_id=85416783

https://api.qubole.com/v2/analyze?command_id=85416915

The first one returns results via the JDBC driver.  
The second one fails to return results, and returns the error: [QuboleJDBC]null

Please keep in mind that in the Qubole console, both queries are reported to have completed successfully.  Their is expectation for the second one to return a non-error result.",21/Jul/17 4:06 AM;sakshia; This is an issue in parsing the results. One of the row values in the result of 'Adult Swim' contains '/t' in the end. This is misleading the parser and giving an error since we use '/t' to detect new column.. The same happens in UI which doesn't give an error but shows incorrect values for such cases.,"24/Jul/17 12:28 AM;sakshia;This has to solved at the middleware level, hence transferring the JIRA.","24/Jul/17 7:39 AM;ekang;Okay, thanks Sakshi. Can we get an ETA on when this will get resolved? Do I need to tell the client to replace all \t to work around this issue?","24/Jul/17 8:39 AM;sakshia;Replacing the '/t's is a temporary workaround for this, though this issue should be solved from our side too.

[~sumitm] Have transferred this to Middleware. ",31/Jul/17 12:23 PM;ekang;Speaking with client now and they are asking if there's an update. This is becoming more of a problem for them. Thanks.,"01/Aug/17 12:02 AM;sumitm;Assigning to [~aswina] as this falls under Analyst vertical. 

cc - [~ravil]","03/Aug/17 4:20 AM;aswina;This looks very similar to MW-1220.  [Pasting comment from there|https://qubole.atlassian.net/browse/MW-1220?focusedCommentId=141750&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-141750]:

I looked at the code that's used to retrieve results to display them in UI.

We have the following in `lib/download_handler/result_handler.rb`:

{code}
  def download_result_file
    ...
      if status && data.present?
        if !@raw
          data = data.gsub(""\001"", ""\t"")
          data = data.gsub(""\n"", ""\r\n"")
        end
    ...
  end

  ...

  def get_formatted_results
    ...
    if @formatted
      oplist.each{|row| results << row.split(""\t"")}
    else
      ...
    ...
  end
{code}

A workaround would be to download raw results and replace Ctrl+A characters with tab characters or another delimiter.

cc: [~sumitm] [~vagrawal] [~hiyer] [~jssarma] for historical perspective.","03/Aug/17 11:09 AM;aswina;I had a discussion with [~stagra] on this.  As a short-term fix, we have agreed on the following:

# commands API would accept a `raw` parameter.  If that's passed, raw results that are ^A separated will be returned inline.
# JDBC (and potentially ODBC) driver will need the following change:
## Make small change in API call to pass the `raw` parameter.
## Instead of splitting by `\t`, split by ^A.

I'll assign this jira to [~sakshia] once the API changes are ready so that the corresponding driver changes can be made as well.","06/Aug/17 11:51 PM;sureshr;Thanks for the update, [~aswina]. I would recommend creating a separate JIRA for the API changes and moving this to the JDBC project. What is the ETA for the API changes?

Also, removed this ticket from Presto Sprint 26 since it is blocked by the API changes. Hope that is ok, [~stagra].","07/Aug/17 12:04 AM;aswina;[~sureshr] - I'll be opening a PR for API changes today.  It is tracked as part of MW-1255.

Assigning this jira to [~sakshia] for driver changes. cc: [~stagra]","08/Aug/17 4:17 AM;aswina;[~stagra] [~sakshia] - There's one more problem. 

# Hive commands with <1000 results contain tab separated results always.  But >=1000 results are ^A separated.  cc [~asomani] for more inputs
# Presto commands' results are always ^A separated.

So even after making API changes in JDBC to fetch raw results, once results are retrieved, you should check whether the first row contains ^A.  If so, split by it else split by tab.  This will make it work in a command agnostic way.

Good thing is, for the problematic command 85416915, result columns are ^A delimited.  So checking for ^A and splitting by it should work as expected.",09/Aug/17 12:48 AM;aswina;API support for raw results is available in master.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Address already in use,SPAR-1784,67734,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Critical,,mahuja,satyavathib,satyavathib,13/Jul/17 6:13 PM,07/Aug/17 4:30 AM,09/Aug/17 5:27 AM,,,,,0,jira_escalated,,,,,,,,"Command ID : 83967196
Account ID : 4927
referring to HIVE -1149

-> jets3t Used , s3a disabled
->Spark-2.0.2
->m3. 2x large nodes used
->Consistently reproducable

Log :
{code}
17/07/13 16:26:40 Executor task launch worker-1 ERROR Executor: Exception in task 1.0 in stage 7.0 (TID 132)
java.io.IOException: Error reading file: s3n://iv-qubole/data_platform/warehouse/analysis.db/em2_record_linkage_name_sim_output_20170605/000001
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:1046)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.next(OrcRawRecordMerger.java:264)
	at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.next(OrcRawRecordMerger.java:550)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$1.next(OrcInputFormat.java:1299)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$1.next(OrcInputFormat.java:1283)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$NullKeyRecordReader.next(OrcInputFormat.java:1210)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$NullKeyRecordReader.next(OrcInputFormat.java:1196)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:396)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:342)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:147)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)
	at java.net.Socket.bind(Socket.java:644)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.commons.httpclient.protocol.ReflectionSocketFactory.createSocket(ReflectionSocketFactory.java:139)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:125)
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1361)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)
	at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRequest(RestStorageService.java:371)
	at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRestGet(RestStorageService.java:936)
	at org.jets3t.service.impl.rest.httpclient.RestStorageService.getObjectImpl(RestStorageService.java:2014)
	at org.jets3t.service.impl.rest.httpclient.RestStorageService.getObjectImpl(RestStorageService.java:1951)
	at org.jets3t.service.S3Service.getObject(S3Service.java:2625)
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieve(Jets3tNativeFileSystemStore.java:256)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:250)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at org.apache.hadoop.fs.s3native.$Proxy7.retrieve(Unknown Source)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.reopen(NativeS3FileSystem.java:272)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.seek(NativeS3FileSystem.java:259)
	at org.apache.hadoop.fs.BufferedFSInputStream.seek(BufferedFSInputStream.java:98)
	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:63)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.readDiskRanges(RecordReaderUtils.java:249)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readPartialDataStreams(RecordReaderImpl.java:964)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:793)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.next(RecordReaderImpl.java:1042)
	... 22 more
17/07/13 16:26:40 dispatcher-event-loop-7 INFO CoarseGrainedExecutorBackend: Got assigned task 140
{code}

{code}
17/07/13 16:26:40 Executor task launch worker-1 ERROR PythonRunner: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/media/ephemeral1/yarn/local/usercache/sourabh.surana/appcache/application_1499923760478_0032/container_1499923760478_0032_01_000003/pyspark.zip/pyspark/worker.py"", line 157, in main
    is_sql_udf = read_int(infile)
  File ""/media/ephemeral1/yarn/local/usercache/sourabh.surana/appcache/application_1499923760478_0032/container_1499923760478_0032_01_000003/pyspark.zip/pyspark/serializers.py"", line 545, in read_int
    raise EOFError
EOFError

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

Please Let me know If any details required.
",,mahuja,rohitk,satyavathib,sourabhg,vagrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,insideview,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05b9r:,,,,,,,,,,,,,,,,,,,1.0,15752,2017-07-18 19:16:40.395,,,18/Jul/17 4:54 PM;satyavathib;Could someone please help me on this ?,"18/Jul/17 7:16 PM;mahuja;[~satyavathib] the stack appears to be in jets3t call. Adding [~hiyer], [~abhishekmodi], [~sourabhg] - if they have seen this earlier. 

 cc [~vagrawal]","20/Jul/17 1:04 AM;vagrawal;First attempt of the task fails with IOException and takes ~56s. Subsequent attempts fails with address already in use and takes < 1sec.


{code:java}
	131	0	FAILED	PROCESS_LOCAL	2 / ip-172-28-3-37.ec2.internal	2017/07/13 16:25:44	56 s	2 s	47.4 MB / 594998	1 s	0.0 B / 0	java.io.IOException: Error reading file: s3n://iv-qubole/data_platform/warehouse/analysis.db/em2_record_linkage_name_sim_output_20170605/000000 +details
0	142	1	FAILED	PROCESS_LOCAL	4 / ip-172-28-3-37.ec2.internal	2017/07/13 16:26:40	60 ms		0.0 B / 0		0.0 B / 0	java.net.BindException: Address already in use +details
0	145	2	FAILED	PROCESS_LOCAL	4 / ip-172-28-3-37.ec2.internal	2017/07/13 16:26:40	0.1 s		0.0 B / 0		0.0 B / 0	java.net.BindException: Address already in use +details
0	147	3	FAILED	PROCESS_LOCAL	2 / ip-172-28-3-37.ec2.internal	2017/07/13 16:26:40	0.1 s		0.0 B / 0		0.0 B / 0	java.net.BindException: Address already in use +details
1	132	0	FAILED	PROCESS_LOCAL	4 / ip-172-28-3-37.ec2.internal	2017/07/13 16:25:44	56 s	2 s	43.2 MB / 544998	0.8 s	0.0 B / 0	java.io.IOException: Error reading file: s3n://iv-qubole/data_platform/warehouse/analysis.db/em2_record_linkage_name_sim_output_20170605/000001 +details
1	141	1	FAILED	PROCESS_LOCAL	2 / ip-172-28-3-37.ec2.internal	2017/07/13 16:26:40	99 ms		0.0 B / 0		0.0 B / 0	java.net.BindException: Address already in use +details
1	144	2	FAILED	PROCESS_LOCAL	4 / ip-172-28-3-37.ec2.internal	2017/07/13 16:26:40	81 ms		0.0 B / 0		0.0 B / 0	java.net.BindException: Address already in use +details
1	146	3	FAILED	PROCESS_LOCAL	4 / ip-172-28-3-37.ec2.internal	2017/07/13 16:26:40	0.1 s		0.0 B / 0		0.0 B / 0	java.net.BindException: Address already in use
{code}
","20/Jul/17 1:18 AM;vagrawal;from executor logs, it looks like things fell apart while reading from S3.

{code:java}
7/07/13 16:26:39 Executor task launch worker-1 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://iv-qubole.s3.amazonaws.com/data_platform%2Fwarehouse%2Fanalysis.db%2Fem2_record_linkage_name_sim_output_20170605%2F000001
17/07/13 16:26:39 Executor task launch worker-1 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://iv-qubole.s3.amazonaws.com/data_platform%2Fwarehouse%2Fanalysis.db%2Fem2_record_linkage_name_sim_output_20170605%2F000001
17/07/13 16:26:39 Executor task launch worker-1 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://iv-qubole.s3.amazonaws.com/data_platform%2Fwarehouse%2Fanalysis.db%2Fem2_record_linkage_name_sim_output_20170605%2F000001
17/07/13 16:26:39 Executor task launch worker-1 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://iv-qubole.s3.amazonaws.com/data_platform%2Fwarehouse%2Fanalysis.db%2Fem2_record_linkage_name_sim_output_20170605%2F000001
17/07/13 16:26:39 Executor task launch worker-1 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://iv-qubole.s3.amazonaws.com/data_platform%2Fwarehouse%2Fanalysis.db%2Fem2_record_linkage_name_sim_output_20170605%2F000001
17/07/13 16:26:39 Executor task launch worker-1 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://iv-qubole.s3.amazonaws.com/data_platform%2Fwarehouse%2Fanalysis.db%2Fem2_record_linkage_name_sim_output_20170605%2F000001
17/07/13 16:26:39 Executor task launch worker-1 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://iv-qubole.s3.amazonaws.com/data_platform%2Fwarehouse%2Fanalysis.db%2Fem2_record_linkage_name_sim_output_20170605%2F000001
17/07/13 16:26:40 Executor task launch worker-1 INFO s3OperationsLog: Method=GET ResponseCode=206 URI=http://iv-qubole.s3.amazonaws.com/data_platform%2Fwarehouse%2Fanalysis.db%2Fem2_record_linkage_name_sim_output_20170605%2F000001
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: I/O exception (java.net.BindException) caught when processing request: Address already in use
17/07/13 16:26:40 Executor task launch worker-1 INFO HttpMethodDirector: Retrying request
17/07/13 16:26:40 Executor task launch worker-1 WARN RestUtils: Retried connection 13 times, which exceeds the maximum retry count of 12
17/07/13 16:26:40 Executor task launch worker-1 ERROR Executor: Exception in task 1.0 in stage 7.0 (TID 132)
{code}

It can be due to too many open connections - https://stackoverflow.com/questions/2914218/bindexception-too-many-file-open-while-using-httpclient-under-load

Stage 7 was running on 3 containers and all three of them were running on same EC2 node and all tasks (reading from S3) started at same time - 2017/07/13 16:25:44 

cc [~abhishekmodi], [~hiyer]

",07/Aug/17 3:01 AM;satyavathib;We could close this JIRA as the customer is no longer facing this issue.,"07/Aug/17 4:30 AM;rohitk;See HIVE-1149

https://qubole.atlassian.net/browse/HIVE-1149?focusedCommentId=85607&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-85607
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oracle Spark Cluster at Max Capacity idle with jobs queued,ACM-1340,67721,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,sourabhg,wvaldez,wvaldez,13/Jul/17 8:29 AM,18/Jul/17 10:48 PM,09/Aug/17 4:38 AM,,,,,1,jira_escalated,,,,,,,,"On 6/30 Oracle reported that one of their Spark clusters had been running at max capacity for quite some time and it was mostly idle with jobs getting queued.  We need to understand what was the root cause.

Account Id: 2551
Cluster Id: 24191
Cluster Inst Id: 523189
Command Id: 81515580 (reported by customer)
Commands that ran for 14-16 hrs and were canceled during this time: 81424374, 81427593, 81427992, and 81429206 

Need to understand if these jobs failed due to a problem with QDS.
",api,adubey,ajayb,drose@qubole.com,sourabhg,wvaldez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jul/17 1:58 PM;wvaldez;autoscaling.log.2017063015.log.gz;https://qubole.atlassian.net/secure/attachment/45607/autoscaling.log.2017063015.log.gz,13/Jul/17 1:58 PM;wvaldez;autoscaling.log.2017063016.log.gz;https://qubole.atlassian.net/secure/attachment/45608/autoscaling.log.2017063016.log.gz,18/Jul/17 4:41 PM;wvaldez;cluster_load.png;https://qubole.atlassian.net/secure/attachment/45808/cluster_load.png,13/Jul/17 1:58 PM;wvaldez;hustler.log;https://qubole.atlassian.net/secure/attachment/45609/hustler.log,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-nodemanager-ip-10-108-50-144.log.2017062920.log.gz;https://qubole.atlassian.net/secure/attachment/45610/yarn-yarn-nodemanager-ip-10-108-50-144.log.2017062920.log.gz,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-nodemanager-ip-10-108-50-144.log.2017062921.log.gz;https://qubole.atlassian.net/secure/attachment/45611/yarn-yarn-nodemanager-ip-10-108-50-144.log.2017062921.log.gz,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-nodemanager-ip-10-108-50-144.log.2017062922.log.gz;https://qubole.atlassian.net/secure/attachment/45612/yarn-yarn-nodemanager-ip-10-108-50-144.log.2017062922.log.gz,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-nodemanager-ip-10-108-57-29.log.2017062920.log.gz;https://qubole.atlassian.net/secure/attachment/45613/yarn-yarn-nodemanager-ip-10-108-57-29.log.2017062920.log.gz,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-nodemanager-ip-10-108-57-29.log.2017062921.log.gz;https://qubole.atlassian.net/secure/attachment/45614/yarn-yarn-nodemanager-ip-10-108-57-29.log.2017062921.log.gz,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-nodemanager-ip-10-108-57-29.log.2017062922.log.gz;https://qubole.atlassian.net/secure/attachment/45615/yarn-yarn-nodemanager-ip-10-108-57-29.log.2017062922.log.gz,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-resourcemanager-ip-10-108-19-214.log.2017063014.log.gz;https://qubole.atlassian.net/secure/attachment/45616/yarn-yarn-resourcemanager-ip-10-108-19-214.log.2017063014.log.gz,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-resourcemanager-ip-10-108-19-214.log.2017063015.log.gz;https://qubole.atlassian.net/secure/attachment/45617/yarn-yarn-resourcemanager-ip-10-108-19-214.log.2017063015.log.gz,13/Jul/17 1:58 PM;wvaldez;yarn-yarn-resourcemanager-ip-10-108-19-214.log.2017063016.log.gz;https://qubole.atlassian.net/secure/attachment/45618/yarn-yarn-resourcemanager-ip-10-108-19-214.log.2017063016.log.gz,AWS,,,,,None,Oracle,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05b6v:,,,,,,,,,,,,,,,,,,,1.0,15543,2017-07-13 10:22:34.144,,,13/Jul/17 10:22 AM;ajayb;[~sourabhg] is hadoop on-call this week and will take first look.,13/Jul/17 1:59 PM;wvaldez;Ashish Dubey suggested that this might be a HADTWO jira.,"14/Jul/17 1:57 AM;sourabhg;Looked into the issue with help from [~abhishekmodi]. 

Based on the analysis of failed command: 81424374, we found out that MR app master for this job was getting killed again and again. It was happening because customer has set 
{code}
yarn.app.mapreduce.am.resource.mb=7552
{code}

in Hadoop overrides. What this means is that max memory AM would consume is 7552. AM will be killed if it consumes memory beyond this.

However the actual jvm launched for this AM has the following values set for min and max heap:
{code}
$JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=536870912 -Dyarn.app.container.log.backups=1 -Dhadoop.root.logger=INFO,CRLA -server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx4070m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr
{code}
Here we can see value of Xmx is 4070 mb. 

As a result of this, when MRApp master jvm hits limit of 7552, it is killed by node manager and then RM launches it again. The continues forever. 

Thats why the application is not getting completed and also the cluster is not downscaling because new app attempts keep getting launched and they keep consuming resources.

The fix this, customer has to set this property in Hadoop overrides: 
{code}
yarn.app.mapreduce.am.command-opts=-Xmx6042m
{code}

We set max heap size: 0.8*yarn.app.mapreduce.am.resource.mb
 


cc - [~abhishekmodi]",16/Jul/17 10:26 PM;wvaldez;[~sourabhg] [~abhishekmodi] [~adubey]  I am not following the explanation.  You are saying that the MR App Master was getting killed again and again.  And that a reason for this would be if it consumed memory beyond yarn.app.mapreduce.am.resource.mb=7552.  You also saying that JVM for the AM was set at 4070 mb.  I don't see how it could have used more than 7552 mb if the max was set to 4070 mb.,17/Jul/17 11:41 AM;adubey;Yes it is confusing to me too. lower Xmx causing container kill? Or am i not reading it correctly  [~sourabhg],"18/Jul/17 12:12 AM;sourabhg;[~wvaldez] [~adubey]: My bad. I interpretted jvm Xmx and yarn.app.mapreduce.am.resource.mb wrongly. I tried to look into it today but looks like the customer has removed logs. For example the command 81424374 and 81427593 were run on clusterid: 24191 and cluster insts 523189 and the logs don't exist on s3 anymore. 
{code}
[ec2-user@ip-10-108-60-124 tmp_logs]$ hadoop dfs -ls s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/
Found 23 items
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/525189
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/526788
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/527316
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/529637
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/530017
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/531613
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/533022
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/534245
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/534771
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/535216
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/535748
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/536304
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/537714
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/539137
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/542068
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/542989
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/543029
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/543621
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/544292
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/544764
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/545358
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/546009
drwxrwxrwx   - ec2-user          0 2017-07-18 00:40 s3n://dlx-prod-analytics/qubole/logs/hadoop/24191/546371
{code}

Let me know if the customer has moved logs to some other location. 

I am also writing my observations from my previous debugging: 

In RM logs we saw that AM was getting killed and relaunched again and again. I tried to find the logs of node manager (on which AM was launched) and as well as AM but both were missing around that time (2017-06-30 02:00:00 - 2017-06-30 04:00:00). 

Pasting RM logs of one such AM: 
{code}
017-06-30 02:30:42,493 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Command to launch container container_1498730786116_0244_01_000001 : $JAVA_HOME/bin/java -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=<LOG_DIR> -Dyarn.app.container.log.filesize=536870912 -Dyarn.app.container.log.backups=1 -Dhadoop.root.logger=INFO,CRLA -server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx4070m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr

2017-06-30 02:44:35,463 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1498730786116_0244_01_000001 Container Transitioned from RUNNING to KILLED
2017-06-30 02:44:35,463 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Completed container: container_1498730786116_0244_01_000001 in state: KILLED event:KILL
2017-06-30 02:44:35,463 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=robert.greenberg OPERATION=AM Released Container TARGET=SchedulerApp     RESULT=SUCCESS  APPID=application_1498730786116_0244    CONTAINERID=container_1498730786116_0244_01_000001
2017-06-30 02:44:35,463 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1498730786116_0244_01_000001 of capacity <memory:7552, vCores:1> on host ip-10-108-63-99.ec2.internal:45454, which currently has 1 containers, <memory:7552, vCores:1> used and <memory:234112, vCores:63> available, release resources=true
{code}

In short, following can be reasons why AM is getting killed: 
1. Jvm Xmx is higher than yarn.app.mapreduce.am.resource.mb. In that case, jvm would be killed by node manager. This option is ruled out since we saw in logs that Xmx for AM is 4070 which is less than yarn.app.mapreduce.am.resource.mb. 
2. JVM itself requires more heap than 4GB and it running out of memory. Can we set  Xmx for AM jvm to a higher value (probably the same as yarn.app.mapreduce.am.resource.mb) and re run their queries ? ",18/Jul/17 8:12 AM;adubey;regarding #2 - what type of queries or jobs were they running and were all of them stuck ( [~wvaldez] ). Ideally 4G xmx should be good enough to get things going ( it should never block the scheduling of job itself ). Even if 4G falls short it would basically reflect after running that job for a while. Also it is quite unlikely that all the jobs will require >4g memory. ,"18/Jul/17 4:48 PM;wvaldez;According to the cluster stats (see attached cluster_load.png) there was zero cluster utilization.  For long periods of time, no jobs were being executed (every now and then it seems like a job would get through).  A restart of the cluster cleared the problem so there was no issue with the jobs themselves but I am thinking maybe there was some job that got the cluster in that bad state.","18/Jul/17 10:31 PM;sourabhg;[~adubey] [~wvaldez]: Its hard to say anything for sure since logs for the previous cluster insts (on which the issue was reported) were present on s3 earlier and are now missing. Can we ask the customer to not clean up their s3 location?  

Also please let us know if the issue happens again.   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commands in waiting state since April 20,MW-1189,67539,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,satyavathib,satyavathib,11/Jul/17 10:14 AM,11/Jul/17 11:12 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Many commands triggered from API are in waiting state since April 30. If they were waiting state in DJ then they are supposed to get killed at least after some time but It did not happen.

This link has all the commands running since long :

https://api.qubole.com/v2/analyze?status=InProgress&interface=API&qbol_user_id=All

RDS details for one of the command :

{code}
production-replica> select * from query_hists where id=65225505 \G;
*************************** 1. row ***************************
                         id: 65225505
               qbol_user_id: 11913
                submit_time: 1491545811
                   end_time: NULL
                   progress: 0
                    cube_id: NULL
                 created_at: 2017-04-07 06:16:51
                 updated_at: 2017-04-07 06:16:51
                       path: /tmp/2017-04-07/1756/65225505
                     status: waiting
                  host_name: NULL
                   user_loc: 1
            qbol_session_id: 3683513
                 command_id: 50785953
               command_type: HiveCommand
                       qlog: NULL
            periodic_job_id: NULL
                      wf_id: NULL
             command_source: API
            resolved_macros: NULL
                status_code: NULL
                        pid: NULL
             editable_pj_id: NULL
                   template: generic
        command_template_id: NULL
command_template_mutable_id: NULL
                 can_notify: 0
             num_result_dir: 0
                 start_time: NULL
                       pool: NULL
                    timeout: NULL
                        tag: hadoop2
                       name: Insert into uacf_prod_derived.mmf_challengeteamscoreupdated_avro Derived Avro Tables
     saved_query_mutable_id: NULL
                 account_id: 1756
{code}


Please Let me know If any details required.",,satyavathib,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,myfitnesspal,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05abj:,,,,,,,,,,,,,,,,,,,1.0,15680,2017-07-11 10:56:16.08,,,11/Jul/17 10:56 AM;sumitm;We added logic to requeue such missed commands. [~satyavathib] is there any new such issue got reported?,"11/Jul/17 11:12 AM;satyavathib;This issue is reported by ""my fitness"" alone yesterday and I have told them that these commands have not started and no cost issues and also communicated that they could kill those commands If they wish to. Do we have the reason for this why these commands were not killed since then [~sumitm]. I would update If any other customers report the same issue.

Customer Email ID : mprice@myfitnesspal.com
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Good subject line,ESC-9,67426,Bug,To Do,ESC,ESCALATIONS,software,adityak,,,Major,,,venkatak,addon_zendesk_for_jira,10/Jul/17 4:58 AM,10/Jul/17 4:58 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_sustenance,,,,,,,"Issue description:

change to appropriate title:

change description as required here..

Command Id/ Notebook Id:
223234

Environment Details:
AWS production

Do we have approvals from customer to rerun/ test the commands:
yes/ no

Summary of the troubleshooting steps (so far):
step 1 done
step 2 done
..

Actions suggested to customer:

action 1 suggested
action 2 suggested
..
Next possible actions:

Can we suggest this 1?
Can we suggest this one 2?

Specific request/ questions to Sustenance/ Engineering:

I do not know how this works in hive. Could you please confirm the behavior?

",,addon_zendesk_for_jira,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z059o7:,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/17 4:58 AM;venkatak;-- This notification was sent from JIRA ESC-9 to all linked Zendesk Support tickets by Venkata Krishnan.--

Comment from Sustenance/ Engineering team goes here...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test jira - will be deleted,HIVE-2325,67205,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,psrinivas,venkatak,addon_zendesk_for_jira,07/Jul/17 12:48 AM,07/Jul/17 1:23 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_sustenance,,,,,,,test jira - will be deleted,,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z0591j:,,,,,,,,,,,,,,,,,,,,,2017-07-07 00:49:13.845,,,"07/Jul/17 12:49 AM;addon_zendesk_for_jira;-- This notification was sent from Zendesk Support ticket #15657 to all linked JIRA issues by Venkata Krishnan S. --

this is what i found
these are my next steps
this is my specific question:

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with different family instance types b/w master and slave ( in case of avro tables backed by avro schema file in s3 ),SPAR-1772,67173,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,adubey,adubey,06/Jul/17 2:05 PM,04/Aug/17 10:09 AM,09/Aug/17 5:30 AM,,,,,0,jira_escalated,,,,,,,,"cc [~venkats] [~psrinivas]

This seems like very very special case when :

1. Avro table is used with avsc file on S3
2. Master has EBS backed instance type like C4 family where it will have a mount point as ""media/ebs2""
3. Slaves do not have any EBS attached so all the volumes are named differently and there wont be any ebs2 dir 
4. Notebook is used for running spark ( it wont be applicable in analyze command case ) because driver runs on master and submit the job to executors on slaves with the plan having ebs2 in the s3ncache location 

Master was c4.xl
Slave i3.xl

Repro case - run a SELECT * query on avro table with avsc file on S3..

I am raising this here because this case is rare but we might wanna assess if there are any other scenarios and see how can we fix this",,adubey,hiyer,mahuja,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Jul/17 2:04 PM;adubey;i3-UAissue.png;https://qubole.atlassian.net/secure/attachment/45158/i3-UAissue.png,,,,,,,,,,,,,AWS,,,,,None,chartboost,underarmour,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z058uf:,,,,,,,,,,,,,,,,,,,1.0,16282,2017-07-24 10:11:42.748,,,"24/Jul/17 10:11 AM;mahuja;[~hiyer], [~abhishekmodi], [~sourabhg] - shouldn't the reference to S3 native cache in the stack be pointing to /media/ephemeral0 for diskless instances?","24/Jul/17 10:28 AM;hiyer;i3 has instance store volumes, and I guess we consider it as non-ebs attachable. That would explain why we attached ebs volumes to the master but not the slaves. ","24/Jul/17 2:25 PM;mahuja;That is correct. The issue will be because the driver runs on c4 node type. The cache dir config on this node will be the following, which is not available on slaves.

{code}
  <property>
    <name>fs.s3n.cache.dir</name>
    <value>/media/ebs2/s3ncache</value>
    <final>true</final>
  </property>
{code}

Shouldn't the final marked value on slaves (with /media/ephemeral0/ path) still apply?","04/Aug/17 10:06 AM;adubey;Chartboost also hit this issue - ( master was m4 instance and slave had m2 )
this time it happened during insert into parq - 

{code:java}
CREATE TABLE `tmp1234.test6`( `impression_id` string, `dt` string, `hr` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ( 'path'='s3n://cb-cubole/warehouse/tmp1234.db/test6') STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 's3n://cb-cubole/warehouse/tmp1234.db/test6' TBLPROPERTIES ( 'spark.sql.sources.provider'='parquet', 'spark.sql.sources.schema.numParts'='1', 'spark.sql.sources.schema.part.0'='{\""type\"":\""struct\"",\""fields\"":[{\""name\"":\""impression_id\"",\""type\"":\""string\"",\""nullable\"":true,\""metadata\"":{}},{\""name\"":\""dt\"",\""type\"":\""string\"",\""nullable\"":true,\""metadata\"":{}},{\""name\"":\""hr\"",\""type\"":\""string\"",\""nullable\"":true,\""metadata\"":{}}]}', 'transient_lastDdlTime'='1501864237')
{code}",04/Aug/17 10:09 AM;adubey;[~mahuja] can we plan a fix here ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S3 based to virtual foldering migration script issue,ZEP-1130,67172,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Blocker,,tanishg,mmajithia,mmajithia,06/Jul/17 1:46 PM,12/Jul/17 12:55 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Ran the script to move s3 foldering to virtual foldering for low risk customer: 6586

- Migration ran successfully

Issue 1. One of the home folder explore call fails for user ddyer@zvelo.com:
{code:java}
Request URL:https://api.qubole.com/api/latest/folders/explore.json?location=Users%2Fddyer%40zvelo.com%2Fddyer%40zvelo.com&type=notes&folder_id=53743&_=1499372617290
Request Method:GET
Status Code:422 Unprocessable Entity

{""error"":{""error_code"":422,""error_message"":""Location is not present""}}
{code}

Issue 2: 
a. ddyer@zvelo.com folder is present in two location after migration but location is same while parent_folder_id is different for both

b. 53744's location is also not set properly
{code:java}
| 53742 | ddyer@zvelo.com      |       6586 |               -2 | Note              | 2017-07-06 20:14:48 | 2017-07-06 20:14:48 | Users                        | NULL                |
| 53743 | ddyer@zvelo.com      |       6586 |            53742 | Note              | 2017-07-06 20:14:55 | 2017-07-06 20:14:55 | Users                        | NULL                |

| 53744 | public internal      |       6586 |            53743 | Note              | 2017-07-06 20:14:55 | 2017-07-06 20:14:55 | Users/ddyer@zvelo.com/       | NULL                |
{code}



*Script Result*
{code:java}

[ec2-user@ip-10-155-27-168 tapp]$ RAILS_ENV=qubole rails runner db/scripts/migrate_s3_folders_to_virtual_folders.rb only 6586
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/lib/patches/presto-ruby-client/presto-client-models.rb:5: warning: already initialized constant Presto::Client::Models::QueryResults
/usr/lib/ruby/gems/2.1.0/gems/presto-client-0.4.12/lib/presto/client/models.rb:798: warning: previous definition of QueryResults was here
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/smtp.rb:806: warning: already initialized constant Net::SMTPSession
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/smtp.rb:806: warning: previous definition of SMTPSession was here
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/pop.rb:687: warning: already initialized constant Net::POP
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/pop.rb:687: warning: previous definition of POP was here
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/pop.rb:688: warning: already initialized constant Net::POPSession
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/pop.rb:688: warning: previous definition of POPSession was here
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/pop.rb:689: warning: already initialized constant Net::POP3Session
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/pop.rb:689: warning: previous definition of POP3Session was here
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/pop.rb:702: warning: already initialized constant Net::APOPSession
/usr/lib/ruby/gems/2.1.0/gems/tlsmail-0.0.1/lib/net/pop.rb:702: warning: previous definition of APOPSession was here
/usr/lib/ruby/gems/2.1.0/gems/signalfx-0.1.0/lib/signalfx/conf.rb:3:in `<top (required)>': Use RbConfig instead of obsolete and deprecated Config.
DEPRECATION WARNING: The InstanceMethods module inside ActiveSupport::Concern will be no longer included automatically. Please define instance methods directly in DeviseSecurityExtension::Controllers::Helpers instead. (called from <top (required)> at /usr/lib/qubole/packages/tapp2-44.38.0/tapp/config/application.rb:8)
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/config/initializers/default_settings.rb:22: warning: already initialized constant CRED_INFO
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/config/initializers/default_settings.rb:12: warning: previous definition of CRED_INFO was here
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/config/initializers/default_settings.rb:27: warning: already initialized constant VERSION_INFO
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/config/initializers/default_settings.rb:13: warning: previous definition of VERSION_INFO was here
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/config/initializers/default_settings.rb:30: warning: already initialized constant INSTANCE_INFO
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/config/initializers/default_settings.rb:14: warning: previous definition of INSTANCE_INFO was here
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/app/models/note.rb:22: warning: character class has '-' without escape: /\A[ a-zA-Z0-9-_'@.]+\z/
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/app/models/notebook_dashboard.rb:16: warning: character class has '-' without escape: /\A[ a-zA-Z0-9-_'@.]+\z/
DEPRECATION WARNING: ActiveAdmin::Dashboard is deprecated and will be removed in the next version
DEPRECATION WARNING: ActiveAdmin::Dashboard is deprecated and will be removed in the next version
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/app/models/folder.rb:11: warning: character class has '-' without escape: /\A[ a-zA-Z0-9-_'@.+|]+\z/
/usr/lib/qubole/packages/tapp2-44.38.0/tapp/lib/binding_module.rb:3: warning: undefining `object_id' may cause serious problems
Account id: 6586 Folders and notebooks under Common location are successfully migrated...
Account id: 6586 Folders and notebooks under Users location are successfully migrated...
Account id: 6586 Creating NotebookDashboard type folder for ddyer@zvelo.com..
Account id: 6586 Creating Note type folder for mrobinson@zvelo.com..
Account id: 6586 Creating NotebookDashboard type folder for mrobinson@zvelo.com..
Account id: 6586 Creating NotebookDashboard type folder for lnabarrete@zvelo.com..
Account id: 6586 Creating NotebookDashboard type folder for awilinksy@zvelo.com..
Account id: 6586 Creating NotebookDashboard type folder for awilinsky@zvelo.com..
Account id: 6586 Foldering migration successfully done.
{code}



*Notebook S3 Location Backup*
{code:java}
production-replica> select id, name, location from notes where account_id = 6586 and deleted_at is null;
+-------+---------------------------------+----------------------------+
| id    | name                            | location                   |
+-------+---------------------------------+----------------------------+
| 24761 | ExampleNote                     | NULL                       |
| 24762 | PrestoExampleNote               | NULL                       |
| 24763 | Example-SparkSQL-HackerNewsData | NULL                       |
| 33210 | zvelonet_EDA                    | NULL                       |
| 34437 | ztag_integration_mining         | NULL                       |
| 34476 | python-regex-znet-matcher       | NULL                       |
| 39762 | QuboleTest                      | Users/ddyer@zvelo.com      |
| 39924 | test                            | Users/lnabarrete@zvelo.com |
| 40046 | Sandbox202                      | Users/ddyer@zvelo.com/dave |
| 40047 | EDA-zveloNet                    | Users/ddyer@zvelo.com/dave |
| 40048 | EDA-zTag                        | Users/ddyer@zvelo.com/dave |
| 40049 | H2O                             | Users/ddyer@zvelo.com/dave |
| 40051 | AIaaS Mining - Clone            | Users/ddyer@zvelo.com/dave |
| 40052 | Presto - AIaaS Phase 2 - Clone  | Users/ddyer@zvelo.com/dave |
| 40054 | EDA-IoT-P                       | Users/ddyer@zvelo.com/dave |
| 40151 | Sandbox210                      | Users/ddyer@zvelo.com/dave |
| 40169 | bubbletest                      | Users/ddyer@zvelo.com/dave |
| 40282 | test_scatterplot                | Users/ddyer@zvelo.com/dave |
| 40323 | Luke-MASCat-sampling            | Users/ddyer@zvelo.com/dave |
| 40377 | zTagPartitionFix                | Common/quboleAdmin         |
| 40592 | EDA-zTagP                       | Users/ddyer@zvelo.com/dave |
| 41043 | EDA-IoT-S                       | Users/ddyer@zvelo.com/dave |
| 41060 | BayAreaBikeShare_Analysis       | Users/ddyer@zvelo.com      |
| 41065 | Dev IoT Status                  | Common                     |
| 41066 | IoT2                            | Common/Public Internal     |
| 41067 | tasdf                           | Common/Public Internal     |
| 41069 | plotly_scatter_demo             | Users/ddyer@zvelo.com/dave |
+-------+---------------------------------+----------------------------+
{code}

*New DB Folders*
{code:java}
production-replica> select * from folders where account_id=6586;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    15463538
Current database: rstore

+-------+----------------------+------------+------------------+-------------------+---------------------+---------------------+------------------------------+---------------------+
| id    | name                 | account_id | parent_folder_id | folder_type       | created_at          | updated_at          | location                     | deleted_at          |
+-------+----------------------+------------+------------------+-------------------+---------------------+---------------------+------------------------------+---------------------+
| 53736 | Public Internal      |       6586 |               -1 | Note              | 2017-07-06 20:14:40 | 2017-07-06 20:14:40 | Common                       | NULL                |
| 53737 | quboleAdmin          |       6586 |               -1 | Note              | 2017-07-06 20:14:45 | 2017-07-06 20:14:45 | Common                       | NULL                |
| 53738 | awilinksy@zvelo.com  |       6586 |               -2 | Note              | 2017-07-06 20:14:48 | 2017-07-06 20:14:48 | Users                        | NULL                |
| 53739 | Notebooks            |       6586 |            53738 | Note              | 2017-07-06 20:14:48 | 2017-07-06 20:14:48 | Users/awilinksy@zvelo.com    | NULL                |
| 53740 | awilinsky@zvelo.com  |       6586 |               -2 | Note              | 2017-07-06 20:14:48 | 2017-07-06 20:14:48 | Users                        | NULL                |
| 53741 | Notebooks            |       6586 |            53740 | Note              | 2017-07-06 20:14:48 | 2017-07-06 20:14:48 | Users/awilinsky@zvelo.com    | NULL                |
| 53742 | ddyer@zvelo.com      |       6586 |               -2 | Note              | 2017-07-06 20:14:48 | 2017-07-06 20:14:48 | Users                        | NULL                |
| 53743 | ddyer@zvelo.com      |       6586 |            53742 | Note              | 2017-07-06 20:14:55 | 2017-07-06 20:14:55 | Users                        | NULL                |
| 53744 | public internal      |       6586 |            53743 | Note              | 2017-07-06 20:14:55 | 2017-07-06 20:14:55 | Users/ddyer@zvelo.com/       | NULL                |
| 53745 | dave                 |       6586 |            53742 | Note              | 2017-07-06 20:14:55 | 2017-07-06 20:14:55 | Users/ddyer@zvelo.com        | NULL                |
| 53746 | public               |       6586 |            53742 | Note              | 2017-07-06 20:15:55 | 2017-07-06 20:15:55 | Users/ddyer@zvelo.com        | 2017-07-06 20:19:03 |
| 53747 | public internal      |       6586 |            53746 | Note              | 2017-07-06 20:15:55 | 2017-07-06 20:15:55 | Users/ddyer@zvelo.com/public | 2017-07-06 20:18:44 |
| 53748 | lnabarrete@zvelo.com |       6586 |               -2 | Note              | 2017-07-06 20:15:55 | 2017-07-06 20:15:55 | Users                        | NULL                |
| 53749 | ddyer@zvelo.com      |       6586 |               -2 | NotebookDashboard | 2017-07-06 20:15:59 | 2017-07-06 20:15:59 | Users                        | NULL                |
| 53750 | mrobinson@zvelo.com  |       6586 |               -2 | Note              | 2017-07-06 20:15:59 | 2017-07-06 20:15:59 | Users                        | NULL                |
| 53751 | mrobinson@zvelo.com  |       6586 |               -2 | NotebookDashboard | 2017-07-06 20:15:59 | 2017-07-06 20:15:59 | Users                        | NULL                |
| 53752 | lnabarrete@zvelo.com |       6586 |               -2 | NotebookDashboard | 2017-07-06 20:15:59 | 2017-07-06 20:15:59 | Users                        | NULL                |
| 53753 | awilinksy@zvelo.com  |       6586 |               -2 | NotebookDashboard | 2017-07-06 20:15:59 | 2017-07-06 20:15:59 | Users                        | NULL                |
| 53754 | awilinsky@zvelo.com  |       6586 |               -2 | NotebookDashboard | 2017-07-06 20:15:59 | 2017-07-06 20:15:59 | Users                        | NULL                |
| 53755 | Public Internal      |       6586 |               -1 | NotebookDashboard | 2017-07-06 20:23:11 | 2017-07-06 20:23:11 | Common                       | NULL                |
+-------+----------------------+------------+------------------+-------------------+---------------------+---------------------+------------------------------+---------------------+
20 rows in set (0.01 sec)

{code}

cc: [~rgupta], [~sumitm], [~mahuja], [~mohan], [~monikak]",,mahuja,mmajithia,monikak,rgupta,tanishg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z058u7:,,,,,,,,,,,,,,,,,,,1.0,15254,2017-07-06 21:34:29.416,,,"06/Jul/17 9:34 PM;rgupta;wat is the way around this?
Is this reqd to fix it bfr we ship virtual foldering to rest of s2 foldering accs?","07/Jul/17 7:10 AM;tanishg;I checked with the same set of folders structure and notebooks and there was no issue. All virtual folders are created successfully and explore call is working fine too.
I think lets go ahead with the script after taking snapshot of notebooks location. Monika already ran it in all the qa environments and it worked fine.Also above type of error never occurred in all those accounts. Moreover we already ran this script in more than 90+ accounts and it worked fine.  IMO lets go ahead with the script. If there is such intermittent issue we can check it accordingly or lets do a proper qa testing for it.
cc [~monikak]  [~mahuja] [~rgupta]
","07/Jul/17 10:40 AM;rgupta;[~mmajithia] wat was the issue....i m bot lost here guys....

I think imo lets not take chance here...lets discuss this on monday and finalize it","07/Jul/17 12:47 PM;mmajithia;[~tanishg], [~rgupta]: 
I have seen the issue with Mojave and Zvelo account, even if the script ran successfully. 
For all the existing internal accounts for which I have already run the script, it was mostly education accounts which are dead.
Moreover, all the internal accounts are not in use and have limited data. 

I don't want to take any chance and continue with this state.
I am inclined to spend some more time and figure out what happened and how to validate it so that we don't end up in a state where we will have many notebooks/user folders which we have to fix manually.

I will spend some time myself to investigate before we resume the migration.","12/Jul/17 1:59 AM;tanishg;[~mmajithia] In account 5269 3-4 notebooks out of 25 notebooks are not copied successfully may be because download notebooks was not done successfully(retries are already there). In such cases once we run the migration it will be good if we can save the logs too. If notebook is not downloaded successfully Error message will be shown something like 
{code:java}
Account id: #{acc.id} : Failed to download file from #{source}..
{code}


This will help us to copy notebooks manually if the download operation is failed.

To give it as an example : 

{code:java}
/usr/lib/hive_scripts/storagecli.py --account-id=5269 --cmd=""-lsr s3://qubole-knijjer/account/zeppelin/notebook/""
drwxrwxrwx   -          0 1970-01-01 00:00 /account/zeppelin/notebook/12WDKUB4BF1481835894
-rwxrwxrwx   1          0 2017-06-12 19:35 /account/zeppelin/notebook/12WDKUB4BF1481835894/note.json
drwxrwxrwx   -          0 1970-01-01 00:00 /account/zeppelin/notebook/15491SEV2B1473816058
-rwxrwxrwx   1       3976 2017-06-12 19:35 /account/zeppelin/notebook/15491SEV2B1473816058/note.json
{code}

Here size of 12WDKUB4BF1481835894 notebook is 0. 
But when I tried it from console it was downloaded successfully

{code:java}
 file_resp = QbolHelper::download_file(creds_map, local_path, ""s3://qubole-knijjer/account/qubole_folder_5269/Users/kulbir@qubole.com/Notebooks/12WDKUB4BF1481835894.json"", 3,1000*1024*1024)
=> {:data_not_available=>false, :msg=>nil}
irb(main):013:0> file_resp.present?
=> true
irb(main):014:0> file_resp.size
=> 2
irb(main):015:0> File.size(local_path)
=> 1261528
{code}

We can also inform others to save logs who are doing migration.



","12/Jul/17 2:21 AM;rgupta;1. [~tanishg] can u do the change of adding relevant logs and update the docs on how to save the logs for CSM or who ever runs this script
2. Add a validation script. This script can make sure that after a migration has happened no notes in new location have size zero. If there is zero sized notes then those are reported back.

Sync with [~mmajithia] also if he has any feedback...",12/Jul/17 2:25 AM;mahuja;[~tanishg] [~sumitm] we should check for size mismatch when moving notebooks. We had run into similar issues with S3 based foldering migration. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet filewriter fails for idle S3 socket and finally causes job failure due to FileAlreadyExists exception,SPAR-1769,67164,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,venkats,adubey,adubey,06/Jul/17 12:58 PM,08/Aug/17 10:20 AM,09/Aug/17 5:27 AM,,,,,0,jira_escalated,,,,,,,,"cc [~mahuja]

Just raising this to track it here - Underarmours reported a job 82545335 which failed due to FileAlreadyExists reason  but when i checked the tasks the previous attempt actually failed due to 


{code:java}
Caused by: com.qubole.com.amazonaws.services.s3.model.AmazonS3Exception: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout; Request ID: DEE397400AF8276E), S3 Extended Request ID: bev7BOwyBZY1FJgWLCSYGYCm11gn3qYVRKYXcIyJKa/SGm98IA47iq4tfvvxr966CxySjttknZw=
{code}


Have we seen this before and is there anything which is a root cause of this - any tuning tip here?

cc [~abhishekmodi]",,abhishekmodi,adubey,bharatb,mdaurangzeb,prachim,puneetg,rohitk,venkats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,optimizely,underarmour,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z058sf:,,,,,,,,,,,,,,,,,,,1.0,16014,2017-07-07 06:47:05.985,,,"07/Jul/17 6:47 AM;puneetg;It looks like exception got triggered from the first executor fail because of RPC timeout.

{code:java}
17/07/06 08:15:19 dispatcher-event-loop-4 WARN HeartbeatReceiver: Removing executor 76 with no recent heartbeats: 140658 ms exceeds timeout 120000 ms
{code}

I could see this in driver logs. Maping stage was successfully done, then in reducer stage driver must have taken more time in collecting shuffle info, hence executors started timing out.

[~adubey]
Can we ask them to set timeout value to 20 mins then retry the job?

{code:java}
spark.network.timeout 1200s
{code}
",07/Jul/17 6:48 AM;Alex;sorry not sure why this is assigned to me - I think you mean AD? ,07/Jul/17 10:12 AM;adubey;[~puneetg] - thanks for digging - yes network timeout will be helpful to avoid this. however to me it looks like the problem of idle socket timeout problem seems completely unrelated to first exec problem? ,"07/Jul/17 8:53 PM;bharatb;I think the sequence of events is:-

1. map output path collection timed out
2. this led to reducers being retried and since DFOC was enabled, the file was already present and the second attempt failed. 

I think we did not check if reducers were retried only once or more than once.","09/Jul/17 11:17 PM;puneetg;I spend some time on this. Here are some findings.
Around 14 reducer failed because of connection close with shuffle service with the following exception:
{code:java}
17/07/06 08:38:14 task-result-getter-3 WARN TaskSetManager: Lost task 2.0 in stage 4.0 (TID 22650, ip-172-31-16-234.ec2.internal, executor 27): org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.shuffle.FetchFailedException: Connection from ip-172-31-31-106.ec2.internal/172.31.31.106:7337 closed
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:357)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:332)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
{code}
But most of them were successfully completely in 2nd or 3rd retry.

Task 2.0 failed but in 2nd retry s3 timeout happened while writing to the file.
{code:java}
17/07/06 09:25:10 task-result-getter-3 WARN TaskSetManager: Lost task 2.1 in stage 4.0 (TID 22719, ip-172-31-31-106.ec2.internal, executor 162): org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.qubole.com.amazonaws.services.s3.model.AmazonS3Exception: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout; Request ID: DEE397400AF8276E), S3 Extended Request ID: bev7BOwyBZY1FJgWLCSYGYCm11gn3qYVRKYXcIyJKa/SGm98IA47iq4tfvvxr966CxySjttknZw=
	at com.qubole.com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1389)
	at com.qubole.com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:902)
	at com.qubole.com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:607)
	at com.qubole.com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:376)
	at com.qubole.com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:338)
{code}

Once this retry failed, the subsequent retry failed with exception:
{code:java}
17/07/06 09:27:03 task-result-getter-2 WARN TaskSetManager: Lost task 2.2 in stage 4.0 (TID 22731, ip-172-31-27-244.ec2.internal, executor 1605): org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: s3://uacf-prod-sot-us-east-1/databricks-warehouse/sotV3/sot_compacted_mfp_food/snapshots/version_1/process_timestamp=2017-07-06T07H45M02S/dt=2017-01-27/part-00002-56682f28-741d-408a-b1cf-04469690cb26.snappy.parquet already exists
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:527)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:914)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:895)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:792)
{code}
Basically since DFOC is enabled, when the s3 timeout happened in 2nd retry. It didn't delete file from s3 and the subsequent retry failed because of file already present.

I am not sure what exact workaround will work in this case. But increasing timeout will surely help.
* It will reduce these unnecessary retries.
* Since, there are no retries, it might reduce the bandwidth usage of s3. Which can avoid s3 timeouts.
Hence, According to me increasing timeout can be good start. Once we have solved that issue, we can look deeper into deeper root cause if it occurs again.

cc: [~adubey], [~venkats]
",24/Jul/17 12:12 PM;venkats;[~adubey] Is there anything to be done as part of this JIRA? Can this be closed?,"25/Jul/17 11:36 PM;mdaurangzeb;[~venkats] 
cc- [~adubey] [~abhishekmodi]

It seems Optimizely is facing a similar issue for a Hadoop job (command id 85885006).

Reducers failed with the below error.

Error: com.qubole.com.amazonaws.services.s3.model.AmazonS3Exception: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed. (Service: Amazon S3; Status Code: 400; Error Code: RequestTimeout; Request ID: 853E0BEEDB612E77), S3 Extended Request ID: Im12fth/sCYhbXTlobNVF1IFr+u+JQMFmqKLRoJ2UiB+xRIOquvtCisGQAN/2s9rT8eEaWUuF78= at com.qubole.com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1389) at com.qubole.com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:902) at com.qubole.com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:607) at com.qubole.com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:376) at com.qubole.com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:338) at com.qubole.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:287) at com.qubole.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3826) at com.qubole.com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1405) at com.qubole.com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:131) at com.qubole.com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:123) at com.qubole.com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:139) at com.qubole.com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:47) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)

Subsequent retry failed with FileAlreadyExistsException:

Error: org.apache.hadoop.fs.FileAlreadyExistsException: s3://optimizely-export-ng/export-staging/20170722/processed-r-00117.gz already exists at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:582) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:914) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:895) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:792) at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135) at org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat$LazyRecordWriter.write(LazyOutputFormat.java:113) at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:457) at com.optimizely.backend.export.RawData2Exporter$ExportReducer.reduce(RawData2Exporter.java:252) at com.optimizely.backend.export.RawData2Exporter$ExportReducer.reduce(RawData2Exporter.java:205) at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:165) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:160)

Do we have any workaround available for this?  ",26/Jul/17 8:44 AM;abhishekmodi;[~mdaurangzeb] could you move this issue to Hadoop. This is independent jira. Could you disable DFOC for the customer for time being.,26/Jul/17 8:34 PM;mdaurangzeb;Thanks [~abhishekmodi],"28/Jul/17 5:47 AM;mdaurangzeb;Hi [~prachim]

This timeout is valid for hadoop 2 cluster ?
spark.network.timeout 1200s ","28/Jul/17 6:53 AM;prachim;Hi [~mdaurangzeb] ,

I don't think this will work for Hadoop cluster. I was looking for the appropriate property to set but couldn't find which one. Maybe [~abhishekmodi] can help here.","28/Jul/17 7:07 AM;prachim;Also may be you can try disabling DFOC first as suggested by [~abhishekmodi] , may be the retries will pass.
I missed seeing Abhishek's comment while suggesting to set network timeout.","08/Aug/17 10:20 AM;adubey;Disabling DFOC in this case will impact the performance very badly. I think we really need to give them a reasonable solution here - 

I was checking https://github.com/aws/aws-sdk-java/issues/1101
looks like there is history of this issue although our SDK version is different but that does not seem to be playing a role here. 

Should we recommend increasing the socket timeout here . ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
writing from temporary to final location is missing one output part,HAD-666,67160,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,hiyer,megha,megha,06/Jul/17 11:12 AM,07/Jul/17 12:18 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,Sustenance,,,,,,,"Command id: 80713570
The job is successful.. 
There were 26 mappers, which wrote 26 output files in the output location: 

s3://dlx-prod-core-shared/prod/data/dlx/core/consumer/individual/individual/master/timestamp=20170626031620/_temporary/_attempt_2243.201706250000_2796_m_000007_0/

As dfoc was disabled, the final output was committed by AM mapper, which wrote all but one file to output location:
s3://dlx-prod-core-shared/prod/data/dlx/core/consumer/individual/individual/master/timestamp=20170626031620


7th mapper i.e https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fip-10-108-32-180.ec2.internal%3A50060%2Ftasklog%3Ftaskid%3Dattempt_2243.201706250000_2796_m_000007_0%26all%3Dtrue&clusterInst=517673 suggests data was written to prod/data/dlx/core/consumer/individual/individual/master/timestamp=20170626031620/_temporary/_attempt_2243.201706250000_2796_m_000007_0/part-m-00007.avro

but looking at final write location above, part-m-00007.avro file is missing in the output..

There are no traces of write failure in the AM mapper
https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fec2-54-197-16-55.compute-1.amazonaws.com%3A50030%2Fjobdetails.jsp%3Fjobid%3Djob_2243.201706250000_2795&clusterInst=517673

As this is data loss issue, marking this as critical.. This has happened a few times..
",,asomani,hiyer,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,oracle,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z058rj:,,,,,,,,,,,,,,,,,,,1.0,15554,2017-07-06 20:52:12.423,,,"06/Jul/17 8:52 PM;hiyer;This is hadoop1, so there is no AM here. The task output is written directly by the tasks. From the logs it seems to me that we did try to copy the file to the final location:
{code}
2017-06-25 21:30:37,078 -1 INFO mapred.Task:918 (Task Thread for Task: attempt_2243.201706250000_2796_m_000007_0): Task:attempt_2243.201706250000_2796_m_000007_0 is done. And is in the process of commiting
2017-06-25 21:30:37,258 -1 INFO mapred.Task:1063 (Task Thread for Task: attempt_2243.201706250000_2796_m_000007_0): Task attempt_2243.201706250000_2796_m_000007_0 is allowed to commit now
2017-06-25 21:30:37,750 -1 INFO output.FileOutputCommitter:174 (Task Thread for Task: attempt_2243.201706250000_2796_m_000007_0): Saved output of task 'attempt_2243.201706250000_2796_m_000007_0' to s3://dlx-prod-core-shared/prod/data/dlx/core/consumer/individual/individual/master/timestamp=20170626031620
{code}

However it's suspicious that this operation finished almost instantaneously, since for all other tasks I saw this operation take upwards of a minute. Can we try enabling debug logs for the map task (-D mapreduce.map.log.level=DEBUG should work, though would be better to verify this once).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java version issue with sqoop hadoop 1,SQOOP-119,67089,Bug,Open,SQOOP,SQOOP,software,sumitm,,,Major,,ksr,megha,megha,05/Jul/17 11:36 AM,05/Jul/17 4:25 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Oracle uses sqoop with shell command.. example ids: 82413655,82361409
They're getting exceptions: 
{code:java}
2017-07-05 08:29:39,182 -1 FATAL mapred.Child:262 (main): Error running child : java.lang.UnsupportedClassVersionError: dbo_storebanner : Unsupported major.minor version 52.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:270)
    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:812)
    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:857)
    at org.apache.sqoop.mapreduce.db.DBConfiguration.getInputClass(DBConfiguration.java:405)
    at org.apache.sqoop.mapreduce.db.DataDrivenDBInputFormat.createDBRecordReader(DataDrivenDBInputFormat.java:233)
    at org.apache.sqoop.mapreduce.db.DBInputFormat.createRecordReader(DBInputFormat.java:263)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:602)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:310)
    at org.apache.hadoop.mapred.Child$3.run(Child.java:205)
{code}

This is because we changed the sqljdbc jar in sqoop repo to resolve a connectivity issue. But that jar required jre8 hence, we had made the upgrade only in sqoop and not the other sqoop repos.

New sqoop location is /usr/lib/sqoop1/bin/sqoop..

We're providing oracle with a custom bootstrap to fix temporarily.. what should be a long term fix? 

",,adubey,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,"{repository={count=1, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":1,""lastUpdated"":""2011-07-22T07:04:20.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z058br:,,,,,,,,,,,,,,,,,,,1.0,15613,,,,05/Jul/17 4:25 PM;megha;Oracle started using the new sqoop location and it seems to be working fine ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need cron to clean up leftover HDFS data for failed/killed jobs,HAD-665,66896,Bug,Open,HAD,Hadoop,software,hiyer,,,Critical,,hiyer,Kulbir,Kulbir,03/Jul/17 11:51 AM,10/Jul/17 3:56 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Saw an issue in Pinterest data cluster where almost 25 TB of data was left sitting around in HDFS, due to which no jobs were running on cluster:
{code}
[ec2-user@ip-10-1-1-79 ~]$ hadoop dfs -du /tmp/
Found 582 items
763             hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/0125f552-5fd7-4b2d-b306-b7c220e20be2
388831          hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/01605df2-eae9-49f7-99b1-1dfcb44fd4df
418736          hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/016e2c41-d407-42e0-a4d7-6a9f25baf843
231860          hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/01cabe19-0d15-401c-88b2-ae618dc47197
418736          hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/030d7edb-85d0-4069-bbd5-57abbc433fe6
418499          hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/031ea38b-478c-461f-8914-71a8835be649
418736          hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/036f73e2-aa5e-4b64-965c-bcda0193ad53
394816          hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/041e3905-e757-4a59-9beb-80c3d2c79fde
399596          hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/056f49c0-2e39-4551-9a5c-20fb6b04da0c
*25139328994077  hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/060e60e0-f4d7-4c02-a00a-de3de8f9ca8f*

Further breakdown

[ec2-user@ip-10-1-1-79 ~]$ hadoop dfs -du /tmp/060e60e0-f4d7-4c02-a00a-de3de8f9ca8f
Found 3 items
2195836         hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/060e60e0-f4d7-4c02-a00a-de3de8f9ca8f/files
0               hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/060e60e0-f4d7-4c02-a00a-de3de8f9ca8f/output
*25139326798241  hdfs://ec2-54-157-221-221.compute-1.amazonaws.com:9000/tmp/060e60e0-f4d7-4c02-a00a-de3de8f9ca8f/tempspace*


[ec2-user@ip-10-1-1-79 logs]$ hadoop dfs -ls /tmp/060e60e0-f4d7-4c02-a00a-de3de8f9ca8f/tempspace
Found 4199 items
-rw-rw-rw-   2 yyin@pinterest.com supergroup  6002203351 2017-06-23 10:57 /tmp/060e60e0-f4d7-4c02-a00a-de3de8f9ca8f/tempspace/0000e60d-e919-4571-9f5b-796aed92cee0a4af1247-c845-4c8a-bd82-96f7637fdd45
-rw-rw-rw-   2 yyin@pinterest.com supergroup  5828796916 2017-06-23 02:22 /tmp/060e60e0-f4d7-4c02-a00a-de3de8f9ca8f/tempspace/0008fcfd-a53a-45e6-8c18-44d297e361aec1231893-d044-4467-8e82-666d98e363df
-rw-rw-rw-   2 yyin@pinterest.com supergroup  6025387965 2017-06-24 02:12 

{code}

On further investigation this seems to be generated by a failed job(https://api.qubole.com/v2/analyze?command_id=80169637) for which I will file a separate JIRA, meanwhile for this issue ask is if we can have a cron job to automatically purge data for such cases, for e.g:

hadoop collectfiles --older-than 40 /tmp/*  ?

We do have similar cron to clean data under /tmp/hive-ec2-user and /tmp/temp so perhaps above will still be fine or else we can change it to look for exact directory name pattern for such jobs.

cc [~mpatel]",,hiyer,Kulbir,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,Pinterest,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z057lr:,,,,,,,,,,,,,,,,,,,1.0,15567,2017-07-03 21:28:16.835,,,"03/Jul/17 9:28 PM;hiyer;I don't know if it's correct to add this cron generically, for two reasons:  
1. Some customers may dump data in /tmp for further processing by subsequent jobs. I know Hike used to do this. It's probably not good practice but there it is.  
2. For smaller clusters 40 hours would probably be too late anyway, and the cluster may run out of space before the cron kicks in. I've seen this happen for a couple of customers.   

IMO alerting and preventive action, viz. EBS upscaling would be a better way to handle this case in a generic way. For specific cases cron can be added via node bootstrap or other means. Thoughts?","03/Jul/17 10:57 PM;Kulbir;Thanks [~hiyer], those are valid points however given big chunk of leftover data(in tempspace folder) seems to only happen for failed commands, if we can come up with specific pattern to clean for only these jobs, 40 hours is still safe and useful duration as backup, atleast for Pinterest(Had1) environment and won't affect any other data in tmp.

For e.g I assume something like this will work ?
{code}
hadoop collectfiles --older-than 40 /tmp/*/files/*
hadoop collectfiles --older-than 40 /tmp/*/output/*
hadoop collectfiles --older-than 40 /tmp/*/tempspace/*
{code}

Bootstrapping is possible but then requires manual effort to maintain it across releases\environments which becomes added burden on admin. FYI i have also filed HADTWO-1003 to make sure for S3DistCp we don't leave leftover data in first place.

Perhaps we only add new cron jobs for Had-1 AMI's which would primarily be Pinterest only ?
What do you think ?
",03/Jul/17 11:28 PM;hiyer;Cron on specific patterns sounds reasonable. I'll add this cron as suggested.,03/Jul/17 11:33 PM;hiyer;Need to check if collectfiles works with multiple wildcards though :-),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reference to default database where the query does not use default db,HIVE-2310,66872,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,psrinivas,venkatak,venkatak,03/Jul/17 2:08 AM,03/Jul/17 2:11 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,Sustenace,,,,,,,"h2. Issue:

Check command id:

75931844

Error:

{code}
Added resource: /media/ephemeral0/s3ncache/13deb68e0fb236a9f9e07dccaa4dfabd/lz4-1.3.0.jar
OK
Time taken: 1.838 seconds
FAILED: SemanticException [Error 10072]: Database does not exist: default
org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist: default
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1343)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1332)
at org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.addEntities(FunctionSemanticAnalyzer.java:162)
at org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.analyzeCreateFunction(FunctionSemanticAnalyzer.java:86)
at org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.analyzeInternal(FunctionSemanticAnalyzer.java:59)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:368)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:446)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:344)
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1013)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1084)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:949)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:939)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:319)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:256)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:478)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:511)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:557)
at org.apache.hadoop.hive.cli.CliDriver.processInitFiles(CliDriver.java:575)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:873)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:818)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:736)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.main(RunJar.java:163)
Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist: default
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getDatabase(BaseSemanticAnalyzer.java:1339)
... 26 more
error processing file s3://qubole-pinterest/search/scripts/bootstrap
contents of the file are :
----------------------------------
{code}

Not sure why the message refers to default db when query does not refer to that database.

Is this a metastore query timeout issue? If it is, why is it referring to default db and not the db in use in that command?

Can this be caused by UDFs that pinterest uses? They use a huge hive bootstrap with so many UDFs added for each hive statement.

Wondering if UDFs are creating some trouble with this. I am confused why they would not see this consistently if so.",,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z057gf:,,,,,,,,,,,,,,,,,,,1.0,13764,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log messages are not indicative of the issue,HIVE-2308,66863,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,psrinivas,venkatak,venkatak,03/Jul/17 12:29 AM,07/Jul/17 9:14 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"h2.Issue:

Check the outcome of the following command ids:

1. 66714725 (failed)
2. 66721320 (success)

The failed command has failed with;

{code}
2017-04-15 01:05:58,958 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 49139.97 sec
MapReduce Total cumulative CPU time: 0 days 13 hours 38 minutes 59 seconds 970 msec
Ended Job = job_247.201702240442_127230
Loading data to table platform.all_audience_table partition (dt=null, eventtype=null)
Failed with exception unexepected condition during load. Please contact Qubole support. false
org.apache.hadoop.hive.ql.metadata.HiveException: unexepected condition during load. Please contact Qubole support. false
at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:2775)
at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1501)
at org.apache.hadoop.hive.ql.metadata.Hive.loadDynamicPartitions(Hive.java:1819)
at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:354)
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:98)
at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1628)
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1355)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1132)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:949)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:939)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:319)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:256)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:478)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:511)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:557)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:888)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:818)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:736)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.main(RunJar.java:163)
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask
MapReduce Jobs Launched:
Job 0: Map: 196 Cumulative CPU: 49139.97 sec HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 days 13 hours 38 minutes 59 seconds 970 msec
/media/ephemeral0/tmp/tapp/tmp/2017-04-15/247/66714725.md 100% || Time: 00:00:00 0.00 B/s
{code}

Though it is obvious that the hive parameters have caused the command to succeed, the messages in the logs are not useful to customer.

h2.Expected result:

The log messages are more helpful and we capture the exception well/ better.
",,drose@qubole.com,jssarma,psrinivas,rvenkatesh,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z057ef:,,,,,,,,,,,,,,,,,,,1.0,14035,2017-07-06 10:00:29.119,,,06/Jul/17 10:00 AM;rvenkatesh;[~venkatak] this is on hive 0.13. We will talk to [~jssarma] about a policy on working on these JIRAs. We have moved on to hive 2.1 but understand that there is no deprecation message and Pinterest is important. ,"07/Jul/17 3:56 AM;jssarma;This doesn't sound like a blocker. Nice to have. Isn't this one easy to reject?

I agree we need to tell them 0.13 is deprecated and we aren't taking enhancement requests and only doing critical/blocker stuff. Cc [~Kulbir] [~drose@qubole.com] ",07/Jul/17 9:14 AM;drose@qubole.com;[~jssarma] we are working to re-engage Pinterest at a higher level. One key objective is to get them using our recent features as they aren't utilizing much of our new devekopemebt.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Something went wrong - try again"" error occurs while attempting to download results",MW-1161,66857,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,venkatak,addon_zendesk_for_jira,02/Jul/17 11:00 PM,03/Jul/17 12:08 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer's issue:
---
Many time when I want to see the result, it says the something went wrong and to try again. That time i am unable to download the result.

If I go to some other command and comeback then everything went fine.I am able to see the result.
---

I am not sure if the web node that actually took the command submission goes down and that causes this.

Customer says that this often happens when they leave a long running query running and come back after 1/2 hours or later to check the results and they end up getting this.

Question:

If the client session is kept open, do we try to use the same web node to get the status of the command? 

Why should this happen?",,addon_zendesk_for_jira,aswina,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z057d3:,,,,,,,,,,,,,,,,,,,1.0,15049,2017-07-02 23:08:45.787,,,02/Jul/17 11:08 PM;aswina;[~sumitm] - Could you triage this please?,"03/Jul/17 12:08 AM;sumitm;[~venkatak] can you gather some more information like who is the customer, for which command ids it happened, is it reproducible, what's the error code sent by MW (if any) etc. 

With the given information its impossible to provide any help. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scheduler Re-run triggering the same instance multiple times.,SCHED-153,66855,Bug,Open,SCHED,Scheduler,software,sumitm,,,Major,,sumitm,satyavathib,satyavathib,02/Jul/17 10:55 PM,21/Jul/17 11:16 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"When the ""re-run"" button in schedulers is clicked just once , the job instance is triggered multiple times continuously. Customer has shared a screenshot for the same :
 !image-2017-07-03-11-15-08-014.png|thumbnail! 

This is a very intermittent issue and the customer complains that he has faced this multiple times.

I have also confirmed from customer that the ""re-run"" button is not clicked mutiple times since it is taking time to generate a command ID after the re-run is clicked.

Customer email : atul.jha@optimizely.com
Example scheduler : 17851

Please Let me know If any detail required.
",,aswina,nimitk,psrinivas,rvenkatesh,satyavathib,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jul/17 10:45 PM;satyavathib;image-2017-07-03-11-15-08-014.png;https://qubole.atlassian.net/secure/attachment/45017/image-2017-07-03-11-15-08-014.png,,,,,,,,,,,,,AWS,,,,,None,optimizely,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z057cn:,,,,,,,,,,,,,,,,,,,1.0,15520,2017-07-02 23:15:26.071,,,"02/Jul/17 11:15 PM;aswina;[~satyavathib] - Can you check whether 'skip missed instances' is checked or unchecked in the scheduled job?

cc: [~rvenkatesh] [~psrinivas] for more inputs",02/Jul/17 11:30 PM;rvenkatesh;Assigning to [~sumitm]. AFAIK he is the owner. Scheduler logs may have some clues. ,"03/Jul/17 12:00 AM;sumitm;Its a UI issue which had come up earlier as well. As Scheduler is under ETL now, we'll fix it as soon as we get a UI resource. ",03/Jul/17 12:03 AM;aswina;[~sumitm] - Can you give some details on why this could be an UI issue?  The API is triggered from UI only once as per jira description.,"03/Jul/17 12:05 AM;sumitm;[~aswina] when a user clicks on re-run button there is no response on UI if the api call has been submitted or not. Ideally, there should be a loading icon as soon as someone clicks on it. 

Unknowingly user clicks on it a couple of times, which results in multiple api calls being sent at same time. ",21/Jul/17 11:16 AM;satyavathib;[~sumitm]Could you please let me know if we have any ETA to communicate to customer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notebook reverting when switching between clusters,ZEP-1109,66823,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,tanishg,mstolpner,mstolpner,30/Jun/17 2:49 PM,01/Aug/17 10:50 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"This is an edge scenario and the issue is intermittent. Steps to reproduce:
1. Have a notebook on cluster A.
2. Update the notebook with changing the associated cluster to cluster B.
3. Change the notebook code
4. After some time, switch notebook back to cluster A. Changes maybe lost and the notebook reverted back to the state it was on step 1.",,beria,monikak,mstolpner,,,,,,,,,,,,,,,,,,,,,,,,,,,ZEP-1245,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05h3q:y,,,,,,nb-RB-47,,,,,,,,,,,,,1.0,15521,2017-06-30 16:01:20.212,,,"30/Jun/17 4:01 PM;beria;[~mstolpner] One thing here is that any code changes made are not synced to server instantaneously, but does so after 10 seconds (this is done for good javascript performance). When the code has not been synced, one would see a orange coloured bar on the left side of the paragraph. In case one changes the cluster of note during that interval, this might happen (since server has not received the changes yet). Can you please check if that is the case you are facing (i.e., time gap between changing code and attaching to different cluster is less than 10 seconds).","30/Jun/17 5:47 PM;mstolpner;[~beria] I am sure it was longer than 10sec. The issue is intermittent and hard to reproduce, but I witnessed it.","30/Jun/17 5:53 PM;beria;This could be similar to https://qubole.atlassian.net/browse/ZEP-872 , I just realized it has not released yet, will be coming in RB-45. But mostly this will also need tapp side changes cc [~namanm] [~rgupta]",30/Jun/17 9:23 PM;monikak;This seems similar to MW-1097.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not able to access some us-east-2 buckets from Explore,MW-1159,66819,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,tanishg,snamburu,snamburu,30/Jun/17 1:14 PM,08/Aug/17 11:25 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Customer: Sovrn
Email: lhesterberg@sovrn.com
Account: sovrn-aws-ingest

Issue: Customer's s3 buckets are hosted in us-east-2. We are able to upload/download files to these buckets but we are not able to access the folders inside certain buckets.

Case 1: 
Bucket name: sovrn-datascience
AWS location: us-east-2
Analyze: Able to access(command id: 80820069)
Explore: not able to access folder structure

Case 2:
Bucket name: sovrn-qubole-wip-us-east-2
AWS location: us-east-2
Analyze: Able to acess
Explore: able to access folder structure but getting the same read error for that bucket if I try to see  data from Explore.

Relevant jira: https://qubole.atlassian.net/browse/MW-834
cc: [~adubey]",,drose@qubole.com,snamburu,sumitm,tanishg,,,,,,,,,,,,,,,,,,,,,,,,HADTWO-905,,,,,,30/Jun/17 1:11 PM;snamburu;Screen Shot 2017-06-30 at 1.04.05 PM.png;https://qubole.atlassian.net/secure/attachment/45010/Screen+Shot+2017-06-30+at+1.04.05+PM.png,30/Jun/17 1:11 PM;snamburu;Screen Shot 2017-06-30 at 1.11.38 PM.png;https://qubole.atlassian.net/secure/attachment/45009/Screen+Shot+2017-06-30+at+1.11.38+PM.png,,,,,,,,,,,,AWS,,,,,None,sovrn,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z0574n:,,,,,,,,,,,,,,,,,,,1.0,15431,2017-07-07 07:09:20.93,,,"07/Jul/17 7:09 AM;sumitm;[~snamburu] I think the role they have given to us doesn't have access to bucket *sovrn-datascience*. For 2nd issue assigning to [~namratas] & [~tanishg].

{noformat}
irb(main):049:0> AwsCredsHelper::get_s3_endpoint('sovrn-qubole-wip-us-east-2', creds_map)
S3 client configured for ""us-east-1"" but the bucket ""sovrn-qubole-wip-us-east-2"" is in ""us-east-2""; Please configure the proper region to avoid multiple unecessary redirects and signing attempts
=> [""https://s3-us-east-2.amazonaws.com:443"", ""us-east-2""]

irb(main):050:0> AwsCredsHelper::get_s3_endpoint('sovrn-datascience', creds_map)
=> [nil, nil]
{noformat}

","07/Jul/17 10:43 AM;snamburu;[~sumitm] Regarding the 1st issue, we are able to access ""sovrn-datascience"" bucket from Analyze(command id: 80820069). This should mean the role has access to their bucket right..",10/Jul/17 1:19 PM;snamburu;[~sumitm] [~namratas] any updates on this?,"10/Jul/17 1:24 PM;drose@qubole.com;[~sumitm][~namratas] this is a new customer building out their architecture, is there a way to confirm quickly is this is a permissions issue on their end?","14/Jul/17 8:59 AM;drose@qubole.com;[~namratas][~sumitm] We are getting pressure from the customer after sharing the feedback on the JIRA last week, can you assist based on last comment? [~adubey]
","16/Jul/17 10:46 PM;tanishg;Will try to look into it by the end of the day.
[~rgupta]","28/Jul/17 10:26 AM;drose@qubole.com;[~tanishg] Do we have an update on this. ""Will try to look into it by the end of the day from 7/16/17."" [~namratas]","01/Aug/17 1:58 PM;drose@qubole.com;[~sureshr][~ravil] [~namratas] the team need to get this resolved, can we please provide an udpate this is a new customer that is getting frustrated.","01/Aug/17 11:45 PM;tanishg;Looked into it:
[~drose@qubole.com] Can you ask customer if they have relevant permission to access bucket. I think they have given list permission to sovrn-datascience bucket but not other relevant permissions as mentioned on this post.
http://docs.qubole.com/en/latest/faqs/general-questions/policy-use-qubole-use-my-iam-credentials.html#sample-policy-for-iam-roles


{code:java}

irb(main):021:0> bucket = ""sovrn-datascience""
=> ""sovrn-datascience""
irb(main):022:0> bucket = s3.buckets[""sovrn-datascience""]
=> #<AWS::S3::Bucket:sovrn-datascience>
irb(main):023:0> location = bucket.location_constraint
AWS::S3::Errors::AccessDenied: Access Denied
	from /usr/lib/ruby/gems/2.1.0/gems/aws-sdk-1.40.3/lib/aws/core/client.rb:375:in `return_or_raise'
	from /usr/lib/ruby/gems/2.1.0/gems/aws-sdk-1.40.3/lib/aws/core/client.rb:476:in `client_request'
	from (eval):3:in `get_bucket_location'
	from /usr/lib/ruby/gems/2.1.0/gems/aws-sdk-1.40.3/lib/aws/s3/bucket.rb:258:in `location_constraint'
	from (irb):23
	from /usr/lib/ruby/gems/2.1.0/gems/railties-3.2.17/lib/rails/commands/console.rb:47:in `start'
	from /usr/lib/ruby/gems/2.1.0/gems/railties-3.2.17/lib/rails/commands/console.rb:8:in `start'
	from /usr/lib/ruby/gems/2.1.0/gems/railties-3.2.17/lib/rails/commands.rb:41:in `<top (required)>'
	from script/rails:6:in `require'
	from script/rails:6:in `<main>'
{code}


Will look into the second case why they are not able to read data from explore
","02/Aug/17 3:44 AM;tanishg;For 2nd case why user is not able to read data from explore :

As us-east-2 is in v4 region which doesn't support get sample data command .Tracking jira for this is 
https://qubole.atlassian.net/browse/HADTWO-905.","03/Aug/17 12:30 PM;snamburu;[~tanishg] They added all the permission settings mentioned in our doc and here is the current policy setting for ""QuboleOneRole"" IAM role :
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:DeleteObject"",
        ""s3:GetObject"",
        ""s3:GetObjectAcl"",
        ""s3:PutObject"",
        ""s3:PutObjectAcl"",
        ""s3:GetBucketAcl"",
        ""s3:GetBucketLocation"",
        ""s3:ListBucket""
      ],
      ""Resource"": [
        ""arn:aws:s3:::sovrn-qubole-wip-us-east-2/*"",
        ""arn:aws:s3:::sovrn-qubole-wip-us-east-2"",
        ""arn:aws:s3:::com-sovrn-datalake-prod-ingest-us-east-2/*"",
        ""arn:aws:s3:::com-sovrn-datalake-prod-ingest-us-east-2"",
        ""arn:aws:s3:::com-sovrn-datalake-prod-content-us-east-2/*"",
        ""arn:aws:s3:::com-sovrn-datalake-prod-content-us-east-2"",
        ""arn:aws:s3:::com-sovrn-datalake-stage-ingest-us-east-2/*"",
        ""arn:aws:s3:::com-sovrn-datalake-stage-ingest-us-east-2"",
        ""arn:aws:s3:::sovrn-datascience/*"",
        ""arn:aws:s3:::sovrn-datascience""
      ]
    },
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": [
        ""*""
      ]
    },
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:ListAllMyBuckets""
      ],
      ""Resource"": [
        ""*""
      ]
    }
  ]
}

But even after this setting, they are unable to access the ""sovrn-datascience"" bucket from Explore. They are able to access the folder structure under other buckets mentioned in this policy settings.
Any specific AWS permissions you think that we need to verify?","03/Aug/17 9:24 PM;tanishg;We have resolved the issue for now. It was related to wrong location fetched while running s3cmd command. Tracking jira is 
https://qubole.atlassian.net/browse/MW-1125

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive 0.13 meta query in hive tier suppresses error and shows successful,AN-82,66814,Bug,Open,AN,Analyst,software,aswina,,,Major,,,navdeepp,navdeepp,30/Jun/17 8:30 AM,30/Jun/17 8:30 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Query: 81531151 (hive 0.13, hadoop1, hive tier)

Alter table recover partition for this table should fail as the data location is inaccessible for the account which throws correct error with hive1.2 (81436927), but with hive 0.13 marks it successful.

Hive tier logs:
{code}
2017-06-30 15:07:29,138 [command:81531151] INFO  metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:<init>(161)) - Ignoring the Mysql compatibility check
2017-06-30 15:07:30,956 [command:81531151] ERROR s3native.NativeS3FileSystem (NativeS3FileSystem.java:listStatus(1383)) - retrieveMetadata failed for : data/organization_id=101577 failed with error : org.jets3t.service.S3ServiceException: Request Error. HEAD '/data%2Forganization_id%3D101577' on Host 'mm-prod-platform-attributed-events.s3.amazonaws.com' @ 'Fri, 30 Jun 2017 15:07:30 GMT' -- ResponseCode: 403, ResponseStatus: Forbidden
2017-06-30 15:07:31,724 [command:81531151] INFO  exec.Task (SessionState.java:printInfo(659)) - Recovered 0 partitions
2017-06-30 15:07:31,725 [command:81531151] INFO  exec.Task (SessionState.java:printInfo(659)) - Removed 0 phantom partitions
{code}

This behaviour seems inconsistent as the query should fail as in hive1.2",,navdeepp,nimitk,psrinivas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z0573j:,,,,,,,,,,,,,,,,,,,1.0,15533,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hustler is giving up on cluster before RM, NM processes come up",ACM-1307,66759,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,Jtrail,sbadam,sbadam,29/Jun/17 8:53 PM,26/Jul/17 1:44 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Hustler is giving up on cluster before RM, NM processes come up

First set of commands - 80696624, 80696658, 80696535, 80696729
Cluster ID, Inst - 23781, 518607


{code:java}
s3://tdcprod-logs/qubole/turner-tdc-etl/logs/hadoop/23781/518607/
{code}



Another set of commands - 80993113, 80993139, 80993130, 80993128
Cluster ID, Inst - 29087, 520822

Logs are here, Account ID: 6184

{code:java}
s3://tdcprod-logs/qubole/turner-tdc-etl/logs/hadoop/29807/520822/
{code}

We are seeing errors before RM, NM comes up online on cluster.

{code:java}
...
ERROR - Cluster not in working condition, giving up
...
java.io.IOException: Failed to run job : Application application_1498437705259_0020 failed 2 times due to Error launching appattempt_1498437705259_0020_000002. Got exception: org.apache.hadoop.yarn.exceptions.NMNotYetReadyException: Rejecting new containers as NodeManager has not yet connected with ResourceManager
{code}


In general, do we wait for RM/ NM processes to be up and running before we schedule any job on cluster?
",api.qubole.com,sbadam,somyak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,turner,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z056wv:,,,,,,,,,,,,,,,,,,,1.0,15430,2017-06-30 04:47:23.714,,,"30/Jun/17 4:47 AM;somyak;Looks like the command was submitted to one of the nodes and the containers were initialized. However, the nodemanager was shutdown because of an exception - 


{code:java}
017-06-26 00:43:48,705 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
com.qubole.com.amazonaws.AmazonClientException: Failed to parse XML document with handler class com.qubole.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListBucketHandler
        at com.qubole.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:127)
        at com.qubole.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseListBucketObjectsResponse(XmlResponsesSaxParser.java:270)
        at com.qubole.com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsUnmarshaller.unmarshall(Unmarshallers.java:67)
        at com.qubole.com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsUnmarshaller.unmarshall(Unmarshallers.java:57)
        at com.qubole.com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62)
        at com.qubole.com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31)
        at com.qubole.com.amazonaws.http.AmazonHttpClient.handleResponse(AmazonHttpClient.java:1256)
        at com.qubole.com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:870)
        at com.qubole.com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:607)
        at com.qubole.com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:376)
        at com.qubole.com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:338)
        at com.qubole.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:287)
        at com.qubole.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3826)
        at com.qubole.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3778)
        at com.qubole.com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:610)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1178)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:1046)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.verifyAndCreateRemoteLogDir(LogAggregationService.java:216)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:321)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:458)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:67)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 5; XML document structures must start and end within the same entity.
        at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)
        at org.apache.xerces.util.ErrorHandlerWrapper.fatalError(Unknown Source)
        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)
        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)
        at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)
        at org.apache.xerces.impl.XMLScanner.reportFatalError(Unknown Source)
        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.endEntity(Unknown Source)
        at org.apache.xerces.impl.XMLDocumentScannerImpl.endEntity(Unknown Source)
        at org.apache.xerces.impl.XMLEntityManager.endEntity(Unknown Source)
        at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)
        at org.apache.xerces.impl.XMLEntityScanner.skipSpaces(Unknown Source)
        at org.apache.xerces.impl.XMLScanner.scanPIData(Unknown Source)
        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanPIData(Unknown Source)
        at org.apache.xerces.impl.XMLScanner.scanPI(Unknown Source)
        at org.apache.xerces.impl.XMLDocumentScannerImpl$PrologDispatcher.dispatch(Unknown Source)
        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
        at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
        at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
        at com.qubole.com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:114)
        ... 23 more
2017-06-26 00:43:48,708 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
2017-06-26 00:43:48,813 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042
2017-06-26 00:43:48,914 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Applications still running : [application_1498437705259_0003, 
{code}



 [~rohang] Relevant logs can be found in - s3://tdcprod-logs/qubole/turner-tdc-etl/logs/hadoop/23781/518607/ec2-52-90-202-21.compute-1.amazonaws.com.node0001/yarn/yarn-yarn-nodemanager-ip-10-65-4-82.log.2017062600.log.gz
","03/Jul/17 11:46 AM;sbadam;For this particular instance(520822), there are only two nodes(master, 1 slave) are up and running at the start. But I don't see any fatal messages in master and slave before we print ""giving up"" messages on UI. Are we seeing any problems in Tunnel connections before we log those errors? 

{code:java}
production-replica> select * from cluster_nodes where cluster_inst_id=520822;
+------------+---------------------+--------------------------------------------+----------+----------------------+---------------+---------------------+---------------------+---------------------+------------+-----------------------------+-----------------+--------+--------------------+---------------------+
| id         | ec2_instance_id     | hostname                                   | role     | amazon_instance_type | spot_instance | up_time             | down_time           | last_seen_time      | account_id | private_ip                  | cluster_inst_id | status | private_ip_address | termination_reason  |
+------------+---------------------+--------------------------------------------+----------+----------------------+---------------+---------------------+---------------------+---------------------+------------+-----------------------------+-----------------+--------+--------------------+---------------------+
| 1623506286 | i-0a844d8c8eee591fa | ec2-54-242-208-151.compute-1.amazonaws.com | master   | r4.8xlarge           |             0 | 2017-06-27 17:20:36 | 2017-06-27 23:53:33 | 2017-06-27 23:53:33 |       6184 | ip-10-65-1-50.ec2.internal  |          520822 | NULL   | 10.65.1.50         | Cluster Terminated. |
| 1623506288 | i-084a1623f12061ddf | ec2-54-90-219-85.compute-1.amazonaws.com   | node0001 | r4.8xlarge           |             0 | 2017-06-27 17:20:54 | 2017-06-27 23:53:33 | 2017-06-27 23:53:33 |       6184 | ip-10-65-6-45.ec2.internal  |          520822 | NULL   | 10.65.6.45         | Cluster Terminated. |
{code}

Locations to check for:

{code:java}
Master - s3://tdcprod-logs/qubole/turner-tdc-etl/logs/hadoop/29807/520822/ec2-54-242-208-151.compute-1.amazonaws.com.master/yarn/yarn-yarn-resourcemanager-ip-10-65-1-50.log.2017062717.log.gz
Slave - s3://tdcprod-logs/qubole/turner-tdc-etl/logs/hadoop/29807/520822/ec2-54-90-219-85.compute-1.amazonaws.com.node0001/yarn/yarn-yarn-nodemanager-ip-10-65-6-45.log.2017062717.log.gz
{code}



","03/Jul/17 11:55 AM;sbadam;Tunnel server logs show fine for this instance(520822).

{code:java}
2017-06-27 17:20:35,108 INFO  Thread-32529522 init_tunnel_server.py:256 - _get_port_ppid - [acc_id=29807, cluster=qbol_acc6184_cl29807]: Parent process using port (24965) = 12851
2017-06-27 17:20:35,108 INFO  Thread-32529522 init_tunnel_server.py:340 - _validate_port_owner - [acc_id=29807, cluster=qbol_acc6184_cl29807]: Validation PASSED
2017-06-27 17:20:35,108 INFO  Thread-32529522 init_tunnel_server.py:639 - _release_lock - [acc_id=29807, cluster=qbol_acc6184_cl29807]: Releasing lock on /media/ephemeral0/logs/tunneling/29807/qbol_acc6184_cl29807/ip-10-171-122-249/dynamic/lock
{code}

",03/Jul/17 5:24 PM;sbadam;I have asked customer if they have been seeing this issue consistently. I keep it posted here.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hotels.com tracker,SOL-202,66749,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,megha,megha,megha,29/Jun/17 11:50 AM,29/Jun/17 11:56 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"This is to track configs, issues for hotels.com, which can help in debugging issues in future",,gmargabanthu,megha,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z056un:,,,,,,,,,,,,,,,,,,,1.0,15506,,,,"29/Jun/17 11:56 AM;megha;Hotels.com has an external whitelisting service, which controls outbound access to various services including ec2.amazonaws.com

Summarizing https://qubole.zendesk.com/agent/tickets/15506, which occured due to failure of this service:
Clusters were failing to start, and for already running clusters, we were unable to push nodes.

For start issue, the nodes were coming up fine but namenode was failing to start. As per /media/ephemeral0/logs/others/hustler_user_data.log, two error symptoms were:

{code:java}
Attempt 5 to create tmp directory...
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/06/29 15:05:41 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
mkdir: Invalid host name: local host is: (unknown); destination host is: ""FAIL-FAST-IF-THIS-IS-NOT-REPLACED"":9000; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
{code}

{code:java}
++++(/usr/lib/hustler/bin/lib/publish_event.sh:20): publish_event():  2017-06-29 15:03:35 logger -t hustler-udf-event -s
Traceback (most recent call last):
  File ""/usr/lib/hustler/lib/py/hadoop_setup/node_init_using_master_instid.py"", line 549, in <module>
    main()
  File ""/usr/lib/hustler/lib/py/hadoop_setup/node_init_using_master_instid.py"", line 340, in main
    ec2 = utils.get_ec2_connection()
  File ""/usr/lib/hustler/lib/py/hadoop_setup/utils.py"", line 22, in get_ec2_connection
    myregion = ec2.get_all_regions([region])[0]
  File ""/usr/local/lib/python2.6/site-packages/boto/ec2/connection.py"", line 3494, in get_all_regions
    [('item', RegionInfo)], verb='POST')
  File ""/usr/local/lib/python2.6/site-packages/boto/connection.py"", line 1170, in get_list
    response = self.make_request(action, params, path, verb)
  File ""/usr/local/lib/python2.6/site-packages/boto/connection.py"", line 1116, in make_request
    return self._mexe(http_request)
  File ""/usr/local/lib/python2.6/site-packages/boto/connection.py"", line 1030, in _mexe
    raise ex
error: [Errno 111] Connection refused
{code}

from master node, following call was failing:

{code:java}
root@ip-10-27-149-41 others]# ping ec2.amazonaws.com
PING ec2.amazonaws.com (54.239.28.168) 56(84) bytes of data.
{code}

For the running clusters in this account, when tried to push a new config: hustler log show following error:
{code:java}
PID: 84762  2017-06-29 15:25:21,105 ERROR  cli.py:257 - main - Unable to connect: [Errno 111] Connection refused
PID: 84762  2017-06-29 15:25:21,105 ERROR  cli.py:258 - main - Check your internet connection?
{code}


Basically while starting the cluster we get some information from aws api, and those calls were failing, hence namenode couldnt start up.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyze page goes unresponsive and crashes,UI-6312,66710,Bug,Open,UI,UI,software,aswina,,,Major,,aswina,satyavathib,satyavathib,29/Jun/17 2:09 AM,21/Jul/17 12:44 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Commands : 

https://api.qubole.com/v2/analyze?command_id=80891769 
https://api.qubole.com/v2/analyze?command_id=80722072 
https://api.qubole.com/v2/analyze?command_id=80566617

When we open the following commands , The browser totally goes unresponsive and crashes. 

Please Let me know If any further details required on this.

Unable to even INSPECT the page to find the possible reason.",,aswina,gmargabanthu,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z056lz:,,,,,,,,,,,,,,,,,,,1.0,15465,2017-06-29 04:02:29.769,,,29/Jun/17 4:02 AM;aswina;I'm able to consistently reproduce this issue.  I'll get back with actionable items on this tomorrow.,17/Jul/17 3:33 PM;gmargabanthu;Hi [~aswina] - Any updates on this one? Can we please have an ETA. Thanks!,21/Jul/17 12:44 AM;aswina;Sorry [~gmargabanthu].  I couldn't get back to this earlier.  I'll check this next week and get back.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saml login logs missing on log server?,MW-1148,66586,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sumitm,megha,megha,28/Jun/17 4:52 PM,28/Jun/17 4:52 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"I'm trying to get logs for saml on the log server, but I'm not finding it
I'm looking in /media/ebs/logs/production/webapp/controller , /media/ebs/logs/production/webapp/nginx ,/media/ebs/logs/production/web/controller,
/media/ebs/logs/production/web/nginx

I'm looking for logs for 'syang@lyft.com'

Attached is the screenshot from time when the user tried to login.. i.e timestamp is 2017-06-27 03:05 PST



I was able to find the logs a few days back, for the same user (in /media/ebs/logs/production/webapp/controller) but that was after web node that got the request went down..  I also want the access.log for this request, which I couldn't find within following directory.. i.e access.log files are present, but didnt have any information about 'syang@lyft.com' /media/ebs/logs/production/webapp/nginx

Do we backup logs to log server only after webapp tier node goes down? If so , is there a way to identify which webapp tier got the request?",,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Jun/17 4:47 PM;megha;Pasted image at 2017_06_27 03_05 PM.png;https://qubole.atlassian.net/secure/attachment/44866/Pasted+image+at+2017_06_27+03_05+PM.png,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z056br:,,,,,,,,,,,,,,,,,,,1.0,14839,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cmd-id on analyse page is in running state for 3hrs while hadoop job has completed,UI-6307,66478,Bug,Open,UI,UI,software,aswina,,,Major,,aswina,biswajit,biswajit,27/Jun/17 9:24 PM,27/Jun/17 9:25 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"cmd-id :- 81042516
Last Hadoop job for the cmd-id succeeded:- job_1498246817607_5403
The cmd-id started at 2017-06-28 00:10:32
The last Hadoop job completed at 2017-06-28 00:32:07



Please do let me know if you need any more details. ",,biswajit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z055o7:,,,,,,,,,,,,,,,,,,,1.0,15479,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Misleading spark log: ""conf 'spark.qubole.queryhist.fileprefix' is not passed hence not able to log error msg!""",SPAR-1741,66447,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,megha,megha,27/Jun/17 10:47 AM,27/Jun/17 10:48 AM,09/Aug/17 5:29 AM,,,,,0,jira_escalated,,,,,,,,"I saw many commands that had different failure reasons.. All these have a common log message: 
""App > 17/06/25 15:11:03 main INFO Utils: conf 'spark.qubole.queryhist.fileprefix' is not passed hence not able to log error msg!""

Is this a harmless error message? Or is it indeed not able to log error, and as a result, we can't see the actual error? 

Sample command ids: 80641645 (customer: expedia, spark-2.0 latest), 79096669(customer: gannet,spark 2.0.0) , 80982556(customer: gannet, spark 2.0.0)


",,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,gannett,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z055hr:,,,,,,,,,,,,,,,,,,,1.0,15469,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""NoSuchMethodError: copyPrivateRequestParameters: After feature rollout today",SPAR-1740,66434,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,puneetg,navdeepp,navdeepp,27/Jun/17 6:24 AM,10/Jul/17 8:24 AM,09/Aug/17 5:29 AM,,,,,0,jira_escalated,,,,,,,,"Failing commands:

80907258(schedule: 17869),
80936590(19982),
80936618(16978)

Stacktrace,

{code}
App > Exception in thread ""main"" java.lang.NoSuchMethodError: com.amazonaws.AmazonWebServiceRequest.copyPrivateRequestParameters()Ljava/util/Map;
App > at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3653)
App > at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3623)
App > at com.amazonaws.services.s3.AmazonS3Client.getBucketLifecycleConfiguration(AmazonS3Client.java:1930)
App > at com.databricks.spark.redshift.Utils$.checkThatBucketHasObjectLifecycleConfiguration(Utils.scala:127)
App > at com.databricks.spark.redshift.RedshiftWriter.saveToRedshift(RedshiftWriter.scala:343)
App > at com.databricks.spark.redshift.DefaultSource.createRelation(DefaultSource.scala:106)
App > at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:429)
App > at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)
App > at com.poshmark.spark.helpers.Redshift$.upsertDfV2(Redshift.scala:146)
App > at com.poshmark.spark.etl.amicus.DressingRoomHourlyKPI$.doJob(DressingRoomHourlyKPI.scala:199)
App > at com.poshmark.spark.etl.ETLJob$$anonfun$main$1.apply$mcVI$sp(ETLJob.scala:63)
App > at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
{code}

This seems to be related to the feature roll done today for poshmark(4126)

Feature: spark.list_packages_on_s3

https://qubole.atlassian.net/wiki/display/RM/Feature+Rollout+Notes#FeatureRolloutNotes-spark.list_packages_on_s3 

The method ""copyPrivateRequestParameters"" is removed for the AWS sdk versions 1.10.x, 

Any suggestions to unblock the customer?",,bharatb,hiyer,navdeepp,puneetg,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z055ev:,,,,,,,,,,,,,,,,,,,1.0,15459,2017-06-27 07:42:52.012,,,"27/Jun/17 7:42 AM;bharatb;{code}
[ec2-user@ip-10-0-0-56 ~]$ hdfs dfs -get s3://production-dwh/qubole-default/logs/hadoop/30531/500389/*master/others/hustler_user_data.log
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/06/27 14:29:33 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[ec2-user@ip-10-0-0-56 ~]$ less hustler_user_data.log
[ec2-user@ip-10-0-0-56 ~]$ grep PACKAGE_ID hustler_user_data.log
+++(/usr/lib/hustler/bin/uncompressed_hadoop_node_init.sh:369): configure_hadoop():  2017-06-11 08:aws_props[69]=SPARK_S3_PACKAGE_ID=00002-0.13483170.1.tar.gz
{code}

I am seeing that even on 11th june 2017, the same package is being used. So I am not sure why anything is failing now.

Looking at account_features, it looks like it was enabled long ago.

{code}
production-replica> select * from account_features where account_id = 4126 and feature_name like '%spark.list%';
+-------+------------+---------+---------------------------+------------+---------------------+---------------------+--------------+
| id    | account_id | user_id | feature_name              | is_enabled | created_at          | updated_at          | segment_name |
+-------+------------+---------+---------------------------+------------+---------------------+---------------------+--------------+
| 14498 |       4126 |    NULL | spark.list_packages_on_s3 |          1 | 2016-08-10 11:04:59 | 2016-08-10 11:04:59 | NULL         |
+-------+------------+---------+---------------------------+------------+---------------------+---------------------+--------------+
{code}

Also checking the overall flag:-

{code}
production-replica> select * from account_features where feature_name = 'spark.list_packages_on_s3';
+-------+------------+---------+---------------------------+------------+---------------------+---------------------+--------------+
| id    | account_id | user_id | feature_name              | is_enabled | created_at          | updated_at          | segment_name |
+-------+------------+---------+---------------------------+------------+---------------------+---------------------+--------------+
| 14361 |       3874 |    NULL | spark.list_packages_on_s3 |          1 | 2016-07-27 22:25:46 | 2016-07-27 22:25:46 | NULL         |
| 14362 |       5586 |    NULL | spark.list_packages_on_s3 |          1 | 2016-07-27 22:51:31 | 2016-07-27 22:51:31 | NULL         |
| 14363 |       4204 |    NULL | spark.list_packages_on_s3 |          1 | 2016-07-28 00:16:59 | 2016-07-28 00:16:59 | NULL         |
| 14373 |       2336 |    NULL | spark.list_packages_on_s3 |          1 | 2016-07-29 10:03:33 | 2016-07-29 10:03:33 | NULL         |
| 14383 |       3902 |    NULL | spark.list_packages_on_s3 |          1 | 2016-07-29 21:25:04 | 2016-07-29 21:25:04 | NULL         |
| 14387 |       5616 |    NULL | spark.list_packages_on_s3 |          1 | 2016-07-29 22:11:31 | 2016-07-29 22:11:31 | NULL         |
| 14486 |       5269 |    NULL | spark.list_packages_on_s3 |          1 | 2016-08-09 01:52:19 | 2016-08-09 01:52:19 | NULL         |
| 14489 |       4885 |    NULL | spark.list_packages_on_s3 |          1 | 2016-08-09 08:03:28 | 2016-08-09 08:03:28 | NULL         |
| 14497 |       3960 |    NULL | spark.list_packages_on_s3 |          1 | 2016-08-10 10:08:04 | 2016-08-10 10:08:04 | NULL         |
| 14498 |       4126 |    NULL | spark.list_packages_on_s3 |          1 | 2016-08-10 11:04:59 | 2016-08-10 11:04:59 | NULL         |
| 14505 |       5726 |    NULL | spark.list_packages_on_s3 |          1 | 2016-08-10 21:36:59 | 2016-08-10 21:36:59 | NULL         |
| 14507 |       4793 |    NULL | spark.list_packages_on_s3 |          1 | 2016-08-11 07:46:06 | 2016-08-11 07:46:06 | NULL         |
| 14551 |       5793 |    NULL | spark.list_packages_on_s3 |          1 | 2016-08-15 22:34:20 | 2016-08-15 22:34:20 | NULL         |
| 15984 |       5886 |    NULL | spark.list_packages_on_s3 |          1 | 2016-09-13 08:59:37 | 2016-09-13 08:59:37 | NULL         |
| 20915 |       6364 |    NULL | spark.list_packages_on_s3 |          1 | 2016-10-27 17:43:28 | 2016-10-27 17:43:28 | NULL         |
| 20949 |       6367 |    NULL | spark.list_packages_on_s3 |          1 | 2016-10-28 05:12:07 | 2016-10-28 05:12:07 | NULL         |
| 22389 |       6601 |    NULL | spark.list_packages_on_s3 |          1 | 2016-12-13 13:05:29 | 2016-12-13 13:05:29 | NULL         |
| 29632 |       7135 |    NULL | spark.list_packages_on_s3 |          1 | 2017-03-22 06:57:25 | 2017-03-22 06:57:25 | NULL         |
| 32307 |       7385 |    NULL | spark.list_packages_on_s3 |          1 | 2017-05-16 06:26:28 | 2017-05-16 06:26:28 | NULL         |
| 55845 |       NULL |    NULL | spark.list_packages_on_s3 |          1 | 2017-06-20 13:53:07 | 2017-06-20 13:53:07 | S1           |
| 55846 |       NULL |    NULL | spark.list_packages_on_s3 |          1 | 2017-06-20 13:53:17 | 2017-06-20 13:53:17 | S2           |
| 56231 |       NULL |    NULL | spark.list_packages_on_s3 |          1 | 2017-06-27 07:26:18 | 2017-06-27 07:26:18 | S3           |
+-------+------------+---------+---------------------------+------------+---------------------+---------------------+--------------+
22 rows in set (0.16 sec)
{code}

I am suspecting this is due to S3A enablement and not related to list_packages flag. cc [~hiyer]
","27/Jun/17 7:52 AM;hiyer;Looking at the error, it seems likely they're using some custom jar that includes a version of aws sdk that doesn't have this method.","27/Jun/17 7:57 AM;navdeepp;do not see ""hadoop2.enable_s3a_filesystem"" feature enabled for the account:
{code}
production-replica> select * from account_features where account_id=4126;
+-------+------------+---------+--------------------------------------+------------+---------------------+---------------------+--------------+
| id    | account_id | user_id | feature_name                         | is_enabled | created_at          | updated_at          | segment_name |
+-------+------------+---------+--------------------------------------+------------+---------------------+---------------------+--------------+
| 11509 |       4126 |    NULL | tapp.ui_enable_cpl_v2                |          1 | 2015-09-02 17:09:09 | 2015-09-02 17:09:09 | NULL         |
| 11510 |       4126 |    NULL | hadoop.aggressive_termination        |          1 | 2015-09-02 17:09:09 | 2015-09-02 17:09:09 | NULL         |
| 14498 |       4126 |    NULL | spark.list_packages_on_s3            |          1 | 2016-08-10 11:04:59 | 2016-08-10 11:04:59 | NULL         |
| 15228 |       4126 |    NULL | zeppelin.buffer_append_output        |          1 | 2016-09-08 06:10:18 | 2016-09-08 06:10:18 | NULL         |
| 15517 |       4126 |    NULL | tapp.ui_enable_node_bootstrap_editor |          1 | 2016-09-08 06:12:21 | 2016-09-08 06:12:21 | NULL         |
| 16487 |       4126 |    NULL | zeppelin.process_qlog                |          1 | 2016-09-15 06:27:36 | 2016-09-15 06:27:36 | NULL         |
| 16776 |       4126 |    NULL | cluster.encrypt_keys                 |          1 | 2016-09-15 06:28:54 | 2016-09-15 06:28:54 | NULL         |
| 16873 |       4126 |    NULL | tapp.enable_aws_roles                |          1 | 2016-09-15 06:30:19 | 2016-09-15 06:30:19 | NULL         |
| 17797 |       4126 |    NULL | hadoop2.use_username_as_ugi          |          1 | 2016-09-23 05:26:46 | 2016-09-23 05:26:46 | NULL         |
| 19931 |       4126 |    NULL | spark.qubole_split_computation       |          1 | 2016-09-29 06:44:32 | 2016-09-29 06:44:32 | NULL         |
| 20211 |       4126 |    NULL | spark.defaults_v2                    |          1 | 2016-09-29 06:46:03 | 2016-09-29 06:46:03 | NULL         |
| 26371 |       4126 |    NULL | tapp.enable_analyze_nav_away_warning |          1 | 2017-02-07 09:23:13 | 2017-02-07 09:23:13 | NULL         |
| 26789 |       4126 |    NULL | tapp.enable_customer_cluster_dd      |          1 | 2017-02-07 09:38:35 | 2017-02-07 09:38:35 | NULL         |
| 32053 |       4126 |    NULL | tapp.enable_virtual_foldering        |          1 | 2017-05-11 00:06:09 | 2017-05-11 00:06:09 | NULL         |
| 33823 |       4126 |    NULL | tapp.ui_enable_spark_apps_ui         |          1 | 2017-06-10 04:20:38 | 2017-06-10 04:20:38 | NULL         |
{code}",27/Jun/17 7:58 AM;navdeepp;customer has reported that they have not changed any bootstrp/job jars,27/Jun/17 7:59 AM;navdeepp;Is there any workaround that can help in this case,"27/Jun/17 8:03 AM;bharatb;{code}
[ec2-user@ip-10-0-0-56 hadoop]$ grep -irn s3a /usr/lib/hadoop2/etc/hadoop/*
/usr/lib/hadoop2/etc/hadoop/core-site.xml:51:    <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
/usr/lib/hadoop2/etc/hadoop/core-site.xml:56:    <value>org.apache.hadoop.fs.s3a.S3A</value>
/usr/lib/hadoop2/etc/hadoop/core-site.xml:60:    <value>org.apache.hadoop.fs.s3a.S3A</value>
/usr/lib/hadoop2/etc/hadoop/core-site.xml:124:    <name>fs.s3a.buffer.dir</name>
/usr/lib/hadoop2/etc/hadoop/core-site.xml:125:    <value>/media/ebs2/s3a</value>
/usr/lib/hadoop2/etc/hadoop/core-site.xml:147:    <name>fs.s3a.connection.maximum</name>
/usr/lib/hadoop2/etc/hadoop/core-site.xml:154:    <name>fs.s3a.connection.ssl.enabled</name>
/usr/lib/hadoop2/etc/hadoop/core-site.xml:169:    <name>fs.s3a.multipart.threshold</name>
{code}

So s3a is being used by them. And it seems to be there in hadoop overrides at cluster level. So that has also not changed.","27/Jun/17 8:57 AM;bharatb;The schedule called ""Hourly ETL"" (17869)  is the most instructive since it is hourly. It started failing at 27 Jun 2017 04:30 IST -- much before any feature was enabled today I think. It also ran successfully a couple of times but is mostly failing. So this probably nothing to do with out feature enablements and most likely related to their jar itself and its dependencies. 

One potential problem from customer's side could be https://github.com/awslabs/amazon-sqs-java-messaging-lib/issues/18. Basically they upgraded their aws sdk. ",27/Jun/17 9:17 AM;bharatb;Checked many of their jars and none of them seem to have been updated in the recent past.,"27/Jun/17 9:29 AM;bharatb;All jars with the relevant class.

{code}
/usr/lib/hadoop2/share/hadoop/tools/lib/hadoop-aws-2.6.0-qds-0.4.6-SNAPSHOT.jar
  com/qubole/com/amazonaws/AmazonWebServiceRequest.class
  com/qubole/com/amazonaws/AmazonWebServiceRequest$1.class

/usr/lib/hadoop2/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar
   com/amazonaws/AmazonWebServiceRequest.class

/usr/lib/spark/lib/streaming/spark-streaming-kinesis-assembly_2.11-2.1.0-SNAPSHOT.jar
  com/amazonaws/AmazonWebServiceRequest.class
  com/amazonaws/AmazonWebServiceRequest$1.class

/usr/lib/spark/lib/streaming/spark-streaming-kinesis-assembly_2.11-2.0.0.jar
  com/amazonaws/AmazonWebServiceRequest.class
  com/amazonaws/AmazonWebServiceRequest$1.class
{code}
","27/Jun/17 9:39 AM;bharatb;I have removed the following two jars from the slave nodes.

/usr/lib/spark/lib/streaming/spark-streaming-kinesis-assembly_2.11-2.1.0-SNAPSHOT.jar
/usr/lib/spark/lib/streaming/spark-streaming-kinesis-assembly_2.11-2.0.0.jar

Now the job seems to be running fine.","27/Jun/17 10:39 AM;bharatb;[~navdeepp] please make sure that all their relevant clusters have those two jars removed via their node bootstraps (and inform them that kinesis will not work).

We will think of an actual fix in code.",27/Jun/17 10:40 AM;bharatb;Strange thing is that it started failing just now. But nothing has really changed in the recent past (or we don't know what changed).,"27/Jun/17 11:40 AM;navdeepp;Suggested the workaround, will follow up further
thanks [~bharatb]",27/Jun/17 9:09 PM;rohitk;[~bharatb] The stack trace seems to suggest writes to Redshift. Does the Redshift jar packages has a dependency on latest version of AWS sdk? Can we downgrade the redshift package?,"03/Jul/17 12:26 AM;navdeepp;Customer is expecting a long term solution without the workaround, please suggest on the next steps","05/Jul/17 4:39 AM;puneetg;[~navdeepp]
I am trying to reproduce this issue in house. I have setup everything exactly same as them Still not able to reproduce this issue. Can we ask them for the code snippet, it will help.","05/Jul/17 8:26 AM;navdeepp;[~puneetg] The job is submitted from a compiled package, do we need the source code or package?","05/Jul/17 9:55 AM;puneetg;[~navdeepp], Source code would help better.",06/Jul/17 10:21 PM;puneetg;[~navdeepp] Any update from customer?,"07/Jul/17 11:26 AM;navdeepp;Customer response:
"" I am not sure about the code as such.
But you can take the jar from our bootstrap/analyze command. And run it on a cluster without the `rm` hack and the issue will be reproducible. ""

I have asked further permissions to rerun the same job in their ac as there can be data duplication.","10/Jul/17 8:24 AM;navdeepp;Customer has granted permission to run the following job if needed,

""You can run https://api.qubole.com/v2/analyze?command_id=82869209 against a cloned cluster.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cloning a workflow template failing,UI-6304,66429,Bug,Open,UI,UI,software,aswina,,,Major,,aswina,satyavathib,satyavathib,27/Jun/17 5:47 AM,29/Jun/17 11:36 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"When trying to clone a workflow template It is failing with the below exception:


Please Let me know If any details required.
",,aswina,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Jun/17 5:46 AM;satyavathib;Screen Shot 2017-06-27 at 6.15.42 PM.png;https://qubole.atlassian.net/secure/attachment/44814/Screen+Shot+2017-06-27+at+6.15.42+PM.png,,,,,,,,,,,,,AWS,,,,,None,blackbook,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z055dr:,,,,,,,,,,,,,,,,,,,1.0,15366,2017-06-29 23:29:37.914,,,29/Jun/17 11:29 PM;aswina;Can you give me the id of the command template that you are trying to clone?,"29/Jun/17 11:36 PM;satyavathib;Actually, It does not need a specific template. We could just create a workflow template and just add ""show tables"" and try to clone and that fails.

Just for reference :

Customer Template : 3038,3202

Internally Tested Template : 3216",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent result headers for Analyze when hive query does select with create table statements,HIVE-2329,66310,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,rvenkatesh,navdeepp,navdeepp,26/Jun/17 8:26 AM,10/Jul/17 8:57 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer query: 80321736

Reproduced in-house:

Command(expected results): 80801148
Bad headers: 80800843

Eg2: 80799184
Eg3: 80786547(should have 5 fields) [Although results are correctly written on s3] [https://api.qubole.com/api/v1.2/commands/80786547/status_with_results has correct values for results parameters]
",,aswina,navdeepp,nimitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jun/17 8:26 AM;navdeepp;Capture.JPG;https://qubole.atlassian.net/secure/attachment/44707/Capture.JPG,,,,,,,,,,,,,AWS,,,,,None,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z054uf:,,,,,,,,,,,,,,,,,,,1.0,15407,2017-07-10 08:57:02.627,,,"10/Jul/17 8:57 AM;aswina;[~navdeepp] - This looks like a bug in Hive.  We use `QBOL-QUERY-SCHEMA` that's present in qlog output to understand the columns to be displayed.  In case of 80800843, all 5 columns are returned.  But in case of 80801148, only 2 are returned.

Assigning this to [~rvenkatesh] for further triage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command status is failed even after it's MR job is successful,SOL-195,66266,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,sbadam,sbadam,25/Jun/17 7:25 PM,26/Jun/17 3:15 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command status is failed even after it's MR job is successful

https://box-prod-1.qubole.com/v2/analyze#, We don't have direct access to UI but I am capturing all required logs from their DFLOC.
Commands: 

https://box-prod-1.qubole.com/v2/analyze?command_id=37726
https://box-prod-1.qubole.com/v2/analyze?command_id=38059

For 37726:

Please access Hive JVM logs from this location(from box web node):

{code:java}
/usr/lib/hive_scripts/storagecli.py -a 8 -c=""-ls s3://dwh-qubole-prod/logs/hadoop/190/446/*.master/hive1.2/hive.log""
{code}

I have also downloaded on web node:

{code:java}
[ec2-user@ip-10-0-1-85 ~]$ ls -ltrh /tmp/37726_UI.log /tmp/37726.log
-rw-rw-r-- 1 ec2-user ec2-user  42K Jun 25 00:36 /tmp/37726_UI.log
-rw-rw-r-- 1 ec2-user ec2-user 570K Jun 25 00:45 /tmp/37726.log
[ec2-user@ip-10-0-1-85 ~]$
{code}

I checked both the files but there is no reason in logs 
",https://box-prod-1.qubole.com/v2/analyze#,Kulbir,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,box,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z054qv:,,,,,,,,,,,,,,,,,,,1.0,15377,2017-06-26 15:15:13.258,,,"26/Jun/17 2:49 PM;sbadam;38059:

UI logs are stored at: 

{code:java}
/usr/lib/hive_scripts/storagecli.py -a 8 -c=""-get s3://dwh-qubole-prod/tmp/2017-06-22/8/38059.err /tmp/38059_UI.log""
{code}

Hive JVM logs are here:

{code:java}
/usr/lib/hive_scripts/storagecli.py -a 8 -c=""-get s3://dwh-qubole-prod/logs/hadoop/190/452/*.master/hive1.2/hive.log /tmp/38059_hive.log""
{code}



","26/Jun/17 2:53 PM;sbadam;Error in hive log, but not they will lead to overall job failures.


{code:java}
[ec2-user@ip-10-0-1-85 ~]$ cat /tmp/38059_hive.log  | grep -i ""] error""
2017-06-22 10:08:29,669 [command:38388] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38388.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:08:45,436 [command:38388] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:08:56,757 [command:38389] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38389.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:09:12,542 [command:38389] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:09:31,516 [command:38388] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38388.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:09:46,989 [command:38388] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:10:14,945 [command:38389] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38389.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:10:30,418 [command:38389] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:10:39,445 [command:38388] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38388.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:10:55,045 [command:38388] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:11:30,871 [command:38389] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38389.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:11:46,418 [command:38389] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:15:49,373 [command:38389] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38389.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/007"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:15:54,984 [command:38388] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38388.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/007"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:16:04,814 [command:38389] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:16:10,488 [command:38388] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:19:02,740 [command:38388] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38388.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/009"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/007"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:19:18,459 [command:38388] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:21:18,996 [command:38388] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38388.dir/011"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/009"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38388.dir/007"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:21:37,898 [command:38388] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:27:19,500 [command:38389] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38389.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/009"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/007"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:27:34,945 [command:38389] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-22 10:32:39,162 [command:38389] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-22/8/38389.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/011"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/009"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/007"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}],""/tmp/2017-06-22/8/38389.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""_col0""},{""ColumnType"":""string"",""ColumnName"":""_col1""},{""ColumnType"":""string"",""ColumnName"":""_col2""},{""ColumnType"":""string"",""ColumnName"":""_col3""}]}}
2017-06-22 10:32:54,592 [command:38389] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
[ec2-user@ip-10-0-1-85 ~]$
{code}
","26/Jun/17 2:54 PM;sbadam;In another command(37726):

{code:java}
[ec2-user@ip-10-0-1-85 ~]$ cat /tmp/37726.log | grep -i ""] error""
2017-06-21 06:13:25,876 [command:37726] ERROR session.SessionState (SessionState.java:copyAtomicallyToSSCache(1720)) - Exception in copying file s3://dwh-qubole-prod/scripts/hive/8/13/bootstrap
2017-06-21 06:13:40,504 [command:37726] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-21/8/37726.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}]}}
2017-06-21 06:13:58,453 [command:37726] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-21 06:16:42,471 [command:37726] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-21/8/37726.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}]}}
2017-06-21 06:17:00,551 [command:37726] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-21 06:23:38,804 [command:37726] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-21/8/37726.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}]}}
2017-06-21 06:23:54,859 [command:37726] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-21 06:26:31,631 [command:37726] ERROR session.SessionState (SessionState.java:copyAtomicallyToSSCache(1720)) - Exception in copying file s3://dwh-qubole-prod/scripts/hive/8/13/bootstrap
2017-06-21 06:26:39,284 [command:37726] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-21/8/37726.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}]}}
2017-06-21 06:26:55,812 [command:37726] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-21 06:29:30,255 [command:37726] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-21/8/37726.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}]}}
2017-06-21 06:29:46,397 [command:37726] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-21 06:31:24,890 [command:37726] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-21/8/37726.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}]}}
2017-06-21 06:31:41,433 [command:37726] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
2017-06-21 06:33:16,422 [command:37726] ERROR SessionState (SessionState.java:printError(1337)) - {""QBOL-QUERY-SCHEMA"":{""/tmp/2017-06-21/8/37726.dir/001"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/003"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/005"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}],""/tmp/2017-06-21/8/37726.dir/007"":[{""ColumnType"":""string"",""ColumnName"":""event_guid""},{""ColumnType"":""int"",""ColumnName"":""_c1""}]}}
2017-06-21 06:33:32,970 [command:37726] ERROR mr.ExecDriver (ExecDriver.java:execute(409)) - yarn
[ec2-user@ip-10-0-1-85 ~]$
{code}

","26/Jun/17 3:15 PM;Kulbir;Slack chat:
{code:java}
sbadam 
[2:55 PM] 
Hi Kulbir..

[2:55] 
can you help me on - https://qubole.atlassian.net/browse/SOL-195 ?

[2:55] 
I am not able to find exact reason for command failure..

kulbir
[3:07 PM] 
hey

[3:08] 
most likely it's same reason as 15378

[3:08] 
if u don't find anything obvious in hive logs plus web tier logs

[3:09] 
on master node you can check for system logs

[3:09] 
`messages` and see if kernel killed any java processes due to system OOM during the time when this command was running

sbadam 
[3:10 PM] 
at /var/log/messages ?

kulbir
[3:10 PM] 
Box has modified this cluster `1_parity` to use bigger master node type and reduce concurrent commands to handle such failures

[3:10] 
yes

[3:12] 
I will check with end user if they have seen such issues since then

[3:13] 
meanwhile anything in controller\hivecli logs indicating any sort of client failure ?
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User is not receiving Scheduler notifications(emails) for failed runs,SCHED-150,66261,Bug,Open,SCHED,Scheduler,software,sumitm,,,Major,,ksr,sbadam,sbadam,25/Jun/17 6:35 PM,10/Jul/17 9:23 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"User is not receiving Scheduler notifications(emails) for failed runs

I tested it in my local account, I didn't receive any email too.
One user from Optimizely informed about this error. I tested in their account and gave my email id to receive notifications upon failure, I have also observed no email is received upon failure.

One Command ID: 

{code:java}
https://api.qubole.com/super_admin/query_hists/80698230/redirect 
https://api.qubole.com/v2/scheduler#!/summary/20126?status=Killed&user_id=All&type=All&id=&name=
{code}

",api.qubole.com,sbadam,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jun/17 12:07 AM;sumitm;Screen Shot 2017-06-26 at 12.34.39 PM.png;https://qubole.atlassian.net/secure/attachment/44620/Screen+Shot+2017-06-26+at+12.34.39+PM.png,,,,,,,,,,,,,AWS,,,,,None,optimizely,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z054pr:,,,,,,,,,,,,,,,,,,,1.0,15335,2017-06-26 00:07:32.401,,,"26/Jun/17 12:07 AM;sumitm;[~sbadam] I also scheduled a job in Scheduler and its sending emails to me. See attached screen shot of such mail. 

!Screen Shot 2017-06-26 at 12.34.39 PM.png|thumbnail! 

","30/Jun/17 3:28 PM;sbadam;Thanks Sumit, I have seen it is happening for few commands(not all). Can we check any logs on Scheduler node for sending email notifications? ","03/Jul/17 10:02 AM;sbadam;Customer comments:


{code:java}
just got email for failiure from https://api.qubole.com/v2/analyze?command_id=81534539

im curious why it doesnt work for the other commands i posted earlier - is it the type of error that is not allowing the email to be sent?

can you confirm from any logs on your side that the email was indeed sent to dw_eng@optimizely.com
{code}
",03/Jul/17 5:18 PM;sbadam;I have asked customer for command id if they see this issue again. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encountered java.lang.ArrayIndexOutOfBoundsException when uncompressing an ORC table,SOL-193,66260,Bug,To Do,SOL,Solutions,software,Jtrail,,,Critical,,Jtrail,sbadam,sbadam,25/Jun/17 5:38 PM,25/Jun/17 6:44 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Encountered java.lang.ArrayIndexOutOfBoundsException when uncompressing an ORC table

Command: https://api.qubole.com/super_admin/query_hists/80157834/redirect

There is problem in uncompressing ORC data. It is running into ArrayIndexOutOfBoundsException when processing an input table.

{code:java}
17/06/26 00:21:27 Executor task launch worker-7 ERROR Executor: Exception in task 18.0 in stage 11.0 (TID 4405)
java.lang.ArrayIndexOutOfBoundsException: 262144
	at org.iq80.snappy.SnappyDecompressor.incrementalCopy(SnappyDecompressor.java:332)
	at org.iq80.snappy.SnappyDecompressor.decompressAllTags(SnappyDecompressor.java:177)
	at org.iq80.snappy.SnappyDecompressor.uncompress(SnappyDecompressor.java:76)
	at org.iq80.snappy.Snappy.uncompress(Snappy.java:43)
	at org.apache.hadoop.hive.ql.io.orc.SnappyCodec.decompress(SnappyCodec.java:71)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.readHeader(InStream.java:216)
	at org.apache.hadoop.hive.ql.io.orc.InStream$CompressedStream.available(InStream.java:253)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringDictionaryTreeReader.readDictionaryStream(TreeReaderFactory.java:1684)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringDictionaryTreeReader.startStripe(TreeReaderFactory.java:1649)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StringTreeReader.startStripe(TreeReaderFactory.java:1382)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$MapTreeReader.startStripe(TreeReaderFactory.java:2307)
	at org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory$StructTreeReader.startStripe(TreeReaderFactory.java:2040)
....
{code}


What I tried so far:

1. I don't see a proper reason in stack trace for the failure. 
2. I searched relevant flags with 262144 as the values because that's the OutofIndex. I found few configs from here:

{code:java}
https://api.qubole.com/cluster-proxy?clusterInst=518523&encodedUrl=http%3A%2F%2F10.23.4.205%3A8042%2Fconf
{code}

tfile.fs.input.buffer.size, tfile.fs.output.buffer.size. As spark is using Hive 0.13, I searched it's default template and found that hive.exec.orc.default.buffer.size also has 262144.

3. I found these links useful but they didn't help to solve the problem:

{code:java}
https://stackoverflow.com/questions/41830831/hive-snappy-uncompressed-length-must-be-less/41873255
https://issues.apache.org/jira/browse/SPARK-16334 
{code}

",api.qubole.com,mstolpner,psrinivas,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z054pj:,,,,,,,,,,,,,,,,,,,1.0,15394,,,,"25/Jun/17 5:50 PM;sbadam;We have permission to re-run queries. I tried to run multiple times with different config but they didn't help either:

80670332, 80686312, 80690699

cc: [~gmargabanthu], [~mstolpner]","25/Jun/17 6:14 PM;sbadam;I tried changing hive.exec.orc.default.buffer.size to 524288 and turned on DEBUG logs at log4j.rootCategory at 

{code:java}
/usr/lib/spark/conf/log4j.properties, /usr/lib/spark/conf/log4j.properties.qubole
{code}
on cluster master.

But I didn't get debug logs in driver and all executors to understand the issue because job was run on a random slave node. Not sure how I can set logs in this case.

Latest command - https://api.qubole.com/v2/analyze?command_id=80695232

Failure in one executor:

{code:java}
https://api.qubole.com/cluster-proxy?clusterInst=518523&encodedUrl=http%3A%2F%2Fec2-54-161-77-15.compute-1.amazonaws.com%3A19888%2Fws%2Fv1%2Fhistory%2Flogs%2F10.23.5.53%3A45454%2Fcontainer_1498430598063_0008_01_000101%2Fapplication_1498430598063_0008%2Fs-qubole-ede-tdr%2Fstderr
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Successful pig query shows a false alarm ( FNF ),QPIG-71,65845,Bug,Open,QPIG,qbol pig,software,bharatb,,,Major,,sureshr,adubey,adubey,20/Jun/17 10:39 AM,20/Jun/17 11:58 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"example:68009246

This causes lot of confusion to user where they report this error as the query failure reason but in reality the root cause will be different 

",,adubey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jun/17 10:38 AM;adubey;Screen Shot 2017-06-20 at 10.37.52 AM.png;https://qubole.atlassian.net/secure/attachment/44307/Screen+Shot+2017-06-20+at+10.37.52+AM.png,,,,,,,,,,,,,AWS,,,,,None,optimizely,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z052uv:,,,,,,,,,,,,,,,,,,,1.0,15333,,,,20/Jun/17 10:43 AM;adubey;[~sureshr] can you reassign to someone who may be working on this ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clear datadog alert when node goes down,QBOL-6176,65836,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,sumitm,megha,megha,20/Jun/17 8:53 AM,20/Jun/17 8:53 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Is there a way to clear datadog alert when node goes down due to spot loss, or downscaling..
",,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z052sv:,,,,,,,,,,,,,,,,,,,1.0,15276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWS::S3::Errors::AuthorizationHeaderMalformed encountered while writing to S3 location,MW-1114,65834,Bug,Open,MW,Middleware,software,sumitm,,,Major,,tanishg,mdaurangzeb,mdaurangzeb,20/Jun/17 8:40 AM,22/Jun/17 11:58 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer cloned one of their accounts 6729->7567. Now they are trying to update default S3 bucket, but getting ""AWS::S3::Errors::AuthorizationHeaderMalformed encountered while writing to S3 location"".

It looks like they are hitting this https://qubole.atlassian.net/browse/MW-990.

Access mode is IAM role.

Role ARN : arn:aws:iam::988204379249:role/QuoboleServiceRole
def location : expedia-risk-modeling-qubole-dev/account_id/7567

The QuboleServiceRole has AmazonS3FullAccess, bucket doesn't have any restrictions either. See attached. The same role is successfully used in another qubole account.","Customer : Expedia
Account id :7567
user: v-umakaranka@expedia.com",mdaurangzeb,rahulg,sumitm,tanishg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jun/17 8:38 AM;mdaurangzeb;2017-06-20_07-06-10.jpg;https://qubole.atlassian.net/secure/attachment/44306/2017-06-20_07-06-10.jpg,20/Jun/17 8:38 AM;mdaurangzeb;2017-06-20_07-06-50.jpg;https://qubole.atlassian.net/secure/attachment/44305/2017-06-20_07-06-50.jpg,20/Jun/17 8:38 AM;mdaurangzeb;2017-06-20_07-07-38.jpg;https://qubole.atlassian.net/secure/attachment/44304/2017-06-20_07-07-38.jpg,20/Jun/17 8:39 AM;mdaurangzeb;account-6729.png;https://qubole.atlassian.net/secure/attachment/44303/account-6729.png,20/Jun/17 8:39 AM;mdaurangzeb;account-7567.png;https://qubole.atlassian.net/secure/attachment/44302/account-7567.png,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z052sf:,,,,,,,,,,,,,,,,,,,1.0,15266,2017-06-22 01:58:49.733,,,"22/Jun/17 12:47 AM;mdaurangzeb;[~sumitm]

Customer is waiting for our update on this, Since more users are getting blocked due to this. Please prioritize.

","22/Jun/17 1:58 AM;sumitm;[~rahulg] can you plz check this. Also, check if the account wasn't having SSE enabled and having some issues cause of that. ","22/Jun/17 5:55 AM;rahulg;[~sumitm] i checked both the accounts and `s3_server_side_encryption` in not abled for these.

Since, its using IAM roles, assigning it to [~tanishg] to look further on this.","22/Jun/17 11:58 PM;tanishg;We found out that somehow the region of bucket which is returning from code was wrong. And this region was saved in cache.So we deleted the cache which was fetching wrong s3 region and then save the settings of the account 7567. It worked fine and fetched correct region.
Also I have raised jira to further investigate the issue
https://qubole.atlassian.net/browse/MW-1125",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notebook is not running due to a problem with interpreter bindings,ZEP-1054,65364,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Critical,,beria,sbadam,sbadam,14/Jun/17 3:48 PM,10/Jul/17 1:36 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Notebook is not running due to a problem with interpreter bindings

User: shorsewood@scrippsnetworks.com, Account - 6654, Notebook ID - 31290, Cluster - 27141
Cluster has custom Zeppelin package - MW-335

Issue is that paragraphs are not executable even though bindings were set properly from UI. I see below logs in Zeppelin:


{code:java}
easy_hustler --cluster-id 27141 sshmaster qbol_acc6654_cl2741
{code}

logs are:
{code:java}
INFO [2017-06-14 22:37:32,612] ({qtp1320510275-13534} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-14 22:42:33,014] ({qtp1320510275-13940} NotebookServer.java[onClose]:300) - Closed connection to 10.49.229.97 : 34640. (1000) Idle for 300402ms > 300000ms
 WARN [2017-06-14 22:45:23,605] ({qtp1320510275-14243} SecurityRestApi.java[ticket]:89) - {""status"":""OK"",""message"":"""",""body"":{""principal"":""anonymous"",""ticket"":""anonymous"",""roles"":""[]""}}
 INFO [2017-06-14 22:45:24,501] ({qtp1320510275-14243} NotebookServer.java[onOpen]:143) - New connection from 10.49.229.97 : 34658
 INFO [2017-06-14 22:45:24,537] ({qtp1320510275-14072} NotebookServer.java[onMessage]:181) - RECEIVE << GET_NOTE
 INFO [2017-06-14 22:45:24,537] ({qtp1320510275-14072} NotebookServer.java[sendNote]:520) - New operation from 10.49.229.97 : 34658 : anonymous : GET_NOTE : RQ35CNT5XP1489066292
 INFO [2017-06-14 22:45:24,542] ({qtp1320510275-14072} NotebookServer.java[onMessage]:181) - RECEIVE << GET_USER_ID
 INFO [2017-06-14 22:45:25,246] ({qtp1320510275-14243} NotebookServer.java[onMessage]:181) - RECEIVE << GET_LOCK_STATUS
{code}








",api.qubole.com,aabbas,beria,drose@qubole.com,rgupta,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Jun/17 5:02 PM;sbadam;JGreeen_House_Interpreter_dump.txt;https://qubole.atlassian.net/secure/attachment/44157/JGreeen_House_Interpreter_dump.txt,16/Jun/17 5:02 PM;sbadam;Zeppelin_thread_dump_during_Jgreen_House_Interpreter_stuck.txt;https://qubole.atlassian.net/secure/attachment/44158/Zeppelin_thread_dump_during_Jgreen_House_Interpreter_stuck.txt,,,,,,,,,,,,AWS,,,,,None,scripps,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z050ov:,,,,,,,,,,,,,,,,,,,2.0,"15182,15702",2017-06-14 16:39:10.128,,,"14/Jun/17 4:31 PM;sbadam;I restarted Zeppelin and things are fine on Notebook. I took thread dump before I restarted. Logs are stored at: 

{code:java}
s3://sni-qubole-ad-marketing-services/defloc/logs/hadoop/27141/504530/ec2-54-236-23-157.compute-1.amazonaws.com.master/Zepplin_freeze_stack_trace

{code}

But the original problem is, scheduler 16057 is throwing below errors:



{code:java}
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
Qubole > Failed to Start Running Notebook.
{code}




","14/Jun/17 4:39 PM;rgupta;Its a close getting stuck issue. They are hitting it on MW-335 package. Its an old package. We will get rid of it post rb45 push.
Till then we dont have much choice. BTW this wont happen very often also",16/Jun/17 7:26 AM;drose@qubole.com;[~aabbas][~rgupta][~mahuja] The customer has escalated this as it is causing scheduled notebooks to fail that the customer considers business critical. Considering we are many weeks away from release how can we help assert stability today?,16/Jun/17 7:36 AM;aabbas;[~rgupta] it happened again and in other accounts. Can you join our 8am call with Scripps?,"16/Jun/17 2:18 PM;sbadam;There is a problem with tunnel server connects with master. From log server:

{code:java}
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] Starting actual execution...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] qid: 79027355, Logs available at /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] Run query in Async mode: false...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] Monthly usage limit: {""ODBC""=>0, ""SCHEDULER""=>1, ""enable_orc_format""=>1.0, ""Hive version""=>1.2, ""HiveClientMaxHeap""=>4096.0, ""Max periodic job instances per day""=>24.0, ""max_heap_mb""=>5120.0, ""PrivilegeLevel""=>-1.0, ""S3 server side encryption""=>1.0, ""foldering_enabled""=>1.0, ""Use Experimental DJ""=>0.0, ""UseHiveTier""=>1.0}
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] Monthly usage: {""QCUH""=>0.0, ""CONNECTOR_INVOCATIONS""=>0, ""USERS""=>0, ""SCHEDULER""=>0, ""ODBC""=>0}
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] ret_val is: {:plan_expired=>false, :qcuh_exceeded=>false, :connector_invocations_exceeded=>false, :users_exceeded=>false, :alert_message=>""OK"", :message=>""OK""}
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] Deferring run_composite_command to thread
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] Beginning execution of composite command: 772149
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] {log_str} db_tap_query : #<DbTapQueryCommand id: 608302, query: ""CALL spStartUpdate;"", db_tap_id: 2589, account_id: 6654, created_at: ""2017-06-16 08:30:18"", updated_at: ""2017-06-16 08:30:19"", md_cmd: false>
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] Running command...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057]  Running command... /usr/lib/hive_scripts/dbtapquerycli.py --cid 608302 --qid 79027355 --sid 2558445 --md_file_path /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.md --output_file_path /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.tmp 6654  2>>/media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.err.tmp > /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.tmp --cid 608302 --qid 79027355 --sid 2558445 --md_file_path /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.md  6654 2>>/media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.err.tmp > /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.tmp
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] Active Record is disconnected
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] qid: 79027355, cmd: /usr/lib/hive_scripts/dbtapquerycli.py --cid 608302 --qid 79027355 --sid 2558445 --md_file_path /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.md --output_file_path /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.tmp 6654  2>>/media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.err.tmp > /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.tmp
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:19 ] [command:79027355] [account:6654] [periodic_job:16057] qid: 79027355, pid: 6886
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] qid: 79027355, cmd done with status: pid 6886 exit 0
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] result is status 0 ret_val
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] log_path: /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355, err file: /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.err
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] log_path: /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355, output_file: /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] stage 3
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] Getting job URLs for hadoop2 cluster
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] stage 4
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] Upload done
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:28 ] [command:79027355] [account:6654] [periodic_job:16057] stage 5
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:29 ] [command:79027355] [account:6654] [periodic_job:16057] Upload done
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:29 ] [command:79027355] [account:6654] [periodic_job:16057] stage 6
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:29 ] [command:79027355] [account:6654] [periodic_job:16057] Starting cluster:, cmd: /usr/lib/hive_scripts/must_get_master_dns.py --cluster-id 27141 --qbol-user-id 24409
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:29 ] [command:79027355] [account:6654] [periodic_job:16057] Starting cluster:, pid: 6990
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:35 ] [command:79027355] [account:6654] [periodic_job:16057] Starting cluster:, cmd done with status: pid 6990 exit 0
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:35 ] [command:79027355] [account:6654] [periodic_job:16057] Starting cluster:, stderr: No config file specified - defaulting to hustler/configs/config.default for config file
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:35 ] [command:79027355] [account:6654] [periodic_job:16057] Running notebook with id: 31290
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:35 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.139.89.103
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:30:36 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 29995 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:31:36 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:31:36 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:31:36 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:31:46 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.155.161.3
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:31:46 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 20722 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:32:46 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:32:46 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:32:46 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:32:56 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.155.161.3
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:32:56 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 20722 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:33:57 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:33:57 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:33:57 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:34:07 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.139.89.103
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:34:07 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 29995 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:35:07 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:35:07 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:35:07 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:35:17 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.123.197.50
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:35:17 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 21165 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:36:17 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:36:17 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:36:17 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:36:28 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.139.89.103
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:36:28 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 29995 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:37:28 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:37:28 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:37:28 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:37:38 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.155.161.3
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:37:38 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 20722 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:38:38 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:38:38 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:38:38 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:38:48 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.155.161.3
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:38:49 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 20722 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:39:49 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:39:49 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:39:49 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:39:59 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.139.89.103
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:39:59 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 29995 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:40:59 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:40:59 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:40:59 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:41:09 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.139.89.103
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:41:10 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 29995 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:42:10 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:42:10 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:42:10 ] [command:79027355] [account:6654] [periodic_job:16057] Will retry start running notebook again in 10 seconds...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:42:20 ] [command:79027355] [account:6654] [periodic_job:16057] Getting proxy connection for account: 6654, cluster: 27141, node: ec2-52-91-243-12.compute-1.amazonaws.com, tunnel_server: 10.139.89.103
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:42:20 ] [command:79027355] [account:6654] [periodic_job:16057] SOCKS proxy port: 29995 for issuing request http://ec2-54-175-241-209.compute-1.amazonaws.com:8082/api/notebook/job/RQ35CNT5XP1489066292
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:20 ] [command:79027355] [account:6654] [periodic_job:16057] Exception occurred while connecting to the server: Net::ReadTimeout
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:43:20 ] [command:79027355] [account:6654] [periodic_job:16057] Failed to Start Running Notebook.
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:20 ] [command:79027355] [account:6654] [periodic_job:16057] qid: 79027355  execution completed. Success? false
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:20 ] [command:79027355] [account:6654] [periodic_job:16057] log_path: /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355, err file: /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355.err
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:20 ] [command:79027355] [account:6654] [periodic_job:16057] log_path: /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355, output_file: /media/ephemeral0/tmp/tapp/tmp/2017-06-16/6654/79027355
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:20 ] [command:79027355] [account:6654] [periodic_job:16057] stage 3
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:20 ] [command:79027355] [account:6654] [periodic_job:16057] Getting job URLs for hadoop2 cluster
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:20 ] [command:79027355] [account:6654] [periodic_job:16057] stage 4
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] Upload done
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] stage 5
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] Upload done
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] stage 6
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] Status of composite command : 772149
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] 2017-06-16T08:43:21+0000: [Worker(delayed_job.4 host:ip-10-184-181-204 pid:13224)] PjuWorker#run_pju completed after 783.5809
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] 2017-06-16T08:43:21+0000: [Worker(delayed_job.4 host:ip-10-184-181-204 pid:13224)] 1 jobs processed at 0.0013 j/s, 0 failed ...
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057]
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] Finishing the composite command execution : 772149
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] attempt number 0 to save query_hist entry
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] qid: 79027355 finalizing command and saving its status into db.
ip-10-184-181-204 [ERROR pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] qh: 79027355 failed to update the md_cmd
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] CompositeCommand is successfully recorded. With status = error
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] Updating changed status and progress in cache for qhist error and 100: 79027355
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] Killing all the child jobs
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 08:43:21 ] [command:79027355] [account:6654] [periodic_job:16057] Finished execution for PJUnit: #<PeriodicJobUnit id: 4451668, query_hist_id: 79027355, objects_json: nil, created_at: ""2017-06-16 08:30:08"", updated_at: ""2017-06-16 08:30:19"", periodic_job_unit_id: nil, periodic_job_id: 16057, sequence_id: 107, nominal_time: ""2017-06-16 08:30:00"", is_rerun_of: nil, editable_periodic_job_id: 51905, dependencies: {""found""=>[], ""not_found""=>[]}, rerun_number: 1, status: ""done"", done: true>
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 09:00:23 ] [command:79027355] [account:6654] [periodic_job:16057] created Query Hist object #<QueryHist id: nil, qbol_user_id: 18891, submit_time: 1497603623, end_time: nil, progress: 0, cube_id: nil, created_at: nil, updated_at: nil, path: nil, status: ""waiting"", host_name: nil, user_loc: true, qbol_session_id: nil, command_id: nil, command_type: ""HiveCommand"", qlog: nil, periodic_job_id: nil, wf_id: nil, command_source: nil, resolved_macros: nil, status_code: nil, pid: nil, editable_pj_id: nil, template: ""generic"", command_template_id: nil, command_template_mutable_id: nil, can_notify: false, num_result_dir: 0, start_time: nil, pool: nil, timeout: nil, tag: nil, name: nil, saved_query_mutable_id: nil, account_id: 5967>
ip-10-184-181-204 [INFO  pid: 13224: 17-06-16 09:00:23 ] [command:79027355] [account:6654] [periodic_job:16057] dependency initialized for account #<Account:0x00557216f759f8>
{code}

","16/Jun/17 5:06 PM;sbadam;Zeppelin is not running Notebook due to problem with Bindings:

From s3://sni-qubole-ad-marketing-services/defloc/logs/hadoop/27141/506363/ec2-54-175-241-209.compute-1.amazonaws.com.master/zeppelin/logs/zeppelin-root-ip-10-49-231-33.log.2017-06-16

{code:java}
INFO [2017-06-16 08:30:36,137] ({qtp1320510275-22878} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:30:36,137] ({qtp1320510275-22878} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:30:36,137] ({qtp1320510275-22878} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:30:36,138] ({qtp1320510275-22878} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:31:46,527] ({qtp1320510275-23010} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:31:46,527] ({qtp1320510275-23010} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:31:46,527] ({qtp1320510275-23010} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:31:46,527] ({qtp1320510275-23010} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:32:56,937] ({qtp1320510275-23012} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:32:56,937] ({qtp1320510275-23012} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:32:56,938] ({qtp1320510275-23012} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:32:56,938] ({qtp1320510275-23012} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:34:07,399] ({qtp1320510275-23130} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:34:07,399] ({qtp1320510275-23130} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:34:07,399] ({qtp1320510275-23130} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:34:07,399] ({qtp1320510275-23130} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:35:17,910] ({qtp1320510275-23132} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:35:17,911] ({qtp1320510275-23132} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:35:17,911] ({qtp1320510275-23132} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:35:17,911] ({qtp1320510275-23132} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:36:28,343] ({qtp1320510275-23261} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:36:28,343] ({qtp1320510275-23261} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:36:28,343] ({qtp1320510275-23261} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:36:28,343] ({qtp1320510275-23261} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:37:38,809] ({qtp1320510275-23387} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:37:38,809] ({qtp1320510275-23387} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:37:38,810] ({qtp1320510275-23387} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:37:38,810] ({qtp1320510275-23387} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:38:49,311] ({qtp1320510275-23389} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:38:49,312] ({qtp1320510275-23389} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:38:49,312] ({qtp1320510275-23389} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:38:49,312] ({qtp1320510275-23389} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:39:59,723] ({qtp1320510275-23440} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:39:59,724] ({qtp1320510275-23440} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:39:59,724] ({qtp1320510275-23440} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:39:59,724] ({qtp1320510275-23440} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
INFO [2017-06-16 08:41:10,119] ({qtp1320510275-23439} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:41:10,119] ({qtp1320510275-23439} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:41:10,119] ({qtp1320510275-23439} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:41:10,120] ({qtp1320510275-23439} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-06-16 08:42:20,605] ({qtp1320510275-23681} NotebookRestApi.java[runNoteJobs]:706) - run notebook jobs RQ35CNT5XP1489066292
 INFO [2017-06-16 08:42:20,605] ({qtp1320510275-23681} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:297) - Set interpreter bindings for note before para run: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:42:20,606] ({qtp1320510275-23681} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:318) - Bind user's interpreter for note: RQ35CNT5XP1489066292
 INFO [2017-06-16 08:42:20,606] ({qtp1320510275-23681} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
{code}


",16/Jun/17 5:10 PM;sbadam;[~rgupta]- could you please provide a summary based on our analysis(to hand-over to dev) and somebody will provide a custom package soon? ,16/Jun/17 7:22 PM;rgupta;[~beria] For scripps everything was working fne...just 1 interpreter was stuck. That interpreter was not getting restarted.Submission of paras was not doing anything. stacks on zeppelin side do have some stuck closed calls. Can you check and verify if its the same close kinda issue we have fixed?,"19/Jun/17 8:50 AM;beria; [~rgupta]: Sorry for the delay, forgot due to my leave. I checked but this is not related to the recent Remote interpreter close issue we have fixed. They are facing https://qubole.atlassian.net/browse/ZEP-643 . They have a pretty old package. This is fixed both in 0.6.0 and 0.6.2 for time now. [~sbadam]

Copying [~karuppayyar] and [~rgupta] solution from mail thread here:
{noformat}
instead of giving new packags, we can add the following to s3cfg(same as the fix for ACM-1230, will go in R-45)

echo ""server_side_encryption = True"" >> /usr/lib/hustler/s3cfg 
{noformat}","10/Jul/17 1:34 PM;sbadam;We are seeing problem with Interpreter bindings again on Zeppelin package(0.6.0-incubating-SNAPSHOT) for Turner(Acc - 6654):


{code:java}
INFO [2017-07-09 07:15:58,003] ({qtp11707893-21849} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Set interpreter bindings for note before para run: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:15:58,004] ({qtp11707893-21849} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:347) - Bind user's interpreter for note: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:15:58,004] ({qtp11707893-21849} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:355) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-07-09 07:17:08,569] ({qtp11707893-21861} QuboleACLHelper.java[checkAndAddNotebookACL]:155) - ACLs for qbolUserId=24409 and zeppelinId=BH7ST5ZJ4N1493989356 already available.
 INFO [2017-07-09 07:17:08,570] ({qtp11707893-21861} NotebookRestApi.java[runNoteJobs]:708) - run notebook jobs BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:17:08,570] ({qtp11707893-21861} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Set interpreter bindings for note before para run: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:17:08,570] ({qtp11707893-21861} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:347) - Bind user's interpreter for note: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:17:08,570] ({qtp11707893-21861} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:355) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-07-09 07:18:19,494] ({qtp11707893-21863} QuboleACLHelper.java[checkAndAddNotebookACL]:155) - ACLs for qbolUserId=24409 and zeppelinId=BH7ST5ZJ4N1493989356 already available.
 INFO [2017-07-09 07:18:19,494] ({qtp11707893-21863} NotebookRestApi.java[runNoteJobs]:708) - run notebook jobs BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:18:19,495] ({qtp11707893-21863} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Set interpreter bindings for note before para run: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:18:19,495] ({qtp11707893-21863} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:347) - Bind user's interpreter for note: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:18:19,495] ({qtp11707893-21863} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:355) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-07-09 07:19:29,856] ({qtp11707893-21875} QuboleACLHelper.java[checkAndAddNotebookACL]:155) - ACLs for qbolUserId=24409 and zeppelinId=BH7ST5ZJ4N1493989356 already available.
 INFO [2017-07-09 07:19:29,856] ({qtp11707893-21875} NotebookRestApi.java[runNoteJobs]:708) - run notebook jobs BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:19:29,856] ({qtp11707893-21875} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Set interpreter bindings for note before para run: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:19:29,857] ({qtp11707893-21875} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:347) - Bind user's interpreter for note: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:19:29,857] ({qtp11707893-21875} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:355) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
 INFO [2017-07-09 07:20:40,227] ({qtp11707893-21888} QuboleACLHelper.java[checkAndAddNotebookACL]:155) - ACLs for qbolUserId=24409 and zeppelinId=BH7ST5ZJ4N1493989356 already available.
 INFO [2017-07-09 07:20:40,227] ({qtp11707893-21888} NotebookRestApi.java[runNoteJobs]:708) - run notebook jobs BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:20:40,227] ({qtp11707893-21888} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:326) - Set interpreter bindings for note before para run: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:20:40,228] ({qtp11707893-21888} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:347) - Bind user's interpreter for note: BH7ST5ZJ4N1493989356
 INFO [2017-07-09 07:20:40,228] ({qtp11707893-21888} QuboleInterpreterUtils.java[setInterpreterBindingsForMode]:355) - Notebook to interpreter mapping failed. Interpreter mode: USER
 Exception:null
{code}

Command ids: 83258693, 83085565, 83248428, 83077132, 83342938
Scheduler id: 18278 , Notebook: 36540, Cluster - 27141

You can look into zeppelin on present cluster instance: 

{code:java}
easy_hustler --cluster-id 27141 sshmaster qbol_acc6654_cl27141
cd /media/ephemeral0/logs/zeppelin/logs 
[root@ip-10-49-230-180 logs]# ls -ltrh zeppelin-root-ip-10-49-230-180.log.2017-07-09 zeppelin-root-ip-10-49-230-180.log
-rw-r--r-- 1 root root 170K Jul  9 20:08 zeppelin-root-ip-10-49-230-180.log.2017-07-09
-rw-r--r-- 1 root root 1.3M Jul 10 20:34 zeppelin-root-ip-10-49-230-180.log
[root@ip-10-49-230-180 logs]#
{code}



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark Commands (trying to access Hive Metastore) failing with ""HiveAuthzPluginException: Unsupported privilege type ALL""",SPAR-1719,65362,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,shridhar,p.vasa,p.vasa,14/Jun/17 2:58 PM,31/Jul/17 12:44 PM,09/Aug/17 5:29 AM,,,,,0,jira_escalated,jira_update,,,,,,,"*General Information ->*
User ->  v-jskinderskis@expedia.com
Account ID -> 5507
Account Name -> Prod-ETL
Cluster ID -> 31152
Cluster Name -> p-matrx
Cluster Type and Version -> Spark, (2.0 Latest)

*Command IDs ->*
78684318

*Problem Description ->*
Command is failing with the following 2 different StackTrace ->

a) 
{code:java}
Exception in thread ""main"" org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException: Unsupported privilege type ALL
{code}

b) 
{code:java}
App > Caused by: java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.
SQLPrivilegeType.ALL
App > at java.lang.Enum.valueOf(Enum.java:236)
{code}

*Things Tried so far ->*
-> Hive Authorization (or) Spark Authorization is not enabled for this account

-> Some successful runs of similar types of commands -> 
*78641630, 78582335, 78488620, 78464443*

->  I ran a completely different spark job - https://api.qubole.com/v2/analyze?command_id=78741130 on a different spark cluster(spark2_1 in prod-adhoc account) and it failed with the same exception

-> I guess there is something going wrong when spark clusters are trying to reach out to Hive Metastore. In this example, the job failed at an alter table attach partition step.

-> Referred this documentation -> *http://www.openkb.info/2015/07/query-fails-with-error.html*
-> Connected to their “*Custom Metastore*” and found out that there are “*ALL*” privileges present for “*DB_PRIVS*” and “*TBL_PRIVS*”

-> SPARK Application is not being created as the job fails before it logs anything about the Spark Application

-> AM logs on the UI show the same StackTrace",,mpatel,p.vasa,rohitk,sbadam,shridhar,snamburu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,roku,,,,"{repository={count=5, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":5,""lastUpdated"":""2017-07-10T12:34:31.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":5,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z050of:,,,,,,,,,,,,,,,,,,,3.0,"15228,15256,15372",2017-06-14 15:54:50.824,,,"14/Jun/17 3:54 PM;shridhar;I see the following in cluster Hadoop overrides:
hive.security.authorization.enabled=true
hive.security.authorization.createtable.owner.grants=ALL

With r44, we have introduced hive authorization in Spark. Even though we have disabled it by default and gated behind an account feature, it can be manually enabled by setting a few configs - one of which is hive.security.authorization.enabled=true.

Can you please find out why they have set these flags? If they aren't required, you can remove them and the job should succeed.
","14/Jun/17 9:06 PM;p.vasa;[~shridhar] Thanks a lot for looking into this issue.
After removing the 2 Hadoop overrides, the customer has confirmed that their commands succeeded.

I will resolve this ticket.
If they have a need to use those overrides back in the future, I will update you.","15/Jun/17 9:15 AM;mpatel;[~shridhar] [~p.vasa] They had the legacy settings enabled due to a requirement around access to the default hive database. I believe there is some gap in how the sql standards works (need to dig up details...)

hive.security.authorization.enabled=true
hive.security.authorization.createtable.owner.grants=ALL

Is there any other way around this to allow them to keep the above settings?","15/Jun/17 9:42 AM;p.vasa;Some other commands for them (on a different cluster ""26605"") ->
78848114, 78816760 , 78816754 which had the above settings enabled failed with the following StackTrace ->

{code:java}
App > py4j.protocol.Py4JJavaError: An error occurred while calling o59.sql.
App > : org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException: Unsupported privilege type ALL
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivilegeType.getRequirePrivilege(SQLPrivilegeType.java:37)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivTypeGrant.getSQLPrivTypeGrant(SQLPrivTypeGrant.java:79)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges.addPrivilege(RequiredPrivileges.java:39)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.addRequiredPrivs(SQLAuthorizationUtils.java:354)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getRequiredPrivsFromThrift(SQLAuthorizationUtils.java:331)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.getPrivilegesFromMetaStore(SQLAuthorizationUtils.java:210)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.checkPrivileges(SQLStdHiveAuthorizationValidator.java:146)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.checkPrivileges(SQLStdHiveAuthorizationValidator.java:99)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.checkPrivileges(HiveAuthorizerImpl.java:85)
App > at org.apache.spark.sql.hive.DefaultSparkSqlHiveAuthorizer.authorize(DefaultSparkSqlHiveAuthorizer.scala:101)
App > at org.apache.spark.sql.hive.HiveSessionCatalog.authorize(HiveSessionCatalog.scala:108)
App > at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.doExecute(InsertIntoHiveTable.scala:320)
App > at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
App > at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
App > at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
App > at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
App > at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
App > at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
App > at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
App > at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
App > at org.apache.spark.sql.Dataset.(Dataset.scala:186)
App > at org.apache.spark.sql.Dataset.(Dataset.scala:167)
App > at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:65)
App > at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
App > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
App > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
App > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
App > at java.lang.reflect.Method.invoke(Method.java:606)
App > at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
App > at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
App > at py4j.Gateway.invoke(Gateway.java:280)
App > at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
App > at py4j.commands.CallCommand.execute(CallCommand.java:79)
App > at py4j.GatewayConnection.run(GatewayConnection.java:214)
App > at java.lang.Thread.run(Thread.java:745)
App > Caused by: java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivilegeType.ALL
App > at java.lang.Enum.valueOf(Enum.java:236)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivilegeType.valueOf(SQLPrivilegeType.java:24)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLPrivilegeType.getRequirePrivilege(SQLPrivilegeType.java:35)
App > ... 34 more
{code}

In the meanwhile, I will ask them the use case for having those settings and it would be great if we can find a workaround without having them to remove those settings as it looks like they need to use them.
","15/Jun/17 9:52 AM;p.vasa;[~shridhar] Here is what the customer replied ->
""Hi Pratham, we've been using legacy auth for a long time now. Those were the statements given to us on how to enable it.""

""Just to give a little more detail.

The use cases are fairly standard. Basically given permissions to different groups of people to use some databases and not others.

The sticking point has been that with legacy auth we can restrict the default db (to avoid it becoming a dumping ground) and you can't do that with the newer auth...""","15/Jun/17 11:08 AM;p.vasa;The workaround suggested is to add the following to the Node Bootstrap script for the cluster ->

{code:java}
python /usr/lib/spark/hustler/populate_xml_props.py 'hive.security.authorization.enabled=false' /usr/lib/spark/conf/
{code}

Will ask the customer to try this on customer's cluster with a test job and update.

(The aim is to set this as false in Spark's *hive-site.xml* so that it can override the settings in Hadoop Overrides in Spark code flow path and at the same time presence of these overrides on UI will add the details in Hive's *hive-site.xml* file so that Hive queries on Spark clusters are honored)","19/Jun/17 8:07 PM;snamburu;[~shridhar]

Query:
select *
from dev.youtube_video_details
limit 50;

Spark command id: 79589230 (FAIL)
Hive command id: 79589664 (SUCCESS)
Notebook id: 39870 (SUCCESS)

Issue: Query runs fine as Hive query and in Notebooks. It is failing when run as Spark query with below error:
'''''
17/06/19 18:24:47 main INFO Utils: conf 'spark.qubole.queryhist.fileprefix' is not passed hence not able to log error msg!
App > Exception in thread ""main"" java.lang.RuntimeException: Failed to use the Hive authorization components: null
App > at org.apache.spark.sql.hive.DefaultSparkSqlHiveAuthorizer.authorize(DefaultSparkSqlHiveAuthorizer.scala:105)
App > at org.apache.spark.sql.hive.HiveSessionCatalog.authorize(HiveSessionCatalog.scala:108)
App > at org.apache.spark.sql.hive.execution.HiveTableScanExec.doExecute(HiveTableScanExec.scala:144)
App > at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
App > at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
App > at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
App > at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
App > at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
App > at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
App > at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
App > at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:323)
App > at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)
App > at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)
App > at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:58)
App > at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
App > at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)
App > at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2187)
App > at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2187)
App > at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2545)
App > at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2187)
App > at org.apache.spark.sql.Dataset.collect(Dataset.scala:2163)
App > at org.apache.spark.examples.sql.hive.SqlWrapper$$anonfun$main$3.apply(SqlWrapper.scala:51)
App > at org.apache.spark.examples.sql.hive.SqlWrapper$$anonfun$main$3.apply(SqlWrapper.scala:46)
App > at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
App > at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
App > at org.apache.spark.examples.sql.hive.SqlWrapper$.main(SqlWrapper.scala:46)
App > at org.apache.spark.examples.sql.hive.SqlWrapper.main(SqlWrapper.scala)
App > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
App > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
App > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
App > at java.lang.reflect.Method.invoke(Method.java:606)
App > at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:869)
App > at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:202)
App > at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)
App > at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:130)
App > at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
App > Caused by: java.lang.NullPointerException
App > at org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator.getUserName(SessionStateUserAuthenticator.java:46)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.checkPrivileges(SQLStdHiveAuthorizationValidator.java:93)
App > at org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerImpl.checkPrivileges(HiveAuthorizerImpl.java:85)
App > at org.apache.spark.sql.hive.DefaultSparkSqlHiveAuthorizer.authorize(DefaultSparkSqlHiveAuthorizer.scala:101)
App > ... 35 more
''''''
","21/Jun/17 1:22 PM;p.vasa;[~shridhar] I am asking a customer to try the following ->

Initial Workaround ->
a) Remove the following parameters from ""Override Hadoop Configuration Variables"" ->
{code:java}
hive.security.authorization.enabled=true
hive.security.authorization.createtable.owner.grants=ALL
{code}

b) Add the following line in the Node Bootstrap script ->
{code:java}
python /usr/lib/spark/hustler/populate_xml_props.py 'hive.security.authorization.enabled=false' /usr/lib/spark/conf/
{code}

New Workaround ->
a) Move those 2 specific parameters from ""Override Hadoop Configuration Variables"" to Hive Bootstrap script.
b) No need to modify the Node Bootstrap script

Can you please confirm if we can ask the customers to try out the new workaround?


","21/Jun/17 5:01 PM;shridhar;[~p.vasa] Yes. You are right.
Couple of points about the hive bootstrap script:
1. Hive bootstrap is essentially a series of hive commands before every Hive query. So the format in the hive bootstrap should be 
{code}
set hive.security.authorization.enabled=true;
set hive.security.authorization.createtable.owner.grants=ALL
{code}
2. It is account wide, so these configs will be applied to every hive query run on any cluster.

Please make sure you mention point 2 to the customer.","21/Jun/17 5:06 PM;p.vasa;Yes this makes sense.
Thanks a lot [~shridhar]
Will ask the customer to use this workaround and update accordingly.","03/Jul/17 2:47 PM;sbadam;[~shridhar] - moving hive security parameters into hive bootstrap is seems to be not a good idea. SQL Authorization enabled is not enabled for all customer accounts, by default. 

-> I moved all of the parameters into hive bootstrap and saw failures with privileges in my acocunt. 

command ids - 82113598, 82113407, 82113189

I had set below flags in hive bootstrap before I have executed above commands:

{code:java}
set hive.security.authorization.enabled=true;
set hive.server2.authentication=NONE;
set hive.server2.enable.impersonation=true;
set hive.server2.enable.doAs=true;
set hive.users.in.admin.role=pvasa, sbadam;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator;
set hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly;
set hive.security.command.whitelist=set,add,list;
set hive.conf.restricted.list=hive.security.authenticator.manager,hive.security.authorization.manager,hive.security.metastore.authorization.manager,hive.security.metastore.authenticator.manager,hive.users.in.admin.role,hive.server2.xsrf.filter.enabled,hive.security.authorization.enabled;
{code}

","11/Jul/17 11:14 AM;p.vasa;cc -> [~shridhar] [~snamburu][~sbadam]

So according to the latest workaround, we can provide the following custom package to the customer ->
{code:java}
spark_s3_package_name='2.0.0-SPAR-1719'
{code}

Once enabled this under the Cluster Configs table for the customer, Spark queries will not pick up this code flow path.

Limitation to setting this custom package is that the customer will not be able to enable the new feature of ""Spark Hive Authorization"" for their Spark clusters. ","11/Jul/17 11:27 AM;shridhar;One clarification regarding -

""Limitation to setting this custom package is that the customer will not be able to enable the new feature of ""Spark Hive Authorization"" for their Spark clusters."" 

This isn't true, my fix involved simply adding a spark config to enable spark hive authorization (in addition to the hive configs). 
So if the spark config - spark.sql.hive.authorization.enabled - is set to true (in addition to the steps mentioned [here|https://qubole.atlassian.net/browse/KB-315?focusedCommentId=137777&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-137777]) then spark hive authorization will still work. That said, if users want to enable Qubole's authorization of hive objects (in Hive/presto/spark), they are not recommended to use their own custom override hadoop confs.","31/Jul/17 12:44 PM;p.vasa;[~shridhar] Thanks for the detailed information.
You may go ahead and resolve this issue. (All the Zendesk tickets attached to this JIRA have been resolved/closed through the Pending Automation)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data loss through Qubole ODBC connector ,ODBC-146,65313,Bug,Open,ODBC,odbc,software,stagra,,,Major,,udayk,biswajit,biswajit,14/Jun/17 4:56 AM,18/Jul/17 1:12 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Data drop is seen when using Qubole ODBC connector to pull data from s3.

Table Name:lz.lz_partner_mktg_spend 
Records which are dropping: please see attached (file_partner_name like '%restatement.tab%')

Below SQL query which is hitting AWS through Qubole ODBC connector from Qlikview BI tool, and It's not returning all the attached records when executed from Qlikview.

Note: 
- If we add “file partner” filter in the SQL, it’s showing up all the missing records. 
- We don’t see any data drop when execute these queries from Qubole editor.

{Code}

SQL Query Used:

select 
local_date, 
file_partner_name, 
etl_load_tag, 
COALESCE(commission,0)+COALESCE(mtb_fees,0) as lz_spend_local 
from 
( 
Select concat(substr(local_date,5,2), '/' ,substr(local_date,7,2),'/',substr(local_date,1,4)) local_date , 
file_partner_name, 
from_unixtime(cast(etl_load_tag as BIGINT)) etl_load_tag, 
sum (regexp_replace(COALESCE(commission,0), ',' , '')) commission, 
sum (regexp_replace(COALESCE(mtb_fees,0), ',' , '')) as mtb_fees 
from lz.lz_partner_mktg_spend 
where 
local_date>='20170414' 
and file_partner_name like '%restatement.tab%' 
group by 
local_date, 
file_partner_name, 
etl_load_tag 
)a 
;

{Code}

same query run on analyse that gave full result :- https://api.qubole.com/v2/analyze?command_id=75942578

Please do let me know if any info is required",,biswajit,goden,stagra,udayk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z050jj:,,,,,,Presto Sprint 24,Presto Sprint 25,Presto Sprint 26,,,,,,,,,,,1.0,14871,2017-06-14 20:47:05.708,,,"14/Jun/17 8:47 PM;stagra;This seems important, adding it to sprint","20/Jun/17 11:01 AM;udayk;[~biswajit] not sure of the actual reason, but when I`m executing the same query as mentioned above from odbc and qds I`m getting different results, around 9000 rows in odbc(inline result) and where as s3 result(> 20MB) when executed from qds.","20/Jun/17 9:04 PM;stagra;[~udayk] do you see the same behavior via UI? If so, we should pass it to MW.",20/Jun/17 11:50 PM;udayk;I see different behaviour when same query is executed via ODBC and UI,21/Jun/17 8:28 AM;biswajit;[~udayk] Did you get chance to look for the root cause for the different behaviour ?,"18/Jul/17 1:12 AM;udayk;[~biswajit] I tried it again but unable to reproduce the issue, but I`m sensing another resolved issue with odbc since the customer is using older version of driver. Can you ask them to use the latest one and see if it fixes the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster gets random updates when modifying using python SDK,SDK-216,65172,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,abhijitj,wvaldez,wvaldez,13/Jun/17 3:22 PM,02/Aug/17 7:13 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Oracle reported that they have specific settings change unexpectedly when they attempt to update the custom Hadoop settings of a cluster using the Python SDK (Zendesk ticket #[14566|https://qubole.zendesk.com/agent/tickets/14566).

I asked them for their code and I was able to replicate the issue even though is not consistent.  I had a few attempts that were successful.  Then I had a couple bad updates where random settings were changed.  Some settings that I wasn't even setting or resetting in between attempts.

I have attached the script that I used to replicate the issue and 3 text files that show the before, expected change, and after snapshot of the cluster config.",api,abhijitj,wvaldez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jun/17 3:21 PM;wvaldez;Cluster_30764_1_(before_change).txt;https://qubole.atlassian.net/secure/attachment/43917/Cluster_30764_1_%28before_change%29.txt,13/Jun/17 3:21 PM;wvaldez;Cluster_30764_2_(change_submitted).txt;https://qubole.atlassian.net/secure/attachment/43916/Cluster_30764_2_%28change_submitted%29.txt,13/Jun/17 3:21 PM;wvaldez;Cluster_30764_3_(after_change).txt;https://qubole.atlassian.net/secure/attachment/43915/Cluster_30764_3_%28after_change%29.txt,13/Jun/17 3:21 PM;wvaldez;qds-cluster-update.py;https://qubole.atlassian.net/secure/attachment/43914/qds-cluster-update.py,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z0505r:,,,,,,,,,,,,,,,,,,,1.0,14566,2017-08-02 19:13:48.738,,,"02/Aug/17 1:26 PM;wvaldez;[~abhijitj]. Hi, could you give me an idea of when this will be looked at?  I would like to give an update to the customer.

Thanks,
Willie",02/Aug/17 7:13 PM;abhijitj;[~sumitm] [~sourabhg] any takers for this ?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lag over partition by with a custom data type in partition by column fails,HIVE-2243,65171,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,achyuths,megha,megha,13/Jun/17 2:52 PM,28/Jun/17 10:52 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command 71244992 fails with 
{code:java}
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.uncheckedGetKey(LazyBinaryMap.java:274) at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.getMap(LazyBinaryMap.java:308) at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.getMap(LazyBinaryMapObjectInspector.java:46) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:313) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:193) at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:179) at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:266) at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425) at org.apache.hadoop.mapred.Child$3.run(Child.java:205) ] at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:283) at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425) at org.apache.hadoop.mapred.Child$3.run(Child.java:205) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.uncheckedGetKey(LazyBinaryMap.java:274) at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.getMap(LazyBinaryMap.java:308) at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.getMap(LazyBinaryMapObjectInspector.java:46) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:313) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349) at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:193) at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:179) at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:266) at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425) at org.apache.hadoop.mapred.Child$3.run(Child.java:205) ] at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:271) ... 3 more Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.uncheckedGetKey(LazyBinaryMap.java:274) at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.getMap(LazyBinaryMap.java:308) at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.getMap(LazyBinaryMapObjectInspector.java:46) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:324) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:340) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:340) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:340) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:340) at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:340) at org.apache.hadoop.hive.ql.exec.PTFPartition.append(PTFPartition.java:91) at org.apache.hadoop.hive.ql.exec.PTFOperator.processOp(PTFOperator.java:138) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796) at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45) at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:262) ... 3 more
task_247.201702240442_162893_r_000009
{code}


Commands https://api.qubole.com/v2/analyze?command_id=69885290,71205569,71207016,71213854,71226757,71242255,71244992 show that issue occurs only when lag is used inside the select query..

The error is classCastException, which shouldn't occur given that https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics suggests that in Over clause:
`OVER with PARTITION BY and ORDER BY with one or more partitioning and/or ordering columns of any datatype.`
should work.


",,megha,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,pinterest,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z0505j:,,,,,,,,,,,,,,,,,,,1.0,13996,,,,13/Jun/17 2:52 PM;megha;cc: [~mpatel],"14/Jun/17 2:17 PM;megha;Comment from pinterest about datatypes of fields involved:
insertion.timestamp is a bigint (epoch-based timestamp in nanoseconds) 
insertion.candidate.position is a small integer 
insertion.requestid.hb is a string, but insertion.requestid is binary so perhaps that's the issue? ","28/Jun/17 10:52 AM;megha;[~psrinivas], 
I confirmed with pinterest.. ""Insertion"" is a struct..

The definition is as follows:
{code:java}
struct PinPromotionsInsertionEvent { 
// next id: 23 
1: required i64 timestamp,

2: required binary insertionId, 
16: optional i64 insertionIdUniquePortion, 
// the mohawk request 
3: optional binary queryId, 
// the first mohawk request in this scroll (aka session id). 
22: optional binary firstQueryId, 
// the pinboard request 
10: optional binary requestId,

4: optional ViewType viewType, 
6: optional i64 userId, 
8: optional PinPromotionCandidate candidate, 
13: optional map<string,string> untriggeredExperimentGroups, 
9: optional map<string,string> experimentGroups,

// from QueryJoin predicted based on repins, if not available, then predicted based on impressions. 
11: optional list<string> predictedQueryCategories, 
12: optional string host, 
15: optional string mohawkDeployStage, 
// used to join with AdFunnelGroup to compute the success rate of Bethlehem shards 
17: optional string queryIdString,

// Metadata on Mohawk that is helpful to detect revenue regression. 
18: optional string mohawkVersion, 
19: optional map<string, double> mohawkDeciderValues, 
20: optional string userTerrapinVersion, 
21: optional string queryTerrapinVersion,

///////////// deprecated. ///////////////////// 
14: optional list<PinPromotionsPacingCandidateInfo> pacingCandidates, 
////////////// end deprecated //////////////// 
}
{code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveMetadataApi getTableProperties returns 404,SDK-215,65101,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,abhijitj,abhijitj,abhijitj,13/Jun/17 1:38 AM,13/Jun/17 2:33 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"https://github.com/qubole/qds-sdk-java/issues/85

The following code I would expect to get the table properties of a database as per http://docs.qubole.com/en/latest/rest-api/hive_metadata_api/get-table-properties.html

{code:java}
QdsClient client = QdsClientFactory.newClient(new DefaultQdsConfiguration(""<apiToken>""))
println JsonOutput.toJson(client.hiveMetadata().getTableProperties(""<tableName>"").invoke().get())
{code}

However, instead I get a 404 response. I can run this command through the rest api directly no problem using the same token and schema/table name.

cc: [~karthikk] [~satyavathib]",,abhijitj,karthikk,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04zpz:,,,,,,,,,,,,,,,,,,,1.0,14897,,,,"13/Jun/17 1:39 AM;abhijitj;It seems to be an issue with our SDK, the URL formed is having ""properties"" in place of ""table_properties"", also the response is not handled well. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
--class option not working for scala,SPAR-1689,65003,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,megha,megha,12/Jun/17 1:20 PM,12/Jun/17 1:20 PM,09/Aug/17 5:29 AM,,,,,0,jira_escalated,,,,,,,,"Example command id: 77063212 failed with java.lang.ClassNotFoundException: class

The spark submit command being sent is:
 /usr/lib/spark/bin/spark-submit --master yarn-client --conf spark.dynamicAllocation.maxExecutors=10 --conf spark.executor.instances=10 --conf spark.executor.cores=6 --conf spark.sql.qubole.result.formatted=true --class LoadUIS --class class code.jar 

which leads to above error..

",,megha,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04zif:,,,,,,,,,,,,,,,,,,,1.0,15045,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug json serde won't read integer's correctly,SOL-182,64994,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,biswajit,biswajit,12/Jun/17 8:09 AM,21/Jun/17 9:57 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"
{code}

{""test_data"": ""658252425452120148""}

CREATE EXTERNAL TABLE contrib_json_serde_int ( 
test_data BIGINT 
) 
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.JsonSerde' 
LOCATION ""/Users/amalakar/tmp/json_serde/data"";

-- BAD result, notice that the last 3 digits are different from what is in json 
hive> select * from contrib_json_serde_int; 
OK 
658252425452120208

{code}


Note .. there is no command id as they are hitting directly to hive server2. 

Was able to replicate the issue on our cluster. 
Create command id :- 78305750
Select command id :- 78305829

Please do let me know if any other detail is required. 
",,biswajit,drose@qubole.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,None,lyft,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04zgf:,,,,,,,,,,,,,,,,,,,1.0,14840,2017-06-21 09:57:19.169,,,"21/Jun/17 9:57 AM;drose@qubole.com;[~biswajit][~Jtrail][~venkatak] should this be a Hive or Presto JIRA so someone looks at it, this does not look to be getting any attention.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyze filter issue,UI-6222,64916,Bug,Open,UI,UI,software,aswina,,,Major,,aswina,megha,megha,09/Jun/17 5:17 PM,13/Jun/17 10:51 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"While applying filters to search for a specific query, one of the queries (77449344) is not showing up when all users are selected..[screenshot name: working-filter]
But if the filter criteria has the specific user that ran the query, then the query shows up..[screenshot name: non-working-filter]
Attaching screenshots for both cases for more details..

Keyword I'm searching from:
step_004B_obm_eml_summary_uber_view.hql

Date range doesn't matter..

This is happening only for this particular keyword search.. Other keywords work fine with ""all"" users.. For example, for this same account, keyword wr_hdp_lz_omniture_hit_hourly_launch.bash works fine, with all users in filter criteria [screenshot name: case2]

",,aswina,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Jun/17 5:16 PM;megha;case2.png;https://qubole.atlassian.net/secure/attachment/43804/case2.png,09/Jun/17 5:12 PM;megha;non-working-filter.png;https://qubole.atlassian.net/secure/attachment/43806/non-working-filter.png,09/Jun/17 5:12 PM;megha;working-filter.png;https://qubole.atlassian.net/secure/attachment/43805/working-filter.png,,,,,,,,,,,AWS,,,,,,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04yz3:,,,,,,,,,,,,,,,,,,,1.0,15131,2017-06-11 23:16:06.694,,,"11/Jun/17 11:16 PM;aswina;Keyword search today works only in the list of items already displayed in the left pane.  It's a client-side only search.

As part of UI-6155 and UI-5551, we are moving keyword search to backend.  That should fix these discrepancies.

I'll update here once we have those changes in master.","12/Jun/17 10:05 AM;megha;Thanks [~aswina], is there an ETA as to when this will be done? 
","12/Jun/17 9:57 PM;aswina;We have a fix ready and we have just begun to test it on a production
replica.  We can hotfix it once it's ready.  Without hotfix, it'll get
rolled out with R46.

cc: [~sureshr] [~karthikk] [~nimitk]

On Mon, Jun 12, 2017 at 10:35 PM, Megha (JIRA) <jira@qubole.atlassian.net>

","13/Jun/17 10:51 AM;megha;Thanks [~aswina], I have given a couple of workarounds to expedia.. If those don't work out, we might have to push this to hotfix.. Else R46 is fine.. Will update here once I hear back",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.sql.qubole.ignoreFNFExceptions=true not being honored for spark 2.1,SPAR-1686,64842,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,venkats,megha,megha,08/Jun/17 4:17 PM,09/Jun/17 12:09 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Have noticed this a few times for spark 2.0 cluster where even when following parameters are set, commands fail with file not found exceptions (S3 inconsistency):
--conf spark.sql.qubole.split.computation=true
--conf spark.sql.qubole.ignoreFNFExceptions=true


Example command id: 77559376

Will add more commands if I see this..

",,mahuja,megha,mpatel,rohitk,venkats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,expedia,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04yuf:,,,,,,,,,,,,,,,,,,,1.0,14794,2017-06-08 18:21:10.302,,,"08/Jun/17 6:21 PM;mahuja;[~venkats] can you please take a look.

From what I remember this switch is used for ignoring S3 exception during catalog build. Need to check if this code path is same and if it is a different problem.","09/Jun/17 11:56 AM;venkats;[~megha] [~mahuja] It seems to be happening while reading the file splits, something has happened in between the catalog building and then the data processing step. They have a huge python code for this particular command where they're doing lot of dataframe manipulations.

Btw, I checked the Spark app and it seems to have ran on Spark-2.1.0. [~megha] Can you please confirm if its 2.0.0 or 2.1.0? ","09/Jun/17 12:04 PM;megha;[~venkats], Yeah its spark 2.1.0, i got confused with the one of the warning log statement..

It is 2.1","09/Jun/17 12:06 PM;megha;And it seems ignore fnf exception is not available on 2.1.
","09/Jun/17 12:09 PM;venkats;Yeah, thats right we haven't ported it to 2.1. Anyway this issues has nothing to do with that change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2gb bytebuffer limit: https://issues.apache.org/jira/browse/SPARK-6235,SPAR-1684,64837,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,mpatel,mpatel,08/Jun/17 3:12 PM,08/Jun/17 3:12 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"SocialCode, Expedia have both reported hitting some manifestation of https://issues.apache.org/jira/browse/SPARK-6235

OSS Jira is still open.

Example: https://api.qubole.com/v2/analyze?command_id=77163824


",,mpatel,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Expedia,socialcode,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04ytb:,,,,,,,,,,,,,,,,,,,1.0,15033,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Persistent Security Group setting in Account Settings is not applied to clusters,ACM-1274,64740,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Critical,,ajayb,mpatel,mpatel,07/Jun/17 2:58 PM,07/Jul/17 9:47 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"reported by expedia

To reproduce:

1) Go to Account Settings -> Default Compute Settings
2) Provide a valid SG in the ""Security Group"" entry
3) Launch one of the account's clusters. 
4) Check the nodes in EC2 console. The persistent SG is not attached.

",,ajayb,ajithr,anum,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Expedia,ExpediaEAN,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04yef:,,,,,,,,,,,,,,,,,,,1.0,15106,2017-07-07 08:30:36.957,,,07/Jul/17 7:18 AM;mpatel;bumping the priority. EAN is depending on this feature now. can it be picked up soon?,"07/Jul/17 8:30 AM;ajayb;[~mpatel] I believe we test in every release that the PSG specified at cluster level gets applied on all nodes. Can you check if this works for you too? When specifying PSG at account level in default compute settings, are they selecting the checkbox ""Push Compute Settings to all clusters""? Without this the PSGs specified at account level will not be pushed to existing clusters.

[~anum], [~namitag] - I know there is a test case to verify PSG gets applied to all cluster nodes. Can you confirm? Is that only tested by specifying PSG at cluster level or also verified for PSG pushed from account settings to clusters?","07/Jul/17 9:47 AM;anum;[~ajayb] The test only covers pushing psg at the cluster level. While testing capone environment, we had tried using psg at the account level and this was working as expected. But now I was trying to save the psg at the account level and that was failing in production. Api (https://api.qubole.com/accounts/3392.json) is throwing ""We're sorry, but something went wrong. Please contact help@qubole.com and provide the error code 0947d50e.""

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Max requests per destination 1024 exceeded for HttpDestination,PRES-1108,64565,Bug,Open,PRES,Presto,software,stagra,,,Major,,udayk,satyavathib,satyavathib,05/Jun/17 10:35 PM,06/Jun/17 2:14 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command ID's : 77092795 77092806 77093007

{code}
Query 20170606_022736_00163_ung4m failed: Max requests per destination 1024 exceeded for HttpDestination[http://192.169.67.181:8081]@715ed68e,queue=1024,pool=DuplexConnectionPool[c=250/250,a=250,i=0]
com.facebook.presto.spi.PrestoException: Max requests per destination 1024 exceeded for HttpDestination[http://192.169.67.181:8081]@715ed68e,queue=1024,pool=DuplexConnectionPool[c=250/250,a=250,i=0]
at com.facebook.presto.server.remotetask.RequestErrorTracker.requestFailed(RequestErrorTracker.java:105)
at com.facebook.presto.server.remotetask.HttpRemoteTask$UpdateResponseHandler.failed(HttpRemoteTask.java:745)
at com.facebook.presto.server.remotetask.SimpleHttpResponseHandler.onFailure(SimpleHttpResponseHandler.java:83)
at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
at io.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:77)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.RejectedExecutionException: Max requests per destination 1024 exceeded for HttpDestination[http://192.169.67.181:8081]@715ed68e,queue=1024,pool=DuplexConnectionPool[c=250/250,a=250,i=0]
at org.eclipse.jetty.client.HttpDestination.send(HttpDestination.java:209)
at org.eclipse.jetty.client.HttpClient.send(HttpClient.java:540)
at org.eclipse.jetty.client.HttpRequest.send(HttpRequest.java:693)
at org.eclipse.jetty.client.HttpRequest.send(HttpRequest.java:677)
at io.airlift.http.client.jetty.JettyHttpClient.executeAsync(JettyHttpClient.java:400)
at com.facebook.presto.server.remotetask.HttpRemoteTask.sendUpdate(HttpRemoteTask.java:514)
{code}

A java error file was created in the location : s3://tvlk-data-qubole-prod/qubole/account_id/7012/logs/presto/494030/192.169.67.181

seems like its hitting the limit in :
https://github.com/airlift/airlift/blob/master/http-client/src/main/java/io/airlift/http/client/HttpClientConfig.java#L48

Please let us know If we should increase the limit of : http-client.max-requests-queued-per-destination

If so how to do that (presto overrides?).

Please let me know if any further details required.

",,satyavathib,stagra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,traveloka,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04xnz:,,,,,,,,,,,,,,,,,,,1.0,15048,2017-06-05 22:50:04.507,,,"05/Jun/17 10:50 PM;stagra;http-client.max-requests-queued-per-destination should go in config.properties section.

But another thing to look out for is why is this happening? Is this due to high concurrency? If so, we should ask them to add resource queues.","06/Jun/17 2:14 AM;satyavathib;[~stagra] [~udayk] As far as I could see from the 3 queries that the customer shared with us, 2 of them are hitting the same tables in most parts of the query while the 3rd one is different. Is that possible If many queries querying the same table hitting the same node might be creating multiple requests.

What could be the possible cases that we could check from our side to verify the same. Could we suspect this as a caching issue. If we have to increase the limit how could we arrive at an ideal value to avoid the issue in future.

Please Let us know If we could provide additional data on this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lot of jobs failed with meta exceptions in hive tier,SOL-171,64518,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,navdeepp,navdeepp,05/Jun/17 4:43 AM,13/Jun/17 9:54 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Yesterday a lot of commands failed for underarmour with meta exceptions in a short time window around 13:50-14:20 IST. 

sswong@underarmour.com, 
Account: 1759
metastore host: prod-api-metastore1.XXXXX.us-east-1.rds.amazonaws.com (there were some alert msgs for this host in production channel yesterday around the same time; need to know if this was related) https://qubole.slack.com/archives/C0BM9L6N5/p1496565273289042

Few of commands are,

76733464
{code}
FAILED: SemanticException MetaException(message:Timeout when executing method: get_partitions_by_expr)
org.apache.hadoop.hive.ql.parse.SemanticException: MetaException(message:Timeout when executing method: get_partitions_by_expr)
…
Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: MetaException(message:Timeout when executing method: get_partitions_by_expr)
{code}
Hive-tier logs:
{code}
2017-06-04 08:55:27,146 [command:76737543] ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(173)) - Retrying HMSHandler after 2000 ms (attempt 4 of 10) with error: javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MPartition@1e42769e"" using statement ""INSERT INTO `PARTITIONS` (`PART_ID`,`PART_NAME`,`CREATE_TIME`,`TBL_ID`,`SD_ID`,`LAST_ACCESS_TIME`) VALUES (?,?,?,?,?,?)"" failed : Lock wait timeout exceeded; try restarting transaction
2017-06-04 08:55:29,147 [command:76737543] INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(748)) - 0: append_partition : db=uacf_prod_derived tbl=mfp_paid_subscription_created_avro[2017-06-04,07]
{code}
76734100
{code}
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don't support retries at the client level.)
org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:For direct MetaStore DB connections, we don't support retries at the client level.)
{code}

76733361
{code}FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:javax.jdo.JDOUserException: One or more instances could not be made persistent
…
Nested Throwables StackTrace:
java.sql.SQLException: Lock wait timeout exceeded; try restarting transaction
{code}
76733326
{code}org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:For direct MetaStore DB connections, we don't support retries at the client level.)
…
Caused by: MetaException(message:For direct MetaStore DB connections, we don't support retries at the client level.)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.reconnect(HiveMetaStoreClient.java:308)
{code}

For more commands filter by date:4June, failed",api.qubole.com,navdeepp,venkatak,,,,,,,,,,,,,,,,,,,,,,,HIVE-1539,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,underarmour,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04xdr:,,,,,,,,,,,,,,,,,,,1.0,15030,2017-06-13 21:54:23.245,,,13/Jun/17 9:54 PM;venkatak;Linked the existing Hive JIRA for this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark command kept on running for a long time and failed with ""java.io.IOException: Broken pipe""",SOL-169,64390,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,navdeepp,navdeepp,02/Jun/17 3:07 AM,02/Jun/17 7:25 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command: 74334247
Scheduler Job Id: 18563, spark 2.0
Application Logs: s3://as-qubole-prod/logs/hadoop/22809/479334/app-logs/asreport/logs/application_1495570286886_0007

The above job kept on running for a long time(>7hr) then regular run time(1.25 hrs) and finally failed with the following stack trace before it was cancelled by the user. The command produced 54M of logs at s3://as-qubole-prod/tmp/2017-05-24/6285/74334247.err.

{code}
App > 17/05/24 12:17:38 dispatcher-event-loop-29 INFO TaskSetManager: Starting task 37437.0 in stage 29.0 (TID 53426, ip-172-31-41-112.us-west-2.compute.internal, partition 37437, RACK_LOCAL, 5888 bytes)
App > 17/05/24 12:21:12 DataStreamer for file /spark-history/application_1495570286886_0007.lz4.inprogress.las block BP-1198060931-172.31.41.188-1495570244400:blk_1073741979_1155 WARN DFSClient: DataStreamer Exception
App > java.io.IOException: Broken pipe
App >   at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
App >   at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
App >   at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
App >   at sun.nio.ch.IOUtil.write(IOUtil.java:65)
App >   at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:487)
App >   at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
App >   at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
App >   at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
App >   at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
App >   at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
App >   at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
App >   at java.io.DataOutputStream.flush(DataOutputStream.java:123)
App >   at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:644)
App > 17/05/24 12:31:24 DataStreamer for file /spark-history/application_1495570286886_0007.lz4.inprogress block BP-1198060931-172.31.41.188-1495570244400:blk_1073741988_1164 WARN DFSClient: DataStreamer Exception
App > java.io.IOException: Broken pipe
App >   at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
App >   at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
App >   at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
App >   at sun.nio.ch.IOUtil.write(IOUtil.java:65)
App >   at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:487)
App >   at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
App >   at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
App >   at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
App >   at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
App >   at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
App >   at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
App >   at java.io.DataOutputStream.flush(DataOutputStream.java:123)
App >   at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:644)App > 17/05/24 12:55:03 Yarn application state monitor ERROR YarnClientSchedulerBackend: Yarn application has already exited with state FINISHED!
App > 17/05/24 13:16:58 Yarn application state monitor INFO ServerConnector: Stopped ServerConnector@1ae78d87{HTTP/1.1}{0.0.0.0:4040}
App > 17/05/24 13:19:23 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@6aa27802{/stages/stage/kill,null,UNAVAILABLE}
App > 17/05/24 13:20:08 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@cd6cecb{/api,null,UNAVAILABLE}
App > 17/05/24 13:20:36 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@3718104f{/,null,UNAVAILABLE}
App > 17/05/24 13:21:36 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@2e6dfd69{/static,null,UNAVAILABLE}
App > 17/05/24 13:23:42 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@5cf64dcd{/executors/threadDump/json,null,UNAVAILABLE}
App > 17/05/24 13:23:56 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@3bd5f99b{/executors/threadDump,null,UNAVAILABLE}
App > 17/05/24 13:24:38 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@406b2b07{/executors/json,null,UNAVAILABLE}
App > 17/05/24 13:26:53 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@19d830ca{/executors,null,UNAVAILABLE}
App > 17/05/24 13:27:16 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@1fbe97ae{/environment/json,null,UNAVAILABLE}
App > 17/05/24 13:27:59 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@29f72198{/environment,null,UNAVAILABLE}
App > 17/05/24 13:28:31 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@e2143ff{/storage/rdd/json,null,UNAVAILABLE}
App > 17/05/24 13:29:36 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@5afecc75{/storage/rdd,null,UNAVAILABLE}
App > 17/05/24 13:30:17 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@36ee5c8f{/storage/json,null,UNAVAILABLE}
App > 17/05/24 13:30:48 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@1e4c9973{/storage,null,UNAVAILABLE}
App > 17/05/24 13:32:08 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@7581d322{/stages/pool/json,null,UNAVAILABLE}
App > 17/05/24 13:33:23 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@333d2dad{/stages/pool,null,UNAVAILABLE}
App > 17/05/24 13:34:14 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@173d346e{/stages/stage/json,null,UNAVAILABLE}
App > 17/05/24 13:34:56 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@530d7023{/stages/stage,null,UNAVAILABLE}
App > 17/05/24 13:35:48 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@16bd02f6{/stages/json,null,UNAVAILABLE}
App > 17/05/24 13:36:33 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@d5c0e84{/stages,null,UNAVAILABLE}
App > 17/05/24 13:37:31 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@2e38b699{/jobs/job/json,null,UNAVAILABLE}
App > 17/05/24 13:38:14 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@3e9ed70d{/jobs/job,null,UNAVAILABLE}
App > 17/05/24 13:38:39 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@45377fab{/jobs/json,null,UNAVAILABLE}
App > 17/05/24 13:39:03 Yarn application state monitor INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@2232d6{/jobs,null,UNAVAILABLE}
App > 17/05/24 13:43:09 Yarn application state monitor INFO SparkUI: Stopped Spark web UI at http://172.31.41.167:4040
Qubole > 2017-05-24 13:44:26,950 WARNING shellcli.py:359 - signal_handler - Waiting for JVM to terminate ...
Qubole > Shell Command failed, exit code unknownApp > log4j:WARN No appenders could be found for logger (org.apache.hadoop.ipc.Server).
App > log4j:WARN Please initialize the log4j system properly.
{code}",,navdeepp,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04wr3:,,,,,,,,,,,,,,,,,,,1.0,14875,,,,"02/Jun/17 7:25 AM;navdeepp;datanode logs

{code}
2017-05-24 12:19:52,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1198060931-172.31.41.188-1495570244400:blk_1073741979_1155, type=HAS_DOWNSTREAM_IN_PIPELINE
java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2209)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1164)
        at java.lang.Thread.run(Thread.java:745)
2017-05-24 12:19:52,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-1198060931-172.31.41.188-1495570244400:blk_1073741979_1155
java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.31.41.167:50010 remote=/172.31.41.167:47798]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:467)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:781)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:734)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:235)
        at java.lang.Thread.run(Thread.java:745)
2017-05-24 12:19:52,585 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run():
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:496)
at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        at java.io.DataOutputStream.flush(DataOutputStream.java:123)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:1388)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:1327)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1248)
        at java.lang.Thread.run(Thread.java:745)
2017-05-24 12:19:52,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
2017-05-24 12:19:52,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1198060931-172.31.41.188-1495570244400:blk_1073741979_1155, type=HAS_DOWNSTREAM_IN_PIPELINE
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:496)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        at java.io.DataOutputStream.flush(DataOutputStream.java:123)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:1388)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:1327)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1248)
at java.lang.Thread.run(Thread.java:745)
2017-05-24 12:19:52,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1198060931-172.31.41.188-1495570244400:blk_1073741979_1155, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2017-05-24 12:19:52,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1198060931-172.31.41.188-1495570244400:blk_1073741979_1155 received exception java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.31.41.167:50010 remote=/172.31.41.167:47798]
2017-05-24 12:19:52,593 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: ip-172-31-41-167.us-west-2.compute.internal:50010:DataXceiver error processing WRITE_BLOCK operation  src: /172.31.41.167:47798 dst: /172.31.41.167:50010
java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.31.41.167:50010 remote=/172.31.41.167:47798]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:467)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:781)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:734)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:235)
        at java.lang.Thread.run(Thread.java:745)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Notebook UI showing ""Error"" for a successful run in paragraph",ZEP-986,64208,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Minor,,rgupta,navdeepp,navdeepp,31/May/17 2:24 AM,31/May/17 5:12 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Account: 6390 (nxus-data@mpiremedia.com.au)
Notebook: 38378 (Para 2,3)

All paragraphs result in error status after successful completion.

I see the following in Interpreter logs when a paragraph run call happens:

{code}
INFO [2017-05-31 08:54:59,403] ({pool-1-thread-5} FIFOScheduler.java[submit]:103) - Success to submit job : 20170531-025431_824849955
 INFO [2017-05-31 08:54:59,403] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:129) - Job remoteInterpretJob_1496220899403 started by scheduler interpreter_495352832
 INFO [2017-05-31 08:54:59,406] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:135) - Job remoteInterpretJob_1496220899403 finished by scheduler interpreter_495352832
ERROR [2017-05-31 08:54:59,409] ({pool-1-thread-5} TThreadPoolServer.java[run]:296) - Error occurred during processing of message.
org.apache.zeppelin.interpreter.InterpreterException: java.io.IOException: Cannot run program ""R"": error=2, No such file or directory
	at org.apache.zeppelin.interpreter.ClassloaderInterpreter.getScheduler(ClassloaderInterpreter.java:146)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.getScheduler(LazyOpenInterpreter.java:136)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer.getStatus(RemoteInterpreterServer.java:457)
	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Processor$getStatus.getResult(RemoteInterpreterService.java:1483)
	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Processor$getStatus.getResult(RemoteInterpreterService.java:1468)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Cannot run program ""R"": error=2, No such file or directory
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)
	at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69)
	at org.apache.zeppelin.rinterpreter.RContext$.apply(RContext.scala:293)
	at org.apache.zeppelin.rinterpreter.RInterpreter.rContext$lzycompute(RInterpreter.scala:44)
	at org.apache.zeppelin.rinterpreter.RInterpreter.rContext(RInterpreter.scala:44)
	at org.apache.zeppelin.rinterpreter.RInterpreter.getScheduler(RInterpreter.scala:95)
	at org.apache.zeppelin.rinterpreter.KnitR.getScheduler(KnitR.java:98)
	at org.apache.zeppelin.interpreter.ClassloaderInterpreter.getScheduler(ClassloaderInterpreter.java:144)
	... 10 more
Caused by: java.io.IOException: error=2, No such file or directory
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:186)
	at java.lang.ProcessImpl.start(ProcessImpl.java:130)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)
	... 17 more
{code}

When the interpreter initialises it has some warning msgs and errors:

{code}
WARN [2017-05-31 09:01:52,900] ({pool-2-thread-2} AbstractLifeCycle.java[setFailed]:212) - FAILED org.spark_project.jetty.server.Server@2347c5e1: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.spark_project.jetty.server.Server.doStart(Server.java:366)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:367)

...
...
ERROR [2017-05-31 09:02:12,458] ({pool-2-thread-2} Utils.java[invokeMethod]:42) - org.apache.spark.sql.SparkSession.wrapped()
java.lang.NoSuchMethodException: org.apache.spark.sql.SparkSession.wrapped()
	at java.lang.Class.getMethod(Class.java:1665)
	at org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:40)
	at org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:35)
	at org.apache.zeppelin.spark.SparkInterpreter.getSQLContext_2(SparkInterpreter.java:352)
	at org.apache.zeppelin.spark.SparkInterpreter.getSQLContext(SparkInterpreter.java:340)
	at org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:938)
	at org.apache.zeppelin.interpreter.ClassloaderInterpreter.open(ClassloaderInterpreter.java:74)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
	at org.apache.zeppelin.spark.PySparkInterpreter.getSparkInterpreter(PySparkInterpreter.java:533)
	at org.apache.zeppelin.spark.PySparkInterpreter.createGatewayServerAndStartScript(PySparkInterpreter.java:198)
	at org.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:176)
	at org.apache.zeppelin.interpreter.ClassloaderInterpreter.open(ClassloaderInterpreter.java:74)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:316)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:182)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:160)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code} ",,namanm,navdeepp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04vqn:,,,,,,,,,,,,,,,,,,,1.0,14948,2017-05-31 03:59:50.857,,,"31/May/17 3:59 AM;namanm;The above error occurred because R is not present on the cluster. In the node bootstrap, the customer has deleted few R packages and tried to download it from their machine which failed.
```scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -rp ubuntu@172.31.52.115:/home/ubuntu/R /tmp/.```

In node bootstrap logs, we can see:
```ssh: connect to host 172.31.52.115 port 22: Connection timed out^```

When starting spark interpreter JVM, it tries to load R which isn't found, hence the error.

","31/May/17 4:34 AM;navdeepp;Since it is a pyspark command it was successfully finished but the paragraph status was still showing error, because of interpreter errors. Is there a way to mark it successful or do not run the paragraph if one of the executable is missing. As the same interpreter is used for R and pyspark I will try to load both, does not look like if we can do anything about it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.OutOfMemoryError: No enough memory for aggregation,SPAR-1659,64155,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,venkats,mstolpner,mstolpner,30/May/17 4:24 PM,25/Jul/17 12:12 AM,09/Aug/17 6:03 AM,,,,,0,in-r47,jira_escalated,,,,,,,"The problem happens on Spark 2.1. 

Notebook https://api.qubole.com/notebooks#home?id=37757&type=my-notebooks. Cluster 22399. User: fbeekhof@expedia.com

The error message seems very similar to https://issues.apache.org/jira/browse/SPARK-18208. However, we checked and the code fixing this issue is in our Spark 2.1 in bitbucket. 

The error happens when count() or show() is called on a dataset returned by rev_plus_features(). The function is defined in paragraph #3. 

Originally, the first paragraph had limit(100000). I changed it to limit(10) without any effect.

Error message:

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 725, 10.23.11.30, executor 12): java.lang.OutOfMemoryError: No enough memory for aggregation
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:731)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:606)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:163)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:101)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1461)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1449)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1448)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1448)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:828)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:828)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:828)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1676)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1631)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1620)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:633)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2071)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2084)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2097)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:58)
  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)
  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:636)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:595)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:604)
  ... 55 elided
Caused by: java.lang.OutOfMemoryError: No enough memory for aggregation
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:731)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:606)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:163)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:101)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)
  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:99)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
  ... 3 more
{code}

",,gayathrym,mahuja,mpatel,mstolpner,rohitk,venkats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Expedia,,,,,"{repository={count=8, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":8,""lastUpdated"":""2017-07-27T16:30:36.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":8,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04vjb:,,,,,,,,,,,,,,,,,,,1.0,14908,2017-06-30 12:44:18.47,,,"30/May/17 4:26 PM;mstolpner;Container log 
{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 725, 10.23.11.30, executor 12): java.lang.OutOfMemoryError: No enough memory for aggregation
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
	at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:731)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:606)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:163)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:101)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1461)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1449)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1448)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1448)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:828)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:828)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:828)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1676)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1631)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1620)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:633)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2071)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2084)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2097)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:58)
  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)
  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:636)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:595)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:604)
  ... 55 elided
Caused by: java.lang.OutOfMemoryError: No enough memory for aggregation
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:714)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:675)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:837)
  at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:870)
  at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:731)
  at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:606)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:163)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1.apply(SortMergeJoinExec.scala:101)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)
  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:99)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
  ... 3 more
{code}","28/Jun/17 11:47 AM;mstolpner;Moving the notebook to Spark 2.1 seems to solve this issue. However, the same notebook is facing a different issue now. Below is the  stack trace. It seems like a known issue https://issues.scala-lang.org/browse/SI-9218. 

{code}
java.lang.IndexOutOfBoundsException: 2
  at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)
  at scala.collection.immutable.List.apply(List.scala:84)
  at org.apache.spark.sql.catalyst.expressions.BoundReference.doGenCode(BoundAttribute.scala:64)
  at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
  at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$createJoinKey$1.apply(SortMergeJoinExec.scala:335)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$createJoinKey$1.apply(SortMergeJoinExec.scala:335)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.createJoinKey(SortMergeJoinExec.scala:335)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.genScanner(SortMergeJoinExec.scala:370)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce(SortMergeJoinExec.scala:513)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce(SortMergeJoinExec.scala:35)
  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:218)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:146)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:218)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:146)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
  at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
  at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:313)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:354)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:225)
  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:272)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:58)
  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)
  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2405)
  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2404)
  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)
  at org.apache.spark.sql.Dataset.count(Dataset.scala:2404)
  ... 53 elided
{code}","30/Jun/17 12:44 PM;mahuja;This looks very similar to https://issues.apache.org/jira/browse/SPARK-17685. 

[~mstolpner] we can give a consolidated patch for Expedia that combines SPAR-1750 and SPAR-1659. Let us know your thoughts","03/Jul/17 12:02 PM;venkats;[~mstolpner] are they using Spark-2.1.1?

Custom package for this issue on Spark-2.1.0 can be deployed to a cluster by updating cluster_configs table's spark_s3_package_name with

This package has fixes for both SPAR-1659 and SPAR-1660 built on top of Spark-2.1.0

{noformat}
2.1.0-SPAR-1659
{noformat}

{code:java}
update cluster_configs set spark_s3_package_name='2.1.0-SPAR-1659', updated_at=now()  where cluster_id=user-cluster-id and id=latest-cluster-config-id limit 1 ;
{code}
","12/Jul/17 8:01 AM;mstolpner;With the new package `2.1.0-SPAR-1659`, the notebook is back to `No enough memory for aggregation` problem. 

I had to clone the notebook due to a different issue: https://api.qubole.com/notebooks#home?id=41821&type=my-notebooks ","25/Jul/17 12:12 AM;gayathrym;Commit to MASTER :
*  55cda8f263084aef7263816ade2eda66347d6bc0	Mon Jul 24 19:08:59 2017	spark	q-spark-2.1.0	SPAR-1659	vsowrirajan@qubole.com	dev: fix: SPAR-1659: Backport from OS 2.2.0 to 2.1.0. IndexOutOfBoundException in SortMergeJoinExec in createJoinKey					",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cron scheduling error for overlapping jobs with the same interpreter,ZEP-984,64138,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,rgupta,mstolpner,mstolpner,30/May/17 7:53 AM,30/May/17 8:05 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Expedia is getting this error for a cron scheduled notebook with "" auto-restart interpreter on cron execution"":
{code}
Interpreter JVM has stopped responding. This generally happens if spark driver has run out of memory.
Try rerunning paragraph after increasing value of spark.driver.memory in interpreter settings page.
Drop us a mail at help@qubole.com with notebook link for root cause analysis.
{code}

They have another notebook scheduled at the same time. Please see Zendesk https://qubole.zendesk.com/agent/tickets/14902. Notebook https://api.qubole.com/notebooks/#home?id=37209&type=my-notebooks. 

I created two notebooks and scheduled them an the same time with ""auto-restart interpreter on cron execution"". Both notebooks used the same interpreter. I got the following error:
{code}org.apache.thrift.transport.TTransportException{code}",,mstolpner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04vfj:,,,,,,,,,,,,,,,,,,,1.0,14902,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refresh Table Through API is not populating all fields. ,MW-1059,64000,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sumitm,nishantp,nishantp,29/May/17 6:19 AM,17/Jul/17 12:53 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Refresh Table Through API is not populating all fields.  Command id 75479890.
In UI table name is not populated and in DB loader_stable field is not populated.

Ex API Call : 

curl -i -X POST -H ""X-AUTH-TOKEN: ZQyVtb3K3TPx5xVv3dkPseAjTggMEpn5oWcTDEMpeJefVqUCQFLzVZRtqtsgasyr"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -d ' {""db_name"":""default"", ""hive_table"":""default_qubole_memetracker"", ""loader_stable"":""1"", ""loader_stable_mult"":""63"", ""template"":""s3import""} ' ""https://api.qubole.com/api/v1.2/commands""

",api.qubole.com,navdeepp,nishantp,psrinivas,rgupta,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04uyn:,,,,,,,,,,,,,,,,,,,1.0,14893,2017-05-29 22:38:33.236,,,29/May/17 10:10 PM;nishantp;Looks MW is not populating some fields in DB and hence they are not visible on UI.,"29/May/17 10:38 PM;sumitm;Not sure why but looks like there is a need of an extra param called ""*create_loader*"" needed to set table name in DB.

cc - [~rgupta] who wrote [this|https://bitbucket.org/qubole/qbol/src/a8c4ddddfa891cd32a370ee25c8f938230ce3a0f/tapp/app/helpers/hive_command_helper.rb?at=master&fileviewer=file-view-default#hive_command_helper.rb-66] piece of code",29/May/17 11:13 PM;rgupta;this loader stuff i guess i wrote long back. But I think we moved away from this loader thingy. Hadoop cluster is started by hive itself and refresh table command in hive should not start a cluster....,"29/May/17 11:23 PM;nishantp;Yes, it does not require any hadoop job but it start cluster. Due to this customer complains about cluster is always up and running. For workaround suggested to customer to use very small cluster(1 master & 1 slave).
If we can achieve this without starting cluster then it will be great.",29/May/17 11:25 PM;sumitm;[~psrinivas] is this some new change that start cluster for such commands?,30/May/17 1:53 AM;navdeepp;This only happens when hive on master is enabled on account level. HIVE-2213,"30/May/17 1:50 PM;psrinivas;Guys, the definition of HOM is, to run Hive on master node, so we have to start the cluster. ","02/Jun/17 12:55 PM;psrinivas;cc: [~rvenkatesh]
[~navdeepp]: Can you please move this to MW project. ","02/Jun/17 6:08 PM;sumitm;[~psrinivas] [~rgupta] so are we good to remove that check of ""loader"" thing?","17/Jul/17 12:53 AM;navdeepp;This is still an issue for taveloka, 84658615",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Irregular indentation in comments causes queries to return ""blank line"" results instead of  ""Empty""",PRES-1098,63980,Bug,Open,PRES,Presto,software,stagra,,,Minor,,sumitm,navdeepp,navdeepp,29/May/17 2:33 AM,01/Jun/17 4:00 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"When a query returns no records we show it as ""Empty"" in the UI. But when there are irregular indentations in comments, before the start of the query, it returns ""empty line"" instead of ""Empty"" string. Screenshots attached. (Indentation at line 2 in comments before the query)

Looked in to the results file in s3, there is an extra empty line being written to it for the exceptional case.",,aswina,navdeepp,psrinivas,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/May/17 2:30 AM;navdeepp;Screen Shot 2017-05-29 at 2.59.18 PM.png;https://qubole.atlassian.net/secure/attachment/43023/Screen+Shot+2017-05-29+at+2.59.18+PM.png,29/May/17 2:30 AM;navdeepp;Screen Shot 2017-05-29 at 2.59.38 PM.png;https://qubole.atlassian.net/secure/attachment/43022/Screen+Shot+2017-05-29+at+2.59.38+PM.png,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04uu7:,,,,,,,,,,,,,,,,,,,1.0,14797,2017-05-29 03:03:30.164,,,29/May/17 3:03 AM;aswina;`getresult.json` API returns the response & UI is displaying it as expected.  Moving it to MW for further investigation.,29/May/17 8:24 PM;sumitm;Looks like it needs to be handled at Hive side.. cc [~psrinivas],"30/May/17 11:54 AM;psrinivas;[~sumitm]: This is Presto query, not Hive. ",01/Jun/17 4:00 AM;navdeepp;Moving this to PRES as does not happen in hive.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive command was not cancelled though it shows it happened on UI,QTEZ-169,63949,Bug,Open,QTEZ,qbol Tez,software,adas,,,Major,,adas,sbadam,sbadam,26/May/17 4:57 PM,28/May/17 12:23 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Hive was not cancelled though it shows it happened on UI. Looks like AM didn't receive kill command from RM. Took thread dumps of hive_cli and AM process which was running the command.

Command ID: https://api.qubole.com/super_admin/query_hists/74757074/redirect
Cluster: 22808, Cluster Inst: 481849
Logs are stored at: s3://as-qubole-prod/logs/hadoop/22808/481849
Master node: ec2-54-149-21-218.us-west-2.compute.amazonaws.com
AM for this command: ec2-54-187-88-129.us-west-2.compute.amazonaws.com

Hive CLI Dump: s3://as-qubole-prod/logs/hadoop/22808/481849/ec2-54-149-21-218.us-west-2.compute.amazonaws.com.master/hive_cli_pid_50655_cmd_74757074_thread_dump

AM dump1: s3://as-qubole-prod/logs/hadoop/22808/481849/ec2-54-187-88-129.us-west-2.compute.amazonaws.com.node0001/DAGAppMaster_pid_7316_ip-172-31-40-187.us-west-2.compute.internal_ThreadDump.log

AM dump2: s3://as-qubole-prod/logs/hadoop/22808/481849/ec2-54-187-88-129.us-west-2.compute.amazonaws.com.node0001/DAGAppMaster_pid_7316_ip-172-31-40-187.us-west-2.compute.internal_ThreadDump.log.1

You can download from web node using below command:

{code:java}

/usr/lib/hive_scripts/storagecli.py -a 6285 -c=""-get <s3_path>""
{code}



",,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/May/17 12:22 PM;sbadam;Tez_AM_logs.txt.gz;https://qubole.atlassian.net/secure/attachment/43016/Tez_AM_logs.txt.gz,,,,,,,,,,,,,AWS,,,,,,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04unb:,,,,,,,,,,,,,,,,,,,1.0,14912,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select without table name query fails in MR,HIVE-2204,63925,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,nitink,satyavathib,satyavathib,26/May/17 6:00 AM,08/Jun/17 12:07 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command ID : 74457960

This works in beeline but fails in qubole. As discussed with [~adeshr] it is because we convert everything as INSERT OVERWRITE and so the customer need to specify a random table name as:

{code}
`select to_json(named_struct(""hello"",""123"")) from some_table` 
{code}

Please let me know If any other detail required.",,nitink,psrinivas,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,lyft,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04uhz:,,,,,,,,,,,,,,,,,,,1.0,14868,2017-06-07 23:10:53.524,,,"07/Jun/17 11:10 PM;nitink;[~rvenkatesh] [~satyavathib] [~venkatak]

Tried the following and it passed

{code:sql}
- normal IOW  without from
insert overwrite directory 's3://lyftqubole-iad/temp/qubole_temp' select ""1"";

- IOW without from , but with UDF 
insert overwrite directory 's3://lyftqubole-iad/temp/qubole_temp' select factrorial(4);
{code}


The queru user is firing has a custom UDF being used and issue is reproducible with custom UDF user is passing .
The Jar is located at s3://lyftqubole-iad/jars/dataplatform/lyft-udfs-0.1.0-jar-with-dependencies.jar. Can we have the source-code of the UDFs used in query ?

","07/Jun/17 11:19 PM;nitink;In the meantime, I will also try to debug the issue by using their UDF jar in my local setup  ","08/Jun/17 12:07 AM;nitink;I got the custom UDF they are using . To reproduce the issue

{code}
add jar s3://dev.canopydata.com/nitink/jars/udf/brickhouse-0.6.0.jar;
CREATE TEMPORARY FUNCTION to_json AS 'brickhouse.udf.json.ToJsonUDF';
insert overwrite directory 's3://dev.canopydata.com/nitink/temp/nofrom' select to_json(named_struct('hello','123'));
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing Create Schema in Explore,SOL-160,63924,Bug,To Do,SOL,Solutions,software,Jtrail,,,Critical,,Jtrail,mayureshp,addon_zendesk_for_jira,26/May/17 5:42 AM,26/May/17 6:05 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Team Treveloka is facing issue in Qubole Explore section.
Unable to see aws s3 bucket nor able to create schema. 
This is happening for most of users and traveloka account, as per them they were able to do it before. 
AWS S3 Bucket is not available to system-admin user as well.
Few of the system-admin users are not able to see the Account settings options in control panel section as well.






",,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/May/17 5:47 AM;mayureshp;Screen Shot 2017-05-26 at 6.15.17 PM.png;https://qubole.atlassian.net/secure/attachment/43004/Screen+Shot+2017-05-26+at+6.15.17+PM.png,26/May/17 6:02 AM;mayureshp;Screen Shot 2017-05-26 at 6.28.22 PM.png;https://qubole.atlassian.net/secure/attachment/43005/Screen+Shot+2017-05-26+at+6.28.22+PM.png,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04uhr:,,,,,,,,,,,,,,,,,,,1.0,14903,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduled jobs using cron expression working incorrectly,SCHED-146,63838,Bug,Open,SCHED,Scheduler,software,sumitm,,,Major,,sumitm,Kulbir,Kulbir,25/May/17 7:48 PM,30/Jun/17 12:53 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Sample job:
https://api.qubole.com/v2/scheduler#!/summary/19091?status=All&user_id=All&type=All&id=&name=

Repro steps:
-Create a scheduled job initially using no cron, for e.g created one to run every 6 hours
-Verified that in the DB table frequency and time_unit are set correctly, in this case 360 and minutes respectively
-Now edit the scheduled job and change it to use cron instead
-DB will now have cron_expression field set however frequency\time_unit are also kept around

This is problematic because in PeriodicJobMapper.java we evaluate both frequency as well as cron expression and pass it on to FactoryFrequency code, where period overrides the cron expression. As a result scheduled job continues to run using old settings vs. using specified cron expression, contrary to what user will expect.

Doing opposite, i.e creating scheduled job as cron initially and then changing to diff. schedule type works fine and we correctly set the corresponding DB field(cron_expression) to null.

As a solution to this issue it seems like we need to set the frequency\time_unit to be NULL whenever cron expression is specified for a scheduled job and saved.

",,Kulbir,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Samsung,,,,,{},NA,Choose from,,,,,,.,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04u5b:,,,,,,,,,,,,,,,,,,,1.0,14841,,,,"30/Jun/17 12:53 PM;Kulbir;[~sumitm] any updates ?
Any ETA on when this will be fixed ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to download results as csv for commands using python-sdk,SDK-210,63740,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,abhijitj,cbalchandani,cbalchandani,24/May/17 11:54 AM,18/Jun/17 8:39 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"# Commands (especially the ones with larger resultset, for eg 73935164) give error while trying to download the resultset as csv using:

*python download_results.py --token <auth_token> --commandid <commandid>*

error message: 

Downloading result for command id: 73935164
Traceback (most recent call last):
  File ""download_results.py"", line 32, in <module>
    downloadresults(opts.commandid)
  File ""download_results.py"", line 20, in downloadresults
    c.get_results(fp=resultsfp, delim=',', inline=False)
  File ""/usr/local/lib/python2.7/site-packages/qds_sdk/commands.py"", line 245, in get_results
    skip_data_avail_check=isinstance(self, PrestoCommand))
  File ""/usr/local/lib/python2.7/site-packages/qds_sdk/commands.py"", line 1329, in _download_to_local
    complete_data_available = _is_complete_data_available(bucket_paths, num_result_dir)
  File ""/usr/local/lib/python2.7/site-packages/qds_sdk/commands.py"", line 1290, in _is_complete_data_available
    file = int(path[1])
ValueError: invalid literal for int() with base 10: 'cdbfca4f-52e4-42de-88cb-515c14ea29cf_00dd4737-307d-4321-b227-d5fe5ab8edc0'
",,abhijitj,cbalchandani,karthikk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,traveloka,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04v6h:,,,,,,Automation_Jun8-Jun16,Automation_Jun19-Jun29,,,,,,,,,,,,1.0,14788,2017-06-16 02:33:40.699,,,"16/Jun/17 2:33 AM;abhijitj;Haven't got a chance to work on this, will try to take it up in the next sprint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Different output for ""S3"" bucket on explore vs analyze",UI-6104,63654,Bug,Open,UI,UI,software,aswina,,,Major,,paritap,megha,megha,23/May/17 4:11 PM,18/Jul/17 11:36 PM,09/Aug/17 6:03 AM,,,,,0,bootcamp,jira_escalated,,,,,,,"This issue occurs with oracle, when we expand one particular s3 bucket. Explore page clearly displays the child buckets, whereas on analyze ""s3"" the child buckets are not being displayed..Example in attached screenshot 
user: justin.wainwright@oracle.com
Account: dev-tech-core-shared
s3 bucket name: s3://dlx-dev-core-shared

[~sumitm] confirmed that that both pages are calling same API, and getting same API response
but analyse-s3 not rendering it


",,aswina,megha,mmajithia,nimitk,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/May/17 4:08 PM;megha;UIexample.tiff;https://qubole.atlassian.net/secure/attachment/42721/UIexample.tiff,,,,,,,,,,,,,AWS,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04t93:,,,,,,,,,,,,,,,,,,,1.0,14763,2017-07-17 23:57:30.094,,,"17/Jul/17 11:57 PM;aswina;Assigning this to [~aishwarya] for further debugging.

cc: [~sumitm]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Issue with reading from s3 when secret key contains a '/', even if encoded",SPAR-1635,63647,Bug,Reopened,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,hiyer,cbalchandani,cbalchandani,23/May/17 11:00 AM,26/May/17 10:55 AM,09/Aug/17 5:30 AM,,,,,0,jira_escalated,jira_update,,,,,,,"403 error encountered while trying to read from s3 having a '/' in secret key, even if encoded.
(https://issues.apache.org/jira/browse/HADOOP-3733)",,cbalchandani,hiyer,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,reltio,,,,,{},NA,Yes - Documentation,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04t7j:,,,,,,,,,,,,,,,,,,,1.0,14803,2017-05-23 11:25:46.001,,,23/May/17 11:25 AM;rohitk;CC: [~hiyer] [~abhishekmodi] When are you guys upgrading to 2.8?,23/May/17 11:36 AM;hiyer;It will take a couple of months at least. If the fix is urgently required we can pull it into our current hadoop as well.,"23/May/17 11:42 AM;hiyer;Btw this bug is only hit if you use storage location as accesskey:secretkey@bucket. Is that what the user was doing? If so, can they try providing the keys using -D instead?","23/May/17 1:17 PM;rohitk;Thanks [~hiyer]
Does this works for you [~cbalchandani]?
","23/May/17 1:26 PM;cbalchandani;Yes the user is trying to access s3 in exactly same way.
I am not sure how to provide keys using -D. Is there any documentation for the same?","23/May/17 10:30 PM;hiyer;In hadoop it's done using ""-D fs.s3n.awsAccessKeyId=<access-key> -D fs.s3n.awsSecretAccessKey=<secret-key>"". [~rohitk] can you jot down how it should be done for spark in this case?","23/May/17 11:04 PM;rohitk;Sure [~hiyer]. I will take this up from here. Thanks.
[~cbalchandani] First question is what is user trying to accomplish? 
These setting as configured with Qubole Accounts are already part of standard configuration files. So access to s3 should work without specifying these properties. 
The only practical use case is when user wants to access data which it cannot access by using the credentials  associated with the  account. If that is the case, the following options will work.
# Specify credentials as part of the URI. (Not working because of encoding issues, but works otherwise)
# Set these properties with -D mode as described by [~hiyer] for the following spark variables spark.executor.extraJavaOptions  &
spark.driver.extraJavaOptions. These can be set per cluster in cluster config or specified per job in analyse or set in interpreter settings for Zeppelin 
#  Set these properties via spark hadoop job overrides per job or set in interpreter settings for Zeppelin  by passing spark.hadoop.fs.s3.awsAccessKeyId, spark.hadoop.fs.s3.awsAccessKeyId, spark.hadoop.fs.s3n.awsSecretAccessKey and spark.hadoop.fs.s3.awsSecretAccessKey
# Override these properties in the spark/scala code. 
{code:java}
sc.hadoopConfiguration.set(""fs.s3n.awsAccessKeyId"", “KEY”)
sc.hadoopConfiguration.set(""fs.s3.awsAccessKeyId"", “KEY”)
sc.hadoopConfiguration.set(""fs.s3n.awsSecretAccessKey"", “Secret”)
sc.hadoopConfiguration.set(""fs.s3.awsSecretAccessKey"", “Secret”)
{code}","23/May/17 11:25 PM;rohitk;When user wants to access data which they cannot access by using the credentials associated with the account, he following options will work.
1. Specify credentials as part of the URI. (Not working because of encoding issues, but works otherwise)
2. Set these properties with -D mode as described by Hariharan Iyer for the following spark variables spark.executor.extraJavaOptions &
spark.driver.extraJavaOptions. These can be set per cluster in cluster config or specified per job in analyse or set in interpreter settings for Zeppelin
3. Set these properties via spark hadoop job overrides per job or set in interpreter settings for Zeppelin by passing spark.hadoop.fs.s3.awsAccessKeyId, spark.hadoop.fs.s3.awsAccessKeyId, spark.hadoop.fs.s3n.awsSecretAccessKey and spark.hadoop.fs.s3.awsSecretAccessKey
4.Override these properties in the spark/scala code.
sc.hadoopConfiguration.set(""fs.s3n.awsAccessKeyId"", “KEY”)
sc.hadoopConfiguration.set(""fs.s3.awsAccessKeyId"", “KEY”)
sc.hadoopConfiguration.set(""fs.s3n.awsSecretAccessKey"", “Secret”)
sc.hadoopConfiguration.set(""fs.s3.awsSecretAccessKey"", “Secret”)","26/May/17 9:26 AM;cbalchandani;The exact use case is(in customer's words):
""""
Our customers will have files on their AWS account and they want to upload those files to our account.
""""
So two sets of AWS keys are involved and the above approach mentions using only one set of credentials.

This  [open issue in Spark|https://issues.apache.org/jira/browse/SPARK-20153] suggests Hadoop 2.8 as a resolution.
","26/May/17 10:55 AM;rohitk;Assigning to [~hiyer]. Please see if we can port the specific patch from Hadoop 2.8  or support Hadoop 2.8.
CC: [~mahuja]

[~cbalchandani] Can you check with customer:
1)  if their ""proposed"" method actually works when keys don't have  slash character?
2)  Why can't they split their work into into two jobs? One to fetch files from s3 to hdfs and second to upload files from hdfs to s3. Each of the s3 operations can use different credentials. 




",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encountered too many errors talking to a worker node,PRES-1071,63526,Bug,Open,PRES,Presto,software,stagra,,,Critical,,stagra,mdaurangzeb,mdaurangzeb,22/May/17 8:01 PM,08/Aug/17 11:11 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"description:Customer (Traveloka) moved their Main Holistic Reporting Presto Cluster to new Account (7012) to remove any potential of user accessing the cluster and disturb the cluster performance. But this error came up again, so many times this morning. The new cluster is 31466. 
The error command IDs are:

74036074 
74036075 
74036076 
74036077 
74036078 
74036079 
74036080 
74036081

74059947 
74059948

74059950 
74059951 
74059952 
74059953 
74059954 
74059955

74060316

74060320

74060322

74064142

74064617

74064631

74064640

74064644 
74064665

74064969

74064971 
74064972

74064983 
74064984

74078822

74078963

74079118

74084000

74086961

74087618

74087620 
74087621

74088060


Query 20170522_190623_00032_qity5 failed: Encountered too many errors talking to a worker node. The node may have crashed or be under too much load. This is probably a transient issue, so please retry your query in a few minutes. (http://192.169.73.99:8081/v1/task/20170522_190623_00032_qity5.3.5/results/4/0 - requests failed for 60.07s)
com.facebook.presto.operator.PageTransportTimeoutException: Encountered too many errors talking to a worker node. The node may have crashed or be under too much load. This is probably a transient issue, so please retry your query in a few minutes. (http://192.169.73.99:8081/v1/task/20170522_190623_00032_qity5.3.5/results/4/0 - requests failed for 60.07s)
at com.facebook.presto.operator.HttpPageBufferClient$1.onFailure(HttpPageBufferClient.java:376)
at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: io.airlift.http.client.RuntimeIOException: java.net.SocketTimeoutException: Connect Timeout
at io.airlift.http.client.ResponseHandlerUtils.propagate(ResponseHandlerUtils.java:20)
at com.facebook.presto.operator.HttpPageBufferClient$PageResponseHandler.handleException(HttpPageBufferClient.java:541)
at com.facebook.presto.operator.HttpPageBufferClient$PageResponseHandler.handleException(HttpPageBufferClient.java:528)
at io.airlift.http.client.jetty.JettyHttpClient$JettyResponseFuture.failed(JettyHttpClient.java:870)
at io.airlift.http.client.jetty.JettyHttpClient$BufferingResponseListener.onComplete(JettyHttpClient.java:1104)
at org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:193)
at org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:185)
at org.eclipse.jetty.client.HttpExchange.notifyFailureComplete(HttpExchange.java:269)
at org.eclipse.jetty.client.HttpExchange.abort(HttpExchange.java:240)
at org.eclipse.jetty.client.HttpConversation.abort(HttpConversation.java:141)
at org.eclipse.jetty.client.HttpRequest.abort(HttpRequest.java:708)
at org.eclipse.jetty.client.HttpDestination.abort(HttpDestination.java:267)
at org.eclipse.jetty.client.PoolingHttpDestination.failed(PoolingHttpDestination.java:90)
at org.eclipse.jetty.client.DuplexConnectionPool$1.failed(DuplexConnectionPool.java:159)
at org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:84)
at org.eclipse.jetty.client.HttpClient$1$1.failed(HttpClient.java:572)
at org.eclipse.jetty.client.AbstractHttpClientTransport.connectFailed(AbstractHttpClientTransport.java:152)
at org.eclipse.jetty.client.AbstractHttpClientTransport$ClientSelectorManager.connectionFailed(AbstractHttpClientTransport.java:195)
at org.eclipse.jetty.io.ManagedSelector$Connect.failed(ManagedSelector.java:661)
at org.eclipse.jetty.io.ManagedSelector$Connect.access$1300(ManagedSelector.java:628)
at org.eclipse.jetty.io.ManagedSelector$ConnectTimeout.run(ManagedSelector.java:683)
... 7 more
Caused by: java.net.SocketTimeoutException: Connect Timeout
... 8 more","endy.lambey@traveloka.com

Account (7012)

The new cluster is 31466. 

",mdaurangzeb,sbadam,stagra,vamship,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,activision,traveloka,turner,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04spr:,,,,,,,,,,,,,,,,,,,2.0,"14813,16260",2017-05-22 20:28:38.189,,,"22/May/17 8:28 PM;vamship;'hive.max-partitions-per-writers' is overridden with 10000 which is a 100 times the default value. As we have suggested before, please remove this as this creates lots of thread which leads to memory pressure and thread contention.  ",23/May/17 10:01 PM;stagra;this most likely is happening due to PRES-1058,"24/May/17 6:44 AM;stagra;[~mdaurangzeb] hotfix for PRES-1058 has gone out today. It was a major contributor to ""too many errors talking to worker node"" error. 

I am bringing the priority down for this while we observe their setup for next few days.","24/May/17 7:05 AM;mdaurangzeb;[~stagra] Great News ,Thanks a lot and truly appreciate your sincere support on this.","03/Aug/17 4:21 PM;sbadam;Thanks to confluence page (https://qubole.atlassian.net/wiki/display/ENG/Presto+Debugging+Centre) for listing all  possible reasons for the error - ""Encountered too many errors talking to a worker node"".

It says:

{code:java}
Need to check in the node what went wrong
Node OOM -> will show up in launcher.log of slave node
GC pause on node -> will show up in gc.log of slave node
Spot loss -> will  show up in server.log of master
Master too busy to get HB from the node -> will show up in server.log of master
{code}

Commands - 87899884, 87925565, 87926761

87899884 -> error says:


{code:java}
Encountered too many errors talking to a worker node. The node may have crashed or be under too much load. This is probably a transient issue, so please retry your query in a few minutes. (http://172.31.215.174:8081/v1/task/20170803_092833_00242_bf564.5.24/results/1/1585 - requests failed for 60.05s)
...
{code}

I see that node is added very recently i.e., almost before command failed. It is second 


{code:java}
production-replica> select * from cluster_nodes where cluster_inst_id=567205  and private_ip_address like '172.31.215.174'\G
*************************** 1. row ***************************
                  id: 1686473729
     ec2_instance_id: i-0078007ce1117a7bb
            hostname: ec2-54-202-61-7.us-west-2.compute.amazonaws.com
                role: node0248
amazon_instance_type: r4.8xlarge
       spot_instance: 1
             up_time: 2017-08-03 05:12:16
           down_time: 2017-08-03 09:04:28
      last_seen_time: 2017-08-03 09:03:25
          account_id: 6285
          private_ip: ip-172-31-215-174.us-west-2.compute.internal
     cluster_inst_id: 567205
              status: NULL
  private_ip_address: 172.31.215.174
  termination_reason: User initiated
*************************** 2. row ***************************
                  id: 1686820126
     ec2_instance_id: i-0439deb9c3308e08b
            hostname: ec2-34-213-43-229.us-west-2.compute.amazonaws.com
                role: node0334
amazon_instance_type: r4.8xlarge
       spot_instance: 1
             up_time: 2017-08-03 09:41:32
           down_time: 2017-08-03 09:53:41
      last_seen_time: 2017-08-03 09:53:18
          account_id: 6285
          private_ip: ip-172-31-215-174.us-west-2.compute.internal
     cluster_inst_id: 567205
              status: NULL
  private_ip_address: 172.31.215.174
  termination_reason: User initiated
2 rows in set (0.00 sec)
{code}

-> I don't see any OOM in slave node launcher file(s3://as-qubole-prod/logs/presto/567205/172.31.215.174/launcher.log).
-> No spot node loss during command was run
-> I didn't see any longer pauses in slave node(s3://as-qubole-prod/logs/presto/567205/172.31.215.174/gc.log). 
-> I see this particular error in master server.log(s3://as-qubole-prod/logs/presto/567205/master/server.log-2017-08-03.12.log.gz):


{code:java}
2017-08-03T09:56:37.521Z        WARN    TaskInfoFetcher-20170803_095521_00258_bf564.1.19-49459  com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting info for task 20170803_095521_00258_bf564.1.19: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.145:8081/v1/task/20170803_095521_00258_bf564.1.19
2017-08-03T09:56:37.521Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.2.23-49843      com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.2.23: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.145:8081/v1/task/20170803_095521_00258_bf564.2.23
2017-08-03T09:56:37.521Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.1.19-50315      com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.1.19: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.145:8081/v1/task/20170803_095521_00258_bf564.1.19
2017-08-03T09:56:37.699Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.1.2-49841       com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.1.2: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.209:8081/v1/task/20170803_095521_00258_bf564.1.2
2017-08-03T09:56:37.699Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.2.6-49843       com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.2.6: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.209:8081/v1/task/20170803_095521_00258_bf564.2.6
2017-08-03T09:56:38.459Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.2.23-50040      com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.2.23: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.145:8081/v1/task/20170803_095521_00258_bf564.2.23
2017-08-03T09:56:38.459Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.1.19-49843      com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.1.19: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.145:8081/v1/task/20170803_095521_00258_bf564.1.19
2017-08-03T09:56:38.695Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.1.2-50040       com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.1.2: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.209:8081/v1/task/20170803_095521_00258_bf564.1.2
2017-08-03T09:56:38.695Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.2.6-50282       com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.2.6: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.209:8081/v1/task/20170803_095521_00258_bf564.2.6
2017-08-03T09:56:39.022Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.2.23-50040      com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.2.23: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.145:8081/v1/task/20170803_095521_00258_bf564.2.23
2017-08-03T09:56:39.022Z        WARN    ContinuousTaskStatusFetcher-20170803_095521_00258_bf564.1.19-50282      com.facebook.presto.server.remotetask.RequestErrorTracker       Error getting task status 20170803_095521_00258_bf564.1.19: java.net.SocketTimeoutException: Connect Timeout: http://172.31.215.145:8081/v1/task/20170803_095521_00258_bf564.1.19
...
2017-08-03T09:56:49.501Z        ERROR   exchange-client-22      com.facebook.presto.operator.HttpPageBufferClient       Request to delete http://172.31.215.105:8081/v1/task/20170803_095521_00258_bf564.0.0/results/0 failed java.util.concurrent.CancellationException: Task was cancelled.
2017-08-03T09:56:49.663Z        ERROR   exchange-client-6       com.facebook.presto.operator.HttpPageBufferClient       Request to delete http://172.31.215.145:8081/v1/task/20170803_095521_00258_bf564.1.19/results/0 failed io.airlift.http.client.RuntimeIOException: java.net.SocketTimeoutException: Connect Timeout
2017-08-03T09:56:49.686Z        INFO    query-execution-648     com.facebook.presto.event.query.QueryMonitor    TIMELINE: Query 20170803_092833_00242_bf564 :: Transaction:[9e09884d-07c3-4182-bfa3-424c16f9beef] :: elapsed 1575570ms :: planning 4304ms :: scheduling 3359ms :: running 1567933ms :: finishing 0ms :: begin 2017-08-03T09:28:33.702Z :: end 2017-08-03T09:54:49.272Z
2017-08-03T09:56:50.264Z        ERROR   exchange-client-7       com.facebook.presto.operator.HttpPageBufferClient       Request to delete http://172.31.215.209:8081/v1/task/20170803_095521_00258_bf564.1.2/results/0 failed io.airlift.http.client.RuntimeIOException: java.net.SocketTimeoutException: Connect Timeout
{code}

Same errors for other two commands too. Should I recommend customer to set config - exchange.http-client.request-timeout=120s in cluster overrides? Let me know if I suggest anything else. 

","08/Aug/17 11:11 AM;sbadam;We are seeing this error for another command(another customer) - 87771577

-> There is no spot node loss during query execution. Error which is seen in Master node:


{code:java}
2017-08-02T17:26:49.297Z        ERROR   remote-task-callback-1780       com.facebook.presto.execution.StageStateMachine Stage 20170802_171328_00883_yvgdq.1 failed
com.facebook.presto.operator.PageTransportTimeoutException: Encountered too many errors talking to a worker node. The node may have crashed or be under too much load. This is probably a transient issue, so please retry your query in a few minutes. (http://10.65.3.203:8081/v1/task/20170802_171328_00883_yvgdq.2.15/results/12/567 - requests failed for 63.59s)
        at com.facebook.presto.operator.HttpPageBufferClient$1.onFailure(HttpPageBufferClient.java:376)
        at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.util.concurrent.TimeoutException: Total timeout 10000 ms elapsed
        at com.google.common.base.Throwables.propagate(Throwables.java:160)
        at io.airlift.http.client.ResponseHandlerUtils.propagate(ResponseHandlerUtils.java:22)
        at com.facebook.presto.operator.HttpPageBufferClient$PageResponseHandler.handleException(HttpPageBufferClient.java:541)
        at com.facebook.presto.operator.HttpPageBufferClient$PageResponseHandler.handleException(HttpPageBufferClient.java:528)
        at io.airlift.http.client.jetty.JettyHttpClient$JettyResponseFuture.failed(JettyHttpClient.java:870)
        at io.airlift.http.client.jetty.JettyHttpClient$BufferingResponseListener.onComplete(JettyHttpClient.java:1104)
        at org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:193)
        at org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:185)
        at org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:457)
        at org.eclipse.jetty.client.HttpReceiver.abort(HttpReceiver.java:528)
        at org.eclipse.jetty.client.HttpChannel.abortResponse(HttpChannel.java:129)
        at org.eclipse.jetty.client.HttpChannel.abort(HttpChannel.java:122)
        at org.eclipse.jetty.client.HttpExchange.abort(HttpExchange.java:257)
        at org.eclipse.jetty.client.HttpConversation.abort(HttpConversation.java:141)
        at org.eclipse.jetty.client.HttpRequest.abort(HttpRequest.java:708)
        at org.eclipse.jetty.client.TimeoutCompleteListener.run(TimeoutCompleteListener.java:71)
        ... 7 more
....
{code}

-> I have suggested ""query.client.timeout=10m"" to customer. I also saw other I/O related errors in slave node. Not sure how to interpreter these errors:

s3://tdcprod-logs/qubole/revenue-analytics/logs/presto/567126/10.65.3.203/server.log

{code:java}
2017-08-02T17:26:27.654Z        ERROR   async-http-response-497 org.glassfish.jersey.server.ServerRuntime$Responder     An I/O error has occurred while writing a response message entity to the container output stream.
org.glassfish.jersey.server.internal.process.MappableException: org.eclipse.jetty.io.EofException
        at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundWriteTo(MappableExceptionWrapperInterceptor.java:92)
        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:162)
        at org.glassfish.jersey.message.internal.MessageBodyFactory.writeTo(MessageBodyFactory.java:1130)
        at org.glassfish.jersey.server.ServerRuntime$Responder.writeResponse(ServerRuntime.java:711)
        at org.glassfish.jersey.server.ServerRuntime$Responder.processResponse(ServerRuntime.java:444)
        at org.glassfish.jersey.server.ServerRuntime$Responder.process(ServerRuntime.java:434)
        at org.glassfish.jersey.server.ServerRuntime$AsyncResponder$3.run(ServerRuntime.java:934)
        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)
        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:315)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:297)
        at org.glassfish.jersey.internal.Errors.process(Errors.java:267)
        at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)
        at org.glassfish.jersey.server.ServerRuntime$AsyncResponder.resume(ServerRuntime.java:966)
        at org.glassfish.jersey.server.ServerRuntime$AsyncResponder.resume(ServerRuntime.java:922)
        at io.airlift.http.server.AsyncResponseHandler.lambda$toCompletionCallback$1(AsyncResponseHandler.java:101)
        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
        at io.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:77)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.eclipse.jetty.io.EofException
        at org.eclipse.jetty.io.ChannelEndPoint.flush(ChannelEndPoint.java:199)
        at org.eclipse.jetty.io.WriteFlusher.flush(WriteFlusher.java:420)
        at org.eclipse.jetty.io.WriteFlusher.write(WriteFlusher.java:313)
        at org.eclipse.jetty.io.AbstractEndPoint.write(AbstractEndPoint.java:140)
        at org.eclipse.jetty.server.HttpConnection$SendCallback.process(HttpConnection.java:738)
        at org.eclipse.jetty.util.IteratingCallback.processing(IteratingCallback.java:241)
        at org.eclipse.jetty.util.IteratingCallback.iterate(IteratingCallback.java:224)
        at org.eclipse.jetty.server.HttpConnection.send(HttpConnection.java:518)
        at org.eclipse.jetty.server.HttpChannel.sendResponse(HttpChannel.java:724)
        at org.eclipse.jetty.server.HttpChannel.write(HttpChannel.java:775)
        at org.eclipse.jetty.server.handler.gzip.GzipHttpOutputInterceptor.write(GzipHttpOutputInterceptor.java:108)
        at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:235)
        at org.eclipse.jetty.server.HttpOutput.write(HttpOutput.java:219)
        at org.eclipse.jetty.server.HttpOutput.flush(HttpOutput.java:355)
        at io.airlift.http.server.TimingFilter$TimedServletOutputStream.flush(TimingFilter.java:281)
        at org.glassfish.jersey.servlet.internal.ResponseWriter$NonCloseableOutputStreamWrapper.flush(ResponseWriter.java:330)
        at org.glassfish.jersey.message.internal.CommittingOutputStream.flush(CommittingOutputStream.java:292)
...
{code}

-> I don't see long pauses in gc.log of slave node.

cc - [~vamship] (tagging on-call person in Shubham's absence).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qubole docs not in sync with Java SDK for Data Import Command Rest API,SDK-209,63516,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,ksr,cbalchandani,cbalchandani,22/May/17 12:50 PM,05/Jun/17 10:40 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Qubole Docs specify support for mentioning *customer cluster* and *customer cluster label* for Data Import Command for Rest API.
[Qubole Docs|http://docs.qubole.com/en/latest/rest-api/command_api/submit-a-db-import-command.html]

but Java SDK does not provide support for those two variables.
[Github code for Java SDK|https://github.com/qubole/qds-sdk-java/blob/master/src/main/java/com/qubole/qds/sdk/java/api/DbSimpleImportCommandBuilder.java]
",,cbalchandani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,tubemogul,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04snj:,,,,,,,,,,,,,,,,,,,1.0,14757,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduled Notebook Failing Intermittently,ZEP-962,63420,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,rgupta,mpatel,mpatel,20/May/17 2:21 PM,24/May/17 12:50 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Expedia 

Account - Prod-ETL (5507)
Scheduled Job: 18232

The logs are not really available in the command instance: https://api.qubole.com/v2/analyze?command_id=73536025

I was lucky to look at a time where the last instance had failed, so I was able to see the error in the paragraphs:

{code}
/usr/lib64/python2.6/dist-packages/numpy/oldnumeric/__init__.py:11: ModuleDeprecationWarning: The oldnumeric module will be dropped in Numpy 1.9
  warnings.warn(_msg, ModuleDeprecationWarning)
Py4JJavaError: An error occurred while calling o298.cache.
: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:606)
org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:40)
org.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:35)
org.apache.zeppelin.spark.SparkInterpreter.createSparkSession(SparkInterpreter.java:489)
org.apache.zeppelin.spark.SparkInterpreter.getSparkSession(SparkInterpreter.java:331)
org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:925)
org.apache.zeppelin.interpreter.ClassloaderInterpreter.open(ClassloaderInterpreter.java:74)
org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
org.apache.zeppelin.spark.PySparkInterpreter.getSparkInterpreter(PySparkInterpreter.java:533)
org.apache.zeppelin.spark.PySparkInterpreter.createGatewayServerAndStartScript(PySparkInterpreter.java:198)
org.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:176)
org.apache.zeppelin.interpreter.ClassloaderInterpreter.open(ClassloaderInterpreter.java:74)
org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)
org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:316)
org.apache.zeppelin.scheduler.Job.run(Job.java:182)
The currently active SparkContext was created at:
(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:113)
	at org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2243)
	at org.apache.spark.SparkContext.defaultMinPartitions(SparkContext.scala:2252)
	at org.apache.spark.sql.hive.HadoopTableReader.<init>(TableReader.scala:78)
	at org.apache.spark.sql.hive.execution.HiveTableScanExec.<init>(HiveTableScanExec.scala:86)
	at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$$anonfun$3.apply(HiveStrategies.scala:76)
	at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$$anonfun$3.apply(HiveStrategies.scala:76)
	at org.apache.spark.sql.execution.SparkPlanner.pruneFilterProject(SparkPlanner.scala:91)
	at org.apache.spark.sql.hive.HiveStrategies$HiveTableScans$.apply(HiveStrategies.scala:72)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:60)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:60)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:61)
	at org.apache.spark.sql.execution.SparkPlanner.plan(SparkPlanner.scala:47)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:51)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1$$anonfun$apply$1.applyOrElse(SparkPlanner.scala:48)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:308)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at org.apache.spark.sql.execution.SparkPlanner$$anonfun$plan$1.apply(SparkPlanner.scala:48)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:76)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.CacheManager$$anonfun$cacheQuery$1.apply(CacheManager.scala:102)
	at org.apache.spark.sql.execution.CacheManager.writeLock(CacheManager.scala:65)
	at org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:89)
	at org.apache.spark.sql.Dataset.persist(Dataset.scala:2301)
	at org.apache.spark.sql.Dataset.cache(Dataset.scala:2311)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o298.cache.\n', JavaObject id=o299), <traceback object at 0x405fef0>)
{code}

Timestamp from para:
{code}
Took a few seconds. Last updated by s-qbl-disc-svcs@expedia.com at Mon May 15 2017 14:39:01 GMT-0700 (PDT). Last run at Fri May 19 2017 17:45:14 GMT-0700 (PDT)
{code}

The problem is I can't see anything related to why this failed in the zeppelin or interpreter logs (attached)...

I checked /var/log/messages in case of any OOM - nothing there...

Also, the cluster had been up for over a day:
{code}
production-replica> select cluster_state, start_at, down_at from cluster_insts where cluster_id = 26605 and cluster_state='UP' ;
+---------------+---------------------+---------+
| cluster_state | start_at            | down_at |
+---------------+---------------------+---------+
| UP            | 2017-05-18 07:33:28 | NULL    |
+---------------+---------------------+---------+
1 row in set (0.00 sec)
{code}

Thoughts?",,mahuja,mpatel,namanm,rgupta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/May/17 2:20 PM;mpatel;zeppelin--ip-10-23-6-55.log;https://qubole.atlassian.net/secure/attachment/42617/zeppelin--ip-10-23-6-55.log,20/May/17 2:20 PM;mpatel;zeppelin--ip-10-23-6-55.out;https://qubole.atlassian.net/secure/attachment/42618/zeppelin--ip-10-23-6-55.out,20/May/17 2:20 PM;mpatel;zeppelin-interpreter-spark--spark-ip-10-23-6-55.log;https://qubole.atlassian.net/secure/attachment/42616/zeppelin-interpreter-spark--spark-ip-10-23-6-55.log,,,,,,,,,,,AWS,,,,,,Expedia,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04s27:,,,,,,,,,,,,,,,,,,,1.0,14670,2017-05-23 15:03:41.116,,,"23/May/17 3:03 PM;mahuja;[~rgupta] can you please assign it for triage, if it is Zeppelin specific","23/May/17 11:17 PM;rgupta;

We checked this particular cluster inst and seems like we are not syncing up all log files. We needed log file rotated for 20th and its not thr...
 We need to fix this also

{code:java}
[ec2-user@ip-10-73-207-123 tmp]$ /usr/lib/hive_scripts/storagecli.py -a 5507 -c=""-ls s3://analytics-qubole-prod/prod-etl/logs/hadoop/26605/473148/ec2-54-84-18-70.compute-1.amazonaws.com.master/zeppelin/logs/*.log""
-rwxrwxrwx   1     304450 2017-05-22 11:06 /prod-etl/logs/hadoop/26605/473148/ec2-54-84-18-70.compute-1.amazonaws.com.master/zeppelin/logs/zeppelin--ip-10-23-6-55.log
-rwxrwxrwx   1   18368302 2017-05-22 11:03 /prod-etl/logs/hadoop/26605/473148/ec2-54-84-18-70.compute-1.amazonaws.com.master/zeppelin/logs/zeppelin-interpreter-spark--spark-ip-10-23-6-55.log


{code}
","23/May/17 11:49 PM;namanm;I don't see any error in either zeppelin or spark logs attached. The complete logs for 20th May are not available. So it doesn't seem feasible to figure out what could have been the reason for failure. 

From the error logs in the description, it seems the spark driver might have crashed. However, we don't have enough information to conclude anything. A live debug with a running cluster would be helpful.","24/May/17 12:46 AM;rgupta;[~namanm] filed ZEP-965 for checking and fixing the missing log files in s3. This will give u good overview of glue code around core zep that we have/....

lets try to fix this in rb45 sprint..",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS2 queries are not running for Underarmour due to connectivity problems,ACM-1217,63318,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayaa,sbadam,sbadam,19/May/17 9:18 PM,22/May/17 3:11 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"HS2 queries are not running for Underarmour due to connectivity problems

User: sswong@underarmour.com, Account ID - 1756

Created a Hadoop2(Hive) cluster(31450, instance id - 475275) and enabled HS2 through cluster configs.

1. Ran few queries from Analyze - 73532139, 73561652, all are failing with below error:

{code:java}
Error: Could not open client transport with JDBC Uri: jdbc:hive2://ec2-204-236-215-137.compute-1.amazonaws.com:10003: java.net.SocketException: Malformed reply from SOCKS server (state=08S01,code=0)
No current connection
Error: Could not open client transport with JDBC Uri: jdbc:hive2://ec2-204-236-215-137.compute-1.amazonaws.com:10003: java.net.SocketException: Malformed reply from SOCKS server (state=08S01,code=0)
{code}

2. Made sure HS2 is running on Master on port#10003

{code:java}
[root@ip-172-31-22-184 ec2-user]# lsof -i :10003
COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
java    7466 root  327u  IPv4  27413      0t0  TCP *:documentum_s (LISTEN)
{code}

From hive.log:

{code:java}
2017-05-20 00:09:51,372 -1 INFO  service.AbstractService (AbstractService.java:start(104)) - Service:HiveServer2 is started.
2017-05-20 00:09:52,105 -1 INFO  tez.TezSessionPoolManager (TezSessionPoolManager.java:setupPool(101)) - Created new tez session for queue: hiveserver2 with session id: dd86c6e5-b254-477d-ad48-57f0071a1ce3
2017-05-20 00:09:52,106 -1 INFO  tez.TezSessionPoolManager (TezSessionPoolManager.java:setupPool(101)) - Created new tez session for queue: hiveserver2 with session id: fcda19b3-2eea-4b22-8afe-c5777d1c298a
2017-05-20 00:09:52,113 -1 INFO  tez.TezSessionState (TezSessionState.java:open(144)) - User of session id dd86c6e5-b254-477d-ad48-57f0071a1ce3 is ec2-user
{code}

3. No bastion, no VPC for this cluster.

4. Listing account features, limits here to check they have any impact here:


{code:java}
production-replica> select * from account_features where account_id=1756;
+-------+------------+---------+------------------------------------------+------------+---------------------+---------------------+--------------+
| id    | account_id | user_id | feature_name                             | is_enabled | created_at          | updated_at          | segment_name |
+-------+------------+---------+------------------------------------------+------------+---------------------+---------------------+--------------+
|   169 |       1756 |    NULL | tapp.ui_enable_analyze_v2                |          1 | 2014-10-24 23:01:12 | 2014-10-24 23:01:12 | NULL         |
|  2178 |       1756 |    NULL | tapp.ui_enable_cpl_v2                    |          1 | 2015-01-22 06:03:47 | 2015-02-11 07:23:11 | NULL         |
| 10996 |       1756 |    NULL | tapp.enable_aws_roles                    |          0 | 2015-08-19 18:01:13 | 2015-08-19 18:01:13 | NULL         |
| 12533 |       1756 |    NULL | tapp.ui_enable_v2_scheduler              |          1 | 2016-02-24 22:19:45 | 2016-02-24 22:19:45 | NULL         |
| 17019 |       1756 |    NULL | tapp.ui_enable_node_bootstrap_editor     |          1 | 2016-09-15 06:33:44 | 2016-09-15 06:33:44 | NULL         |
| 17124 |       1756 |    NULL | zeppelin.buffer_append_output            |          1 | 2016-09-15 06:35:13 | 2016-09-15 06:35:13 | NULL         |
| 28165 |       1756 |    NULL | tapp.ui_enable_heterogeneous_composition |          1 | 2017-02-24 21:58:11 | 2017-02-24 21:58:11 | NULL         |
+-------+------------+---------+------------------------------------------+------------+---------------------+---------------------+--------------+
7 rows in set (0.01 sec)

production-replica> select * from account_limits where account_id=1756;
+-------+------------+------------------------------------+-------+---------------------+---------------------+--------------+
| id    | account_id | metric_name                        | limit | created_at          | updated_at          | string_value |
+-------+------------+------------------------------------+-------+---------------------+---------------------+--------------+
| 17123 |       1756 | Hive version                       |   1.2 | 2016-01-22 01:22:58 | 2016-01-22 01:22:58 | NULL         |
| 46408 |       1756 | HiveClientMaxHeap                  |  3072 | 2016-03-14 10:51:21 | 2016-03-14 10:51:21 | NULL         |
| 49644 |       1756 | Max concurrent commands            |   100 | 2016-05-12 07:54:36 | 2016-05-12 07:54:36 |              |
|  5860 |       1756 | Max periodic job instances per day |    36 | 2014-05-09 18:36:47 | 2015-05-07 18:57:16 | NULL         |
|  5859 |       1756 | PrivilegeLevel                     |    -2 | 2014-05-09 18:36:47 | 2014-05-09 18:36:47 | NULL         |
|  5858 |       1756 | QCUH this month                    |    50 | 2014-05-09 18:36:47 | 2014-05-09 18:36:47 | NULL         |
| 36328 |       1756 | qubole_mysql_compatibility_check   |     0 | 2015-12-02 11:32:51 | 2015-12-02 11:32:51 | NULL         |
| 30569 |       1756 | UseHiveTier                        |     1 | 2015-09-21 08:20:02 | 2015-09-21 08:20:02 | NULL         |
| 33643 |       1756 | whitelist_ip_check                 |     0 | 2015-11-02 18:26:19 | 2016-04-14 02:07:03 |              |
+-------+------------+------------------------------------+-------+---------------------+---------------------+--------------+
9 rows in set (0.00 sec)

production-replica>
{code}


I tried samething in my account and it worked perfectly fine - 73495711. 




",,adubey,ajayb,drose@qubole.com,nitink,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,underarmour,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04rxj:,,,,,,,,,,,,,,,,,,,1.0,14706,2017-05-19 22:29:19.064,,,"19/May/17 10:29 PM;adubey;Downgrading to Major - the impact is very limited:

They cannot run HS2 queries ( new feature ) from analyze without hive.on.master=true; that means their main workloads are running fine and if they have a tool to connect to Hs2 they can revert things to bootstrap model where they were before r43.. [~sbadam] feel free to increase the priority if impact gets bigger here.","19/May/17 10:54 PM;nitink;{code}
11:05 AM] 
nitink The Public DNS is not getting resolved from web node whereas private IP is working fine .. This normally happens in clusters behind private subnet . In Private subnet case , setting account feature - cluster.disable_public_ip works

[11:06 AM] 
If above feature is enabled, private-ip is used for all communication (ACM team will know more about this)

[11:07 AM] 
This cluster is not inside private subnet, so not sure if bastion is creating this problems….

[11:07 AM] 
But I tried this

[11:07 AM] 
Goto Production web node - 10.150.57.50

[11:08 AM] 
 ```/usr/lib/hive1.2/usr-bin/beeline --socksProxyHost=127.0.0.1 --socksProxyPort=20188 -u jdbc:hive2://ec2-204-236-215-137.compute-1.amazonaws.com:10003 -e ""show tables""
Fails ( after timeout)
```

[11:08 AM] 
 ```/usr/lib/hive1.2/usr-bin/beeline --socksProxyHost=127.0.0.1 --socksProxyPort=20188 -u jdbc:hive2://172.31.22.184:10003 -e ""show tables""
Works fine
```

[11:09 AM] 
Only diff in these two statements is that in 1st case - we are using public DNS , whereas in second case we are using private ip of cluster - 31450

[11:10 AM] 
ACM team can confirm if we can use the account feature - cluster.disable_public_ip  in this case or not (as cluster is not in private subnet)

[11:11 AM] 
Questions ?

[11:11 AM] 
psrinivas hmmm, so only private IPs works is it ?

[11:12 AM] 
and by enabling ” cluster.disable_public_ip” web node also uses private to contact master  for hs2? (edited)

[11:13 AM] 
nitink All cluster communication is via private IPs.. But , this feature was meant for clusters in VPC.. , not sure if it fits here as this cluster is not n VPC, but uses bastion

[11:14 AM] 
@adubey

[11:15 AM] 
can customer use HOM for now ? we can continue discussing with ACM team to find out if above feature can be used for this setup
{code}",22/May/17 3:11 AM;ajayb;[~ajayaa] can you figure out what's happening here? Above comments suggests that dns name resolution of cluster nodes was failing from web nodes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove node from a cluster - incorrect syntax in qubole docs,MW-962,63314,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,cbalchandani,cbalchandani,19/May/17 3:33 PM,30/May/17 2:59 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Qubole docs for deleting a node in the cluster - 
http://docs.qubole.com/en/latest/rest-api/cluster_api/remove-node.html
seem to mention incorrect syntax. 

Executing following command - 
curl -i -X DELETE -H ""X-AUTH-TOKEN:$AUTH_TOKEN"" -H ""Content-Type:application/json"" https://api.qubole.com/api/latest/clusters/31114/nodes?private_dns=ip-10-182-114-25.ec2.internal
gives error - 
{""error"":{""error_code"":422,""error_message"":""wrong number of arguments (1 for 0)""}}

CC: [~adubey] [~megha]",,cbalchandani,drose@qubole.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,clarivate,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04rwn:,,,,,,,,,,,,,,,,,,,1.0,14606,,,,"30/May/17 2:59 PM;cbalchandani;Do we have an ETA for this?
or any alternate way to achieve this?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commands failing on eu-central-1 cluster due to tunnel server disk being full,MW-1265,63285,Bug,Reopened,MW,Middleware,software,sumitm,,,Major,,ksr,navdeepp,navdeepp,19/May/17 5:50 AM,09/Aug/17 3:34 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Environment: Eu-central-1
Cluster:378
Command Ids:48649 , 48640, 48786

{code}
Traceback (most recent call last):
File ""/usr/lib/qubole/packages/hustler-43.6.0/hustler/lib/py/starcluster/cli.py"", line 251, in main
sc.execute(args)
File ""/usr/lib/qubole/packages/hustler-43.6.0/hustler/lib/py/starcluster/commands/clusterinfo.py"", line 249, in execute
tunnel_server, sec_master_dns)
File ""/usr/lib/qubole/packages/hustler-43.6.0/hustler/lib/py/starcluster/commands/clusterinfo.py"", line 198, in perform_curl_check_and_print_master
raise Exception(""proxy_port or RM / JT curl check failed on master"")
Exception: proxy_port or RM / JT curl check failed on master
!!! 2017-05-18 21:10:00,302 ERROR - An error occured while running hustler script
Following messages were logged in stderr:
null
at com.qubole.hadoop.cluster.HadoopInitializer.getAuthorities(HadoopInitializer.java:55)
at org.apache.hadoop.hive.ql.exec.Utilities.getAuthorities(Utilities.java:4134)
at org.apache.hadoop.hive.ql.session.SessionState.populateHadoopAuthorities(SessionState.java:735)
at org.apache.hadoop.hive.ql.Context.populateHadoopAuthorities(Context.java:224)
at org.apache.hadoop.hive.ql.Context.getRelativizedScratchDir(Context.java:346)
at org.apache.hadoop.hive.ql.Context.getMRScratchDir(Context.java:374)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6807)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9195)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:9086)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9931)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9824)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10260)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10271)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:239)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:466)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:330)
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1185)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1239)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1122)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1112)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:248)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:195)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:418)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:349)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:451)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:493)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:845)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:800)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:711)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{code}",,ksr,mdaurangzeb,navdeepp,prakharj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,neustar,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04rqn:,,,,,,,,,,,,,,,,,,,2.0,"14766,16299",2017-05-19 07:27:45.59,,,"19/May/17 7:27 AM;prakharj;One of the tunnel server has disk full - 52.28.127.62
So all get_proxy_port calls to that tunnel server are failing

{noformat}
[ec2-user@ip-172-31-2-144 ~]$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        3.7G   60K  3.7G   1% /dev
tmpfs           3.7G     0  3.7G   0% /dev/shm
/dev/xvda1       16G  4.2G   12G  28% /
/dev/xvdb        30G   15G   14G  54% /media/ephemeral0
{noformat}

","19/May/17 7:30 AM;navdeepp;Marking this resolved, Root cause was disk space being full on eu-central tunnel 52.28.127.62.

Thanks [~prakharj] ","08/Aug/17 5:30 AM;mdaurangzeb;[~prakharj] Neustar is facing this issue intermittently.

Failed Command ids: 63744,62627
Cluster id : 722
accound id : 133

raise Exception(""proxy_port or RM / JT curl check failed on master"")
Exception: proxy_port or RM / JT curl check failed on master
!!! 2017-08-08 11:40:30,515 ERROR - An error occured while running hustler script
Following messages were logged in stderr:
null
at com.qubole.hadoop.cluster.HadoopInitializer.getAuthorities(HadoopInitializer.java:55)
at org.apache.hadoop.hive.ql.exec.Utilities.getAuthorities(Utilities.java:4136)
at org.apache.hadoop.hive.ql.session.SessionState.populateHadoopAuthorities(SessionState.java:751)
at org.apache.hadoop.hive.ql.Context.populateHadoopAuthorities(Context.java:224)
at org.apache.hadoop.hive.ql.Context.getRelativizedScratchDir(Context.java:346)
at org.apache.hadoop.hive.ql.Context.getMRScratchDir(Context.java:374)
at org.apache.hadoop.hive.ql.Context.getMRTmpFileURI(Context.java:430)
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1456)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1272)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1133)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1123)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:250)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:197)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:353)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:497)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:853)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:808)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:715)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)","08/Aug/17 6:35 AM;mdaurangzeb;[~ajayb] [~prakharj]

Disk space is full on one of the tunnel servers. 

[ec2-user@ip-172-31-2-144 ~]$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        3.7G   60K  3.7G   1% /dev
tmpfs           3.7G     0  3.7G   0% /dev/shm
/dev/xvda1       16G  4.0G   12G  26% /
/dev/xvdb        30G   28G     0 100% /media/ephemeral0","09/Aug/17 12:44 AM;mdaurangzeb;[~ksr]

Customer is not happy because they faced this issue in past as well because of same disk space issue. Going forward Could we please ensure that we have proper alerts and logs rotation etc in place to avoid this in future. Appreciate your help on this. Thanks!

cc- [~venkatak]","09/Aug/17 1:30 AM;ksr;[~mdaurangzeb] The log rotation conf was absent on that particular tunnel server. [~joset] helped me add it now and we should not see this again. Also, [~shivurh] is working on adding alerting for this.","09/Aug/17 2:59 AM;mdaurangzeb;[~ksr]

Customer is again started facing the issue. Command id : 63940 , Cluster id 745. account id : 133


File ""/usr/lib/qubole/packages/hustler-45.35.1/hustler/lib/py/starcluster/commands/clusterinfo.py"", line 200, in perform_curl_check_and_print_master
raise Exception(""proxy_port or RM / JT curl check failed on master"")
Exception: proxy_port or RM / JT curl check failed on master
!!! 2017-08-09 08:43:13,484 ERROR - An error occured while running hustler script
Following messages were logged in stderr:
null
at com.qubole.hadoop.cluster.HadoopInitializer.getAuthorities(HadoopInitializer.java:55)
at org.apache.hadoop.hive.ql.exec.Utilities.getAuthorities(Utilities.java:4136)
at org.apache.hadoop.hive.ql.session.SessionState.populateHadoopAuthorities(SessionState.java:751)
at org.apache.hadoop.hive.ql.Context.populateHadoopAuthorities(Context.java:224)
at org.apache.hadoop.hive.ql.Context.getRelativizedScratchDir(Context.java:346)
at org.apache.hadoop.hive.ql.Context.getMRScratchDir(Context.java:374)
at org.apache.hadoop.hive.ql.Context.getMRTmpFileURI(Context.java:430)
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1456)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1272)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1133)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1123)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:250)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:197)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:353)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:455)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:497)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:853)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:808)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:715)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",09/Aug/17 3:34 AM;ksr;[~mdaurangzeb] The disk full issue caused the loss of available ports. I restarted the service and reran the command. It was successful - https://eu-central-1.qubole.com/v2/analyze?command_id=63983,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to use specific serde,HIVE-2184,63246,Bug,Reopened,HIVE,qbol hive,software,psrinivas,,,Major,,psrinivas,mayureshp,addon_zendesk_for_jira,18/May/17 10:40 PM,21/May/17 11:19 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Today 06:10 am
Hi Mayuresh,

Thanks for your reply. Yes, that's the SerDe that I was using: org.openx.data.jsonserde.JsonSerDe

Looking forward to further reply.

Thanks a lot.

Best regards, 
Mathias

 

Mayureshp
Today 04:49 am
Hi Mathias,

The issue which you have pointed out I think is with SerDe library org.openx.data.jsonserde.JsonSerDe, 
I was able to reproduce the exception (Failed with exception java.io.IOException:Row is not a valid JSON Object - JSONException:
Duplicate key ""timestamp"")

Here in the DDL you mentioned, though ""case.insensitive"" = ""false"" is set, library is unable to distinguish between timeStamp and timestamp.

As a workaround to test how it can work for me I have created a tiny json data (with similar keys as yours) and changed the keys from timeStamp 
to time_stamp, and it seems working for the select statement otherwise (with timestamp and timeStamp) throwing same exception.

I will refer this issue with HIVE team after discussion with Support, and see if they can help in this issue.


Regards
Mayuresh.

Qubole Public User Groups/ KBs: https://qubole.zendesk.com/hc/en-us Help Docs: http://docs.qubole.com/en/latest/

 
mathias
Yesterday 05:16 pm
Hi Megha,

Thanks for your reply. First, ok to reduce the priority of this ticket.

Secondly, just want to update on the command id, please have a look in the tvlk-ml-qubole-prod account at command id: 72989138 (for the DDL) and 72989485 (for the select query).

The error message when trying to select is: 
Failed with exception java.io.IOException:Row is not a valid JSON Object - JSONException: Duplicate key ""timestamp""

In the raw file we do have fields: ""timestamp"" and ""timeStamp"", so we need to use the mapping serde properties as specified in: https://github.com/rcongiu/Hive-JSON-Serde

But, eventhough the DDL is executed successfully, we still encounter errors when trying to query the created table. Looking at the changelog the serde, the latest update is on 2017-04-09, so we wonder whether this serde has also been updated on Qubole? If it has not been updated, is there any way for us to use the latest update?

Thanks.

Best regards, 
Mathias

 

Megha Thakkar
Yesterday 04:04 pm
Hi Colin,

Thank you for contacting qubole support. 
Following is our priority definition for Urgent ticket.
Urgent :

Severe Business Impact - Failures that render a substantial portion of the Software inoperative or causes catastrophic failure of the Software, or there is a security breach, where no Workaround is available. Example : Cluster Outage, Data Corruption

Is it ok to reduce the priority of this ticket? 


Irrespective, I looked into the issue.
The select queries have different failure reasons:
72987547 failed, because timestamp is  a keyword in hive, and hence needs to be escaped.
Also, 72987524 failed due to authorization issue. Do you know if hive authorization is enabled for this account?

Thanks,
Megha
Qubole Public User Groups/ KBs: https://qubole.zendesk.com/hc/en-us Help Docs: http://docs.qubole.com/en/latest/

 
Colin Tobing
Yesterday 03:30 pm
We're trying to utilize org.openx.data.jsonserde.JsonSerDe but it's not working. Or probably there's a mistake on our end. Need your advise on this.

Create table command id 72987157 
Select statement 72987524 and 72987547",,addon_zendesk_for_jira,mayureshp,psrinivas,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-2180,,HIVE-2180,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04rhz:,,,,,,,,,,,,,,,,,,,1.0,14731,2017-05-19 19:26:16.229,,,"18/May/17 10:48 PM;mayureshp;SET
role ml_economics
;

SET
hive.exce.parallel = TRUE
;

DROP
    TABLE
        ml_econ.track_inventory_hotel_booking
;

CREATE
    EXTERNAL TABLE
        ml_econ.track_inventory_hotel_booking (
            `AdjustmentMappingType` STRING
            ,`_id` STRING
            ,`AgentBookingId` BIGINT
            ,`CookieID` STRING
            ,`ExternalKeys` STRUCT < `PROVIDERID` : STRING >
            ,`HotelBookingCase` STRING
            ,`HotelBookingId` BIGINT
            ,`HotelId` BIGINT
            ,`kafkaPublishTimestamp` BIGINT
            ,`kafkaSubscribeTimestamp` BIGINT
            ,`nodeId` STRING
            ,`OriginType` STRING
            ,`prevSearchId` STRING
            ,`pricingSpec` STRUCT < `rateType` : STRING
            ,`originalNumOfInfants` : INT
            ,`bookingWindowInDays` : INT
            ,`checkInDate` : BIGINT
            ,`checkInDay` : INT
            ,`checkOutDate` : BIGINT
            ,`cid` : STRING
            ,`clientType` : STRING
            ,`currency` : STRING
            ,`flightTransactionFrequency` : STRING
            ,`flightTransactionFrequencyStatus` : STRING
            ,`flightTransactionRecency` : STRING
            ,`hotelGeoCity` : STRING
            ,`hotelGeoCountry` : STRING
            ,`hotelGeoRegion` : STRING
            ,`hotelGrouping` : INT
            ,`hotelIds` : STRING
            ,`hotelSearchFunnel` : STRING
            ,`hotelTransactionFrequency` : STRING
            ,`hotelTransactionFrequencyStatus` : STRING
            ,`hotelTransactionRecency` : INT
            ,`isBackDate` : STRING
            ,`isBot` : STRING
            ,`lengthOfStay` : INT
            ,`numOfAdults` : INT
            ,`numOfChildren` : INT
            ,`originalNumOfAdults` : INT
            ,`originalNumOfChildren` : INT
            ,`originalNumOfRooms` : INT
            ,`roomCount` : INT
            ,`roomNight` : INT
            ,`searchDate` : BIGINT
            ,`searchDay` : INT
            ,`searchTime` : BIGINT
            ,`userGeoCountry` : STRING
            ,`userLocale` : STRING
            ,`utmCampaign` : STRING
            ,`hotelUserType` : STRING >
            ,`ProviderId` STRING
            ,`RateAfterAdjustment` ARRAY < BIGINT >
            ,`RateBeforeAdjustment` ARRAY < BIGINT >
            ,`RatioUpdate` DOUBLE
            ,`searchId` STRING
            ,`searchType` STRING
            ,`SessionID` STRING
            ,`timestamp` BIGINT
            ,`timestamp_camelcase` BIGINT
        ) PARTITIONED BY (
            `pt_year` STRING
            ,`pt_month` STRING
            ,`pt_day` STRING
            ,`pt_hour` STRING
            ,`pt_minute` STRING
        ) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' WITH SERDEPROPERTIES (
		  
            ""case.insensitive"" = ""false""
            ,'mapping.timestamp' = 'timestamp'
            ,'mapping.timestamp_camelcase' = 'timeStamp'
        ) LOCATION 's3://tvlk-data-datalake-prod/traveloka/data/v1/raw/json/hour_1/track.inventory.hotel.booking/'

ALTER TABLE ml_econ.track_inventory_hotel_booking recover partitions;","19/May/17 7:26 PM;psrinivas;cc: [~rvenkatesh]

[~mayureshp]: Can you please summarize the issue? This seems same as HIVE-2180.  ","21/May/17 11:45 AM;mayureshp;In this particular case Traveloka guys have json with keys 'timestamp' and 'timeStamp', while using 'org.openx.data.jsonserde.JsonSerDe' properties to map with different column of the table to these keys ending up in IOException:Row is not a valid JSON Object - JSONException: Duplicate key ""timestamp"" during select clause.
though they are able to create to table through the above commented DDL. This issue is reproducible. 

HIVE-2180 is also appears to be same issue to me. 

cc: [venkatak@qubole.com]





",21/May/17 11:15 PM;mayureshp;Could you please update some resolution notes (though it is duplicate of HIVE-2180 which is still open)?  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Presto query failed with ""execution expired""",SOL-154,63226,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,megha,megha,18/May/17 2:56 PM,18/May/17 7:29 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Lot of queries failed for SaleCycle. The command logs are:

{code:java}
Executing this query on presto version : 0.157
Log location: s3://salecycle-qubole-metadata/logs/presto/2161
execution expired
Time taken: 60.002215265 seconds
{code}


failed command ids: 48444,48511, 48516, 48521, 48529, 48537, 48539, 48542, 48543, 48544
cluster-inst-id: 2161
Presto version: 0.157, with Rubix enabled.

The error message in dj.log -> [ERROR pid: 11485: 17-05-18 12:00:04 ] [command:48544] [account:73] [StatsD] Errno::ECONNREFUSED: Connection refused - send(2)

Checked server.log and gc.log on master.. There are no gaps in the logs.. i.e there were no gcs etc going on..
The command 48444 is the one that failed at the start of cluster, I looked at the server.log of the only slave (ip 172.24.0.82) around at that time - those logs also dont have any errors or gaps.
",,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,salecycle,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04rdj:,,,,,,,,,,,,,,,,,,,1.0,14748,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sqoop import from Oracle Db fails with  Bad type on operand stack error,SQOOP-110,63192,Bug,Open,SQOOP,SQOOP,software,sumitm,,,Major,,ksr,biswajit,biswajit,18/May/17 5:29 AM,08/Aug/17 11:43 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,SOLSUP,,,,,,,"Sqoop 1.4.6 import fails with java.lang.VerifyError: Bad type on operand stack error. While list table works fine. 

{code}

[ec2-user@ip-10-109-0-153 ~]$ /usr/lib/qubole/packages/sqoop_h2/sqoop-h2/bin/sqoop import -connect 'jdbc:sqlserver://uportaldb.valkyrie.net:1433;database=nexa_prod' --driver ""com.microsoft.sqlserver.jdbc.SQLServerDriver"" --username ""app.ab.core_etl"" --password-file ""s3n://dlx-dev-core-consumer/user/app.ab.core_etl/credentials/uportaldb_app.ab.core_etl"" --table dbo.AddInOrders 
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/05/18 06:04:46 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/sqoop_h2/sqoop-h2/lib/avro-tools-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hbase/hbase/lib/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/18 06:04:47 INFO s3.S3ServiceFactory: Setting s3 endpoint to s3-external-1.amazonaws.com
17/05/18 06:04:47 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:04:47 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:04:47 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:04:47 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:04:47 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:04:47 INFO s3native.NativeS3FileSystem: file size of user/app.ab.core_etl/credentials/uportaldb_app.ab.core_etl is 8
17/05/18 06:04:47 INFO tool.ImportTool: Starting Import Command
17/05/18 06:04:47 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
17/05/18 06:04:48 INFO manager.SqlManager: Using default fetchSize of 1000
17/05/18 06:04:48 INFO tool.CodeGenTool: Beginning code generation
17/05/18 06:04:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM dbo.AddInOrders AS t WHERE 1=0
17/05/18 06:04:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM dbo.AddInOrders AS t WHERE 1=0
Note: /tmp/sqoop-ec2-user/compile/456902bbbb8ac0480af00b9ab6b6ec1b/dbo_AddInOrders.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
17/05/18 06:04:49 INFO mapreduce.ImportJobBase: Beginning import of dbo.AddInOrders
17/05/18 06:04:49 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
Exception in thread ""main"" java.lang.VerifyError: Bad type on operand stack
Exception Details:
  Location:
    org/apache/hadoop/mapred/JobTrackerInstrumentation.create(Lorg/apache/hadoop/mapred/JobTracker;Lorg/apache/hadoop/mapred/JobConf;)Lorg/apache/hadoop/mapred/JobTrackerInstrumentation; @5: invokestatic
  Reason:
    Type 'org/apache/hadoop/metrics2/lib/DefaultMetricsSystem' (current frame, stack[2]) is not assignable to 'org/apache/hadoop/metrics2/MetricsSystem'
  Current Frame:
    bci: @5
    flags: { }
    locals: { 'org/apache/hadoop/mapred/JobTracker', 'org/apache/hadoop/mapred/JobConf' }
    stack: { 'org/apache/hadoop/mapred/JobTracker', 'org/apache/hadoop/mapred/JobConf', 'org/apache/hadoop/metrics2/lib/DefaultMetricsSystem' }
  Bytecode:
    0000000: 2a2b b200 03b8 0004 b0                 

	at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:420)
	at org.apache.hadoop.mapred.JobClient.init(JobClient.java:470)
	at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:455)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:478)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635)
	at org.apache.hadoop.mapreduce.Job.connect(Job.java:476)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:464)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:495)
	at org.apache.sqoop.mapreduce.JobBase.runJob(JobBase.java:473)
	at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:169)
	at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:273)
	at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:745)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:498)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:617)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:173)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:209)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:248)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:257)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:266)
[ec2-user@ip-10-109-0-153 ~]$ which sqoop 
/usr/lib/sqoop/bin/sqoop
[ec2-user@ip-10-109-0-153 ~]$ /usr/lib/sqoop/bin/sqoop import -connect 'jdbc:sqlserver://uportaldb.valkyrie.net:1433;database=nexa_prod' --driver ""com.microsoft.sqlserver.jdbc.SQLServerDriver"" --username ""app.ab.core_etl"" --password-file ""s3n://dlx-dev-core-consumer/user/app.ab.core_etl/credentials/uportaldb_app.ab.core_etl"" --table dbo.AddInOrders 
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/05/18 06:05:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/sqoop_h2/sqoop-h2/lib/avro-tools-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hbase/hbase/lib/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/18 06:05:55 INFO s3.S3ServiceFactory: Setting s3 endpoint to s3-external-1.amazonaws.com
17/05/18 06:05:55 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:05:55 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:05:55 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:05:55 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:05:55 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:05:55 INFO s3native.NativeS3FileSystem: file size of user/app.ab.core_etl/credentials/uportaldb_app.ab.core_etl is 8
17/05/18 06:05:55 INFO tool.ImportTool: Starting Import Command
17/05/18 06:05:55 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
17/05/18 06:05:55 INFO manager.SqlManager: Using default fetchSize of 1000
17/05/18 06:05:55 INFO tool.CodeGenTool: Beginning code generation
17/05/18 06:05:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM dbo.AddInOrders AS t WHERE 1=0
17/05/18 06:05:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM dbo.AddInOrders AS t WHERE 1=0
Note: /tmp/sqoop-ec2-user/compile/86c4d01397b1f97e9aa662427ad9e399/dbo_AddInOrders.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
17/05/18 06:05:57 INFO mapreduce.ImportJobBase: Beginning import of dbo.AddInOrders
17/05/18 06:05:57 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
Exception in thread ""main"" java.lang.VerifyError: Bad type on operand stack
Exception Details:
  Location:
    org/apache/hadoop/mapred/JobTrackerInstrumentation.create(Lorg/apache/hadoop/mapred/JobTracker;Lorg/apache/hadoop/mapred/JobConf;)Lorg/apache/hadoop/mapred/JobTrackerInstrumentation; @5: invokestatic
  Reason:
    Type 'org/apache/hadoop/metrics2/lib/DefaultMetricsSystem' (current frame, stack[2]) is not assignable to 'org/apache/hadoop/metrics2/MetricsSystem'
  Current Frame:
    bci: @5
    flags: { }
    locals: { 'org/apache/hadoop/mapred/JobTracker', 'org/apache/hadoop/mapred/JobConf' }
    stack: { 'org/apache/hadoop/mapred/JobTracker', 'org/apache/hadoop/mapred/JobConf', 'org/apache/hadoop/metrics2/lib/DefaultMetricsSystem' }
  Bytecode:
    0000000: 2a2b b200 03b8 0004 b0                 

	at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:420)
	at org.apache.hadoop.mapred.JobClient.init(JobClient.java:470)
	at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:455)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:478)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635)
	at org.apache.hadoop.mapreduce.Job.connect(Job.java:476)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:464)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:495)
	at org.apache.sqoop.mapreduce.JobBase.runJob(JobBase.java:473)
	at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:169)
	at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:273)
	at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:745)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:498)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:617)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:173)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:209)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:248)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:257)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:266)
[ec2-user@ip-10-109-0-153 ~]$

{code}

List table :- 

{code}

[ec2-user@ip-10-109-0-153 ~]$ /usr/lib/qubole/packages/sqoop_h2/sqoop-h2/bin/sqoop list-tables -connect 'jdbc:sqlserver://uportaldb.valkyrie.net:1433;database=nexa_prod' --driver ""com.microsoft.sqlserver.jdbc.SQLServerDriver"" --username ""app.ab.core_etl"" --password-file ""s3n://dlx-dev-core-consumer/user/app.ab.core_etl/credentials/uportaldb_app.ab.core_etl"" 
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/05/18 06:28:33 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/sqoop_h2/sqoop-h2/lib/avro-tools-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hbase/hbase/lib/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/18 06:28:34 INFO s3.S3ServiceFactory: Setting s3 endpoint to s3-external-1.amazonaws.com
17/05/18 06:28:35 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:28:35 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:28:35 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:28:35 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:28:35 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://dlx-dev-core-consumer.s3-external-1.amazonaws.com/user%2Fapp.ab.core_etl%2Fcredentials%2Fuportaldb_app.ab.core_etl
17/05/18 06:28:35 INFO s3native.NativeS3FileSystem: file size of user/app.ab.core_etl/credentials/uportaldb_app.ab.core_etl is 8
17/05/18 06:28:35 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
17/05/18 06:28:35 INFO manager.SqlManager: Using default fetchSize of 1000
AccountingItems
AccountingItems_deprecated
AccountingItemsXREF_deprecated
AddInModels


{code}

",,adityak,biswajit,ksr,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,oracle,,,,,"{repository={count=1, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":1,""lastUpdated"":""2011-07-22T07:04:19.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04r6f:,,,,,,MW-RB46-Sprint-2,MW-RB46-Sprint-3,,,,,,,,,,,,1.0,12824,2017-06-22 05:52:04.638,,,"18/May/17 5:30 AM;biswajit;original command 

{code}

fs.s3.inputpathprocessor=true;
mapred.job.natives3filesystem.globstatus.use=true;
mapred.output.compression.type=BLOCK;
/usr/lib/qubole/packages/sqoop_h2/sqoop-h2/bin/sqoop import --connect 'jdbc:sqlserver://uportaldb.valkyrie.net:1433;database=nexa_prod' --driver ""com.microsoft.sqlserver.jdbc.SQLServerDriver"" --username ""app.ab.core_etl"" --password-file ""s3n://dlx-dev-core-consumer/user/app.ab.core_etl/credentials/uportaldb"" --table dbo.AddInOrders --target-dir 's3://dlx-dev-dft/test/jk_test/sql_nonpart_test/master/bundle=20170217123623/chgset=20170217123623' --map-column-java AppName=String,StartTime=String,EndTime=String,CopyStart=String,CopyEnd=String,ErrorMessage=String --as-avrodatafile

{code}
","22/Jun/17 5:52 AM;ksr;[~biswajit] There might be an old jar causing this issue. See this - http://stackoverflow.com/questions/30163766/java-lang-verifyerror-with-hadoop

Can we ask them to run import via the UI using the Use Hadoop Cluster option.","28/Jun/17 8:42 AM;sureshr;[~biswajit]: moving to ""NeedMoreInfo"" status. Please reopen after trying out the workaround that [~ksr] has suggested.",05/Jul/17 8:45 AM;adityak;[~biswajit] - Do we have an update from the customer ?,12/Jul/17 11:57 AM;biswajit;[~ksr] The sqoop command was executed on shell command on UI itself. Here is the command id 73226562. ,26/Jul/17 8:52 AM;sureshr;[~ksr]: could you please take a look now?,"26/Jul/17 10:20 PM;ksr;[~sureshr] Sure, I'll look into this.",01/Aug/17 6:02 AM;ksr;Discussed this with [~biswajit] offline. We think we may have figured out the issue. Biswajit is trying the proposed fix.,"07/Aug/17 12:47 AM;sureshr;[~biswajit], [~ksr]: Please post updates on your tests with the proposed fix.",08/Aug/17 11:43 AM;ksr;[~sureshr] We determined this to be a jar issue. The customer is adding the avro-tools jar using node bootstrap. THe jar that they were adding was compiled with hadoop1. We tried using a jar compiled with hadoop2 instead and it worked. cc: [~biswajit],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why thrift metastore would return null to presto query,PRES-1062,63130,Bug,Open,PRES,Presto,software,stagra,,,Critical,,vamship,megha,megha,17/May/17 12:31 PM,25/Jul/17 8:47 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,jira_update,,,,,,,"Lyft uses Mode to run presto queries on Qubole. The query that mode runs to fetch table details is as in command 72895075 
The query threw java.lang.NullPointerException

{code:java}

at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:210)
at com.google.common.base.Throwables.propagate(Throwables.java:159)
at com.facebook.presto.hive.metastore.ThriftHiveMetastore.getAllTables(ThriftHiveMetastore.java:857)
at com.facebook.presto.hive.metastore.BridgingHiveMetastore.getAllTables(BridgingHiveMetastore.java:90)
at com.facebook.presto.hive.metastore.CachingHiveMetastore.getAllTables(CachingHiveMetastore.java:303)
at com.facebook.presto.hive.metastore.CachingHiveMetastore.getAllTables(CachingHiveMetastore.java:303)

{code}

There's no logs in hive_ms.log on master.
What would be the cause for this NPE? 

They do have lot of databases and tables, and this query tries to fetch everything here. 
Can this NPE be due to some timeout? ",,adubey,megha,p.vasa,stagra,vamship,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/May/17 10:08 PM;megha;client.td.1;https://qubole.atlassian.net/secure/attachment/42519/client.td.1,17/May/17 10:08 PM;megha;client.td.2;https://qubole.atlassian.net/secure/attachment/42520/client.td.2,17/Jul/17 8:07 PM;p.vasa;finished_fired_through_mode.pdf;https://qubole.atlassian.net/secure/attachment/45737/finished_fired_through_mode.pdf,17/May/17 10:07 PM;megha;server.td.1;https://qubole.atlassian.net/secure/attachment/42516/server.td.1,17/May/17 10:07 PM;megha;server.td.2;https://qubole.atlassian.net/secure/attachment/42517/server.td.2,17/May/17 10:07 PM;megha;server.td.3;https://qubole.atlassian.net/secure/attachment/42518/server.td.3,,,,,,,,AWS,,,,,,lyft,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04qsn:,,,,,,,,,,,,,,,,,,,2.0,"14708,15807",2017-05-18 08:18:00.055,,,"17/May/17 10:10 PM;megha;Same query initially failed with following exception (example cmd id:: 73166214)

{code:java}
java.lang.RuntimeException: java.net.SocketTimeoutException: Read timed out 
at com.google.common.base.Throwables.propagate(Throwables.java:160) 
at com.facebook.presto.hive.metastore.ThriftHiveMetastore.getAllTables(ThriftHiveMetastore.java:857) 
at com.facebook.presto.hive.metastore.BridgingHiveMetastore.getAllTables(BridgingHiveMetastore.java:90) 
at com.facebook.presto.hive.metastore.CachingHiveMetastore.getAllTables(CachingHiveMetastore.java:303) 
at com.facebook.presto.hive.metastore.CachingHiveMetastore.getAllTables(CachingHiveMetastore.java:303) 
at com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore.getAllTables(SemiTransactionalHiveMetastore.java:2197) 
at com.facebook.presto.hive.HiveMetadata.listTableColumns(HiveMetadata.java:382) 
at com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorMetadata.listTableColumns(ClassLoaderSafeConnectorMetadata.java:166) 
{code}

As the customer is using mode, and mode internally runs this query, customer didnt have option to run show schemas and show tables instead of this query. 
As this was a timeout exception, I suggested them to increase the value for hive.metastore-timeout..
They set it to 10m at cluster level , after which the query started throwing NPE.
I ran the same query again on the cluster with 10 m timeout (by running presto client on master), and have attached threaddumps of client and server here. (command id: 72895075)

Mainly, the client is waiting for get_all_tables call to return.. The server is returning something funky due to which presto client runs into NPE. 

Note: Lyft has lots of schemas and tables, hence timeout is expected.. Is there any parameter/s we can tweak to get results instead of timeouts, NPE etc? 
What is the reason for NPE?

It is ok to re-run the query, and tweak cluster labelled ""qubole-test"" . It is also ok to re-run the query.

 [^server.td.1]  [^server.td.2]  [^server.td.3]  [^client.td.1]  [^client.td.2] 
","18/May/17 8:18 AM;vamship;Couple of observations. 
1. There's no problem in presto talking to metastore server, listing the tables works and completes in couple of secs. Accessing it through information schema is failing.
{noformat}
describe information_schema.columns; 
{noformat}
^ this times out giving NPE. (with a 10m timeout value). This shows that listing operations taking longer time in the case of the information_schema, might be related to JDBC slowness issue we have seen in PRES-820.
2. If we increase the time out value to 1h, we are seeing a different error related to unsupported serde.
https://api.qubole.com/v2/analyze?command_id=73172308

Will keep this thread updated as i look further.  ","18/May/17 5:06 PM;megha;Thanks [~vamship].. PRES-820 seems resolved.. is this related to a regression? Or something more needs to be fixed? 
Also, the unsuppored serde seems to be relevant to some unsupported value being returned from the server right? Could we add more logging to identify what value is being returned? ",19/May/17 10:50 AM;drose@qubole.com;Increasing this issue as a blocker - Lyft is trying to roll out Presto across all users next week.  How can we help make this happen? [~stagra][~vamship][~rvenkatesh],"19/May/17 11:37 PM;vamship;We thought it is related to regression, updated the hive-contrib jar as suggested but the error persists. 
https://api.qubole.com/v2/analyze?command_id=73577208
Need to find an alternative
CC: [~stagra]","21/May/17 4:23 AM;stagra;[~vamship] are you sure that the bootstrap was applied. The culster_inst on which 73577208 had run has been up since ""2017-05-15 20:49:16"" ... not possible for it to have your bootstrap changes.","21/May/17 7:59 PM;vamship;[~stagra] Have not applied it through bootstrap, instead overriden jar in both master and slave. ","21/May/17 9:30 PM;stagra;[~vamship], in that case can you continue with the experiment we talked about on friday to write a driver program for metastore to find which table is causing trouble","21/May/17 9:35 PM;stagra;Also, I just checked PRES-831, it packages hive-contrib 0.13.1 while you are testing with 0.11
(https://bitbucket.org/qubole/presto/commits/f2d4972935578144a1c37ee3415118bef294143e)","21/May/17 10:34 PM;vamship;We are observing 2 issues here,
1. With a bigger time out value (1hour), the queries are failing with unsupported serde error. This is happening because of an older serde plugin. The following boostrap fixes the same.

{noformat}
set -ex
PRESTO_HOME=""/usr/lib/presto""
HIVE_CONTRIB_JAR_LOC=""s3://paid-qubole/tmpjars/hive-contrib-0.13.1.jar""

restart() {
  $PRESTO_HOME/bin/presto server restart
}

change_jars() {
        pushd ${PRESTO_HOME}/plugin/hive-hadoop2/
        mv hive-contrib-* /tmp/
        hadoop dfs -get ${HIVE_CONTRIB_JAR_LOC} ${PRESTO_HOME}/plugin/hive-hadoop2/hive-contrib-0.13.1.jar
        popd
}

change_jars
restart
service monit restart
{noformat}
With this, customer should be unblocked.

2. Queries are failing with NPE when timeout is set to lower values. With the above fix, the query finished in 18 mins(73915306). I will look further to find the root cause for this slowness. 
",22/May/17 9:30 PM;vamship;[~megha] Is the customer unblocked with the fix? Please update the priority accordingly. ,"26/May/17 11:41 AM;megha;I havnt heard from the customer in past few days.. Reducing the priority to critical for now, and will raise it back if required..
",26/May/17 2:19 PM;adubey;[~megha] if they had a test cluster - can we test it ourselves by bring up a 1 node cluster.,"28/May/17 3:35 AM;vamship;[~adubey] I've already re-ran query as mentioned above, it went through fine (ID- 73915306).","28/May/17 9:39 AM;stagra;[~vamship] we still need to investigate the original problem, it can resurface again. I know you were doing some experiments, please update the jira with the progress","07/Jun/17 12:01 AM;stagra;[~udayk] assigning this to you as vamshi is off for rest of the week and this has been brought up by Lyft as an important issue to resolve.

I would suggest you to repro this slowness problem on their cluster and recreate something similar in your vbox so that you can easily debug e.g. if they have 1000s of tables then you can create similar amount of tables in your vbox and run the same query that they ran to see where the slowness is.","13/Jun/17 1:19 AM;vamship;[~udayk] please check these 2 files, 
1. s3://lyftqubole-iad/tmp/error 
2. s3://lyftqubole-iad/tmp/output
I've uploaded the result of my experiments here. There are some invalid databases and tables that were logged as exceptions. We use a thrift api with retry logic, i'm guessing this throws an exception which is being suppressed and repeated for default number of times (hardcoded as 10).  

I've also added logic to extract tables in batches of 4k but haven't seen any significant improvement. ",21/Jun/17 9:59 AM;drose@qubole.com;[~vamship][~udayk]Do we have any updates on this request?,"21/Jun/17 9:47 PM;vamship;[~udayk] did you get a chance to look at this last week? if so, please update with your findings. ","22/Jun/17 3:23 AM;vamship;I had run some experiments to benchmark the retrieval times. Inside presto, to list columns we do the following operations.
1. Get all database names.
2. Get all tables in a database
3. Get all columns from a table.

2 is the expensive operation as it involves pulling all the table objects from metastore. Thrift internally pulls information from multiple places and constructs a unified table object. They have around 250 databases and 2nd op for certain databases is taking long amount of time.
||DB Name  ||Retrieval time(in secs)
|default|  450.04
|hle| 26.8977
|realtime_event|23.5946
|temp |32.0952

There are almost ~16k tables in 'default' database which explains the long retrieval time. So as of now, we don't have any straightforward solution to boost the performance. 

To avoid seeing NPE, we should increase the 'hive.metastore-timeout' parameter.

As a long term solution, i will try to investigate if we can write custom methods in presto to retrieve only column information which is what most of the BI tools need. I'll open a JIRA shortly to keep track of it.
",22/Jun/17 3:23 AM;vamship;CC: [~drose@qubole.com] [~megha],22/Jun/17 9:30 PM;vamship;Opened PRES-1120 to track the new feature.,"17/Jul/17 8:07 PM;p.vasa;[~vamship]
The customer tried setting the metastore timeout parameter to 20 mins and then further increased to 60 minutes.
With the metastore timeout set at 60 minutes, the query fired through Mode (Please see the attached file *""finished_fired_through_mode*"") took about 29 minutes for each stage to complete successfully and the query fired through Qubole (Command ID -> *84831823*) took about 36 minutes to complete.

The customer wants to bring down this time and here was the response ->
""Even if we increased the timeout, this query doesn't complete in less than 29 minutes, which is way too slow. We should reduce the response time for such queries no matter what.""

cc -> [~drose@qubole.com] for prioritization.",17/Jul/17 8:38 PM;vamship;Last time i checked the select queries were taking around 18 minutes. I will take a look again and will see what we can do to reach the target of 29 mins.,17/Jul/17 8:57 PM;vamship;[~p.vasa] Can you share the query ID that took 36 mins? ,"17/Jul/17 9:02 PM;drose@qubole.com;[~vamship] the query information is in Pratham's post.  ""[~vamship]
The customer tried setting the metastore timeout parameter to 20 mins and then further increased to 60 minutes.
With the metastore timeout set at 60 minutes, the query fired through Mode (Please see the attached file *""finished_fired_through_mode*"") took about 29 minutes for each stage to complete successfully and the query fired through Qubole (Command ID -> *84831823*) took about 36 minutes to complete.

The customer wants to bring down this time and here was the response ->
""Even if we increased the timeout, this query doesn't complete in less than 29 minutes, which is way too slow. We should reduce the response time for such queries no matter what.""

cc -> [~drose@qubole.com] for prioritization.""","18/Jul/17 2:37 AM;vamship;I think we found the root cause for the slowness. 

In my driver program to retrieve all tables from metastore i see couple of exceptions related to missing OpenCSV and DynamoDB Serdes. While retrieving tables from metastore presto uses a retry mechanism to overcome intermittent failures like network timeouts. Exceptions like this are getting suppressed and are retried for 10 times. 

I've updated this mechanism internally in presto and ran https://api.qubole.com/v2/analyze?command_id=84909811 to verify this theory and it finished in 17.5 mins!!

I will roll out a bootstrap shortly.
CC: [~stagra] [~drose@qubole.com]","18/Jul/17 3:32 AM;vamship;These are the tables for which serde's are missing.


{noformat}
2017-07-18 07:26:57,023 -1 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(688)) - 48: get_table : db=amalakar tbl=escape_exported
2017-07-18 07:26:57,054 -1 ERROR hive.log (MetaStoreUtils.java:getDeserializer(344)) - error in initSerDe: java.lang.ClassNotFoundException Class org.apache.hadoop.hive.serde2.OpenCSVSerde not found
java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.serde2.OpenCSVSerde not found
--
2017-07-18 07:28:16,436 -1 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(688)) - 48: get_table : db=default tbl=dynamodb
2017-07-18 07:28:16,467 -1 ERROR hive.log (MetaStoreUtils.java:getDeserializer(344)) - error in initSerDe: java.lang.ClassNotFoundException Class com.lyft.hive.serde.DynamoDbSerDe not found
java.lang.ClassNotFoundException: Class com.lyft.hive.serde.DynamoDbSerDe not found
--
2017-07-18 07:34:03,518 -1 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(688)) - 48: get_table : db=default tbl=invoices
2017-07-18 07:34:03,549 -1 ERROR hive.log (MetaStoreUtils.java:getDeserializer(344)) - error in initSerDe: java.lang.ClassNotFoundException Class com.lyft.hive.serde.DynamoDbSerDe not found
java.lang.ClassNotFoundException: Class com.lyft.hive.serde.DynamoDbSerDe not found
--
2017-07-18 07:34:04,727 -1 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(688)) - 48: get_table : db=default tbl=lineitems
2017-07-18 07:34:04,757 -1 ERROR hive.log (MetaStoreUtils.java:getDeserializer(344)) - error in initSerDe: java.lang.ClassNotFoundException Class com.lyft.hive.serde.DynamoDbSerDe not found
java.lang.ClassNotFoundException: Class com.lyft.hive.serde.DynamoDbSerDe not found
--
2017-07-18 07:34:06,846 -1 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(688)) - 48: get_table : db=default tbl=orders
2017-07-18 07:34:06,876 -1 ERROR hive.log (MetaStoreUtils.java:getDeserializer(344)) - error in initSerDe: java.lang.ClassNotFoundException Class com.lyft.hive.serde.DynamoDbSerDe not found
java.lang.ClassNotFoundException: Class com.lyft.hive.serde.DynamoDbSerDe not found
--
2017-07-18 07:37:51,965 -1 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(688)) - 48: get_table : db=default tbl=s3_inventory_info
2017-07-18 07:37:51,997 -1 ERROR hive.log (MetaStoreUtils.java:getDeserializer(344)) - error in initSerDe: java.lang.ClassNotFoundException Class org.apache.hadoop.hive.serde2.OpenCSVSerde not found
java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.serde2.OpenCSVSerde not found
--
2017-07-18 07:37:52,030 -1 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(688)) - 48: get_table : db=default tbl=s3_inventory_info_agp_temp
2017-07-18 07:37:52,061 -1 ERROR hive.log (MetaStoreUtils.java:getDeserializer(344)) - error in initSerDe: java.lang.ClassNotFoundException Class org.apache.hadoop.hive.serde2.OpenCSVSerde not found
java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.serde2.OpenCSVSerde not found
{noformat}
","18/Jul/17 11:22 AM;p.vasa;Thanks [~vamship] for looking into it.
Will wait for a Node Bootstrap script which we can provide to the customers.

I had a question regarding the changes ->
So are you reducing the number of retries through the Node Bootstrap script (or) making sure that these SerDe's are already present before the query is run thereby not producing intermittent Exceptions?","19/Jul/17 5:44 AM;vamship;i've got a chance to analyze this closely, my observations are ..

1.  Like i earlier thought the issue is neither with presto retries nor the serdes (the missing serdes needs a minor fix but that can be picked up later). When these exceptions occur in QDS presto they are skipped silently. The speedup we saw in y'day query(84909811) was probably because of a lesser load on metastore RDS (Checked that caching is disabled so it can't be the reason for the improvement).

2.  getTables() returns list of all table names in a database completed within 1 sec for default database which has around 10k tables. So this shows that n/w latency isn't an issue. Thrift metastore client also takes similar amount of times, so the issues can't be from presto code.

3.  Bulk retrieval of tables is expensive of all. It takes anywhere from 200 sec to 550 secs to extract from ""default"" table alone. 

Next, i will see how get_table_objects_by_name is implemented internally and where it spends most of its time (ex: RDS calls).
","25/Jul/17 11:25 AM;p.vasa;[~vamship] Any updates on this issue?
The customer has asked about the Root Cause for this issue and any workaround if possible.","25/Jul/17 8:47 PM;vamship;[~p.vasa] I went through zendesk ticket and as customer observed the slowdown is sporadic. The query is taking anywhere from 12-36 mins depending on RDS load. Since they have lot of tables and concurrency, we suggest customer to upgrade their metastore to minimize this variance. Even in such case 12 mins is too high for something as simple as metadata. We have identified the issue to be with thrift metastore server (and not in presto). I will update this thread once i've more intel on metastore server.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock found when trying to get lock; try restarting transaction,ACM-1207,63081,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,mdaurangzeb,mdaurangzeb,16/May/17 11:54 PM,07/Jun/17 2:42 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster startup failed And we saw there were dead lock message in the hustler log around that time stamp. Below is the snippet for the example pointing to other cluster ids though,But there are many such messages in the logs.


ip-10-146-80-145 PID: 10530  2017-05-11 22:46:21,684 DEBUG [cluster:31108] [cluster_instance:466119]  cluster.py:5342 - _lock_and_start - cluster_id 31108, cluster_inst_id 466119: exception while bringing up the cluster: cluster 'qbol_acc5680_cl31108' bring up failed and has been marked down

ip-10-146-80-145 PID: 10530  2017-05-11 22:46:23,289 ERROR [cluster:31108] [cluster_instance:466119]  clusterinfo.py:259 - execute - Cluster ""qbol_acc5680_cl31108"" could not be started and marked down: cluster 'qbol_acc5680_cl31108' bring up failed and has been marked down

ip-10-146-80-145 PID: 11012  2017-05-11 22:46:54,875 DEBUG [cluster:31108] [cluster_instance:466120]  cluster.py:5342 - _lock_and_start - cluster_id 31108, cluster_inst_id 466120: exception while bringing up the cluster: cluster 'qbol_acc5680_cl31108' bring up failed and has been marked down

ip-10-146-80-145 PID: 11012  2017-05-11 22:46:57,082 ERROR [cluster:31108] [cluster_instance:466120]  clusterinfo.py:259 - execute - Cluster ""qbol_acc5680_cl31108"" could not be started and marked down: cluster 'qbol_acc5680_cl31108' bring up failed and has been marked down
ip-10-146-80-145 PID: 11270  2017-05-11 22:47:29,068 DEBUG [cluster:31108] [cluster_instance:466121]  cluster.py:5342 - _lock_and_start - cluster_id 31108, cluster_inst_id 466121: exception while bringing up the cluster: cluster 'qbol_acc5680_cl31108' bring up failed and has been marked down
ip-10-146-80-145 PID: 11270  2017-05-11 22:47:30,944 ERROR [cluster:31108] [cluster_instance:466121]  clusterinfo.py:259 - execute - Cluster ""qbol_acc5680_cl31108"" could not be started and marked down: cluster 'qbol_acc5680_cl31108' bring up failed and has been marked down
ip-10-69-56-114                                )) execution failed: (1213, 'Deadlock found when trying to get lock; try restarting transaction'). Re-trying...

ip-10-81-158-141 PID: 8991  2017-05-11 22:48:28,553 DEBUG [cluster:20507] [cluster_instance:466122]  cluster.py:4197 - wait_for_hadoop - Checking RM and HDFSfailed in master.","Cluster : 31177

https://api.qubole.com/v2/analyze?command_id=71871855",mdaurangzeb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04qlb:,,,,,,,,,,,,,,,,,,,1.0,14580,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AM getting stuck when cluster nodes cycle between healthy/unhealthy status,HADTWO-921,63064,Bug,Open,HADTWO,Hadoop2,software,ajayb,,,Major,,sourabhg,Kulbir,Kulbir,16/May/17 4:29 PM,22/May/17 1:16 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Box saw an issue over the weekend where bunch of jobs(kicked via HS2 directly on master) were stuck. For e.g:
https://box-prod-1.qubole.com/cluster-proxy?clusterInst=255&encodedUrl=http%3A%2F%2Fec2-54-201-44-250.us-west-2.compute.amazonaws.com%3A8088%2Fcluster%2Fapp%2Fapplication_1494615717224_22944
https://box-prod-1.qubole.com/cluster-proxy?clusterInst=255&encodedUrl=http%3A%2F%2Fec2-54-201-44-250.us-west-2.compute.amazonaws.com%3A8088%2Fproxy%2Fapplication_1494615717224_23149%2F
https://box-prod-1.qubole.com/cluster-proxy?clusterInst=255&encodedUrl=http%3A%2F%2Fec2-54-201-44-250.us-west-2.compute.amazonaws.com%3A8088%2Fproxy%2Fapplication_1494615717224_23206%2F
https://box-prod-1.qubole.com/cluster-proxy?clusterInst=255&encodedUrl=http%3A%2F%2Fec2-54-201-44-250.us-west-2.compute.amazonaws.com%3A8088%2Fproxy%2Fapplication_1494615717224_23227%2F
https://box-prod-1.qubole.com/cluster-proxy?clusterInst=255&encodedUrl=http%3A%2F%2Fec2-54-201-44-250.us-west-2.compute.amazonaws.com%3A8088%2Fproxy%2Fapplication_1494615717224_23272%2F
https://box-prod-1.qubole.com/cluster-proxy?clusterInst=255&encodedUrl=http%3A%2F%2Fec2-54-201-44-250.us-west-2.compute.amazonaws.com%3A8088%2Fproxy%2Fapplication_1494615717224_23311%2F
https://box-prod-1.qubole.com/cluster-proxy?clusterInst=255&encodedUrl=http%3A%2F%2Fec2-54-201-44-250.us-west-2.compute.amazonaws.com%3A8088%2Fproxy%2Fapplication_1494615717224_23351%2F

I looked into one of the hung jobs- job_1494615717224_22944 where AM was killed and restarted 8 times. Sequence of events seems to be:
-Few of the nodes fill up all the EBS volume space, subsequently disk checker on NM marks node as Unhealthy. 
-At this point RM removes these nodes from cluster and also kills any running tasks on these nodes, as a result of which AM for above job was killed 7 times for e.g:
{code}
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:17:59,247 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application attempt appattempt_1494615717224_22944_000006 released container container_1494615717224_22944_06_000006 on node: host: ip-10-0-11-156.us-west-2.compute.internal:45454 #containers=7 available=2688 used=10752 with event: FINISHED
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:18:26,167 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1494615717224_22944_06_000001 Container Transitioned from RUNNING to KILLED
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:18:26,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Completed container: container_1494615717224_22944_06_000001 in state: KILLED event:KILL
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:18:26,167 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=anonymous	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1494615717224_22944	CONTAINERID=container_1494615717224_22944_06_000001
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:18:26,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1494615717224_22944_06_000001 of capacity <memory:1536, vCores:1> on host ip-10-0-17-212.us-west-2.compute.internal:45454, which currently has 6 containers, <memory:9216, vCores:6> used and <memory:4224, vCores:2> available, release resources=true
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:18:26,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application attempt appattempt_1494615717224_22944_000006 released container container_1494615717224_22944_06_000001 on node: host: ip-10-0-17-212.us-west-2.compute.internal:45454 #containers=6 available=4224 used=9216 with event: KILL
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:18:26,172 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1494615717224_22944_000006 with final state: FAILED, and exit status: -100

{code}

-Few minutes later disk usage goes down and nodes get added back to cluster, repeating same cycle, which is understood.
-However from AM logs there is no clear indication why the 8th attempt in this case never succeeded nor got killed, which is what needs to be investigated.

Log snippets from RM for above attempt only show these messages rolling every second:
*Last successful container for app attempt:*

{code}
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:26:02,171 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_1494615717224_22944_08_000007 of capacity <memory:1536, vCores:1> on host ip-10-0-6-228.us-west-2.compute.internal:45454, which currently has 7 containers, <memory:10752, vCores:7> used and <memory:2688, vCores:1> available, release resources=true
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051302.log.gz:2017-05-13 02:26:02,171 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application attempt appattempt_1494615717224_22944_000008 released container container_1494615717224_22944_08_000007 on node: host: ip-10-0-6-228.us-west-2.compute.internal:45454 #containers=7 available=2688 used=10752 with event: FINISHED
{code}
*Subsequently RM logs simply rolling with these messages every second:*
{code}
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051312.log.gz:2017-05-13 12:01:58,792 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1494615717224_22944_000008
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051312.log.gz:2017-05-13 12:01:58,792 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1494615717224_22944_000008
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051312.log.gz:2017-05-13 12:01:58,792 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: The AMRMToken has been rolled-over. Send new AMRMToken back to application: application_1494615717224_22944
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051312.log.gz:2017-05-13 12:01:59,793 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1494615717224_22944_000008
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051312.log.gz:2017-05-13 12:01:59,793 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1494615717224_22944_000008
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051312.log.gz:2017-05-13 12:01:59,793 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: The AMRMToken has been rolled-over. Send new AMRMToken back to application: application_1494615717224_22944

{code}

*AM killed manually by customer*
{code}
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051317.log.gz:2017-05-13 17:03:57,789 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1494615717224_22944,name=INSERT INTO TABLE zin...gestionSrc.hour\='02'(Stage-1),user=anonymous,queue=root.anonymous,state=KILLED,trackingUrl=http://ec2-54-201-44-250.us-west-2.compute.amazonaws.com:8088/cluster/app/application_1494615717224_22944,appMasterHost=N/A,startTime=1494666758734,finishTime=1494720237789,finalStatus=KILLED
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051317.log.gz:2017-05-13 17:03:57,991 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=ec2-user	IP=10.0.29.152	OPERATION=Kill Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1494615717224_22944
yarn-yarn-resourcemanager-ip-10-0-29-152.log.2017051317.log.gz:2017-05-13 17:03:58,426 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=anonymous	IP=10.0.65.181	OPERATION=Kill Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1494615717224_22944
{code}

This seems similar to HADTWO-610 however in this case it's a very small job, 2 mappers-2 reducers and from AM logs it seems like it was waiting for FS to allocate a mapper slot, can't see why it would get stuck for hours.


On a side note I understand running out of disk is the trigger point so i am working with box to add more EBS storage however still need to understand why AM got hung. Please investigate further.

AM logs: 
s3://dwh-qubole-prod/logs/hadoop/221/255/ec2-54-186-205-80.us-west-2.compute.amazonaws.com.node1545/yarn/application_1494615717224_22944/container_1494615717224_22944_08_000001

cc [~megha] [~p.vasa]",,abhishekmodi,Kulbir,megha,p.vasa,sourabhg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Box,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04qhb:,,,,,,,,,,,,,,,,,,,1.0,14635,2017-05-16 19:05:12.098,,,"16/May/17 7:05 PM;megha;While the stuck job was running, I also noticed that the AM was in ""starting"" state. However I didnt find this state in any logs. Is this a valid container state?  The job was killed by user soon after this so I couldn't get container's thread dumps.",16/May/17 11:26 PM;abhishekmodi;Assigning to on call engineer for initial investigation.,17/May/17 3:30 AM;sourabhg;[~Kulbir]: Can you upload logs of this cluster instance on google drive? I don't have the access to Box env. ,17/May/17 8:00 AM;Kulbir;[~p.vasa] can you help with log upload ? [~sourabhg] meanwhile it will be good if you can work on getting access to Box environment so we can avoid moving logs around. Also which logs are needed ? Uploading logs for entire cluster instance will be hard given size.,"17/May/17 12:03 PM;p.vasa;[~Kulbir] I can definitely sync up with the customer to ask them to provide us the logs.
As asked by Kulbir, [~sourabhg] Can you please provide some information on the specific logs for that cluster instance that you will require so that I can ask the customer to grab the exact logs?

Kulbir,
Can you please verify that ->
(Cluster Instance -> 255)","17/May/17 1:53 PM;Kulbir;[~p.vasa] yes it's 255, we can also see that in the job URL's I posted in issue description.","17/May/17 1:57 PM;p.vasa;Yes I grabbed it from there itself. Just wanted to verify.
Thanks [~Kulbir]",17/May/17 10:31 PM;sourabhg;[~Kulbir] [~p.vasa]: I need master logs for sure. It will be good if we can also get the logs of the nodes on which AM containers (all AM attempts) ran. ,"22/May/17 11:11 AM;p.vasa;[~sourabhg] I have uploaded all the ResourceManager logs to the following location in BOX environment ->
*s3://dwh-qubole-prod/BoxLogs/*

This location is under the <defloc> for account ID -> 8.

The app started at 2 am and was killed at 5 pm so started at 2 and grabbed for same day until 5 pm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reverse Tunnel failing ,MW-938,63030,Bug,Open,MW,Middleware,software,sumitm,,,Major,,ksr,satyavathib,satyavathib,16/May/17 5:34 AM,05/Jun/17 2:00 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Account ID : 5905
Cluster Id :31256

Metastore : Qubole Managed

Log says :

{code}
[ec2-user@ip-10-56-193-158 ~]$ /usr/lib/hive2/bin/thrift-metastore client
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.

Logging initialized using configuration in jar:file:/usr/lib/qubole/packages/hive2/hive2/lib/hive-common-0.13.1.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hive2/hive2/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
hive> show tables;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1422)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1407)
	at org.apache.hadoop.hive.ql.exec.DDLTask.showTables(DDLTask.java:2783)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:384)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:98)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1628)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1355)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1132)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:949)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:939)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:319)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:256)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:478)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:934)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:818)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:736)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:540)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:527)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:955)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy9.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1418)
	... 22 more
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
	... 37 more
{code}

{code}
[ec2-user@ip-10-56-193-158 ~]$ nc -vz localhost 7000
nc: connect to localhost port 7000 (tcp) failed: Connection refused
[ec2-user@ip-10-56-193-158 ~]$ nc -vz 127.0.0.1 8081
Connection to 127.0.0.1 8081 port [tcp/tproxy] succeeded!
{code}

Please let us know If you need further details.",,mdaurangzeb,psrinivas,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,vista,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04q9r:,,,,,,,,,,,,,,,,,,,1.0,14595,2017-06-05 02:00:43.434,,,05/Jun/17 2:00 AM;mdaurangzeb;[~ksr]  Do we have any update on this? Customer is waiting for our response on this. It would be really great if you can prioritize this. Thanks !!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration of Looker is not working RB43,JDBC-63,62942,Bug,Open,JDBC,JDBC,software,stagra,,,Major,,stagra,sbadam,sbadam,15/May/17 4:11 PM,17/May/17 11:58 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Looker is throwing problems on RB43.

Turner reported that their BI tool is throwing errors on RB43 when they are using it for scheduleing. Looker is talking to us through JDBC driver and here is stack trace that they observed:


{code:java}
Java::JavaSql::SQLException: [Qubole][QuboleDSII](100601) Exception: Unexpected character (W) at position 0.
{code}
",,sbadam,stagra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,turner,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04pxb:,,,,,,,,,,,,,,,,,,,1.0,14658,2017-05-15 20:28:44.953,,,"15/May/17 8:28 PM;stagra;Please share the query hist id.
Any other information? like what action was performed on looker? 
You specifically mention R43, was it working before?
[~sbadam]","15/May/17 8:59 PM;sbadam;Yes [~stagra]. It worked before for customer. He told us that there are no problems in our end but the integration is kind of failing from Looker side. He is expecting us to give a fix as he believing that RB43 introduced this problem.

In customer words:

{code:java}
As far as we can tell, the issue is on Looker's side. I did not see any errors generated in Qubole. We were just speculating whether there was a need to upgrade the JDBC driver due to a new release. I think the answer is ""yes"", they should upgrade at some point but the issues we were dealing with (Looker scheduler) is really related to the BI tool and I figure that you guys would rather have them fix it than get you guys involved unless it's necessary.

{code}

I asked him specific steps to reproduce the problem. I will update this ticket as soon as hear anything from his side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimizely jobs are stuck due to AWS-SDK 1.10.x dependency,SOL-146,62720,Bug,To Do,SOL,Solutions,software,Jtrail,,,Blocker,,Jtrail,adubey,adubey,11/May/17 9:03 AM,26/May/17 2:41 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Internal slack discussion - 
https://qubole.slack.com/archives/C44AFQR0W/p1494515238884238

Today we shipped hadtwo-839 , so i tried to validate their jobs blocked on SDK version 1.10.x but this did not solve the problem because job still picks up 1.7.4 version. According to them this job used to wrk fine on their CDH cluster and i think they may have other jars etc. Also this type of jobs can suffer from other customers as well because people now started using SDK version 1.10 in their jars. Can we address this , short term using some bootstrap solution and long term in our packaging .",,abhishekmodi,adubey,ajayb,psrinivas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,optimizely,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04ouf:,,,,,,,,,,,,,,,,,,,1.0,14520,2017-05-11 10:05:19.327,,,"11/May/17 9:11 AM;adubey;https://api.qubole.com/v2/analyze?command_id=71817537 ( after replacing 1.7.4 manually  )
https://api.qubole.com/v2/analyze?command_id=71814955 ( with original state of cluster )
","11/May/17 9:28 AM;adubey;cc [~ajayb] - can we have someone from hadoop team look into this for short and long term solution.

As mentioned - i suggested them shading some time ago but looks like that would be not very smooth option because - 
1. multiple jars
2. jar creation was owned by some other team so team we are talking may not have complete idea 
3. their argument is - why should i do it as it works on my CDH cluster ( they have on-prem 60 node cluster)","11/May/17 10:05 AM;ajayb;[~hiyer] can you look at it & explore alternatives?

[~adubey], just curious does their CDH cluster have aws sdk 1.10.x on it? Wondering because that's one of their arguments to not shade.","11/May/17 10:07 AM;adubey;I think the person we are working with does not have all the details .. so i am trying to figure and also asked him to shade for now.. let's see how that goes. 

","26/May/17 3:51 AM;abhishekmodi;The reason it works on CDH and not our environment is because we shade aws sdk 1.10.4 in our jars and thus it's not available to their jar file. If they are not going to use NativeS3FileSystem, we can provide a node bootstrap based solution to them which will not require changes at their end. Please let me know if we need to provide bootstrap based solution.

cc [~adubey]",26/May/17 2:41 PM;adubey;[~abhishekmodi] - i think it would be good to have a bootstrap based solution ready ( not just for this customer ) - because this is pretty generic issue when people come with their jars compiled with higher version of SDK for some good reasons.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot save a query in workspace for a Mediamath user,UI-5911,62612,Bug,Open,UI,UI,software,aswina,,,Major,,laliti,sbadam,sbadam,10/May/17 2:39 PM,06/Jun/17 8:28 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Can not save any query in workspace for a Mediamath user

Account: mediamath-att
User: jessica.nachtigall@omd.com

I checked her permissions. She has all permissions on Workspace. ""Save"" button doesn't work at all when she tries to save contents. It is easily reproducible in her account. 
",,aswina,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,UI-5909,,,,,,,,,,,,,,,,,AWS,,,,,,mediamath,,,,,{},NA,Choose from,,UI-4372,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04oj3:,,,,,,,,,,,,,,,,,,,1.0,14434,2017-05-10 21:06:50.825,,,10/May/17 9:06 PM;aswina;[~sbadam] - Does this user have any existing saved query in her account?,"10/May/17 10:09 PM;sbadam;There were no saved queries. Tried creating from UI but I couldn't. Created one using RDS query - PCR-23 which solved the problem temporarily(UI-5909)

{code:java}
production-replica> select * from saved_queries where qbol_user_id=30101\G
*************************** 1. row ***************************
            id: 66524
  qbol_user_id: 30101
    is_removed: 0
command_source: UI
          name: Saved_query_from_Qubole
    created_at: 2017-05-10 01:00:00
    updated_at: 2017-05-10 01:01:00
1 row in set (0.27 sec)

production-replica>
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive query stuck in hive tier,SOL-143,62543,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,megha,megha,09/May/17 3:50 PM,25/Jun/17 10:39 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command id 71429794 is stuck in hive tier for about 2 hours now..
I see lot of log lines similar to:
2017-05-09 22:33:48,851 [command:71429794] INFO  exec.Utilities (Utilities.java:getInputPaths(3630)) - Adding input file s3n://p3-guids-map/partner-cookie-map/3609/2016/02/17
2017-05-09 22:33:48,851 [command:71429794] INFO  exec.Utilities (Utilities.java:isEmptyPath(2880)) - Content Summary s3n://p3-guids-map/partner-cookie-map/3609/2016/02/17length: 4910602396 num files: 1 num directories: 0

for about 2 hours now.. 

What is it doing? 
It has not yet reached starting job submission and split computation

Attaching hive log until now

",,adubey,megha,psrinivas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/May/17 3:49 PM;megha;71429794.log;https://qubole.atlassian.net/secure/attachment/42125/71429794.log,09/May/17 4:00 PM;megha;jstack.log;https://qubole.atlassian.net/secure/attachment/42126/jstack.log,,,,,,,,,,,,AWS,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04o4f:,,,,,,,,,,,,,,,,,,,1.0,14507,2017-05-10 11:23:35.806,,,09/May/17 4:01 PM;megha;attached jstack files,"09/May/17 6:04 PM;megha;Have told them to retry the query.. However successful version of this query on hadoop1 is : 68439066, where it took only 1 min in job submission and split computation.., and i don't see any time lag before this phase..

","10/May/17 11:08 AM;megha;The re-run is stuck same way as yesterday: 71598406

Hive log keeps saying:
2017-05-10 18:06:32,503 [command:71598406] INFO  exec.Utilities (Utilities.java:isEmptyPath(2884)) - Content Summary not cached for hdfs://ec2-54-84-76-111.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/hive_2017-05-10_14-52-54_465_3343671741993350071-2/-mr-10002/75
2017-05-10 18:08:31,005 [command:71598406] INFO  exec.Utilities (Utilities.java:createDummyFileForEmptyPartition(3743)) - Changed input file s3n://p3-guids-map/partner-cookie-map/3609/2016/02/03 to empty file hdfs://ec2-54-84-76-111.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/hive_2017-05-10_14-52-54_465_3343671741993350071-2/-mr-10002/76
2017-05-10 18:08:31,005 [command:71598406] INFO  exec.Utilities (Utilities.java:getInputPaths(3630)) - Adding input file hdfs://ec2-54-84-76-111.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/hive_2017-05-10_14-52-54_465_3343671741993350071-2/-mr-10002/76
",10/May/17 11:23 AM;adubey;[~megha] i have not looked into details - but can you give one shot with hive on master mode ( just ruling out connectivity piece ) otherwise this might be some hive code level issue and we can look into that then,"10/May/17 2:54 PM;megha;New command id, with hive on master 71631752, went past the job submission and split computation., phase.
Talked to [~sbhatia] and have asked the customer to send one re-run on hadoop1 to compare the stats.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SyntaxError in Python: Non-ASCII character '\xe2' in file,SPAR-1591,62538,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,shridhar,snamburu,snamburu,09/May/17 11:02 AM,17/May/17 9:33 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Email: juthika.agrawal@tubemogul.com
Cluster: 15426
Cluster_type: Spark cluster

Please check this failed command id: 71447148
It fails with below syntax error:
SyntaxError: Non-ASCII character '\xe2' in file /media/ephemeral0/spark/spark-src-05-09-17-1494370591-76f12da2/script.py on line 8, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details
The query runns fine when i remove ""America's Got Talent"" from line 8. The apostrophe is causing this error.
Stackoverflow -> http://stackoverflow.com/questions/21639275/python-syntaxerror-non-ascii-character-xe2-in-file
Based on ^^ stackoverflow link, we need to add ""# -*- coding: utf-8 -*-"" to our py file.

Query fails in Analyze but works fine in Notebook.



",,adubey,drose@qubole.com,mahuja,snamburu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,tubemogul,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04o3b:,,,,,,,,,,,,,,,,,,,1.0,14483,2017-05-09 11:13:06.381,,,"09/May/17 11:13 AM;adubey;[~snamburu] 

1. Are they using spot instances and if yes what is the ratio on live cluster
2. are they using any fair scheduler xml?",10/May/17 3:19 PM;snamburu;Initial issue got resolved.  Changed the jira description based on another SyntaxError,13/May/17 7:52 PM;mahuja;[~shridhar] assigning to you. We can take this up post Hive authorization changes are done.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port forwarding process dying across all clusters in an account,MW-892,62528,Bug,Open,MW,Middleware,software,sumitm,,,Major,,ksr,ekang,ekang,09/May/17 8:04 AM,02/Aug/17 8:44 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,solsup,,,,,,"Account: Digital Data Services
User: shorsewood

All the clusters aren't able to reach the database due to missing port fowarding process. I went onto master of all the clusters in screenshot and ran 

sudo netstat -tulpn | grep 7000

and nothing showed up. I set up a brand new cluster and ran same command and I saw a process. I instructed the client to restart all their clusters so you may not be able to reproduce my steps. An easy way of creating this problem is to run a kill command on the port forwarding process and start running presto/spark jobs to see the comm link failures.

We have seen this sort of issue many times before and is a major issue because it impacts many clusters at once so larger clients end up seeing this issue in a very magnified way, e.g. Scripps reported dozens of jobs failing and are getting pinged internally by their folks asking what's wrong. We need a better way of managing the port forwarding process rather than running an ssh process that can disappear without notice.",,ashishs,ekang,ksr,ravil,surendranm,sureshr,,,,,,,,,,,,,,,,,,,,,,,,SOL-201,,,,09/May/17 8:00 AM;ekang;Screen Shot 2017-05-09 at 10.58.01 AM.png;https://qubole.atlassian.net/secure/attachment/42112/Screen+Shot+2017-05-09+at+10.58.01+AM.png,,,,,,,,,,,,,AWS,,,,,,scripps,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04o13:,,,,,,MW-RB45-Sprint-4,MW-RB45-Sprint-3,MW-RB46-Sprint-2,MW-RB46-Sprint-3,,,,,,,,,,1.0,14171,2017-05-17 08:39:26.157,,,"17/May/17 8:39 AM;sureshr;[~ksr]: Any updates on this one? Can you please investigate?
cc: [~sumitm]",31/May/17 8:39 AM;sureshr;Adding to current MW sprint for review. [~sumitm]: can you please follow-up on this?,"28/Jun/17 8:44 AM;sureshr;[~surendranm]: This is part of the infra charter. Can you please figure out a transition from Sameer?
cc: [~sumitm]",28/Jun/17 9:28 AM;surendranm;[~ksr] [~ashishs] isn't this already an autossh process? ,"28/Jun/17 10:07 AM;ashishs;from the description, it looks like that the reverse tunnel did not come up during cluster start. that is why there is nothing running on port 7000. there could be multiple reasons for this like connectivity issue, tunnel server being loaded etc. 
yes, it is an autossh process which keeps on spawning ssh process till it max out to 150 retries. 
in this case all the retries failed due to one reason or other. 

our current code right now does not take care to validate whether reverse tunnel got created or not and moves forward even if above reverse tunnel creation failed. 

[~sureshr] [~surendranm] this is what [~ksr] has fixed in his tunnel servers upgrade code, roll out of which is still under discussion. roll out of this fix is what requires a downtime as mentioned in the mail thread. ",04/Jul/17 10:42 AM;surendranm;[~sureshr] Should I assign this to [~ksr] for the moment? ,"04/Jul/17 11:02 AM;sureshr;Yes, I think it makes sense to have this assigned to [~ksr]. He has a fix ready for detecting these failures, but we need to figure out the rollout plan.","26/Jul/17 8:54 AM;sureshr;Any updates, [~ksr]? cc: [~sumitm]",26/Jul/17 8:55 AM;sureshr;also cc: [~ravil]. This needs customer communication due to the downtime. Can you check with CS team on this?,26/Jul/17 8:56 AM;sureshr;Let's nail down a plan and get this out. It's been pending for quite some time.,"26/Jul/17 2:17 PM;ravil;Sameer,

I believe you and Daniel are already on top of this. Plan is to have it
deployed in Aug first week?
I believe you are tracking the list of clients who may be impacted by this.

Please confirm.

Thanks,
Ravi


On Wed, Jul 26, 2017 at 8:55 AM, Suresh Ramaswamy (JIRA) <

","26/Jul/17 10:23 PM;ksr;[~sureshr] [~ravil] Yes, the plan is to deploy this on 1st August on the tunnel servers and the code will be deployed on the web nodes with the deployment of R46. [~gmargabanthu] had confirmed last week that the communication is out.","02/Aug/17 8:44 AM;sureshr;[~ksr]: Now that we have added the error checking to reverse tunnel creation, is there anything further to be done for this JIRA ticket?

cc: [~sumitm] and [~ravil]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cluster starting command fails with ""RemoteHostNotFound""",ACM-1184,62389,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,Ranjana,Ranjana,08/May/17 4:58 PM,27/Jun/17 10:59 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"+General Information+
Customer: Oracle
Submitted by: jeffrey.kamei@oracle.com
Zendesk ticket: #14487
Command ids: 71069525, 70901662, 70732426, 70731376, 70360437, 70360203

Problem description:  These jobs run at approximately same time start the cluster up. The command fails with following thrown in logs in UI. The same cluster instance come up successfully eventually and later re-run of same job is successful. This 1st run which brings up cluster fails.

For command id: 71069525
{code:java}
Qubole > 2017-05-08 03:09:12,888 INFO  shellcli.py:282 - main - Shell CLI Begin
No config file specified - defaulting to hustler/configs/config.default for config file
*** 2017-05-08 03:09:22,546 WARNING - Skipping keypair fingerprint validation...
*** 2017-05-08 03:09:30,614 WARNING - Skipping keypair fingerprint validation...
2017-05-08 03:09:30,615 >>> Cluster ""qbol_acc2243_cl29823"": checking hadoop because stable spot nodes may not be up in 472.615936995 seconds
2017-05-08 03:09:30,616 >>> Creating encrypted channel to master node: ec2-54-146-231-43.compute-1.amazonaws.com
2017-05-08 03:09:30,767 >>> Encrypted channel to cluster established successfully.
2017-05-08 03:09:40,163 >>> Hadoop startup successful
2017-05-08 03:09:40,163 >>> Waiting for Reverse Tunnel
2017-05-08 03:09:40,417 >>> Resource manager on master ec2-54-146-231-43.compute-1.amazonaws.com for cluster qbol_acc2243_cl29823 is accessible.
Qubole > 2017-05-08 03:09:41,166 ERROR shellcli.py:705 - <module> - Traceback (most recent call last):
  File ""/usr/lib/hive_scripts/shellcli.py"", line 681, in <module>
    status = main(None)
  File ""/usr/lib/hive_scripts/shellcli.py"", line 349, in main
    proxy = qconf.get_proxy_port()
  File ""/usr/lib/qubole/packages/hive_scripts-42.41.9/hive_scripts/qbol_config.py"", line 3832, in get_proxy_port
    raise RemoteHostNotFound(cluster_inst_id, self.acid)
RemoteHostNotFound: Remote Host not found for cluster instance: 461353 for account: 2243
{code}

The master node which did come up for this instance is  ec2-54-146-231-43.compute-1.amazonaws.com and it is a spot node.  

Custer Instance's config details as follows:
{code:java}
Cluster id: 29823
Cluster Config id: 94343
production-replica> select * from cluster_configs where cluster_id = '29823' and id = '94343' order by id desc limit 1\G
*************************** 1. row ***************************
                              id: 94343
                      cluster_id: 29823
              compute_access_key: NULL
              compute_secret_key: NULL
              compute_validated: 1
                      aws_region: us-east-1
          aws_availability_zone: Any
              hadoop_master_type: r3.2xlarge
              hadoop_slave_type: r3.2xlarge
            hadoop_initial_nodes: 5
                hadoop_max_nodes: 50
            custom_hadoop_config: yarn.scheduler.maximum-allocation-mb=244224
parquet.enable.summary-metadata=false
spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
mapreduce.compress.map.output=true
mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec
          fair_scheduler_config: NULL
                    default_pool: NULL
      hadoop_slave_request_type: spot
    maximum_bid_price_percentage: 150.0
        timeout_for_spot_request: 10
maximum_spot_instance_percentage: 100
      persistent_security_groups: qubole_vpc_securitygroup
            restrict_ssh_access: 0
            encrypted_ephemerals: 0
                customer_ssh_key: NULL
    disallow_cluster_termination: 0
              ganglia_monitoring: 0
                      created_at: 2017-04-06 17:02:35
                      updated_at: 2017-04-06 17:02:35
                      is_latest: 1
                  presto_jvm_mem: 6144
                presto_task_mem: 1000
                    force_tunnel: 1
            node_bootstrap_file: dft_node_bootstrap_prod_hyperloop_hadoop2.sh
              is_presto_enabled: 0
            custom_presto_config: NULL
            stable_spot_timeout: 10
      stable_spot_bid_percentage: 200.0
            stable_spot_fallback: 1
                          vpc_id: vpc-088e266d
                      subnet_id: subnet-88d870ff
                tunnel_server_ip: NULL
                    use_hadoop2: 1
                ebs_volume_count: 0
                ebs_volume_size: 100
                ebs_volume_type: standard
          use_stable_spot_nodes: NULL
    use_qubole_placement_policy: 1
            fallback_to_ondemand: 1
                custom_ec2_tags: {""Capability"":""Data"",""Team"":""DataFoundation"",""Business"":""Shared"",""Environment"":""Production""}
                      use_hbase: 0
                      use_spark: 1
                  config_errors: --- []

                compute_role_arn: XXXX
            compute_external_id: XXXX
                  ami_overrides: null
              datadog_api_token: XXXX
              datadog_app_token: XXXXXX
      use_account_compute_creds: 0
            custom_spark_config: 
                  spark_version: 2.0-latest
            hadoop_setup_timeout: NULL
          role_instance_profile: NULL
        bastion_node_public_dns: NULL
              master_elastic_ip: NULL
          spark_s3_package_name: NULL
        zeppelin_s3_package_name: NULL
              engine_config_type: NULL
                engine_config_id: NULL
                  presto_version: 0.142
                cloud_config_id: NULL
              cloud_config_type: NULL
            heterogeneous_config: {""memory"":[{""instance_type"":""r3.2xlarge"",""weight"":1.0},{""instance_type"":""r3.xlarge"",""weight"":0.5}]}
      zeppelin_interpreter_mode: legacy
                          is_ha: NULL
            ebs_upscaling_config: null
                    enable_rubix: 0
      compute_validation_result: null
            idle_cluster_timeout: NULL
            spot_block_duration: NULL
                instance_tenancy: NULL
1 row in set (0.00 sec)
{code}

 ",,ajayb,Harsh,Ranjana,wvaldez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04nqn:,,,,,,,,,,,,,,,,,,,1.0,14487,2017-05-09 15:02:36.319,,,"09/May/17 12:49 PM;Ranjana;[~Harsh] Two more command ids from today: 71390998, 71390985","09/May/17 3:02 PM;Harsh;From the command (71390998) logs the cluster hadoop check was successful but there were no nodes in DB. This is a race condition where 

{code:java}
2017-05-09 16:09:34,955 >>> Resource manager on master ec2-54-209-16-150.compute-1.amazonaws.com for cluster qbol_acc2243_cl29823 is accessible.
Qubole > 2017-05-09 16:09:35,739 ERROR shellcli.py:705 - <module> - Traceback (most recent call last):
  File ""/usr/lib/hive_scripts/shellcli.py"", line 681, in <module>
    status = main(None)
  File ""/usr/lib/hive_scripts/shellcli.py"", line 349, in main
    proxy = qconf.get_proxy_port()
  File ""/usr/lib/qubole/packages/hive_scripts-42.41.9/hive_scripts/qbol_config.py"", line 3832, in get_proxy_port
    raise RemoteHostNotFound(cluster_inst_id, self.acid)
RemoteHostNotFound: Remote Host not found for cluster instance: 463124 for account: 224
{code}
At 16:09:35, cmd figured the hadoop check is success and attempts to get node information from DB but the start cluster process has not yet logged nodes into DB. This happens at time 16:09:47:
{code:python}
PID: 28276  2017-05-09 16:09:47,845 DEBUG [cluster:29823] [cluster_instance:463124]  cluster.py:4891 - log_nodes0 - attempting logging for ec2-54-209-16-150.compute-1.amazonaws.com
{code}

While latching to the cluster, we should make sure the log nodes has completed successfully. Either by calling log_nodes or checking the DB too.

ACM-75 code change can help here as well.","14/Jun/17 2:27 PM;wvaldez;[~ajayb] Oracle is asking for an update / ETA on getting this fix.  Is there something I can share?  

Thank you,
Willie
","26/Jun/17 10:08 AM;wvaldez;[~ajayb] I asked the customer per email exchange, how often they are hitting this.  Their reply: ""this happens every time our Spark cluster is brought up.""",27/Jun/17 10:59 AM;ajayb;[~Harsh] can you check why Oracle is lucky to hit this race condition every time? Are we also seeing this for other customer's clusters? Would be good to confirm your earlier diagnosis using current logs of Oracle cluster bringup.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent AmazonClientException reading data from a table,PRES-1053,62384,Bug,Open,PRES,Presto,software,stagra,,,Major,,stagra,megha,megha,08/May/17 1:14 PM,09/Aug/17 2:35 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Example query: 71176907
Exception:
com.facebook.presto.spi.PrestoException
com.amazonaws.AmazonClientException: Unable to execute HTTP request: ds-data-prod.s3-us-west-2.amazonaws.com

A working query for data that is located in same s3 bucket ds-data-prod.s3-us-west-2.amazonaws.com: 71201503

The working query (and the fact that this error is intermittent), proves that the bucket access is allowed from the account as well as cluster.. I can also see the location from explore..

Is this error due to timeouts? If so, do we have retries? And does all retry fail leading to this failure.. Is there a scope to add more retries, or any other suggestions to improve results in this issue? 
Also, is it possible to get aws request id in this case if we need to coordinate with aws? ",,megha,navdeepp,satyavathib,stagra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04npj:,,,,,,,,,,,,,,,,,,,4.0,"14396,14999,16241,16294",2017-05-08 20:17:54.682,,,08/May/17 8:17 PM;stagra;This is due to dns failing to resolve host. There was a discussion on presto-escalations channel regarding the same on friday and we had opened a ticket with aws for recommendation. Right now we havent received any solid recommendation from them and we are looking at other places to bring down load on dns server to avoid this issue (PRES-1015),08/May/17 8:18 PM;stagra;And the request id is not captured in logs,"07/Aug/17 2:33 AM;navdeepp;Traveloka is facing a similar issue with their presto clusters:
7 Aug: (88494741 88503209 88503213 88503371 88503382)
6 Aug: 88353866

All the failed queries have failed for the bucket ""tvlk-data-datalake-prod.s3-ap-southeast-1.amazonaws.com"" where they have all/most of their data.

Caused by: java.net.UnknownHostException: tvlk-data-datalake-prod.s3-ap-southeast-1.amazonaws.com

They are using AWS provided dns for their subnet. 

cc [~vamship]
","09/Aug/17 2:34 AM;satyavathib;the same issue being faced by Activision as well frequently. Sample command ID : 87797921.

{code}
Unable to execute HTTP request: ds-data-prod.s3-us-west-2.amazonaws.com

Caused by: java.net.UnknownHostException
ds-data-prod.s3-us-west-2.amazonaws.com
java.net.InetAddress.getAllByName0(InetAddress.java:1280)
java.net.InetAddress.getAllByName(InetAddress.java:1192)
java.net.InetAddress.getAllByName(InetAddress.java:1126)
com.amazonaws.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:27)
com.amazonaws.http.DelegatingDnsResolver.resolve(DelegatingDnsResolver.java:38)
org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:111)
org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cluster scrubbed by ""health check process"", Logs missing for last hour",ACM-1180,62302,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,megha,megha,05/May/17 3:04 PM,07/Jul/17 12:53 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"It is not an all spot cluster, so master shouldnt have gone down.. Health check failure reason was unable to put data to hdfs...

cluster id: 30720
cluster_inst_id: 456145
s3 log location: s3://analytics-qubole-prod/prod-adhoc/logs/hadoop/30720/456145
cluster down time: 2017-05-05 15:57:44
last available log: s3://analytics-qubole-prod/prod-adhoc/logs/hadoop/30720/456145/ec2-52-54-221-246.compute-1.amazonaws.com.master/yarn/yarn-yarn-resourcemanager-ip-10-23-19-60.log.2017050514.log.gz

example missing logs: s3://analytics-qubole-prod/prod-adhoc/logs/hadoop/30720/456145/ec2-52-54-221-246.compute-1.amazonaws.com.master/yarn/yarn-yarn-resourcemanager-ip-10-23-19-60.log.2017050515.log.gz  i.e logs for 15th hour is missing..

Similarly, timelineserver, hdfs etc logs are missing as well..

I've seen this a few times, also for clusters where it is a user-initiated cluster shutdown.. I will add more such cases as I see them",,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04n7r:,,,,,,,,,,,,,,,,,,,1.0,14458,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
command failed due to service.StorageService internal server error,HAD-661,62301,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,hiyer,megha,megha,05/May/17 2:52 PM,18/May/17 11:33 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Command id that was hanging and then was killed: 70423931
The error message in job logs are a bunch of Internal Server errors as shown below:
{code:java}
App > 17/05/04 16:20:14 WARN -1 service.StorageService: Encountered 1 Internal Server error(s), will retry in 50ms
App > 17/05/04 16:30:08 WARN -1 service.StorageService: Encountered 1 Internal Server error(s), will retry in 50ms
App > 17/05/04 16:40:12 WARN -1 service.StorageService: Encountered 1 Internal Server error(s), will retry in 50ms
{code}

Notice that there is about 10 min gap between each of this error, and the retry was only 2nd time (i.e 50 seconds later). There were only a couple 3rd retries, and rest all calls seemed stuck for 10 or more minutes..

The shell mapper logs were same as the command logs on the UI (mapper logs are no longer available).


Another command that succeeded, but hanged for about 2 hours in between with similar issue: 70508414

As these errors are coming from aws, via jets3t api, I understand we might not be able to get deeper analysis on these issues. However, can we add some more logging to jets3t api so that we can get aws request ids etc to debug if this happens in future.. Some ami similar to one we created  for HADTWO-742 would suffice",,hiyer,Kulbir,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,pinterest,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04n7j:,,,,,,,,,,,,,,,,,,,1.0,14440,2017-05-05 20:41:21.369,,,"05/May/17 8:41 PM;hiyer;Looking at the logs, I don't think the calls were stuck. It would seem the retry succeeded and that's why we did not log anything after that. We can add more logs here to confirm this.","18/May/17 11:33 AM;megha;[~hiyer], is there an estimate on additional logging? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem cancelling a running paragraph,ZEP-916,62233,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,vipulm,mstolpner,mstolpner,04/May/17 8:23 PM,12/Jul/17 8:45 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,notebook-usability,,,,,,,"Congrui from Expedia has an issue that he cannot cancel a running paragraph. He posted description here: https://qubole.zendesk.com/hc/en-us/community/posts/115017703083-cannot-cancel-a-running-paragraph 

[~mpatel]",api.qubole.com,beria,mahuja,mstolpner,rgupta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,expedia,reltio,,,,{},NA,Choose from,,AN-207,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05aou:zzv,,,,,,nb-RB-45,nb-RB-47,,,,,,,,,,,,1.0,15498,2017-05-05 17:35:19.641,,,"05/May/17 5:35 PM;mahuja;[~mstolpner], [~dinesh.nair] - can we run this notebook? It will be helpful if we can get a standalone notebook to reproduce the issue.

","05/May/17 5:42 PM;mstolpner;[~mahuja] Please feel free to run and stop the notebook. The customer made a clone so they can keep working and we can experiment.
Thank you!","05/May/17 5:43 PM;mahuja;Excellent, thanks [~mstolpner]","08/May/17 1:57 AM;beria;This is actually a known issue and is present in OSS as well. When one tries to cancel a spark paragraph, corresponding spark jobs (if any) are attempted to cancel, and driver command statements are not attempted to cancel. 
For eg: Even this cannot be cancelled, and interpreter restart is the only option to stop this paragraph.
{noformat}
%spark
Thread.sleep(20000);
{noformat}

We can probably add this on our end (or OSS also), but this is not a regression.",08/May/17 7:12 AM;mahuja;[~beria] does it need to be investigated from Spark side ? Can you share the API Zeppelin is using for cancellation. ,"08/May/17 11:20 AM;beria;[~mahuja] Mostly not. Basically *SparkContext.cancelJobGroup(groupId);* is called on the driver, when a cancel is clicked on zeppelin. That doesn't do anything because if there are no *spark jobs* running for the job group. I will look into it more.",06/Jun/17 12:48 PM;mahuja;[~vipulm] is this a duplicate of your recent work? Or does it need Scala handling?,28/Jun/17 6:57 PM;beria;[~holden] Here we are tracking it. We have made similar fix for R https://qubole.atlassian.net/browse/ZEP-879 .  cc [~rgupta],"28/Jun/17 9:04 PM;rgupta;[~vipulm] has fixed this for R and py in rb45.

scala will be in rb46.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues despite the Hive thrift server 1.2 running on master node (started by node bootstrap),SOL-138,62070,Bug,To Do,SOL,Solutions,software,Jtrail,,,Blocker,,Jtrail,Ranjana,Ranjana,03/May/17 12:21 PM,07/Jun/17 2:28 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"+General Information+
Command id: 70117599  on api.qubole.com
Ticket submitted by: justin.wainwright@oracle.com
Ticket #: 13608
Cluster id: 10102
Account id: 2553
easy_hustler --cluster-id 10102 sshmaster qbol_acc2553_cl10102
Attachemnts: tunnel server logs, node bootstrap script, logs of node bootstrap script

+Problem description:+ Hive Thrift server 1.2 running, but jobs were failing with ""main WARN metastore: set_ugi() not successful, Likely cause: new client talking to old server. Continuing without it.
org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out"", until Hive thrift server was restarted. 

For cluster id 10102, account 2553, 
tunnel.use_tunnel_discovery is enabled
direct connect from master to metastore is False. 
So we turned ON hive.use_thrift_metastore1_2. But then, the thrift metastore script depended on reverse tunnel being available to start Hive thrift metastore server on master and often the cluster came up with Hive thrift server not running. To work around this, we set up a node bootstrap script, which will replace the default thrift-metastore with https://bitbucket.org/qubole/hive-1.2.0/src/fb74251cf229dc1b7ada5e09cfebe27c02c5db6e/bin/thrift-metastore?at=master&fileviewer=file-view-default which does not do reverse tunnel check. 
Now, the cluster is coming up with Hive thrift server running, but the jobs failed showing logs as follows:
{code:java}
17/05/02 21:43:05 main WARN metastore: set_ugi() not successful, Likely cause: new client talking to old server. Continuing without it.
org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_set_ugi(ThriftHiveMetastore.java:3688)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.set_ugi(ThriftHiveMetastore.java:3674)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:436)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1527)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:88)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:134)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:106)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3450)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3469)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1486)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:182)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:174)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:633)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)
	at org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)
	at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)
	at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)
	at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)
	at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)
	at org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)
	at org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699)
	at org.apache.spark.examples.sql.hive.SqlWrapper$$anonfun$3.apply(SqlWrapper.scala:31)
	at org.apache.spark.examples.sql.hive.SqlWrapper$$anonfun$3.apply(SqlWrapper.scala:31)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.examples.sql.hive.SqlWrapper$.main(SqlWrapper.scala:31)
	at org.apache.spark.examples.sql.hive.SqlWrapper.main(SqlWrapper.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:867)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:193)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:218)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:132)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
	... 74 more
{code}
See command ids: 70117599, 70117835, 70110833, 70096431
Note that the Hive thrift server 1.2 was running on master node at the time. I verified the node bootstrap script. Attaching the node bootstrap script and its logs with this jira. I am also attaching the excerpts from server.log on tunnel server. There was an error log related to the Hive metastore in the node bootstrap log as follows:
{code:java}
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/05/02 20:07:36 INFO conf.HiveConf: HiveConf of name hive.io.override.jsonserde does not exist
17/05/02 20:07:36 INFO conf.HiveConf: HiveConf of name hive.io.file.cache.basedir does not exist
javax.jdo.JDOFatalDataStoreException: Communications link failure

The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:359)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:388)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:285)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:255)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:595)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:573)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:626)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:460)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5753)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5748)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.startMetaStore(HiveMetaStore.java:5981)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:5906)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
NestedThrowablesStackTrace:
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure
{code}


Resolution done:
Restarted the Hive thrift server as follows:
/usr/lib/hive1.2/bin/thrift-metastore server stop
sudo jps -m
/usr/lib/hive1.2/bin/thrift-metastore server start
sudo jps -m
/usr/lib/hive1.2/bin/thrift-metastore server monitor
After that, all these commands started going through fine. 

Metastore details
{code:java}
irb(main):002:0> Metastore.where(:account_id => 2553,:active => 1)
  Metastore Load (0.6ms)  SELECT `metastores`.* FROM `metastores` WHERE `metastores`.`account_id` = 2553 AND `metastores`.`active` = 1
=> [#<Metastore id: 7468, db_name: ""hiveprod"", db_host: ""dlx-hive-q-prod-metastore.cah11koidopd.us-east-1.rd..."", db_user: ""hive"", db_passwd: ""XXXXX"", account_id: 2553, active: true, qubole_managed: false, created_at: ""2017-03-20 22:19:26"", updated_at: ""2017-03-20 22:19:27"", db_port: 3306, bastion_node_public_dns: nil, bastion_node_user: nil, bastion_node_private_key: nil, direct_access_metastore_from_cluster: false>]
{code}

The tunnel we observed from below is 23.21.156.210. The excerpt of logs obtained from this tunnel server is attached. 
{code:java}
[root@ip-10-108-57-192 ~]# netstat -tnpa | grep 'ESTABLISHED.*sshd'
tcp        0      0 10.108.57.192:22            23.21.156.210:33595         ESTABLISHED 12886/sshd: ec2-use 
tcp        0      0 10.108.57.192:54408         10.108.57.192:22            ESTABLISHED 13131/sshd: ec2-use 
tcp        0      0 10.108.57.192:22            54.243.39.255:53748         ESTABLISHED 18748/sshd: ec2-use 
tcp        0      0 10.108.57.192:22            23.23.178.159:33279         ESTABLISHED 13006/sshd: ec2-use 
tcp        0      0 10.108.57.192:22            23.21.156.210:40043         ESTABLISHED 14393/sshd: ec2-use 
tcp        0      0 10.108.57.192:22            54.243.39.255:34514         ESTABLISHED 13137/sshd: ec2-use 
tcp        0    340 10.108.57.192:22            54.204.43.32:51428          ESTABLISHED 13128/sshd: ec2-use 
tcp        0      0 10.108.57.192:56386         10.108.57.192:22            ESTABLISHED 17227/sshd: ec2-use 
tcp        0      0 10.108.57.192:22            23.23.178.159:53482         ESTABLISHED 17225/sshd: ec2-use 
tcp        0      0 10.108.57.192:22            54.204.43.32:47173          ESTABLISHED 48874/sshd: ec2-use
{code}


",,ajayb,drose@qubole.com,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,ACM-1163,,,,03/May/17 12:35 PM;Ranjana;HiveThriftServerIssues.zip;https://qubole.atlassian.net/secure/attachment/41968/HiveThriftServerIssues.zip,,,,,,,,,,,,,AWS,,,,,,Oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04mbb:,,,,,,,,,,,,,,,,,,,1.0,13608,2017-06-07 06:58:03.561,,,"03/May/17 1:01 PM;Ranjana;This issue reported again for cluster id_graph_spark easy_hustler --cluster-id 9432 sshmaster qbol_acc2553_cl9432
revavare [1:09 PM] 
always when the cluster first comes up the job fails
[1:09] 
and restarting the job works
[1:10] 
I did not restart the server from master","03/May/17 7:12 PM;Ranjana;Happened again with cluster id 9432
https://api.qubole.com/v2/analyze?command_id=70317742

The following was done to resolve
sudo /usr/lib/hive1.2/bin/thrift-metastore server stop
sudo /usr/lib/hive1.2/bin/thrift-metastore server start
sudo /usr/lib/hive1.2/bin/thrift-metastore server monitor","17/May/17 8:49 AM;Ranjana;We will continue work on this SOL jira depending on progress of ACM-1163, whichever way we choose to proceed. ",07/Jun/17 6:58 AM;ajayb;Can the solsup label be removed as the related jira ACM-1163 is now resolved?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ganglia throwing connection refused,SOL-137,62068,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,megha,megha,03/May/17 11:11 AM,15/Jun/17 9:56 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Oracle is facing some issues with ganglia page for cluster 29556.. 
Ganglia page throws:
There was an error collecting ganglia data (127.0.0.1:8652): fsockopen error: Connection refused
error..
This happened right after cluster start, and cluster has enough memory/disk space so this isnt due to memory issues..

I do see lot of errors in /var/log/httpd/error_log (attached herewith) - need some help in solving these errors, and why these errors occur..

Also attaching hustler_user_data.log to make sure everything for ganglia was setup correclty..

I restarted ganglia as per kb-257, and that worked fine..Ganglia did come up.. but the issue occurs again when cluster is restarted (not consistently though)",,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/May/17 11:11 AM;megha;error_log;https://qubole.atlassian.net/secure/attachment/41966/error_log,03/May/17 11:11 AM;megha;hustler_user_data.log;https://qubole.atlassian.net/secure/attachment/41965/hustler_user_data.log,,,,,,,,,,,,AWS,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04mav:,,,,,,,,,,,,,,,,,,,1.0,14303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem connecting Tunnel to Cluster Master ,ACM-1170,62004,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ksr,sbadam,sbadam,02/May/17 4:55 PM,19/May/17 2:52 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Problem in connecting Tunnel to Cluster Master 

We have observed this problem for couple of customers(Activision and TubeMogul) recently. Let me explain problem for Tubemogul which happened today.

1. Presto queries failed - 70039322, 70039322
2. By looking at logs, reverse tunnel was not successful.
3. Looking at tunneling logs(23.21.156.210) found that master node couldn't be connected as the process id went missing.


{code:java}
2017-05-02 15:03:44,666 ERROR Thread-26975624 init_tunnel_server.py:147 - _get_pid_from_file - Exception in _get_pid_from_file(/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid): [Errno 2] No such file or directory: '/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid'
2017-05-02 15:03:46,538 ERROR Thread-26975624 init_tunnel_server.py:147 - _get_pid_from_file - Exception in _get_pid_from_file(/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid): [Errno 2] No such file or directory: '/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid'
2017-05-02 15:03:46,542 ERROR Thread-26975624 init_tunnel_server.py:147 - _get_pid_from_file - Exception in _get_pid_from_file(/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid): [Errno 2] No such file or directory: '/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid'
2017-05-02 15:06:03,868 ERROR Thread-26975895 init_tunnel_server.py:147 - _get_pid_from_file - Exception in _get_pid_from_file(/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid): [Errno 2] No such file or directory: '/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid'
2017-05-02 15:08:09,772 ERROR Thread-26976131 init_tunnel_server.py:147 - _get_pid_from_file - Exception in _get_pid_from_file(/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid): [Errno 2] No such file or directory: '/media/ephemeral0/logs/tunneling/26241/qbol_acc648_cl26241/ip-10-171-122-249/dynamic/ec2-54-145-200-202.compute-1.amazonaws.com.pid'
{code}

4. Cluster start logs confirm that it started without any issues - 


{code:java}
                        id: 451766
                     cluster_id: 26241
              cluster_config_id: 96324
                  cluster_state: DOWN
                       start_at: 2017-04-28 07:30:56
                        down_at: 2017-05-02 18:02:52
               terminate_reason: USER-INITIATED
    last_health_check_action_at: NULL
       last_health_check_action: NULL
                     created_at: 2017-04-28 07:22:14
                     updated_at: 2017-05-02 18:02:52
start_cluster_manage_command_id: 119018
                 public_ssh_key: NULL
                private_ssh_key: NULL
          starting_process_host: 10.13.185.203
    starting_process_updated_at: 2017-04-28 07:27:14
{code}

So the problem is, connection broke in between and we couldn't reach to master node(as pid has nothing at that time). What can cause this problem? 
",,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04lxb:,,,,,,,,,,,,,,,,,,,1.0,14381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster doesn't honor Termination from UI - keep retrying ssh into master,ACM-1169,61997,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,sbadam,sbadam,02/May/17 2:51 PM,09/Jun/17 3:24 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster doesn't honor Termination from UI - keep retrying ssh into master

User ID: jason.weber@returnpath.com
Cluster: 26074, 30711

1. Cluster termination was not honored while waiting for Hadoop bring-up on cluster master.
Cluster: 26074, Instance ID: 456102, Command id: 70068706 

{code:java}
                        id: 456102
                     cluster_id: 26074
              cluster_config_id: 85244
                  cluster_state: DOWN
                       start_at: NULL
                        down_at: 2017-05-02 21:27:38
               terminate_reason: USER-INITIATED
    last_health_check_action_at: NULL
       last_health_check_action: NULL
                     created_at: 2017-05-02 20:45:50
                     updated_at: 2017-05-02 21:27:38
start_cluster_manage_command_id: 121411
                 public_ssh_key: NULL
                private_ssh_key: NULL
          starting_process_host: 10.137.126.120
    starting_process_updated_at: 2017-05-02 21:35:52
{code}
Logs: https://api.qubole.com/clusters#/start-logs/26074?instance=70068706

Cluster was asked to terminated at May 2nd, 21:27 GMT but we kept on retrying ""wait for Hadoop bringup""

2. Too many retries for Hadoop bringup status(we are spending almost 90mins before we timeout). 

https://api.qubole.com/clusters#/start-logs/30711?instance=70056554
https://api.qubole.com/clusters#/start-logs/26074?instance=70068706


3. I am not completely sure this is an issue. Two cluster commands are running at the same time for a single cluster. Below commands were run on cluster#26074 at the same time. 

https://api.qubole.com/clusters#/start-logs/26074?instance=70068706
https://api.qubole.com/clusters#/start-logs/26074?instance=70078033


",,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04lvz:,,,,,,,,,,,,,,,,,,,1.0,14325,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Message while accessing job logs - ""Authorization Failed. You need to be part of the account to which this cluster belongs to."" because of node not being present in cluster_nodes table",ACM-1172,61992,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,snamburu,snamburu,02/May/17 1:49 PM,03/Aug/17 4:12 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster: 2594
Command id: 70033320
Email id: tom.olson@shazam.com

Customer is getting ""Authorization failed"" error message when they try to access the Job Tracker task logs. The failed task page initially loads fine but when we click one of the links in the ‘Task Logs’ column, the error message pops up when trying to grab either the 4KB, 8KB, or complete logs.

Things observed so far:
1. Tested from system-admin account but same issue in that case as well.
2. There is only one account used by the customer
3. Suspected if this was occuring because of some permission issue but did not find any error in log server logs. Here is the log snippet
''''''''
ip-10-149-142-125 [INFO  pid: 21834: 17-05-02 20:36:27 ] Started GET ""/qpal/handle_proxy?query=http%3A%2F%2Fec2-54-167-136-120.compute-1.amazonaws.com%3A50030%2Fqboltaskdetailshistory.jsp%3Fjobid%3Djob_2441.201705021608_0001%26taskid%3Dtask_2441.201705021608_0001_m_000035&clusterInst=455836"" for 64.71.10.254 at 2017-05-02 20:36:27 +0000
ip-10-149-142-125 [INFO  pid: 21834: 17-05-02 20:36:27 ]   Parameters: {""query""=>""http://ec2-54-167-136-120.compute-1.amazonaws.com:50030/qboltaskdetailshistory.jsp?jobid=job_2441.201705021608_0001&taskid=task_2441.201705021608_0001_m_000035"", ""clusterInst""=>""455836""}
ip-10-149-142-125 [INFO  pid: 21834: 17-05-02 20:36:27 f978e67a] [user:9268] [account:2441] [su:12882] Fetching jtproxy url: http://localhost:8080/proxy?socks_port=-1&socks_server=&url=http%3A%2F%2Fec2-54-167-136-120.compute-1.amazonaws.com%3A50030%2Fqboltaskdetailshistory.jsp%3Fjobid%3Djob_2441.201705021608_0001%26taskid%3Dtask_2441.201705021608_0001_m_000035&clusterInst=455836
ip-10-149-142-125 [INFO  pid: 20517: 17-05-02 20:36:27 3c476af5] Redirected to https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fec2-54-167-136-120.compute-1.amazonaws.com%3A50030%2Fqboltaskdetailshistory.jsp%3Fjobid%3Djob_2441.201705021608_0001%26taskid%3Dtask_2441.201705021608_0001_m_000035&clusterInst=455836
ip-10-149-142-125 [INFO  pid: 21834: 17-05-02 20:36:27 f978e67a] [user:9268] [account:2441] [su:12882] GET: {""query""=>""http://ec2-54-167-136-120.compute-1.amazonaws.com:50030/qboltaskdetailshistory.jsp?jobid=job_2441.201705021608_0001&taskid=task_2441.201705021608_0001_m_000035"", ""clusterInst""=>""455836""}
ip-10-149-142-125 [INFO  pid: 21834: 17-05-02 20:36:27 f978e67a] [user:9268] [account:2441] [su:12882] {""query""=>""http://ec2-54-167-136-120.compute-1.amazonaws.com:50030/qboltaskdetailshistory.jsp?jobid=job_2441.201705021608_0001&taskid=task_2441.201705021608_0001_m_000035"", ""clusterInst""=>""455836"", ""controller""=>""qpal"", ""action""=>""handle_proxy""}
ip-10-149-142-125 [INFO  pid: 21834: 17-05-02 20:36:27 f978e67a] [user:9268] [account:2441] [su:12882] Handle Proxy - authorized for cluster_node: #<ClusterNode id: 1531751332, ec2_instance_id: ""i-01f7bb5eaf0fe97b4"", hostname: ""ec2-54-167-136-120.compute-1.amazonaws.com"", role: ""master"", amazon_instance_type: ""m1.large"", spot_instance: false, up_time: ""2017-05-02 16:08:36"", down_time: ""2017-05-02 17:05:18"", last_seen_time: ""2017-05-02 17:05:18"", account_id: 2441, private_ip: ""ip-10-63-54-122.ec2.internal"", cluster_inst_id: 455836, status: nil, private_ip_address: ""10.63.54.122"", termination_reason: ""Cluster Terminated."">, URL: http://ec2-54-167-136-120.compute-1.amazonaws.com:50030/qboltaskdetailshistory.jsp?jobid=job_2441.201705021608_0001&taskid=task_2441.201705021608_0001_m_000035
'''''''

",,hiyer,navdeepp,snamburu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/May/17 1:43 PM;snamburu;Screen Shot 2017-05-02 at 1.43.02 PM.png;https://qubole.atlassian.net/secure/attachment/41948/Screen+Shot+2017-05-02+at+1.43.02+PM.png,02/May/17 1:42 PM;snamburu;Screen Shot 2017-05-02 at 11.57.31 AM.png;https://qubole.atlassian.net/secure/attachment/41949/Screen+Shot+2017-05-02+at+11.57.31+AM.png,03/Aug/17 3:43 AM;navdeepp;Screen Shot 2017-08-03 at 4.12.24 PM.png;https://qubole.atlassian.net/secure/attachment/46441/Screen+Shot+2017-08-03+at+4.12.24+PM.png,,,,,,,,,,,AWS,,,,,,shazam,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04luv:,,,,,,,,,,,,,,,,,,,2.0,"14375,16255",2017-05-03 02:41:40.956,,,"03/May/17 2:41 AM;hiyer;There is no record of the node in question (180-171-126) in the cluster nodes table for this cluster instance. As a result, the permission check on tapp side failed. Assigning to ACM team to investigate why this node was never added to the cluster_nodes table. Another interesting observation is that for 2 nodes out of 20 in the cluster_instance, the private_ip column is empty.","03/Aug/17 3:41 AM;navdeepp; !Screen Shot 2017-08-03 at 12.23.48 PM.png|thumbnail! 
Saw this for pinterest. Also, checked for the node, it was present in the cluster_nodes table
{code}
production-replica> select * from cluster_nodes where cluster_inst_id=568340 and private_ip='ip-10-1-129-93.ec2.internal';
+------------+---------------------+-------------------------------------------+----------+----------------------+---------------+---------------------+-----------+---------------------+------------+-----------------------------+-----------------+--------+--------------------+--------------------+
| id         | ec2_instance_id     | hostname                                  | role     | amazon_instance_type | spot_instance | up_time             | down_time | last_seen_time      | account_id | private_ip                  | cluster_inst_id | status | private_ip_address | termination_reason |
+------------+---------------------+-------------------------------------------+----------+----------------------+---------------+---------------------+-----------+---------------------+------------+-----------------------------+-----------------+--------+--------------------+--------------------+
| 1686829025 | i-011cbd2d70be1853a | ec2-34-227-65-212.compute-1.amazonaws.com | node0065 | c3.8xlarge           |             0 | 2017-08-03 09:51:18 | NULL      | 2017-08-03 10:24:40 |       3508 | ip-10-1-129-93.ec2.internal |          568340 | NULL   | 10.1.129.93        | NULL
{code}",03/Aug/17 3:43 AM;navdeepp; !Screen Shot 2017-08-03 at 4.12.24 PM.png|thumbnail! ,"03/Aug/17 4:03 AM;hiyer;Pinterest is using a custom dns domain and the links are correctly pointing to that e.g, https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fip-10-1-128-37.ec2.pin220.com%3A50060%2Ftasklog%3Ftaskid%3Dattempt_4911.201707141711_27616_m_000000_0%26all%3Dtrue&clusterInst=542556

However hustler has stored the host with its ec2.internal name, so we are not able to find it in our query.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Application link not Found ,SOL-136,61951,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,sbadam,sbadam,01/May/17 11:10 PM,26/May/17 12:57 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Spark Application link not Found 

User: bahubali.jain@clarivate.com
Customer: Clarivate/ Thomson Reuters
Command id: 69750294, Cluster ID: 28908, Instance: 454305 

Link which is not working: https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-34-209-165-228.us-west-2.compute.amazonaws.com%3A18080%2Fhistory%2Fapplication_1493618074319_0002%2F1%2F%3Fspark%3Dtrue&clusterInst=454305

Helping points:
1. Logs are present at: s3://ims-poc-data/logs/hadoop/28908/454305
2. Command start time - 1st May 05:50AM GMT, finish - 1st May ~07:13AM GMT
3. Cluster terminated at - 1st May 2017-05-01 08:58:51 GMT. There is enough time to write data into S3. 

But I see that application is not written into S3 completely. Here are the logs:
file: s3://ims-poc-data/logs/hadoop/28908/454305/ec2-34-209-165-228.us-west-2.compute.amazonaws.com.master/spark/spark-yarn-org.apache.spark.deploy.history.HistoryServer-1-ip-172-17-201-57.out has
...

{code:java}
17/05/01 05:56:05 log-replay-executor-0 INFO FsHistoryProvider: Replaying log path: hdfs://ec2-34-209-165-228.us-west-2.compute.amazonaws.com:9000/spark-history/application_1493618074319_0002.lz4.inprogress.las
17/05/01 05:56:06 log-replay-executor-0 WARN ReplayListenerBus: Got JsonParseException from log file hdfs://ec2-34-209-165-228.us-west-2.compute.amazonaws.com:9000/spark-history/application_1493618074319_0002.lz4.inprogress.las at line 2, the file might not have finished writing cleanly.
17/05/01 05:56:06 log-replay-executor-0 WARN FsHistoryProvider: Failed to load application log hdfs://ec2-34-209-165-228.us-west-2.compute.amazonaws.com:9000/spark-history/application_1493618074319_0002.lz4.inprogress.las. The application may have not started.
17/05/01 05:56:16 log-replay-executor-1 INFO FsHistoryProvider: Replaying log path: hdfs://ec2-34-209-165-228.us-west-2.compute.amazonaws.com:9000/spark-history/application_1493618074319_0002.lz4.inprogress.las
{code}

4. I see that mr-history folder is empty on S3.


",,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04llr:,,,,,,,,,,,,,,,,,,,1.0,14191,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scaling.log growing inordinately and filling up master node disk,ACM-1164,61928,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,ekang,ekang,01/May/17 12:27 PM,11/Jul/17 3:46 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"scaling.log doesn't stop growing. Customer complained twice that they saw the log file reaching 70gb and causing their cluster to run out of disk space. Can you implement rolling logs, e.g. logs by day?",,ekang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,turner,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04lgn:,,,,,,,,,,,,,,,,,,,1.0,14352,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler Macro vars within notebook pyspark command,ZEP-906,61816,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,beria,ekang,ekang,28/Apr/17 1:06 PM,12/Jun/17 6:00 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"client is trying to run the following where yesterday is a scheduler macro var. Please advise how this can be done? thanks.

%pyspark
z.put(""yest"",$yesterday$)
yest = z.get(""yest"")
print(yest)

Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark.py"", line 247, in <module>
    compiledCode = compile(final_code, ""<string>"", ""exec"")
  File ""<string>"", line 1
    z.put(""yest"",$yesterday$)
                 ^
SyntaxError: invalid syntax",,ekang,mahuja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,scripps,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04l7z:,,,,,,nb-RB-45,,,,,,,,,,,,,1.0,14318,2017-05-01 23:01:15.166,,,"01/May/17 11:01 PM;mahuja;This looks like a case where parameters need to be passed to notebooks. Scheduler macros are not passed to notebooks today. [~beria] is working on a feature where notebook run API can have parameters.

Given that, it will need changes to scheduler interface to pass the parameters from UI/API.

",02/May/17 3:26 AM;ekang;is there a Jira ticket for Anirudh's new feature so I can track it?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notebook bad gateway 502 error on cluster start,ZEP-905,61805,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,beria,satyavathib,satyavathib,28/Apr/17 4:30 AM,28/Jun/17 12:13 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Got two complaints from expedia and reltio that they are facing 502 bad gateway error on cluster restart in notebooks.

Pasting a screenshot from reltio notebooks :

 !image-2017-04-28-16-59-14-599.png|thumbnail! 

{code}
nebansal [1:38 PM] 
Hi Guys, We are unable to open any notebook connected to cluster id :29388. This is happening since we restarted the cluster. The notebook keeps loading and then terminates with ""502:Bad Gateway error"". Can you please suggest if anything can be done
{code}

Above snippet from expedia channel.

Please let me know If any details required.",,gmargabanthu,mahuja,mmajithia,mpatel,Ranjana,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/May/17 11:37 AM;mmajithia;Screen Shot 2017-05-13 at 12.06.50 AM.png;https://qubole.atlassian.net/secure/attachment/42332/Screen+Shot+2017-05-13+at+12.06.50+AM.png,25/May/17 12:00 AM;satyavathib;Screen Shot 2017-05-25 at 12.19.34 PM.png;https://qubole.atlassian.net/secure/attachment/42834/Screen+Shot+2017-05-25+at+12.19.34+PM.png,28/Apr/17 4:29 AM;satyavathib;image-2017-04-28-16-59-14-599.png;https://qubole.atlassian.net/secure/attachment/41802/image-2017-04-28-16-59-14-599.png,,,,,,,,,,,AWS,,,,,,expedia,hotels.com,reltio,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04l5j:,,,,,,nb-RB-45,,,,,,,,,,,,,3.0,"14316,14452,14849",2017-05-09 14:54:16.963,,,05/May/17 8:07 AM;satyavathib;Same issue reported from Expedia today. Could you please let me know If we have  any possible fix for this or Should I communicate the customer accordingly. This is just to set appropriate expectations to the customer.,"09/May/17 2:54 PM;Ranjana;For the purpose of fix, from ticket #14316 from Reltio, this is happening for cluster 28488 and notebook id is 33954. ","11/May/17 8:28 AM;mmajithia;[~satyavathib], [~Ranjana]: We have made a fix with rb41, which will handle the case when a cluster is UP and zeppelin is starting which result into 502 error. This fix is only available in new notebook page with foldering. Could you please move the customer to use virtual foldering so that this error can be surfaced?

KB to enable virtual foldering: KB-539
","11/May/17 7:04 PM;mahuja;[~Ranjana], [~satyavathib] - do the clusters reported belong to accounts with notebook foldering enabled?","12/May/17 10:17 AM;satyavathib;[~mmajithia][~mahuja] I could see that for expedia account 5497 ""tapp.enable_intelligent_notebook_loading"" and ""tapp.enable_foldering"" are enabled . I am not very clear of how to enable virtual foldering for an already foldering enabled customers from KB-539. It would be great If someone could help me on that. ","12/May/17 10:58 AM;mmajithia;[~satyavathib]: Even if foldering is enabled, changes are available by default without any other flags. As per the attached screenshot, user is facing the issue on old notebook page without foldering. One of the way to figure it out: if url is api.qubole.com/v2/notebooks, changes are not available.

I have quickly verified the account which belongs to the notebook opened in the screenshot doesn't have foldering.
Could you please enable virtual foldering for the customer?

{code:java}
production-replica> select * from notes where id=33954\G;
*************************** 1. row ***************************
                   id: 33954
              note_id: 33R49XZSUH1491512484
                 name: swarna-aa-demo-01
           created_at: 2017-04-06 21:01:24
           updated_at: 2017-04-24 16:23:05
            note_type: spark
           account_id: 5708
           cluster_id: 28488
           deleted_at: NULL
            read_only: 0
               source: FCN
         qbol_user_id: 17270
  space_subscriber_id: NULL
published_notebook_id: NULL
            link_type: NULL
              link_id: NULL
             location: NULL
            file_name: NULL
     parent_folder_id: NULL
1 row in set (0.00 sec)

ERROR: 
No query specified

production-replica> select * from account_features where account_id=5708;
+-------+------------+---------+--------------------------------------+------------+---------------------+---------------------+--------------+
| id    | account_id | user_id | feature_name                         | is_enabled | created_at          | updated_at          | segment_name |
+-------+------------+---------+--------------------------------------+------------+---------------------+---------------------+--------------+
| 14485 |       5708 |    NULL | hive.enable_hive_authorization       |          0 | 2016-08-08 17:26:27 | 2016-08-19 04:57:20 | NULL         |
| 14605 |       5708 |    NULL | enable_s3a_filesystem                |          1 | 2016-08-22 21:02:40 | 2016-08-22 21:02:40 | NULL         |
| 15338 |       5708 |    NULL | zeppelin.buffer_append_output        |          1 | 2016-09-08 06:10:20 | 2016-09-08 06:10:20 | NULL         |
| 15624 |       5708 |    NULL | tapp.ui_enable_node_bootstrap_editor |          1 | 2016-09-08 06:12:23 | 2016-09-08 06:12:23 | NULL         |
| 16598 |       5708 |    NULL | zeppelin.process_qlog                |          1 | 2016-09-15 06:27:38 | 2016-09-15 06:27:38 | NULL         |
| 16845 |       5708 |    NULL | cluster.encrypt_keys                 |          1 | 2016-09-15 06:28:55 | 2016-09-15 06:28:55 | NULL         |
| 16984 |       5708 |    NULL | tapp.enable_aws_roles                |          1 | 2016-09-15 06:30:21 | 2016-09-15 06:30:21 | NULL         |
| 17906 |       5708 |    NULL | hadoop2.use_username_as_ugi          |          1 | 2016-09-23 05:26:49 | 2016-09-23 05:26:49 | NULL         |
| 20040 |       5708 |    NULL | spark.qubole_split_computation       |          1 | 2016-09-29 06:44:34 | 2016-09-29 06:44:34 | NULL         |
| 20320 |       5708 |    NULL | spark.defaults_v2                    |          1 | 2016-09-29 06:46:05 | 2016-09-29 06:46:05 | NULL         |
| 24719 |       5708 |    NULL | ssl.enable_in_cluster                |          1 | 2017-01-26 22:16:49 | 2017-01-26 22:16:49 | NULL         |
| 25315 |       5708 |    NULL | tapp.hadoop_use_shellcli             |          1 | 2017-02-03 02:24:18 | 2017-02-03 02:24:18 | NULL         |
| 26479 |       5708 |    NULL | tapp.enable_analyze_nav_away_warning |          1 | 2017-02-07 09:23:26 | 2017-02-07 09:23:26 | NULL         |
| 26897 |       5708 |    NULL | tapp.enable_customer_cluster_dd      |          1 | 2017-02-07 09:38:44 | 2017-02-07 09:38:44 | NULL         |
+-------+------------+---------+--------------------------------------+------------+---------------------+---------------------+--------------+
{code}

","12/May/17 11:28 AM;satyavathib;[~mmajithia] Yes... I too checked fro reltio and even foldering is not enabled for reltio. But could you clarify me on expedia case. As mentioned above for account : 5497 Notebook ID - 36152 
Cluster ID - 29388 I found the features :

{code}
*************************** 16. row ***************************
          id: 31878
  account_id: 5497
     user_id: NULL
feature_name: zeppelin.latency_improvement
  is_enabled: 1
  created_at: 2017-05-08 22:39:43
  updated_at: 2017-05-08 22:39:43
segment_name: NULL

*************************** 14. row ***************************
          id: 31355
  account_id: 5497
     user_id: NULL
feature_name: tapp.enable_intelligent_notebook_loading
  is_enabled: 1
  created_at: 2017-04-27 17:02:47
  updated_at: 2017-04-27 17:02:47
segment_name: NULL

*************************** 10. row ***************************
          id: 23534
  account_id: 5497
     user_id: NULL
feature_name: tapp.enable_foldering
  is_enabled: 1
  created_at: 2017-01-13 00:09:15
  updated_at: 2017-01-13 00:09:15
segment_name: NULL
*************************** 11. row ***************************
          id: 23535
  account_id: 5497
     user_id: NULL
feature_name: zeppelin.folder_sync
  is_enabled: 1
  created_at: 2017-01-13 00:09:33
  updated_at: 2017-01-13 00:09:33
segment_name: NULL
*************************** 12. row ***************************
          id: 23536
  account_id: 5497
     user_id: NULL
feature_name: zeppelin.enable_acl
  is_enabled: 1
  created_at: 2017-01-13 00:09:53
  updated_at: 2017-01-13 00:09:53
segment_name: NULL
*************************** 13. row ***************************
          id: 24575
  account_id: 5497
     user_id: NULL
feature_name: tapp.ui_show_cluster_interpreter_mode
  is_enabled: 1
  created_at: 2017-01-26 00:08:57
  updated_at: 2017-01-26 00:08:57
segment_name: NULL

{code}","12/May/17 11:37 AM;mmajithia;[~satyavathib]: I am able to open the notebook:
 !Screen Shot 2017-05-13 at 12.06.50 AM.png|thumbnail! 
","12/May/17 11:51 AM;satyavathib;As this issue was reported by expedia on cluster restart , unfortunately we could not have a screenshot for the same.The notebook is accessible as usual after few minutes.  I have requested for a screenshot from them if they face this issue again. ","12/May/17 11:51 AM;mahuja;From my understanding of conversation till now:

Reltio does not have folder enabled and hence the fix for 502 does not apply. Need to move them to folders. cc - [~gmargabanthu]

For Expedia, folders are enabled. [~mmajithia] I believe the notebooks are generally accessible but this error is seen only on start of cluster","12/May/17 11:59 AM;mmajithia;[~mahuja]: 502 error can happen with new notebook page only if it takes more than 3 minutes after the cluster restart, we are planning to bump up it to 5 minutes.",12/May/17 12:19 PM;mahuja;We should look at this cluster start logs to extract the right value. [~mmajithia] you might need help from Zeppelin team to look at the logs cc - [~karuppayyar],"25/May/17 12:01 AM;satyavathib;Expedia has agin faced this gate way error in a HBASE notebook.

Screenshot for the same :  !Screen Shot 2017-05-25 at 12.19.34 PM.png|thumbnail! 

As per [~mmajithia] we could avoid these issues If we could move them to virtual foldering.

[~mpatel][~gmargabanthu] Please let me know If we could do that for expedia.","25/May/17 12:02 AM;mmajithia;[~satyavathib]: Customer are using old notebook page, please move them to virtual foldering in order to surface 502 error.","30/May/17 9:52 AM;mpatel; [~satyavathib] let's enable virtual foldering for the LAE account. 

[~mmajithia] this account only uses notebooks for HBase. Any issues in enabling virtual foldering here?",30/May/17 10:47 AM;mmajithia;[~mpatel]: Virtual foldering should work fine for HBase notebook only account.,"12/Jun/17 5:12 PM;mpatel;[~mmajithia] my understanding is to turn on virtual foldering we need to restart the cluster. How does that work for HBase? 

[~satyavathib] mentioned the customer is still seeing 502s ...","12/Jun/17 10:49 PM;mmajithia;[~mpatel] It seems it was missed with Virtual foldering implementation, ZEP-1038 will fix it for Virtual foldering as well. Even after we have the fix, there is something wrong which shows 502 error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Application link is broken intermittently - leading to 504 errors,SOL-133,61710,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,sbadam,sbadam,27/Apr/17 10:45 PM,26/May/17 12:57 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Spark Application link is broken intermittently - leading to 504 errors

Link: https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-34-209-74-246.us-west-2.compute.amazonaws.com%3A18090%2Fhistory%2Fapplication_1493191098003_0006%2Fjobs%2F&clusterInst=449462

User ID: bahubali.jain@tubemogul.com
Spark Application link for Command ID: 68841322

It is working well for sometime and it is breaking all of a sudden(gives 504 error). I looked into cluster_proxy logs at the time when I faced problem. I couldn't see any error there.

I don't see any problem in cluster_proxy logs when link is accessed. Interestingly, the link works after some time. I captured screenshot when it didn't work. PFA. From cluster_proxy logs, this is what fetched.

{code:java}
ip-10-180-81-53 2017-04-28 04:25:54,321 [qtp1023892928-28] INFO  com.qubole.cluster_proxy.ClusterProxyServlet - Updated Location Header: http://ec2-34-209-74-246.us-west-2.compute.amazonaws.com:18090/history/application_1493191098003_0006/?spark=true&storage_cloud=aws&s3_role_arn=arn%3Aaws%3Aiam%3A%3A509786517216%3Arole%2Fqubole&s3_external_id=GM4TCMDGM5SGG2DF&s3_loc_prefix=&s3_access_key_id=&s3_secret_access_key=
ip-10-180-81-53 2017-04-28 04:25:54,322 [qtp1023892928-28] INFO  com.qubole.cluster_proxy.ClusterProxyServlet - Returned Location Header: https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-34-209-74-246.us-west-2.compute.amazonaws.com%3A18090%2Fhistory%2Fapplication_1493191098003_0006%2F%3Fspark%3Dtrue%26storage_cloud%3Daws%26s3_role_arn%3Darn%253Aaws%253Aiam%253A%253A509786517216%253Arole%252Fqubole%26s3_external_id%3DGM4TCMDGM5SGG2DF%26s3_loc_prefix%3D%26s3_access_key_id%3D%26s3_secret_access_key%3D&clusterInst=449462
ip-10-180-81-53 2017-04-28 04:25:54,322 [qtp1023892928-28] INFO  com.qubole.cluster_proxy.ClusterProxyServlet - Copying Response...
...
...

{code}


Some related logs after some time for this account(3910): 


{code:java}
ip-10-97-204-49 2017-04-28 05:17:48,576 [qtp1023892928-12] INFO  com.qubole.cluster_proxy.cloud.AwsCloud - IAM Roles Feature account override is true for accountId: 3910
ip-10-97-204-49 2017-04-28 05:17:48,576 [qtp1023892928-12] INFO  com.qubole.cluster_proxy.cloud.AwsCloud - IAM Roles are enabled for accountId: 3910
ip-10-97-204-49 2017-04-28 05:17:48,576 [qtp1023892928-12] INFO  com.qubole.cluster_proxy.cloud.AwsCloud - Account: 3910 is using IAM Roles
ip-10-97-204-49 2017-04-28 05:17:48,603 [qtp1023892928-12] ERROR com.qubole.cluster_proxy.MultiJhs - Bad URL for multitenant jobhistory server: ':19888/' or ':18080/' or ':18090/' or ':10500' or ' :8188/' not in URL.
ip-10-97-204-49 2017-04-28 05:17:48,605 [qtp1023892928-12] ERROR com.qubole.cluster_proxy.ClusterProxyServlet - Some problem with reply from SOCKS server. Probably no server running on the other side.
ip-10-97-204-49 org.apache.http.conn.HttpHostConnectException: Connect to localhost:8088 [localhost/127.0.0.1] failed: Connection refused
ip-10-97-204-49         at org.apache.http.impl.conn.HttpClientConnectionOperator.connect(HttpClientConnectionOperator.java:140)
ip-10-97-204-49         at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:318)
ip-10-97-204-49         at org.apache.http.impl.execchain.MinimalClientExec.execute(MinimalClientExec.java:182)
ip-10-97-204-49         at org.apache.http.impl.client.MinimalHttpClient.doExecute(MinimalHttpClient.java:107)
ip-10-97-204-49         at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
ip-10-97-204-49 2017-04-28 05:17:48,532 [qtp1023892928-12] INFO  com.qubole.cluster_proxy.ClusterProxyServlet -
ip-10-97-204-49         at com.qubole.cluster_proxy.ClusterProxyServlet.serviceRequest(ClusterProxyServlet.java:355)
{code}

To: [~mahuja], [~karuppayyar], [~vipulm], [~venkats], [~shridhar]
",,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Apr/17 10:43 PM;sbadam;Screen Shot 2017-04-27 at 9.31.28 PM.png;https://qubole.atlassian.net/secure/attachment/41736/Screen+Shot+2017-04-27+at+9.31.28+PM.png,,,,,,,,,,,,,AWS,,,,,,tubemogul,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04l2v:,,,,,,,,,,,,,,,,,,,1.0,14191,,,,"27/Apr/17 10:57 PM;sbadam;One more important info:

Logs of Application in question(application_1493191098003_0006) is stored at - 
{code:java}
s3://ims-poc-data/logs/hadoop/28908/449462/app-logs/bahubali.jain/logs/application_1493191098003_0006
{code}
 but not at 
{code:java}
s3://ims-poc-data/logs/hadoop/28908/449462/mr-history
{code}

I believe when we display logs after cluster is terminated, we get it from later path. Correct me if I am wrong. 
",01/May/17 11:11 PM;sbadam;This should be dealt in escalation-acm as per UI folks input.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hive query failed in tunnel_utils.py,MW-813,61650,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,ksr,megha,megha,26/Apr/17 3:48 PM,08/Aug/17 11:19 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,solsup,,,,,,,"Query failed with error: 
68705103
{code:java}
2017-04-25 17:07:17,326 ERROR tunnel_utils.py:56 - create_proxy - Cannot get local port for rds: dlx-hive-q-prod-metastore.cah11koidopd.us-east-1.rds.amazonaws.com. Exception: invalid literal for int() with base 10: ''.
{code}

This is in tunnel_utils.py.
The account has dynamic tunnel service enabled, so I checked all four tunnel servers to see if any tunneling/server.log had any traces of this error or any tunnel connections to cluster on which it was run i.e (18906), but there is no log for this..

Customer sees this issue intermittently, but often.

",,drose@qubole.com,ksr,megha,psrinivas,sureshr,wvaldez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04kpj:,,,,,,,,,,,,,,,,,,,1.0,14230,2017-05-17 08:51:20.286,,,"01/May/17 11:18 AM;megha;Some more examples:
68626287, 69201530, 69201353, 69201410",17/May/17 8:51 AM;drose@qubole.com;[~sumitm] can we increase the urgency to finding the root cause?,"14/Jun/17 2:29 PM;wvaldez;Hi [~sumitm],

Oracle is asking for an update / ETA on getting this fix.  Is there something I can share?  

Thank you,
Willie
","26/Jul/17 9:04 AM;sureshr;[~karthikk]/[~surendranm]: with the veritical/scrum team structure, who is taking over ownership of tunnel server infra? Need to transition this from [~ksr]. Can [~yogeshg] take a look at this?","07/Aug/17 12:49 AM;sureshr;Tunnel service has not yet been transitioned to the infra team. So, for now, we still need help from [~ksr]. Assigning back to him for triage. Can you please take a look at this ticket, this week?","08/Aug/17 3:23 AM;ksr;[~sureshr] [~megha] I tried investigating this but the problem is that we have only 10 days of logs on the tunnel server. So, I cannot check the reason for failure on the tunnel server. Could you please ask them for a more recent failure, preferably in the last week?

cc: [~ashishs]","08/Aug/17 11:01 AM;megha;[~ksr], I've requested to get latest command failures, will let you know what I get back.. Do we have tunnel server logs backed up on s3 like we do for logserver? 

","08/Aug/17 11:19 AM;ksr;[~megha] No, we don't. I had made changes to add that but that requires us to run chef-recipe on the running tunnel servers. Any new tunnel servers that we launch will have this change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dependency related failures need to be bubbled up in UI,SCHED-141,61648,Bug,Open,SCHED,Scheduler,software,sumitm,,,Major,,ksr,Kulbir,Kulbir,26/Apr/17 3:12 PM,19/Jun/17 2:18 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Ran into a issue with Scheduled job at Box (https://box-prod-1.qubole.com/v2/scheduler#!/summary/24?status=Active&user_id=All&type=All&id=&name=) where scheduled instances were not running post addition of hive dependency and UI showed no clue whatsoever was the cause, see attached screenshot.

After digging through logs root cause found was below :
{code}
ip-10-0-1-180 [INFO  pid: 11306: 17-04-26 21:21:03 ] [command:16441] [periodic_job:20] [account:8] File dependency initialized
ip-10-0-1-180 [INFO  pid: 11306: 17-04-26 21:21:03 ] [command:16441] [periodic_job:20] [account:8] {:name=>""file1"", :path=>""s3://dwh-edge-prod/tmp/onprem_parity/parity-%Y-%m-%d-00-00-00"", :done_flag=>""_SUCCESS"", :interval=>1 day, :start=>-1, :end=>-1, :tz=>#<ActiveSupport::TimeZone:0x0055f0d5de20a8 @name=""America/Los_Angeles"", @utc_offset=nil, @tzinfo=#<TZInfo::TimezoneProxy: America/Los_Angeles>, @current_period=#<TZInfo::TimezonePeriod: #<TZInfo::TimezoneTransitionInfo: #<TZInfo::TimeOrDateTime: 1489312800>,#<TZInfo::TimezoneOffsetInfo: -28800,3600,PDT>>,#<TZInfo::TimezoneTransitionInfo: #<TZInfo::TimeOrDateTime: 1509872400>,#<TZInfo::TimezoneOffsetInfo: -28800,0,PST>>>>, :initial_instance=>Fri, 21 Apr 2017 19:00:00 UTC +00:00, :dependencies=>[""s3://dwh-edge-prod/tmp/onprem_parity/parity-2017-04-25-00-00-00/_SUCCESS""]}
ip-10-0-1-180 [INFO  pid: 11306: 17-04-26 21:21:03 ] [command:16441] [periodic_job:20] [account:8] Save successfully the pju status to #<PeriodicJobUnit id: 1515, query_hist_id: nil, objects_json: nil, created_at: ""2017-04-26 21:21:03"", updated_at: ""2017-04-26 21:21:03"", periodic_job_unit_id: nil, periodic_job_id: 24, sequence_id: 8, nominal_time: ""2017-04-26 19:00:00"", is_rerun_of: 1510, editable_periodic_job_id: nil, dependencies: ""{}"", rerun_number: 3, status: ""done"", done: true>
ip-10-0-1-180 [FATAL pid: 11306: 17-04-26 21:21:03 ] [command:16441] [periodic_job:20] [account:8] Uncaught exception: AWS::S3::Errors::Forbidden
ip-10-0-1-180 [FATAL pid: 11306: 17-04-26 21:21:03 ] [command:16441] [periodic_job:20] [account:8] [""/usr/lib/ruby/gems/2.1.0/gems/aws-sdk-1.40.3/lib/aws/core/client.rb:375:in `return_or_raise'"", ""/usr/lib/ruby/gems/2.1.0/gems/aws-sdk-1.40.3/lib/aws/core/client.rb:476:in `client_request'"", ""(eval):3:in `head_object'"", ""/usr/lib/ruby/gems/2.1.0/gems/aws-sdk-1.40.3/lib/aws/s3/s3_object.rb:293:in `head'"", ""/usr/lib/ruby/gems/2.1.0/gems/aws-sdk-1.40.3/lib/aws/s3/s3_object.rb:270:in `exists?'"", ""/usr/lib/qubole/packages/tapp2-42.41.4/tapp/app/helpers/qbol_helper.rb:578:

this is because Box uses dual IAM roles, and account level role doesn't have access to dwh-edge-prod bucket,  which is why Scheduled job is failing to start

-Second issue(we will hit once 1 is resolved) is that we seem to append _SUCCESS to the specified Hive Dependency path(s3://dwh-edge-prod/tmp/onprem_parity/parity-2017-04-25-00-00-00/_SUCCESS) which in this case also doesn't exist because upstream job writing into these folders is not a MR job. 
{code}

Ask is to capture these kind of errors and show it in Scheduler UI, would have saved lot of time on both ends.

cc [~mpatel]
",,Kulbir,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Box,,,,,{},NA,Choose from,,SCHED-149,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04kp3:,,,,,,,,,,,,,,,,,,,1.0,14217,2017-05-26 06:46:49.857,,,"27/Apr/17 2:52 PM;Kulbir;[~sumitm] we made two changes to address the issue:
-Change S3 bucket path for Hive dependency so Qubole tier has access to it
-Manually add _SUCCESS file to these paths(needed because these paths are not created by MR JOB and our code looks for it)

However we need to catch the *AWS::S3::Errors::Forbidden* exception on our end and bubble it up in the UI, otherwise there is no feedback to end user why jobs are not running. Please let me know if any questions.",25/May/17 8:01 PM;Kulbir;[~sumitm] any update on this one ?,"26/May/17 6:46 AM;sumitm;[~Kulbir] assigning to [~ksr] for further triage, though we don't have bandwidth to pick this up as of now. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Usage - Reports does not reflect the current status of cluster state,UI-5782,61647,Bug,Open,UI,UI,software,aswina,,,Major,,laliti,cbalchandani,cbalchandani,26/Apr/17 3:10 PM,30/May/17 3:01 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"The Clusters -> Reports feature does not reflect the current status of Cluster State.
Cluster 28030 and 28027 were started and terminated on 04/26/2017 and its activity is captured by RDS (as shown in the attached screenshot) but the same information is not reflected in the Reports. (screenshot taken around 2.40PM PST)

cc [~megha]",,aswina,cbalchandani,mmajithia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Apr/17 3:07 PM;cbalchandani;Clusters.png;https://qubole.atlassian.net/secure/attachment/41717/Clusters.png,26/Apr/17 3:07 PM;cbalchandani;RDS.png;https://qubole.atlassian.net/secure/attachment/41716/RDS.png,26/Apr/17 3:07 PM;cbalchandani;Usage.png;https://qubole.atlassian.net/secure/attachment/41715/Usage.png,,,,,,,,,,,AWS,,,,,,Cimpress,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04kov:,,,,,,,,,,,,,,,,,,,1.0,14211,2017-04-27 05:07:36.723,,,"27/Apr/17 5:07 AM;aswina;Some of this information is cached and it'll eventually get refreshed.  

[~laliti] - Can you investigate this a bit and find out how long the API response is cached?",30/May/17 3:01 PM;cbalchandani;Do we have an ETA for this? [~laliti],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive bootstrap location got wiped out,MW-814,61628,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sumitm,sam,sam,26/Apr/17 9:25 AM,17/May/17 2:53 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"This is a Mediamath issue that has occurred twice in the last 5-6 months. Details are in the ZD: https://qubole.zendesk.com/agent/tickets/14163

Need some help investigating to formulate a response to MM client. 

Thank you ",,psrinivas,rvenkatesh,sam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,MediaMath,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04kkn:,,,,,,,,,,,,,,,,,,,1.0,14163,2017-04-26 10:17:24.548,,,"26/Apr/17 9:29 AM;sam;It is possible that the location was deleted, changed accidentally by the client, but given that they sat only one person has access and no changes were made, I thought it is worth for us to take a look into. 

I asked for date and time of the initial failure, so we can compare with release records to see if that may have been the cause. If times do not match, then not worth looking into that angle.. 

If there are records that we can produce of config changes to audit who changed the config just prior to failures, that may help. If no changes were made by a person, then it may be some strange bug.. or maybe s3 bucket permissions change? Basically I could use some help here... thanks guys ",26/Apr/17 10:17 AM;rvenkatesh;[~sam] Have they looked at cloudtrail logs ? ,"26/Apr/17 10:33 AM;sam;I'll ask, but I do not think they did",26/Apr/17 10:34 AM;sam;I pinged John on the ZD about it.,"26/Apr/17 10:43 AM;rvenkatesh;The hive devs should confirm. I do not think there is any code in hive that writes to hive bootstrap. It only reads. OTOH, Middleware or tapp layer writes to hive bootstrap. ","26/Apr/17 11:35 AM;sam;It's not into Hive bootstrap.. it is the hive boostrap location field in cluster configs, the one points to the file in S3. That field got wiped out.",26/Apr/17 11:35 AM;sam;Could be a UI issue too.. hard to tell,26/Apr/17 12:01 PM;psrinivas;[~sam]: You mean node_bootstrap file in cluster_configs table? ,"26/Apr/17 12:17 PM;sam;No, the location of the Hive bootstrap on the bootstrap config page, above the actual Editor. Sorry I misspoke, not the cluster config ","26/Apr/17 12:26 PM;psrinivas;So, you mean to say the ""Base Bootstrap Location"" on [Hive bootstrap|https://api.qubole.com/v2/control-panel#hive-bootstrap] page. 

To understand correctly, do the contents pointed by the s3 location are cleaned? Or the s3 path is removed from our config tables? 

","26/Apr/17 6:31 PM;sam;The path is removed from the tables. The contents of the file are in tacts. The client was able to fix the problem by filling the location in again, so this is not a blocker, however, troublesome if this happens several times. The interesting anecdotal evidence is that I have not heard any other client complain about such an issue. Still does not mean that it is not an issue, but just more context - just happened to MM twice.
",26/Apr/17 6:34 PM;psrinivas;[~sam]: This is a middleware bug. Can you please move it to their plate. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wide open SG is created even after enabling Tunnel Discovery feature,ACM-1428,61555,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,Done,ajayb,sbadam,sbadam,25/Apr/17 9:27 PM,03/Aug/17 10:55 PM,09/Aug/17 6:03 AM,22/Jun/17 4:14 PM,,,,0,jira_escalated,,,,,,,,"Wide open SG is created even after enabling Tunnel Discovery feature. Customer is unhappy with this. 

User: ron@ibotta.com
Cluster: 29874(cluster is private VPC)

It doesn't have any Tunnel Server specified in Clusters configs when this issue is seen.
Cluster start logs:


{code:java}
.....
2017-04-26 01:57:20,164 >>> Creating security group @sc-qbol_acc6796_cl29874...
2017-04-26 01:57:21,512 >>> Opening tcp port range 22-22 for CIDR 0.0.0.0/0
2017-04-26 01:57:21,628 >>> Opening tcp port range 22-22 for CIDR 23.21.156.210/32
2017-04-26 01:57:21,747 >>> Opening tcp port range 22-22 for CIDR 54.204.43.32/32
2017-04-26 01:57:21,875 >>> Opening tcp port range 22-22 for CIDR 54.243.39.255/32
2017-04-26 01:57:21,988 >>> Opening tcp port range 22-22 for CIDR 23.23.178.159/32
*** 2017-04-26 01:57:22,210 WARNING - Unable to revoke tcp protocol for cidr 0.0.0.0/0 in the group @sc-qbol_acc6796_cl29874
*** .....
{code}

Question: Why are creating wide open SG when we can access from all four Tunnel Servers and why did it try to revoke it(but it failed)?

As a workaround, we have asked customer to give one Tunnel IP in cluster configs.
",,ajayaa,sbadam,siyengar,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Aug/17 10:34 PM;sbadam;Capture.JPG;https://qubole.atlassian.net/secure/attachment/46479/Capture.JPG,03/Aug/17 10:33 PM;sbadam;Screen Shot 2017-08-03 at 10.17.33 PM.png;https://qubole.atlassian.net/secure/attachment/46477/Screen+Shot+2017-08-03+at+10.17.33+PM.png,,,,,,,,,,,,AWS,,,,,,ibotta,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04kbj:,,,,,,,,,,,,,,,,,,,2.0,"14169,16051",2017-04-25 22:03:30.887,,,25/Apr/17 10:03 PM;venkatak;Did they delete the security group by hand once before restarting the cluster [~sbadam]? Does that solve this problem?,25/Apr/17 10:05 PM;sbadam;yeah they deleted all wide open SGs and stared the cluster. Observed a violating SG with tag - qbol_acc6796_cl29874 as per customer,"26/Apr/17 12:58 PM;siyengar;I think the problem is that if tunnel ip is left blank in cluster configs page, the sg with wide open rule is created. We should have some way to specify at an account level to avoid that (not sure but may be account feature). Not sure how restrict_ssh_access works.","09/May/17 12:23 AM;ajayaa;[~sbadam] Most likely the customer didn't give permission to revoke security group rules. If the customer still sees this problem, can we check with them? If that's the case, the short term fix is to ask the customer to give permission to revoke security group rules. If tunnel discovery is enabled, we check if port 22 is open to world and then revoke it. We shouldn't add that rule in first place. This is a bug.

cc [~ashishs] [~ajayb]","03/Aug/17 10:32 PM;sbadam;[~ajayaa] - customer is concerned we are still creating a wide open SG even after enabling Tunnel Discovery.
Logs:

{code:java}
PID: 19169  2017-08-03 15:20:18,442 INFO  [cluster:34021] [cluster_instance:568681]  cluster.py:3145 - create_cluster - udf_size: Size of udf while checking whether to upload to s3 or not is 14285 bytes
PID: 19169  2017-08-03 15:20:18,463 DEBUG [cluster:34021] [cluster_instance:568681]  config.py:898 - get_tunnel_machine_to_use - Discovered Tunnel Machine: 54.243.39.255
PID: 19169  2017-08-03 15:20:18,568 WARNING [cluster:34021] [cluster_instance:568681]  awsutils.py:648 - revoke_group_permission - Unable to revoke tcp protocol for cidr 0.0.0.0/0 in the group @sc-qbol_acc6271_cl34021
PID: 19169  2017-08-03 15:20:18,568 DEBUG [cluster:34021] [cluster_instance:568681]  awsutils.py:352 - conn - creating self._conn w/ connection_authenticator kwargs = {'path': '/', 'region': RegionInfo:us-east-1, 'port': None, 'is_secure': True}
{code}

-> As [~Kulbir] pointed, Is there any chance wide-open access has been retained which was present before Tunnel Discovery was enabled for customer. On May 29th, the feature was enabled for account#6071.

User: patty.vonick@returnpath.com, ACC: 6071, Cluster ID: 34021, Cluster start commands -  87944754,  87944757. 

-> Is there any reason we have two cluster start commands in succession for one cluster? 
-> I believe we will have enough permissions to remove SGs, I mean we don't need explicit permission from customer?
","03/Aug/17 10:43 PM;ajayaa;[~sbadam] [Here|http://docs.qubole.com/en/latest/faqs/general-questions/policy-use-qubole-use-my-iam-credentials.html#policy-use-qubole-use-my-iam-credentials-aws] is the recommended Qubole policy. I don't see `ec2:RevokeSecurityGroupIngress` listed over here. How did you conclude that we have enough permission to remove ingress rules from security group?

Our docs should be updated to include `ec2:RevokeSecurityGroupIngress` as a required permission. cc [~veenamj]

Two cluster manage commands should not be a problem for the customer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debugging Spark issues via Analyze and Notebooks,SPAR-1561,61405,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,venkatak,addon_zendesk_for_jira,24/Apr/17 4:25 AM,22/Jun/17 1:48 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Expedia's teams are facing lots of issues troubleshooting their PySpark code via Analyze and Notebook. 

The scenario:

1. In Expedia, administrators create the clusters and at times the interpreters and the users have very little freedom to update their cluster and/ or interpreter settings

2. They face several issues debugging their code (typically Pyspark):

a. Analyze takes a long time to fail a bad code they might have written (and which runs on a larger dataset for example) and they are unable to easily see the logs surfaced up on the command log output. It is hidden deep inside the logs (single mapper driver log has the error at times). *They want these errors to be surfaced to the command log*

example command ids: 67286138, 67370922 , 67352198,67348248,67317163,67286138. (command ids where we could have surfaced the error logs)

At times their logs grow to 500 mbs in size which is becoming impossible to debug. 

b. When we encouraged them use the notebook for pyspark debugging, this is what they had to say:

Also, in notebooks we face a few issues quite often:

* multiple para get triggered using shift enter 
* queries once triggered take a lot of time to cancel 
* the codes get lost and notebook becomes inaccessible in case of cluster outage 
* the help section to raise a ticket would only accept a command id

Overall they are too dissatisfied with the user's ability to debug issues on their own with Notebooks or Analyze. They have multiple users working as developers at expedia, and they have raised several tickets in recent times raising their concerns on being able to debug in Qubole. They have given really nasty UI feedbacks in recent times.

Can we address this sooner?

cc [~mpatel][~gmargabanthu][~adityak]",,addon_zendesk_for_jira,gmargabanthu,karuppayyar,mahuja,mpatel,rohitk,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,autodesk,expedia,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04jev:,,,,,,,,,,,,,,,,,,,1.0,14065,2017-04-24 04:57:18.561,,,"24/Apr/17 4:57 AM;karuppayyar;Few thought/queries on notebook points
*multiple para get triggered using shift enter* - How consistent is this?(May be related to ZEP-811)
*queries once triggered take a lot of time to cancel* - This happens when the spark jobs complete and there is some processing happening within driver.
(eg:*Alter table recover partition*)
I think if we give appropriate UI feedback., it will help [~mahuja]","24/Apr/17 11:52 PM;mahuja;[~prachim] Can you please look at Analyze related error reporting issues. [~vipulm], [~karuppayyar] can you look at notebook ones

I have been requesting direct feedback from Expedia users for many months. We do not need to wait for the users to be unhappy before we address it. [~gmargabanthu], [~venkatak] can you please compile the feedback till now and let's walk through it. cc [~mohan], [~mpatel]",27/Apr/17 12:02 AM;venkatak;Hi [~mahuja] It is best for the Spark team have an interactive session at both ends of the world to have an interactive session with Expedia team to make this really useful exercise. The ticket linked to this JIRA has contacts that you will need on the expedia side (or the vendors who actually work for expedia developing these code),"27/Apr/17 12:03 AM;venkatak;[~gmargabanthu] If you want to open this conversation since you are the CSM/ TAM, I am willing to solicit/ co-ordinate and orchestrate things from this part of the world",27/Apr/17 12:45 AM;mahuja;[~venkatak] we are meeting part of Expedia team next week. ,"02/May/17 1:16 AM;venkatak;[~mahuja] thats a good move before they come back through their management teams. Let me know if I should join in that call, so that there is a local representative...",02/May/17 1:18 AM;mahuja;[~venkatak] adding you to all the meetings.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster is not terminating even when no jobs are running,SOL-126,61335,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,sbadam,sbadam,21/Apr/17 5:47 PM,17/May/17 12:48 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"User from TheTradeDesk experienced that their cluster is up for 15 hours even though there are no jobs in the cluster.

User: marina.lepikhina@thetradedesk.com
Cluster ID, Instance ID: 4285, 444512

Logs are here: s3://thetradedesk-useast-qubole/logs/hadoop/11271/444512/ 

Customer experienced this situation intermittently. Provided one such instance here(444512). Can somebody look into this problem?
",,hiyer,psrinivas,rvenkatesh,sbadam,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,thetradedesk,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04izb:,,,,,,,,,,,,,,,,,,,2.0,"14165,14392",2017-04-21 19:42:46.826,,,"21/Apr/17 7:42 PM;hiyer;Cluster auto-termination is controlled by MW + hustler, not by Hadoop.",21/Apr/17 11:41 PM;sbadam;Please hold-on this issue. Looks like it is not a problem. ,"23/Apr/17 9:09 PM;sbadam;[~sumitm]: can you please look into this issue? 

Problem: When background Metadata queries are running on a cluster,  it is not auto-terminating if it is already up. If it not up, jobs are running fine without starting it. Customer is expecting to auto-terminate despite background jobs. What do you say about it?

https://api.qubole.com/v2/analyze?qbol_user_id=All&cluster_label=default&show=history&start_date=1490250224&end_date=1492842224#","24/Apr/17 12:13 AM;sumitm;[~sbadam] actually tapp has no way to know before hand if the command requires cluster or not, hence it associate a session with the command. Later while terminating a cluster we checks if there are any active session associated with the cluster or not. So not sure what we can change from MW side. Tagging [~psrinivas] [~rvenkatesh] for suggestion wrt to hive. 

The same issue had happened earlier as well with some other customer (Grofers).
",24/Apr/17 1:24 PM;sbadam;Customer is expecting to have cluster auto-terminated in spite of background metadata jobs.,"24/Apr/17 4:38 PM;psrinivas;cc: [~rvenkatesh]
Right now, there is nothing that can be fixed from Hive side here. It is by design. ",24/Apr/17 11:44 PM;hiyer;Can the SQL parser that the UI uses not be enhanced to return a value indicating a query is metadata-only (or otherwise)?,"25/Apr/17 2:34 PM;psrinivas;Yes, it can be enhanced. ",25/Apr/17 8:42 PM;hiyer;So it *can* be fixed then :-). If multiple customers are asking for this it seems to me we should get it done if it's not too much effort ...,"25/Apr/17 9:20 PM;sbadam;Thank you [~hiyer] for your suggestion :). Shall I create two tickets, one for UI and other one for Hive ?
to: [~psrinivas], [~rvenkatesh]","25/Apr/17 9:35 PM;rvenkatesh;Are these ""alter table recover partitions"" ? Run from the scheduler ? 
(I should be using super admin to click around in the account but its not working for some reason).
 ","25/Apr/17 9:52 PM;hiyer;[~rvenkatesh] no, one command I saw was `show partitions thetradedesk.ttd_bulk_dataimport_v1;`. There may be others as well.    
[~sbadam] one JIRA for hive and one for MW. There should not be any change required on UI unless the change to syntax checker API is a breaking one.","26/Apr/17 12:13 AM;sumitm;[~psrinivas] as of now tapp uses a naive parser to identify if the command is md cmd or not. So we gonna need some api/script which behaves in same way in which hive would behave. Also the api/script should be very fast, as we do that checking while the user is submitting command, cause if that takes time, there would be delay in command submission. ","26/Apr/17 12:44 AM;hiyer;[~sumitm] as discussed above, the checkSyntax thrift call can be enhanced to return information on whether a particular command is a metadata command or not.","26/Apr/17 10:45 AM;psrinivas;[~hiyer]/[~sumitm]: Yes, checkSyntax(or another method) can be enhanced to determine the metadata commands. For now, can we use the tapp md cmd option to update the session? ",03/May/17 2:52 PM;sbadam;[~sumitm] - can please provide your inputs?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pinterest command took long to submit and run,MW-787,61333,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sumitm,mpatel,mpatel,21/Apr/17 4:20 PM,01/May/17 4:10 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"See: 67866077

Was this related to the db issue?",,Kulbir,mpatel,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Pinterest,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04iyv:,,,,,,,,,,,,,,,,,,,,,,,,01/May/17 4:10 PM;mpatel;[~sumitm] / [~sureshr] .. just want to check if the above command was related to the db issues?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"lock wait exception writing to hive's ""partitions"" table",HIVE-2106,61330,Bug,Reopened,HIVE,qbol hive,software,psrinivas,,,Major,,psrinivas,megha,megha,21/Apr/17 1:32 PM,06/Jul/17 10:13 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"*strong text*Command:67717321,67482721, 67598927 and few others failed with following exception:

 Failed with exception MetaException(message:javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MPartition@3cb2ff35"" using statement ""INSERT INTO `PARTITIONS` (`PART_ID`,`TBL_ID`,`PART_NAME`,`SD_ID`,`CREATE_TIME`,`LAST_ACCESS_TIME`) VALUES (?,?,?,?,?,?)"" failed : Lock wait timeout exceeded; try restarting transaction

Ideally, this would happen if simultaneous updates are done on same hive tables.. however there are no simultaneous updates to the tables involved in these queries.

For instance for 67717321, I ran following rds query to retrieve all queries runnign at the same time as this command:
select * from hive_commands where id in (select command_id from query_hists where submit_time<=1492674221 and end_time>=1492673839 and account_id=1756)\G

There were 10 commands  running on this account, but none of them were accessing same tables as in 67717321

Also, this is qubole managed metastore, and no other account is using this metastore, hence there is no other interface accessing the metastore at the same time.

I wouldn't expect lock timeouts in such cases.. Why would this happen? ",,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-994,,HIVE-1539,,,,,,,,,,,,,,,,,AWS,,,,,,underarmour,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04iy7:,,,,,,,,,,,,,,,,,,,2.0,"14095,14107",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Datastore connection fails with ""Failed to Connect"", connection works from tunnels",QBOL-6150,61262,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,sumitm,megha,megha,20/Apr/17 5:26 PM,20/Apr/17 9:05 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Tunnel.use_tunnel_discovery is enabled, for account 6285, dbsink id: 2243
The details that they're entering on UI are all fine..

Following works:
telnet to db host, port works from all tunnel servers (23.21.156.210,54.204.43.32,54.243.39.255,23.23.178.159)
psql connection to the db, i used information from rails console DbSink.find(2433).db_passwd, DbSink.find(2433)

The datastore region is us-west-2


",,Kulbir,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,activision,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04inz:,,,,,,,,,,,,,,,,,,,,,2017-04-20 21:00:13.132,,,"20/Apr/17 9:00 PM;Kulbir;This is addressed now.
Basically on Redshift side SSL was required and hence DB Tap was not working, added this string to DB name to get it working:
?ssl=true&sslfactory=org.postgresql.ssl.NonValidatingFactory

[~sumitm] as a followup item, I am sure postgres Driver would have came back and thrown some connection exception, can we bubble this up in the UI vs. simply telling user to contact support ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Node was not in use by RM , but was kept up as it was used by hdfs",ACM-1131,61200,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,megha,megha,19/Apr/17 5:06 PM,20/Apr/17 4:23 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"This was for cluster 28441 for expedia, where the cluster is up since 27 days.
The node ec2-54-227-156-197.compute-1.amazonaws.com	 was started 23 days back, and is up since.
The node was however never a part of RM , but was in use by HDFS. RM never identified this node..
Will attach complete Node manager logs shortly, but following is the snippet of exceptions, after which NM shutdown (this happened a few times):

{code:java}
state STARTED; cause: java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto$Builder.setHost(YarnProtos.java:19145)
        at org.apache.hadoop.yarn.api.records.impl.pb.NodeIdPBImpl.setHost(NodeIdPBImpl.java:56)
        at org.apache.hadoop.yarn.api.records.NodeId.newInstance(NodeId.java:42)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.buildNodeId(ContainerManagerImpl.java:487)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStart(ContainerManagerImpl.java:447)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:264)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:463)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:509)
2017-04-19 23:53:19,863 INFO org.apache.hadoop.ipc.Server: Stopping server on 45454
2017-04-19 23:53:19,868 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 45454
2017-04-19 23:53:19,870 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService: org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit
2017-04-19 23:53:19,870 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
{code}

How can we avoid this situation or atleast identify such situation and terminate the node? 
",,abhishekmodi,hiyer,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,expedia,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04ia7:,,,,,,,,,,,,,,,,,,,,,2017-04-19 22:42:44.798,,,"19/Apr/17 10:42 PM;hiyer;This should be fixed with the fix for HADTWO-853, since we now use slave.host.name - which is never NULL - for the hostName.","20/Apr/17 3:31 AM;abhishekmodi;We checked and found AWS local metadata service was not started on this node for quite some time. Due to this our UDF was not able to fetch values from that and didn't fill slave.host.name and other values. Due to this Hadoop couldn't fetch all these details and failed.

{code}
++++(/usr/lib/hustler/bin/uncompressed_hadoop_node_init.sh:424): save_nodeinfo():  2017-03-27 11:23python /usr/lib/hustler/lib/py/hadoop_setup/get_info_using_instid.py i-0c1552d4b902b0d2c 0
Traceback (most recent call last):
  File ""/usr/lib/hustler/lib/py/hadoop_setup/get_info_using_instid.py"", line 38, in <module>
    main()
  File ""/usr/lib/hustler/lib/py/hadoop_setup/get_info_using_instid.py"", line 32, in main
    ec2 = utils.get_ec2_connection()
  File ""/usr/lib/hustler/lib/py/hadoop_setup/utils.py"", line 11, in get_ec2_connection
    f, dummy = urllib.urlretrieve(""http://169.254.169.254/latest/meta-data/placement/availability-zone"")
  File ""/usr/lib64/python2.6/urllib.py"", line 93, in urlretrieve
    return _urlopener.retrieve(url, filename, reporthook, data)
  File ""/usr/lib64/python2.6/urllib.py"", line 239, in retrieve
    fp = self.open(url, data)
  File ""/usr/lib64/python2.6/urllib.py"", line 207, in open
    return getattr(self, name)(url)
  File ""/usr/lib64/python2.6/urllib.py"", line 346, in open_http
    h.endheaders()
  File ""/usr/lib64/python2.6/httplib.py"", line 967, in endheaders
    self._send_output()
  File ""/usr/lib64/python2.6/httplib.py"", line 831, in _send_output
    self.send(msg)
  File ""/usr/lib64/python2.6/httplib.py"", line 790, in send
    self.connect()
  File ""/usr/lib64/python2.6/httplib.py"", line 771, in connect
    self.timeout)
  File ""/usr/lib64/python2.6/socket.py"", line 567, in create_connection
    raise error, msg
IOError: [Errno socket error] [Errno 111] Connection refused
{code}

Moving to ACM team for further looking into it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Role - ""Deny-Account-all"" still allows a user to get API token and clone the account",UI-5740,61190,Bug,Open,UI,UI,software,aswina,,,Major,,tabraiz,sbadam,sbadam,19/Apr/17 11:11 AM,20/Apr/17 4:53 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Role - ""Deny-Account-all"" still allows a user to get API token and clone the account

Steps to reproduce:
1. Invite a new user into your account using ""Manage Users"" tab. 
2. Create a new Role and a new policy(Deny-Account-all)
3. Create a new Group and above role. Make sure the user is not part of any other group
4. Here comes the problem, the user is still able to clone account and see API token in spite of denying all permissions.

",,aswina,nimitk,sbadam,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,returnpath,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04i7z:,,,,,,UI Sprint 34 (7-Jun - 20-Jun),,,,,,,,,,,,,,,2017-04-20 02:11:52.95,,,"20/Apr/17 2:11 AM;sumitm;[~sbadam] the api token is an user property, so it would be visible. I tried clicking on clone and I got a access denied message from backend. So I am moving this to UI to do not show clone link if user not have the permissions. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qubole API call failed to return a master node,MW-769,61133,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sumitm,navdeepp,navdeepp,19/Apr/17 12:10 AM,03/May/17 12:07 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Pintrest [Ac:3734] [Cluster: 29320]

1.master node did not appear in the cluster ui
2. the cluster control page showed 0 (0) next to Nodes (On clicking this link it shows some nodes but not master) 
3. Job tracker/namenode page was not accessible 
4. we could not submit jobs and we saw ""Exception: Qubole API call to http://api.qubole.com/api/v1.2/hadoop_cluster failed to return a master node, please wait and retry and/or email help@qubole.com with this message.""",,Kulbir,mpatel,navdeepp,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,pinterest,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04hvb:,,,,,,,,,,,,,,,,,,,,,2017-04-19 10:18:58.913,,,19/Apr/17 12:13 AM;navdeepp;Its working fine now but customer needs RCA,"19/Apr/17 10:18 AM;Kulbir;Hi [~sumitm],
Marking this one as critical given the impact to Pinterest workflows.
Both UI and API calls were impacted and they are concerned about what caused this and how we can mitigate going forward.

Can I please get an update by tomorrow ?

-Kulbir

cc [~mpatel]","19/Apr/17 10:41 AM;Kulbir;Could this be somehow related to RDS CPU spikes yesterday ?
https://qubole.slack.com/archives/C11DBLQNS/p1492596303027689
","21/Apr/17 7:29 AM;sumitm;[~Kulbir] can you ask the customer that where they saw the msg mentioned in point 4. Is it shown in query logs or when they viewing cluster details in cluster page or somewhere else. Please get the command id, if this error was appeared in some query. 

I tried to find out the reason, but it seems very momentary thing, with apparently no traces in huge web logs we generate everyday. 
","21/Apr/17 7:22 PM;Kulbir;Have asked for it [~sumitm] but just wondering, if this was returned from some API call then won't we log this in webapp tier logs in nginx or controller logs ? Did we check there already ?
Sorry been tied with other stuff so couldn't dig down more on my end.

cc [~mpatel]","24/Apr/17 6:54 AM;sumitm;[~Kulbir] I found that the above mentioned api (http://api.qubole.com/api/v1.2/hadoop_cluster) is a custom api, which we might have developed for Pinterest a long ago. AFAIK our codebase (UI/SDKs) doesn't use that api anywhere. 

I also saw that everyday we get thousands of request for this api, and we deliver the response properly. I tried to dig up the webapp logs of that day, which was enormous 25gb, so could not find the real issue. 

I did find some 401 (unauthorised) calls for these api, but no 500s so far. 

{noformat}
ip-10-37-170-165 [INFO  pid: 11962: 17-04-17 22:41:09 ] Started GET ""/api/v1.2/hadoop_cluster"" for 52.204.101.198 at 2017-04-17 22:41:09 +0000
ip-10-37-170-165 [INFO  pid: 11962: 17-04-17 22:41:09 ] Processing by Api::V12::HadoopClustersController#show as */*
ip-10-37-170-165 [ERROR pid: 11962: 17-04-17 22:41:09 ] Invalid Token!
ip-10-37-170-165 [INFO  pid: 11962: 17-04-17 22:41:09 ] Completed 401 Unauthorized in 8.8ms (Views: 6.9ms | ActiveRecord: 0.0ms)
{noformat}

In short it looks like that may be due to high db load, we were failed to respond some of the api calls. 


","03/May/17 12:07 PM;Kulbir;Thanks [~sumitm], sorry lost track of this update.
[~navdeepp] please share this with Pinterest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error reading SSH protocol banner,PRES-1021,60947,Bug,Open,PRES,Presto,software,stagra,,,Critical,,ajayb,satyavathib,satyavathib,13/Apr/17 9:46 PM,22/May/17 8:24 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command Id's : 64927023,64926761,64927301, 66411088

Log from one of the commands :

{code}
2017-04-05 17:48:59,542 ERROR transport.py:1428 - _log - Exception: Error reading SSH protocol banner
2017-04-05 17:48:59,545 ERROR transport.py:1426 - _log - Traceback (most recent call last):
2017-04-05 17:48:59,545 ERROR transport.py:1426 - _log - File ""/usr/lib/python2.6/dist-packages/paramiko/transport.py"", line 1582, in run
2017-04-05 17:48:59,545 ERROR transport.py:1426 - _log - self._check_banner()
2017-04-05 17:48:59,545 ERROR transport.py:1426 - _log - File ""/usr/lib/python2.6/dist-packages/paramiko/transport.py"", line 1709, in _check_banner
2017-04-05 17:48:59,545 ERROR transport.py:1426 - _log - raise SSHException('Error reading SSH protocol banner' + str(e))
2017-04-05 17:48:59,545 ERROR transport.py:1426 - _log - SSHException: Error reading SSH protocol banner
2017-04-05 17:48:59,545 ERROR transport.py:1426 - _log -
2017-04-05 17:48:59,589 INFO cmd_utils.py:290 - _ssh_and_execute_direct - SSH Exception encountered :SSH exception Error reading SSH protocol banner
2017-04-05 17:48:59,589 INFO cmd_utils.py:293 - _ssh_and_execute_direct - retries left :2 sleeping for 1 seconds before retrying
2017-04-05 17:49:42,201 ERROR transport.py:1428 - _log - Exception: Error reading SSH protocol banner
2017-04-05 17:49:42,201 ERROR transport.py:1426 - _log - Traceback (most recent call last):
2017-04-05 17:49:42,201 ERROR transport.py:1426 - _log - File ""/usr/lib/python2.6/dist-packages/paramiko/transport.py"", line 1582, in run
2017-04-05 17:49:42,202 ERROR transport.py:1426 - _log - self._check_banner()
2017-04-05 17:49:42,202 ERROR transport.py:1426 - _log - File ""/usr/lib/python2.6/dist-packages/paramiko/transport.py"", line 1709, in _check_banner
2017-04-05 17:49:42,202 ERROR transport.py:1426 - _log - raise SSHException('Error reading SSH protocol banner' + str(e))
2017-04-05 17:49:42,202 ERROR transport.py:1426 - _log - SSHException: Error reading SSH protocol banner
2017-04-05 17:49:42,202 ERROR transport.py:1426 - _log -
2017-04-05 17:49:42,217 INFO cmd_utils.py:290 - _ssh_and_execute_direct - SSH Exception encountered :SSH exception Error reading SSH protocol banner
2017-04-05 17:49:42,217 INFO cmd_utils.py:293 - _ssh_and_execute_direct - retries left :1 sleeping for 2 seconds before retrying
2017-04-05 17:50:21,839 ERROR transport.py:1428 - _log - Exception: Error reading SSH protocol banner
2017-04-05 17:50:21,839 ERROR transport.py:1426 - _log - Traceback (most recent call last):
2017-04-05 17:50:21,840 ERROR transport.py:1426 - _log - File ""/usr/lib/python2.6/dist-packages/paramiko/transport.py"", line 1582, in run
2017-04-05 17:50:21,840 ERROR transport.py:1426 - _log - self._check_banner()
2017-04-05 17:50:21,840 ERROR transport.py:1426 - _log - File ""/usr/lib/python2.6/dist-packages/paramiko/transport.py"", line 1709, in _check_banner
2017-04-05 17:50:21,840 ERROR transport.py:1426 - _log - raise SSHException('Error reading SSH protocol banner' + str(e))
2017-04-05 17:50:21,840 ERROR transport.py:1426 - _log - SSHException: Error reading SSH protocol banner
2017-04-05 17:50:21,840 ERROR transport.py:1426 - _log -
2017-04-05 17:50:21,894 INFO cmd_utils.py:290 - _ssh_and_execute_direct - SSH Exception encountered :SSH exception Error reading SSH protocol banner
2017-04-05 17:50:21,895 INFO cmd_utils.py:297 - _ssh_and_execute_direct - retries expired, possibly too many connections being established simultaneously for long periods
2017-04-05 17:50:21,895 ERROR prestocli.py:145 - execute_cmd - Error while executing Presto Query: SSH exception Error reading SSH protocol banner
{code}

they are facing this error repeatedly in the last 15 days. I am not able to re-open the JIRA https://qubole.atlassian.net/browse/PRES-692. So creating a new one as the customer is facing this issue almost every alternate day.

Also adding the load is not that too much as per the customer. Few times its just one command running at the moment but still the commands fail.

Please Let me know If any details requirred.

",,ankitd,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04gqn:,,,,,,,,,,,,,,,,,,,1.0,13846,2017-04-17 22:15:44.567,,,"13/Apr/17 10:00 PM;satyavathib;66413083:

Log says :

{code}
Error running command: Server refused connection: http://127.0.0.1:8081/v1/statement
io.airlift.http.client.RuntimeIOException: Server refused connection: http://127.0.0.1:8081/v1/statement
at io.airlift.http.client.ResponseHandlerUtils.propagate(ResponseHandlerUtils.java:17)
at io.airlift.http.client.FullJsonResponseHandler.handleException(FullJsonResponseHandler.java:55)
at io.airlift.http.client.FullJsonResponseHandler.handleException(FullJsonResponseHandler.java:35)
at io.airlift.http.client.jetty.JettyHttpClient.execute(JettyHttpClient.java:326)
at com.facebook.presto.client.StatementClient.(StatementClient.java:120)
at com.facebook.presto.cli.QueryRunner.startInternalQuery(QueryRunner.java:126)
at com.facebook.presto.cli.QueryRunner.startQuery(QueryRunner.java:121)
at com.facebook.presto.cli.Console.process(Console.java:138)
at com.facebook.presto.cli.Console.executeCommand(Console.java:120)
at com.facebook.presto.cli.Console.run(Console.java:297)
at com.facebook.presto.cli.Presto.main(Presto.java:32)
Caused by: java.net.ConnectException: Connection refused
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
at org.eclipse.jetty.io.SelectorManager.finishConnect(SelectorManager.java:337)
at org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:342)
at org.eclipse.jetty.io.ManagedSelector.access$900(ManagedSelector.java:57)
at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:279)
at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:171)
at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:171)
at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:102)
at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:137)
at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654)
at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
at java.lang.Thread.run(Thread.java:745)
2017-04-13 10:55:01,536 ERROR ssh.py:679 - execute - command ' echo $$ > /media/ephemeral0/pids/66413083.pid ; /usr/lib/presto/bin/presto client --output-format TSV_JSON --show-progress --execute ""insert OVERWRITE TABLE stats.pr_timezone_idf2_report_orc
{code}",17/Apr/17 10:15 PM;ankitd;Pinged on https://qubole.atlassian.net/browse/ACM-523. This ticket is related to that one,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select queries with join failing with classcastexception,SOL-120,60943,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,megha,megha,13/Apr/17 5:20 PM,13/Jun/17 2:52 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Query 1 succeeds, with no join to db_advertisers: 
https://api.qubole.com/v2/analyze?command_id=66430916

Query 2 fails, and the only difference is that it adds the join: 
https://api.qubole.com/v2/analyze?command_id=66433944  [failed reducer log at: https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fip-10-1-10-89.ec2.internal%3A50060%2Ftasklog%3Ftaskid%3Dattempt_247.201702240442_121654_r_000242_0%26all%3Dtrue&clusterInst=386778 ] 
pinterest user: andrea@pinterest.com

User tried following:
- no join at all - success! - https://api.qubole.com/v2/analyze?command_id=59144561 
- join to extracted_db_users - failure - https://api.qubole.com/v2/analyze?command_id=59134798 
- join to db_users - failure - https://api.qubole.com/v2/analyze?command_id=59289217 
- join to db_users_snapshots - failure - https://api.qubole.com/v2/analyze?command_id=59290372

The base query is as shown below, it's just a matter of removing or re-adding the commented lines joining to (extracted_)db_users.

For what it's worth, this related query joins ad_query and extracted_db_users perfectly well:
https://api.qubole.com/v2/analyze?command_id=58768244

I tried to narrow down the working join query but have been unsuccessful. 
The failure is in reducer. Attached error logs for one such failure.",,megha,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-2243,,,,18/Apr/17 5:22 AM;mpatel;explain_67357857.txt;https://qubole.atlassian.net/secure/attachment/41460/explain_67357857.txt,13/Apr/17 5:20 PM;megha;reducer-exception.log;https://qubole.atlassian.net/secure/attachment/41408/reducer-exception.log,,,,,,,,,,,,AWS,,,,,,pinterest,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04gpr:,,,,,,,,,,,,,,,,,,,1.0,13996,2017-04-18 05:22:08.514,,,18/Apr/17 5:22 AM;mpatel;sorry for the delay.. i will look into it over the next 1-2 days [~megha],"18/Apr/17 6:46 AM;mpatel;I think it has to do with the join key being a complex type:

on a.gid = e.insertion.candidate.gadvertiserid

com.pinterest.ads.thrift.PinPromotionsInsertionEvent

Will need to dig more. See: https://qubole.atlassian.net/browse/HIVE-824 for something similar.","02/May/17 1:55 PM;megha;[~mpatel]
Which command did you check that had above join clause? 
all three failed queries: 
join to extracted_db_users - failure - https://api.qubole.com/v2/analyze?command_id=59134798
join to db_users - failure - https://api.qubole.com/v2/analyze?command_id=59289217
join to db_users_snapshots - failure - https://api.qubole.com/v2/analyze?command_id=59290372

have join on id, user_id etc, which are bigint columns..
did I probably miss something? 

","12/May/17 4:29 PM;megha;Tried a few more tests:
https://api.qubole.com/v2/analyze?command_id=69885290,71205569,71207016,71213854,71226757,71242255,71244992,72066519

I noticed that in general commands succeed to select all columns. But when "" lag"" is applied to any columns, it is resulting in failures. The failure error is same:

{code:java}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.uncheckedGetKey(LazyBinaryMap.java:274)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap.getMap(LazyBinaryMap.java:308)
	at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.getMap(LazyBinaryMapObjectInspector.java:46)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:313)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349)
	at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:349)
	at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:193)
	at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:179)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:266)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425)
	at org.apache.hadoop.mapred.Child$3.run(Child.java:205)
 ]
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:271)
	... 3 more
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blank/null results not displaying correctly in results table,ZEP-863,60934,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,mmajithia,ekang,ekang,13/Apr/17 10:16 AM,12/Jul/17 8:45 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,notebook-usability,,,,,,,"Account: TDC Data Sciences Spark
User: Virginia
Notebook: https://api.qubole.com/notebooks#home?id=34530&type=my-notebooks

When you run the select distinct query, the results come out wrong in the table. See attached screenshot. When I filter out the null/blank value it works fine.

",,ekang,navdeepp,rgupta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Apr/17 10:16 AM;ekang;request_13945.docx;https://qubole.atlassian.net/secure/attachment/41404/request_13945.docx,,,,,,,,,,,,,AWS,,,,,,turner,,,,,{},NA,Choose from,,AN-207,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z05aou:zzi,,,,,,nb-RB-47,,,,,,,,,,,,,1.0,13945,2017-05-20 22:26:22.176,,,"20/May/17 10:26 PM;rgupta;Looks like equivalent OS issue: https://github.com/apache/zeppelin/pull/1093

Not sure if they closed without merging?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive results not displaying properly,MW-750,60931,Bug,Open,MW,Middleware,software,sumitm,,,Major,,ksr,ekang,ekang,13/Apr/17 9:17 AM,17/Apr/17 10:06 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Account: Revenue Analytics
User: Virginia

Below are 2 queries displaying results very differently. To narrow the results down to a single value for both the good/bad queries, you can look at eventtime='20170323082609'

This query displays incorrectly
https://api.qubole.com/v2/analyze?command_id=66454413

This query displays correctly
https://api.qubole.com/v2/analyze?command_id=66451980

Based on how the query is written changes how it's displayed.",,aswina,ekang,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,turner,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04gn3:,,,,,,,,,,,,,,,,,,,,,2017-04-16 23:42:47.235,,,16/Apr/17 11:42 PM;aswina;This seems like a MW because of the way results are processed/returned.  Assigning this to [~sumitm] for further triage.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lock wait timeout exceeded ... : INSERT INTO `delayed_jobs` ,MW-740,60859,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sureshr,mpatel,mpatel,12/Apr/17 9:55 PM,26/Apr/17 12:31 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"This happened for Expedia. Prod-ETL account 5507. Re-run worked fine.

There were 2 of them:

{""error"":true,""message"":""SyntaxError: Unexpected token W"",""raw"":""We're sorry, but something went wrong. Please contact help@qubole.com and provide the error code 3804d371.""}

{""error"":true,""message"":""SyntaxError: Unexpected token W"",""raw"":""We're sorry, but something went wrong. Please contact help@qubole.com and provide the error code e22423e4.""}

From the rails 500 log:

{code}
500_errors.log-2017-04-08_1491621782.gz:ip-10-30-25-199 [ERROR pid: 26496: 17-04-07 06:16:23 3804d371] ErrorType: ActiveRecord::StatementInvalid,      Hostname: ip-10-30-25-199,      URL: POST https://api.qubole.com/api/latest/commands,      Params: {""label"":""p-prtnr-mktg"",""command_type"":""HiveCommand"",""query"":"" set hive.on.master=true; use lz; alter table lz.lz_partner_mktg_spend recover partitions; "",""action"":""create"",""controller"":""api/latest/commands"",""command"":{""label"":""p-prtnr-mktg"",""command_type"":""HiveCommand"",""query"":"" set hive.on.master=true; use lz; alter table lz.lz_partner_mktg_spend recover partitions; ""},""auth_token"":""ejkcH9JtZstyXRp2rjcU5o9ZSABxxaq7Lgsxnq4qHDstp2QKhTEG8MUsKqRq1YGq"",""command_source"":""API""},      Message: Mysql2::Error: Lock wait timeout exceeded; try restarting transaction: INSERT INTO `delayed_jobs` (`account_id`, `attempts`, `created_at`, `failed_at`, `handler`, `host_name`, `last_error`, `locked_at`, `locked_by`, `priority`, `queue`, `run_at`, `updated_at`) VALUES (5507, 0, '2017-04-07 06:15:32', NULL, '--- !ruby/object:Delayed::PerformableMethod\nobject: !ruby/object:QpalWorker\n  logstream: \nmethod_name: :query_run_method\nargs:\n- :acc_id: 5507\n  :log_path: \""/media/ephemeral0/tmp/tapp/tmp/2017-04-07/5507/65225265\""\n  :dest: s3://analytics-qubole-prod/prod-etl/tmp/2017-04-07/5507/65225265\n  :query_hist_id: 65225265\n  :ugi: s-qubole-ede-pm@expedia.com,default_group\n  :qbol_user_id: 17472\n', NULL, NULL, NULL, NULL, 20, 'qpal_worker', '2017-04-07 06:15:32', '2017-04-07 06:15:32'),      StackTrace: [""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_mysql_adapter.rb:245:in `query'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_mysql_adapter.rb:245:in `block in execute'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_adapter.rb:280:in `block in log'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/notifications/instrumenter.rb:20:in `instrument'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_adapter.rb:275:in `log'"", ""/usr/lib/ruby/gems/2.1.0/gems/newrelic_rpm-3.16.2.321/lib/new_relic/agent/instrumentation/active_record.rb:57:in `log_with_newrelic_instrumentation'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_mysql_adapter.rb:245:in `execute'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/mysql2_adapter.rb:213:in `execute'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/mysql2_adapter.rb:236:in `exec_insert'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/database_statements.rb:90:in `insert'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/query_cache.rb:14:in `insert'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/relation.rb:66:in `insert'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/persistence.rb:367:in `create'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/timestamp.rb:58:in `create'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/callbacks.rb:268:in `block in create'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:403:in `_run__1954721656522789937__create__4605477117537275830__callbacks'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in `__run_callback'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:385:in `_run_create_callbacks'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:81:in `run_callbacks'"", ""/usr/lib/ruby/gems/2.1.0/gems/airbrake-5.5.0/lib/airbrake/rails/active_record.rb:22:in `run_callbacks'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/callbacks.rb:268:in `create'""]


and:


500_errors.log-2017-04-08_1491621782.gz:ip-10-228-67-179 [ERROR pid: 24186: 17-04-07 06:17:22 e22423e4] ErrorType: ActiveRecord::StatementInvalid,      Hostname: ip-10-228-67-179,      URL: POST https://api.qubole.com/api/latest/commands,      Params: {""label"":""default"",""command_type"":""ShellCommand"",""script_location"":""s3://bex-analytics-softwares/builds/ede-tdr-cesc-map-hourly-qubole-mapreduce.git/6437a18b721781187e7a712a0ce2cce1a71b1324/scripts/bash/step_00_remove_dynamic_dir.bash"",""macros"":[{""ENVIRONMENT"":""'prod'""}],""action"":""create"",""controller"":""api/latest/commands"",""command"":{""label"":""default"",""command_type"":""ShellCommand"",""script_location"":""s3://bex-analytics-softwares/builds/ede-tdr-cesc-map-hourly-qubole-mapreduce.git/6437a18b721781187e7a712a0ce2cce1a71b1324/scripts/bash/step_00_remove_dynamic_dir.bash"",""macros"":[{""ENVIRONMENT"":""'prod'""}]},""auth_token"":""Ec95bPUU5CYSq3hHoTxuWCrPr7rdjUubnKxy1Fe6iU33NmoscVji5dVGNhyeWCnz"",""command_source"":""API""},      Message: Mysql2::Error: Lock wait timeout exceeded; try restarting transaction: INSERT INTO `delayed_jobs` (`account_id`, `attempts`, `created_at`, `failed_at`, `handler`, `host_name`, `last_error`, `locked_at`, `locked_by`, `priority`, `queue`, `run_at`, `updated_at`) VALUES (5507, 0, '2017-04-07 06:16:31', NULL, '--- !ruby/object:Delayed::PerformableMethod\nobject: !ruby/object:QpalWorker\n  logstream: \nmethod_name: :query_run_method\nargs:\n- :acc_id: 5507\n  :log_path: \""/media/ephemeral0/tmp/tapp/tmp/2017-04-07/5507/65225438\""\n  :dest: s3://analytics-qubole-prod/prod-etl/tmp/2017-04-07/5507/65225438\n  :query_hist_id: 65225438\n  :ugi: s-qubole-ede-arm@expedia.com,default_group\n  :qbol_user_id: 17460\n', NULL, NULL, NULL, NULL, 20, 'qpal_worker', '2017-04-07 06:16:31', '2017-04-07 06:16:31'),      StackTrace: [""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_mysql_adapter.rb:245:in `query'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_mysql_adapter.rb:245:in `block in execute'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_adapter.rb:280:in `block in log'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/notifications/instrumenter.rb:20:in `instrument'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_adapter.rb:275:in `log'"", ""/usr/lib/ruby/gems/2.1.0/gems/newrelic_rpm-3.16.2.321/lib/new_relic/agent/instrumentation/active_record.rb:57:in `log_with_newrelic_instrumentation'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_mysql_adapter.rb:245:in `execute'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/mysql2_adapter.rb:213:in `execute'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/mysql2_adapter.rb:236:in `exec_insert'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/database_statements.rb:90:in `insert'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/query_cache.rb:14:in `insert'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/relation.rb:66:in `insert'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/persistence.rb:367:in `create'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/timestamp.rb:58:in `create'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/callbacks.rb:268:in `block in create'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:403:in `_run__371726933028238488__create__107062086868019492__callbacks'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in `__run_callback'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:385:in `_run_create_callbacks'"", ""/usr/lib/ruby/gems/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:81:in `run_callbacks'"", ""/usr/lib/ruby/gems/2.1.0/gems/airbrake-5.5.0/lib/airbrake/rails/active_record.rb:22:in `run_callbacks'"", ""/usr/lib/ruby/gems/2.1.0/gems/activerecord-3.2.17/lib/active_record/callbacks.rb:268:in `create'""]
{code}

We recently increased the concurrent commands for this account (5507) last Thurs from 80 to 120. then again from 120-> 200 on 4/12",,mpatel,sumitm,surendranm,yogeshg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Expedia,turner,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04gdr:,,,,,,,,,,,,,,,,,,,,,2017-04-12 21:59:18.761,,,12/Apr/17 9:59 PM;sumitm;[~yogeshg]/[~surendranm] can one of you plz have a look on it. ,12/Apr/17 10:01 PM;yogeshg;I will take a look into it.,"12/Apr/17 11:00 PM;surendranm;This has been around for a long time. High burst rate (commands submissions/s) or decay rate (commands finishes/s) seems to fail because of this lock time out in delayed jobs table. 

At this point I am leaning towards Won't Fix, since this should go away with async. Work around is to ask customers to throttle requests as bit but still not reliable. CC [~sureshr] for input. ",13/Apr/17 9:49 AM;mpatel;Thanks [~surendranm].. is Async Architecture ready for them to start testing in their test accounts?,"15/Apr/17 1:46 AM;surendranm;Hey Minesh, From R43 CP, we should be able to start rolling out async architectures to customers with high concurrency and burst rates. GA is planned for R45. 

That said, since this is an architecture redesign, we are being cautious about our roll out. QA team is working on certifying async release in R43, so we will be able to take a better call. 

Does that help? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data Exchange - Import fails for a subscribed space ,MW-741,60849,Bug,Reopened,MW,Middleware,software,sumitm,,,Major,,rupeshb,sbadam,sbadam,12/Apr/17 3:08 PM,04/Aug/17 3:32 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Data Exchange - Import fails for a subscribed space 

User details: boblanton@myfitnesspal.com 
Scenario: He has access to two accounts(uacf, uacf-beta). Using Data Exchange feature in explore, he has shared a database(uacf_prod_derived) from uacf to a space. Account uacf-beta is subscribed to it. When he tries to import tables from uacf-beta, they are failing. PFA. What is the problem here?

To: [~sumitm], [~yogeshg]

",,ashishs,drose@qubole.com,ksr,p.vasa,psrinivas,sbadam,sumitm,vagrawal,,,,,,,,,,,,,,,,,,,,,,,,,,12/Apr/17 3:06 PM;sbadam;Screen Shot 2017-04-12 at 3.05.35 PM.png;https://qubole.atlassian.net/secure/attachment/41330/Screen+Shot+2017-04-12+at+3.05.35+PM.png,03/May/17 3:23 PM;p.vasa;data_import_fail.png;https://qubole.atlassian.net/secure/attachment/41971/data_import_fail.png,,,,,,,,,,,,AWS,,,,,,myfitnesspal,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04gbj:,,,,,,MW-RB44-Sprint-2,,,,,,,,,,,,,2.0,"13956,14279",2017-04-12 22:06:54.946,,,12/Apr/17 10:06 PM;sumitm;[~ashishs] can you plz check it. ,"13/Apr/17 2:14 AM;ashishs;there is some issue in log server sync so i could not find the exact place of this failure. 
I tried an api call myself using uacf-beta account and I am getting ""Access Denied"".
API: http://api.qubole.com/api/latest/subscribed_hivetables?published_hivetable_id=42&schema_name=uacf_prod_derived
Response:
{code}
<Error><Code>AccessDenied</Code><Message>Access Denied</Message>
{code}

I looked at the error code we get in response, that error code points to the location where we try to fetch the ddl file for the published hivetable.

For them ddl location is: 
{code}
ddl_location: s3://uacf-prod-derived-us-east-1/uacf_prod_derived/uacf_clientevent_avro/1491941394.ddl
{code}

i tried getting this ddl file using roles provided and that gave Forbidden, Acess Denied.
{code}
[ec2-user@ip-10-63-175-146 ~]$ hadoop dfs -Dfs.s3.awsAccessKeyId=AKIAJ7CZYQRSEND5PURQ -Dfs.s3.awsSecretAccessKey=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 -Dfs.s3.awsPrimaryRoleToBeAssumed=arn:aws:iam::257920104501:role/qubole-uacf-beta-cross -Dfs.s3.awsPrimaryExternalId=qubole_user -get s3://uacf-prod-derived-us-east-1/uacf_prod_derived/uacf_clientevent_avro/1491941394.ddl .
get: org.jets3t.service.S3ServiceException: Request Error. HEAD '/uacf_prod_derived%2Fuacf_clientevent_avro%2F1491941394.ddl' on Host 'uacf-prod-derived-us-east-1.s3.amazonaws.com' @ 'Thu, 13 Apr 2017 08:54:53 GMT' -- ResponseCode: 403, ResponseStatus: Forbidden
{code}

looks like ddl file could not be fetched using the roles given. 
interesting point to note here is we are able to list the base location using the given role but could not get granular files inside base location.

Please ask them to give proper access for those locations.
 ",14/Apr/17 1:37 PM;sbadam;Thanks [~ashishs] for quick analysis. Issue is resolved after adding permission. Customer gave feedback saying that error message should have been better. You can close it.,"03/May/17 3:21 PM;p.vasa;[~ashishs] The same user is not able to do a ""Data Import"" on a different table.
Here is all the information ->

TICKET ID (Zendesk) -> #14279

*General Description ->*
User -> boblanton@myfitnesspal.com

*-> Publisher Details ->*
a) Space Name -> “Uacf Integ Derived”
b) Table Name -> “mmf_timeseriesfeatureset_avro“
c) Publisher Account Name -> “uacf”
d) Publisher Account ID -> 1756
e) S3 Location of the table -> s3://uacf-integ-derived-us-east-1/mmf/TimeSeriesFeatureSet/avro
f) Location of the Space -> s3://uacf-integ-derived-us-east-1
g) Location of the DDL File -> s3://uacf-integ-derived-us-east-1/uacf_integ_derived/mmf_timeseriesfeatureset_avro/1493243683.ddl



*-> Subscriber Details ->*
a) Other’s Spaces Selected -> “Uacf Integ Derived”
b) Table Selected from the Other’s Spaces -> “mmf_timeseriesfeatureset_avro”
c) Hive Schema Selected to import -> “uacf_integ_derived_import”
d) Subscriber Account Name -> “uacf-beta”
e) Subscriber Account ID -> 4482


*Problem Definition ->*
Data Import from a table within Other’s Spaces is failing during Data Exchange process. (Please refer the screenshot attached for error message)

*Things Tried So Far ->*
-> Checked the policy permissions of the Publisher side and they look good (Note that here the bucket location for the Hive Table and DDL file under the Space are the same in terms of the base bucket location)

-> Here are the permissions from the Publisher end ->

{code:java}
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""Stmt1444052180000"",
            ""Effect"": ""Allow"",
            ""Action"": ""s3:GetObject"",
            ""Resource"": [
                ""arn:aws:s3:::uacf-prod-derived-us-east-1/cognitivecoaching*"",
                ""arn:aws:s3:::uacf-prod-derived-us-east-1/uacf_prod_derived/*"",
                ""arn:aws:s3:::uacf-prod-derived-us-east-1/dataseries*"",
                ""arn:aws:s3:::uacf-prod-derived-us-east-1/mfp/ClientEvent*"",
                ""arn:aws:s3:::uacf-prod-derived-us-east-1/mmf/ClientEvent*"",
                ""arn:aws:s3:::uacf-prod-events-derived-us-east-1/warehouse/*"",
                ""arn:aws:s3:::uacf-prod-derived-us-east-1/warehouse/*"",
                ""arn:aws:s3:::uacf-integ-derived-us-east-1/uacf_integ_derived/*"",
                ""arn:aws:s3:::uacf-integ-*""
            ]
        },
        {
            ""Sid"": ""Stmt144405218001"",
            ""Effect"": ""Allow"",
            ""Action"": [
                ""s3:GetBucketLocation"",
                ""s3:ListBucket""
            ],
            ""Resource"": [
                ""arn:aws:s3:::uacf-prod-derived-us-east-1*"",
                ""arn:aws:s3:::uacf-prod-events-derived-us-east-1/warehouse"",
                ""arn:aws:s3:::uacf-integ-*""
            ]
        }
    ]
}

{code}

-> The customer also has assigned this policy to a role which has then been assumed by the Subscriber.

-> Also asked the customer to try this command from one of the clusters in the Subscriber Account ->


{code:java}
-> hadoop dfs -Dfs.s3.awsAccessKeyId=<AWS_Access_Key_ID> -Dfs.s3.awsSecretAccessKey=<AWS_Secret_Key> -Dfs.s3.awsPrimaryRoleToBeAssumed=arn:aws:iam::<12_digit_AWS_account_number>:role/<role_name> -Dfs.s3.awsPrimaryExternalId=qubole_user -get s3://uacf-integ-derived-us-east-1/uacf_integ_derived/mmf_timeseriesfeatureset_avro/1493243683.ddl .

{code}

The above command was run from within a cluster belonging to the Subscriber account and it succeeded.

Here are the logs (from the Customer) ->

{code:java}
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/05/03 21:57:10 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
17/05/03 21:57:19 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://uacf-integ-derived-us-east-1.s3.amazonaws.com/uacf_integ_derived%2Fmmf_timeseriesfeatureset_avro%2F1493243683.ddl
17/05/03 21:57:19 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://uacf-integ-derived-us-east-1.s3.amazonaws.com/uacf_integ_derived%2Fmmf_timeseriesfeatureset_avro%2F1493243683.ddl
17/05/03 21:57:19 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://uacf-integ-derived-us-east-1.s3.amazonaws.com/uacf_integ_derived%2Fmmf_timeseriesfeatureset_avro%2F1493243683.ddl
17/05/03 21:57:19 INFO s3OperationsLog: Method=HEAD ResponseCode=200 URI=http://uacf-integ-derived-us-east-1.s3.amazonaws.com/uacf_integ_derived%2Fmmf_timeseriesfeatureset_avro%2F1493243683.ddl
17/05/03 21:57:19 INFO s3native.NativeS3FileSystem: file size of uacf_integ_derived/mmf_timeseriesfeatureset_avro/1493243683.ddl is 1799
{code}

I tried checking the ""*controller.log*"", ""*dj logs*"", ""*rails_log*"" under *webapp/* and *web/* on Log Server but could not find anything specific and some logs were also not synced.

Can you please help me with this issue?

",03/May/17 3:24 PM;p.vasa;Tagging some folks as Ashish is absent -> [~ksr],"03/May/17 10:14 PM;ksr;[~p.vasa] Okay, I'll check this today.","04/May/17 3:45 PM;p.vasa;[~ksr] Thank you. 
Can you please look into this today as the customer has asked for an update?
Please let me know if you need any additional information.","05/May/17 4:01 AM;ksr;[~p.vasa] I looked at the webapp logs for this and found the following error:

{code:java}
[ERROR pid: 2406: 17-05-05 09:33:34 554d7f81] [user:11217] [account:4482] [su:6216] ExprEvaluator:, cmd done with status: pid 2957 exit 64
[INFO  pid: 2406: 17-05-05 09:33:34 554d7f81] [user:11217] [account:4482] [su:6216] ExprEvaluator:, stderr: 2017-05-05 09:33:23,942 INFO  hivecli.py:161 - __init__ - Default hive version of the account is set as 1.2
2017-05-05 09:33:24,192 INFO  hivecli.py:741 - getQHSHostName - Using Hive tier.
2017-05-05 09:33:24,192 INFO  hivecli.py:463 - getStandaloneCmd - Using hive version 1.2 for hadoop2 cluster
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/05/05 09:33:25 INFO conf.HiveConf: HiveConf of name hive.io.override.jsonserde does not exist
17/05/05 09:33:25 INFO conf.HiveConf: HiveConf of name hive.io.file.cache.basedir does not exist
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/tez-42.2.0/tez/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hadoop2-42.41.1/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

Processing file s3://uacf-beta-qubole-us-east-1/scripts/bootstrap
Added [/media/ephemeral0/s3ncache/1488f23fcec6ca88df447ad23a1490a1/original-json-1.3-shaded.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/original-json-1.3-shaded.jar]
Added [/media/ephemeral0/s3ncache/ed5a991fac0177679e1cf2e2dc3b8a8a/original-json-serde-cdh4-shim-1.3-shaded.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/original-json-serde-cdh4-shim-1.3-shaded.jar]
Added [/media/ephemeral0/s3ncache/dab4fb2171578bb8bdf995152fae7ba9/original-json-serde-1.3-shaded.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/original-json-serde-1.3-shaded.jar]
Added [/media/ephemeral0/s3ncache/f681eaf47027ea550029e42833408a8a/mfp-hive-serde-json-1.0-SNAPSHOT.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/mfp-hive-serde-json-1.0-SNAPSHOT.jar]
Added [/media/ephemeral0/s3ncache/5970f54883b4831b24b97f1125ba27e6/commons-codec-1.6.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/commons-codec-1.6.jar]
Added [/media/ephemeral0/s3ncache/226f5207543c490f10f234e82108b998/joda-time-2.2.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/joda-time-2.2.jar]
Added [/media/ephemeral0/s3ncache/f81cd606325198d88253158de710d7f8/sm-hive-udf-1.0-SNAPSHOT.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/sm-hive-udf-1.0-SNAPSHOT.jar]
Added [/media/ephemeral0/s3ncache/a75989fde037adf0d4ec848c34ad5dae/csv-serde-1.1.2-0.11.0.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/csv-serde-1.1.2-0.11.0.jar]
Added [/media/ephemeral0/s3ncache/5c29018dbf14c02de34c4a66e5bd2c63/mfp-csv-serde-1.0.0-all.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/mfp-csv-serde-1.0.0-all.jar]
Added [/media/ephemeral0/s3ncache/2da3693fb483013b132112b5618286ff/hive-udfs-1.0.3.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/udfs/hive-udfs-1.0.3.jar]
Added [/media/ephemeral0/s3ncache/57a2d7651c52da3abe68f2270819b084/mfp-escapedline-inputformat-1.0.0.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/mfp-escapedline-inputformat-1.0.0.jar]
Added [/media/ephemeral0/s3ncache/2bd396368872badfd965d80675e84b0a/avro-1.7.7.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/avro-1.7.7.jar]
Added [/media/ephemeral0/s3ncache/a810a857bd691db1139291131d24178a/avro-mapred-1.7.7.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/avro-mapred-1.7.7.jar]
Added [/media/ephemeral0/s3ncache/513c3a06e2fc687a2123abcd9e6ec080/avro-json-1.0-SNAPSHOT.jar] to class path
Added resources: [s3://quoble.aws.myfitnesspal.com/jars/avro-json-1.0-SNAPSHOT.jar]
OK
Time taken: 1.178 seconds
OK
Time taken: 0.005 seconds
OK
Time taken: 0.005 seconds
OK
Time taken: 0.005 seconds
OK
Time taken: 0.005 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.006 seconds
OK
Time taken: 0.005 seconds
OK
Time taken: 1.248 seconds
NoViableAltException(142@[])
        at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.identifier(HiveParser_IdentifiersParser.java:10871)
        at org.apache.hadoop.hive.ql.parse.HiveParser.identifier(HiveParser.java:47761)
        at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameColonType(HiveParser.java:39058)
        at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameColonTypeList(HiveParser.java:37189)
        at org.apache.hadoop.hive.ql.parse.HiveParser.structType(HiveParser.java:40554)
        at org.apache.hadoop.hive.ql.parse.HiveParser.type(HiveParser.java:39502)
        at org.apache.hadoop.hive.ql.parse.HiveParser.listType(HiveParser.java:40443)
        at org.apache.hadoop.hive.ql.parse.HiveParser.type(HiveParser.java:39487)
        at org.apache.hadoop.hive.ql.parse.HiveParser.colType(HiveParser.java:39214)
        at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameType(HiveParser.java:38898)
        at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameTypeList(HiveParser.java:37050)
        at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:5428)
        at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2680)
        at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1682)
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1122)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:327)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1164)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1218)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1101)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1091)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:248)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:195)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:418)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:349)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:838)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:800)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:711)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:1082 cannot recognize input near 'interval' ':' 'struct' in column specification
org.apache.hadoop.hive.ql.parse.ParseException: line 1:1082 cannot recognize input near 'interval' ':' 'struct' in column specification
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:327)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1164)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1218)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1101)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1091)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:248)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:195)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:418)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:349)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:838)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:800)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:711)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
[INFO  pid: 2406: 17-05-05 09:33:34 ] Completed 422 Unprocessable Entity in 13622.3ms (Views: 10.4ms | ActiveRecord: 0.0ms)
{code}

This is the command that is causing this issue

{code:java}
/usr/lib/hive_scripts/hivecli.py --hive-version=1.2 --timeout=75 -c hive.fetch.task.conversion=more -e 'set hive.mapred.mode=nonstrict;use uacf_integ_derived_import; set fs.s3.awsBucketToRoleMapping={""uacf-integ-derived-us-east-1"":{""roleName"":""arn:aws:iam::257920104501:role/qubole-uacf-beta-cross"",""externalId"":""257920104501""}}; set fs.s3n.awsBucketToRoleMapping={""uacf-integ-derived-us-east-1"":{""roleName"":""arn:aws:iam::257920104501:role/qubole-uacf-beta-cross"",""externalId"":""257920104501""}}; CREATE EXTERNAL TABLE `mmf_timeseriesfeatureset_avro`(   `header_schema_id` string COMMENT ""from deserializer"",    `envelope_header__id` string COMMENT ""from deserializer"",    `envelope_header__namespace` string COMMENT ""from deserializer"",    `envelope_header__event_type` string COMMENT ""from deserializer"",    `envelope_header__creation_date` bigint COMMENT ""from deserializer"",    `envelope_header__schema_id` string COMMENT ""from deserializer"",    `envelope_header__trace_path` array<struct<message_id:string,namespace:string,event_type:string,zone:string>> COMMENT ""from deserializer"",    `envelope_header__library_version` string COMMENT ""from deserializer"",    `payload__feature_set_name` string COMMENT ""from deserializer"",    `payload__source` string COMMENT ""from deserializer"",    `payload__feature_set_version` string COMMENT ""from deserializer"",    `payload__feature_set` array<struct<name:string,type:string,value:string>> COMMENT ""from deserializer"",    `payload__time_series_set` array<struct<address:struct<data_type_id:string,tags:map<string,string>>,name:string,interval:struct<start:string,end:string>,meta:map<string,string>,tags:array<string>>> COMMENT ""from deserializer"") PARTITIONED BY (    `dt` string,    `hr` string) ROW FORMAT SERDE    ""org.apache.hadoop.hive.serde2.avro.AvroSerDe""  STORED AS INPUTFORMAT    ""org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat""  OUTPUTFORMAT    ""org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat"" LOCATION   ""s3://uacf-integ-derived-us-east-1/mmf/TimeSeriesFeatureSet/avro"" TBLPROPERTIES (   ""avro.schema.url""=""s3:\/\/uacf-integ-derived-us-east-1\/mmf\/TimeSeriesFeatureSet\/avro\/derived_TimeSeriesFeatureSet.avsc"",    ""avro.schema.md5sum""=""79ca57c72775a1b9803cf906c0b4a41d"",    ""transient_lastDdlTime""=""1492015251"")' -c hive.log.qbol.info.from.driver=/tmp/mdfile20170505-2406-gqkon6 --use-hive-tier  --qbol-user-id 29109  --ugi 'boblanton@myfitnesspal.com,default_group'      1> /tmp/expeval20170505-2406-14djlsi 4482
{code}

This seems to be some error in hive. cc: [~psrinivas] [~nitink]","12/May/17 4:21 PM;p.vasa;[~ksr] Thanks a lot for all the information.
[~psrinivas] [~nitink] Can you please look into this issue? -> The customer has asked for an update on the same.
Please let me know if you need any additional information.","12/May/17 5:27 PM;psrinivas;[~p.vasa]: ""Interval"" is a keyword in Hive. https://issues.apache.org/jira/browse/HIVE-9792

This is a parse exception. If you escape the keywords, the query should work. 

In this particular case, I don't think, it will work as the column is a struct datatype. They have to change the name of the struct variables. 

I will try to dig deep, but, for now, we should ask them to change. ","12/May/17 5:28 PM;psrinivas;Probably similar stuff here: http://stackoverflow.com/questions/36468387/hive-create-table-for-json-struct-error, escaping might help. ",12/May/17 5:54 PM;p.vasa;Thanks [~psrinivas],"22/May/17 4:22 PM;p.vasa;[~psrinivas] I asked them to try out either of these 2 solutions ->
1) Avoid using Reserved Keywords as table column names 
2) Escape the column names which are Reserved Keywords during table creation statement

It seems like they cannot do this themselves and asked the following question ->

{code:java}
""Hmm, this is something we cannot really do, as our table schemas are inferred from AVRO schema files. We don't ""create the tables"" using the full type information, instead we let the AVRO serde figure those pieces out automatically. Since it seems you are doing some internal ""create external table"" stuff from the linked account, is it possible that that mechanism ""quote"" columns. I understand this can be particularly tricky as the keyword in use (interval) is embedded in a complex type.

Or just not ""define types"" for the table in the ""create table"" world if the serde is Avro (which may be easier, but not sure)""
{code}

Is it possible for us to support any of these customer requirements?","22/May/17 4:37 PM;psrinivas;[~p.vasa]My point is to change our side in dbimport/dbexport, not on customer side. ","22/May/17 5:38 PM;p.vasa;[~ashishs] Based on (cc -> [~psrinivas]) comments -> Is it possible that we do some modifications on our end to not include/not consider the column information (name and data type etc.) from the ""CREATE TABLE"" statement when the table is getting created from a deserializer using AVRO SerDe?

And then apply escaping logic (or) any other logic from our end in the ""Data Import"" code, so that such tables can be imported successfully?

Example of ""CREATE TABLE"" ->
Command ID -> *74076616*

{code:java}
CREATE EXTERNAL TABLE `uacf_integ_derived.mmf_timeseriesfeatureset_avro_1`


PARTITIONED BY ( 
  `dt` string, 
  `hr` string)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION
  's3://uacf-integ-derived-us-east-1/mmf/TimeSeriesFeatureSet/avro'
TBLPROPERTIES (
  'avro.schema.url'='s3:\/\/uacf-integ-derived-us-east-1\/mmf\/TimeSeriesFeatureSet\/avro\/derived_TimeSeriesFeatureSet.avsc', 
  'avro.schema.md5sum'='e3ac2adcf2fd261ceeba16697039ebfa', 
  'transient_lastDdlTime'='1492015251')
{code}

This is what ""SHOW CREATE TABLE <table_name>"" looks like for the same table ->
Command ID -> *74076723*
{code:java}
-> show create table `uacf_integ_derived.mmf_timeseriesfeatureset_avro_1`

->
CREATE EXTERNAL TABLE `uacf_integ_derived.mmf_timeseriesfeatureset_avro_1`(
  `header_schema_id` string COMMENT 'from deserializer', 
  `envelope_header__id` string COMMENT 'from deserializer', 
  `envelope_header__namespace` string COMMENT 'from deserializer', 
  `envelope_header__event_type` string COMMENT 'from deserializer', 
  `envelope_header__creation_date` bigint COMMENT 'from deserializer', 
  `envelope_header__schema_id` string COMMENT 'from deserializer', 
  `envelope_header__trace_path` array<struct<message_id:string,namespace:string,event_type:string,zone:string>> COMMENT 'from deserializer', 
  `envelope_header__library_version` string COMMENT 'from deserializer', 
  `envelope_header__trace__trace_implementation` string COMMENT 'from deserializer', 
  `envelope_header__trace__data` map<string,string> COMMENT 'from deserializer', 
  `payload__feature_set_name` string COMMENT 'from deserializer', 
  `payload__source` string COMMENT 'from deserializer', 
  `payload__feature_set_version` string COMMENT 'from deserializer', 
  `payload__feature_set` array<struct<name:string,type:string,value:string>> COMMENT 'from deserializer', 
  `payload__time_series_set` array<struct<address:struct<data_type_id:string,tags:map<string,string>>,name:string,interval:struct<start:string,end:string>,meta:map<string,string>,tags:array<string>>> COMMENT 'from deserializer')
PARTITIONED BY ( 
  `dt` string, 
  `hr` string)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION
  's3n://uacf-integ-derived-us-east-1/mmf/TimeSeriesFeatureSet/avro'
TBLPROPERTIES (
  'avro.schema.url'='s3:\/\/uacf-integ-derived-us-east-1\/mmf\/TimeSeriesFeatureSet\/avro\/derived_TimeSeriesFeatureSet.avsc', 
  'avro.schema.md5sum'='e3ac2adcf2fd261ceeba16697039ebfa', 
  'transient_lastDdlTime'='1495498793')
{code}



","30/May/17 4:25 PM;p.vasa;[~ashishs] Can you please provide any update based on the previous comment?
Let me know if you need any additional information.","31/May/17 2:37 AM;ashishs;[~p.vasa] I am not much aware of the data import intricacies on how the command gets formed and what we can do to avoid those as per requirement. 
I will let [~ksr] answer this as he is well aware of the details.

[~ksr] can you check and update here on what the possible options we have in here.?","05/Jun/17 1:11 PM;p.vasa;[~ksr] Can you please provide some information on the above question (any possible options available to modify the data import process so as not to include table definitions and then apply escaping logic to them to avoid the above issue of using reserved keywords as table column names)?
",09/Jun/17 3:24 PM;drose@qubole.com;[~ashishs] how do we help the customer help move forward? Sounds like we need to make a change on our end to allow them to utilize this. [~p.vasa],"11/Jun/17 10:42 PM;ksr;[~p.vasa] [~psrinivas] This is not an issue in dbimport/export. AFAIK, dbimport/export (sqoop) is not used here. This follows a different code path I think. Assigning to [~sumitm] for priority/re-assignment.","31/Jul/17 2:10 PM;p.vasa;[~sumitm] Can you please reassign this JIRA as required?
(Please let me know if you need any additional information)",04/Aug/17 2:19 AM;sumitm;[~rupeshb] making you the new owner of Data Exchange feature :). Please talk to [~ksr] or [~ashishs] or [~vagrawal] to understand the feature and the bug. ,"04/Aug/17 3:32 AM;vagrawal;[~sumitm] - dont we want to deprecate data exchange. If there is such plan, there is no point in supporting it.  cc [~rangasayeec]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Browser crashes with notebook of huge size,ZEP-855,60808,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,vipulm,satyavathib,satyavathib,12/Apr/17 3:23 AM,11/Jul/17 5:25 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,notebook-reliability,,,,,,,"Notebook ID : 32674

Cluster ID : 24969

Customer Email : bahtiyar.babanazarov@turner.com

Notebook Size : 88M

When the cluster is down, the notebook loads half and then the browser freezes totally and crashes.

When the cluster is up the notebook loads fully but the navigation becomes very slow and the page freezes.

Just thinking If we can handle large size notebooks in a better way rather than crash the page without any clue why that is happening.

Could we restrict the customer from creating a new notebook when the size exceeds the limit? Just a small thought.",,aabbas,karuppayyar,mahuja,mmajithia,rgupta,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Apr/17 3:20 AM;satyavathib;Screen Shot 2017-04-12 at 3.12.21 PM.png;https://qubole.atlassian.net/secure/attachment/41313/Screen+Shot+2017-04-12+at+3.12.21+PM.png,,,,,,,,,,,,,AWS,,,,,,turner,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04g2f:,,,,,,,,,,,,,,,,,,,1.0,13907,2017-04-12 14:39:49.436,,,12/Apr/17 4:00 AM;satyavathib;Also the customer is asking if there is a way to handle this type of huge notebooks without getting crashed.,"12/Apr/17 2:39 PM;mahuja;cc - [~beria], [~vipulm]
","13/Apr/17 8:49 AM;aabbas;Bahtiyar from turner asking if this is related or should be a different ticket:

I will probably submit a ticket for this but here is an issue:

I submitted a large block of sql code to create a table with many variables. I just realized that a particular variable had not been created in that query and I did not get any error message either.

The variable that was not created is called valscr_change_inc_l19_24 and it is in the Notebook called sqls_as_vector, the cluster number is 24969.

I run the query while back, the table I was creating was called temp_user_store.bb_tcm_table_ones_recoded

So, this could be related to the issue you just said, the paragraph being too large. Could you please take a look into it and determine whether there is another additional issue other than paragraph being too large?


","27/Jun/17 9:46 PM;rgupta;In zep 0.6.2 we have done lot of fixes to make large notebooks faster. Also there is global limit to not show on UI very large results. 

Not sure when was this issue raised, but I think with zep0.6.2 release these should largely be fixed/improved now. [~karuppayyar] [~vipulm] wat u guys think?","27/Jun/17 9:53 PM;karuppayyar;*zeppelin.output_size_limit.* is the acoounlimit and is set to 30KB.
This is [rolled|https://qubole.atlassian.net/browse/ROLL-86?focusedCommentId=103749&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-103749] out for everyone,
If the issue is still there, we can check if the cluster has very old zeppelin package which doesnot honour this limit
","27/Jun/17 9:58 PM;karuppayyar;

{code:java}
production-replica> select zeppelin_s3_package_name from cluster_configs where cluster_id=24969 and is_latest=1;
+--------------------------+
| zeppelin_s3_package_name |
+--------------------------+
| pb-45                    |
+--------------------------+
1 row in set (0.02 sec)
{code}
{code:java}
[karuppayyar@nandi ~]$ /usr/bin/s3cmd ls s3://paid-qubole/archives/zeppelin/zeppelin-pb-45/
2017-06-07 22:07 321296895   s3://paid-qubole/archives/zeppelin/zeppelin-pb-45/00002-0.94066127.27.tar.gz
2017-06-05 21:01      3233   s3://paid-qubole/archives/zeppelin/zeppelin-pb-45/install.sh
{code}

The package was uploaded in early June, this jira has been created before that.
We can check if the problem is still there",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master node not updated to DB,ACM-1097,60731,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,satyavathib,satyavathib,11/Apr/17 5:24 AM,19/Jun/17 1:03 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster Id : 26106
Account_ID : 2336

Cluster Start log ;

{code}
2017-04-11 12:01:10,460 >>> Creating encrypted channel to master node: ec2-54-164-194-16.compute-1.amazonaws.com
*** 2017-04-11 12:02:16,061 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:03:23,907 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:04:31,853 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
2017-04-11 12:04:43,863 >>> Creating encrypted channel to master node: ec2-54-164-194-16.compute-1.amazonaws.com
*** 2017-04-11 12:05:51,925 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:06:59,711 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:08:07,546 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
2017-04-11 12:08:19,555 >>> Creating encrypted channel to master node: ec2-54-164-194-16.compute-1.amazonaws.com
*** 2017-04-11 12:09:25,224 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:10:33,041 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:11:40,762 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
2017-04-11 12:11:52,774 >>> Creating encrypted channel to master node: ec2-54-164-194-16.compute-1.amazonaws.com
*** 2017-04-11 12:12:58,305 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:14:05,907 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:15:13,587 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
2017-04-11 12:15:25,594 >>> Creating encrypted channel to master node: ec2-54-164-194-16.compute-1.amazonaws.com
*** 2017-04-11 12:16:31,125 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:17:38,768 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
*** 2017-04-11 12:18:46,286 WARNING - Couldn't create encrypted channel to cluster: qbol_acc2336_cl26106. Retrying.
2017-04-11 12:18:58,296 >>> Creating encrypted channel to master node: ec2-54-164-194-16.compute-1.amazonaws.com
The command resulted in timeout.
{code}

1. Hustler log on log server not updated :

{code}
[ec2-user@ip-10-182-127-68 hustler]$ grep 26106 hustler.log
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,344 DEBUG  config.py:602 - load - Loading config
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,345 DEBUG  config.py:135 - _get_fp - Loading file: /usr/lib/qubole/packages/hustler-41.24.0/hustler/configs/config.default
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,740 INFO  [cluster:25123] [cluster_instance:434500]  clusterinfo.py:232 - execute - Cluster id '25123', account id '184' marked UP in DB. Trying to latch ...
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,351 DEBUG [cluster:25123] [cluster_instance:434500]  cli.py:244 - main - Command: CmdClusterInfo, Args: ['25123', 'qbol_acc184_cl25123'], Opts: {'can_launch_new': 'True', 'hustler_package_version': None, 'qubole_command_id': 0}
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,740 DEBUG [cluster:25123] [cluster_instance:434500]  utils.py:610 - get_proxy_port - Executing:
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,738 DEBUG [cluster:25123] [cluster_instance:434500]  cluster.py:5939 - get_heterogeneous_config - Heterogeneous config for cluster qbol_acc184_cl25123: {""memory"": [{""instance_type"": ""c3.xlarge"", ""stable_spot_bid"": null, ""weight"": 1.0, ""volatile_spot_bid"": 1.05}, {""instance_type"": ""c3.2xlarge"", ""stable_spot_bid"": null, ""weight"": 2.0, ""volatile_spot_bid"": 2.1}, {""instance_type"": ""c3.4xlarge"", ""stable_spot_bid"": null, ""weight"": 4.0, ""volatile_spot_bid"": 4.2}, {""instance_type"": ""c3.8xlarge"", ""stable_spot_bid"": null, ""weight"": 8.0, ""volatile_spot_bid"": 8.4}]}
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,846 DEBUG [cluster:25123] [cluster_instance:434500]  utils.py:623 - get_proxy_port - Established/Using encrypted channel to cluster qbol_acc184_cl25123 (port: 20346)
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,846 DEBUG [cluster:25123] [cluster_instance:434500]  utils.py:552 - access_url - Executing:
ip-10-167-91-123 PID: 26106  2017-04-11 03:36:01,871 INFO  [cluster:25123] [cluster_instance:434500]  clusterinfo.py:335 - perform_curl_check - Resource manager on master ec2-54-172-47-55.compute-1.amazonaws.com for cluster qbol_acc184_cl25123 is accessible.
{code}


Log from cluster start log on UI :

{code}
2017-04-11 11:20:00,932 >>> Validating cluster template settings...
*** 2017-04-11 11:20:01,005 WARNING - Skipping keypair fingerprint validation...
2017-04-11 11:20:01,217 >>> Cluster template settings are valid
2017-04-11 11:20:01,285 >>> cluster_id 26106, cluster_inst_id 434902:: extending start loop time by 1260 seconds since it uses stable spot nodes
2017-04-11 11:20:01,304 >>> Starting cluster...
2017-04-11 11:20:01,304 >>> Launching a 3-node cluster...
2017-04-11 11:20:01,448 >>> Launching master node (ami: ami-3218a924, type: m1.xlarge, zone: us-east-1b)...
2017-04-11 11:20:01,448 >>> Placing a bid on 1 spot instance(s) for $0.525 with a timeout of 10 minute(s)in us-east-1b zone
2017-04-11 11:20:02,047 >>> SpotInstanceRequest:sir-ftwi7ipg
2017-04-11 11:20:02,048 >>> Tracking spot request id: sir-ftwi7ipg
0/1 | | 0% 0/1 | | 0% 2017-04-11 11:20:12,884 >>> Spot request id: sir-ftwi7ipg, allotted instance id: i-0952533261b2c8a33
1/1 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100% 
2017-04-11 11:20:22,952 >>> Public DNS is attached to the master node(s)
2017-04-11 11:20:22,952 >>> Zone selected for master node: us-east-1b
2017-04-11 11:20:22,953 >>> Trying to bringup slave in AZ - us-east-1b
2017-04-11 11:20:22,953 >>> Placing a bid on 2 spot instance(s) for $0.525 with a timeout of 10 minute(s)in us-east-1b zone
2017-04-11 11:20:23,632 >>> SpotInstanceRequest:sir-rapg4azj
2017-04-11 11:20:23,632 >>> SpotInstanceRequest:sir-biyi436g
2017-04-11 11:20:23,633 >>> Tracking spot request id: sir-rapg4azj
2017-04-11 11:20:23,633 >>> Tracking spot request id: sir-biyi436g
0/2 | | 0% 0/2 | | 0% 2017-04-11 11:20:34,861 >>> Spot request id: sir-rapg4azj, allotted instance id: i-0f43d61442bf4a050
2017-04-11 11:20:34,862 >>> Spot request id: sir-biyi436g, allotted instance id: i-0b8ad70ba599577cb
2/2 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100% 
2017-04-11 11:20:38,362 >>> Waiting for cluster to come up... (updating every 10s)
2017-04-11 11:20:45,652 >>> Waiting for all nodes to be in a 'running' state...
0/3 | | 0% 0/3 | | 0% 1/3 |---------------------- | 33% 1/3 |\\\\\\\\\\\\\\\\\\\\\\ | 33% 1/3 ||||||||||||||||||||||| | 33% 1/2 |///////////////////////////////// | 50% 1/2 |--------------------------------- | 50% 1/1 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100% 
2017-04-11 11:22:09,890 >>> Recording nodes information to the database
2017-04-11 11:22:12,476 >>> ==> Waiting for Hadoop to come up ...
{code}

But seems like this master info is not updated to DB :

{code}
production-replica> select * from cluster_nodes where cluster_inst_id=434902 \G;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    2058345
Current database: rstore

*************************** 1. row ***************************
                  id: 1498477284
     ec2_instance_id: i-0b8ad70ba599577cb
            hostname: ec2-52-90-23-121.compute-1.amazonaws.com
                role: node0002
amazon_instance_type: m1.xlarge
       spot_instance: 1
             up_time: 2017-04-11 11:20:31
           down_time: NULL
      last_seen_time: 2017-04-11 11:22:12
          account_id: 2336
          private_ip: ip-10-100-1-240.ec2.internal
     cluster_inst_id: 434902
              status: NULL
  private_ip_address: 10.100.1.240
  termination_reason: NULL
1 row in set (0.07 sec)

ERROR:
No query specified



production-replica> select * from cluster_nodes where ec2_instance_id=""i-0952533261b2c8a33"" \G;
Empty set (0.00 sec)

ERROR:
No query specified

{code}

{code}
[ec2-user@ip-10-143-41-101 ~]$ easy_hustler --cluster-id 26106 sshmaster -u 'ec2-user' qbol_acc2336_cl26106 \G;
No config file specified - defaulting to hustler/configs/config.default for config file
!!! 2017-04-11 12:11:41,123 ERROR - Could not find Master DNS from DB to SSH. Making AWS call to find out master DNS.
*** 2017-04-11 12:11:43,436 WARNING - Skipping keypair fingerprint validation...
!!! 2017-04-11 12:11:45,752 ERROR - node 'master' does not exist
[ec2-user@ip-10-143-41-101 ~]$
{code}",,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04flb:,,,,,,,,,,,,,,,,,,,1.0,13936,,,,"11/Apr/17 5:27 AM;satyavathib;Cluster_Configs :

{code}
production-replica> select * from cluster_configs where cluster_id = 26106 order by id desc limit 5 \G;
*************************** 1. row ***************************
                              id: 94852
                      cluster_id: 26106
              compute_access_key: aRrZLcpKuCFDTv3nJhX9z9dgcYK1cZn8+Cwgb6WaxNE=
              compute_secret_key: IkIgZbslq1KGTTJEqVuacxgqgaIsHeWYD+l1B5qJhG4f9v46XMTLkmTnXplLMd3V
               compute_validated: 1
                      aws_region: us-east-1
           aws_availability_zone: us-east-1b
              hadoop_master_type: m1.xlarge
               hadoop_slave_type: m1.xlarge
            hadoop_initial_nodes: 2
                hadoop_max_nodes: 180
            custom_hadoop_config: yarn.scheduler.maximum-allocation-vcores=8
mapred.hustler.nodes.lease.period=3600000
spark.sql.parquet.output.committer.class=org.apache.spark.sql.execution.datasources.parquet.DirectParquetOutputCommitter
mapred.output.committer.class=org.apache.hadoop.mapred.DirectFileOutputCommitter
hustler.resource.checker.min.resourcerequest.ignore-ms=0
fs.s3n.multipart.uploads.enabled=true
fs.s3n.multipart.streaming.uploads.maxpartsize.mb=10
fs.s3n.multipart.uploads.concurrency.factor=4
fs.s3n.multipart.copy.block.size=3000000000
spark.files.useFetchCache=false
spark.yarn.max.executor.failures=100
yarn.app.mapreduce.am.resource.mb=6720
           fair_scheduler_config: NULL
                    default_pool: NULL
       hadoop_slave_request_type: spot
    maximum_bid_price_percentage: 100.0
        timeout_for_spot_request: 10
maximum_spot_instance_percentage: 100
      persistent_security_groups: NULL
             restrict_ssh_access: 0
            encrypted_ephemerals: 0
                customer_ssh_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDVGWvqdkSPEduLd7tx7DIptG67gS6WQFy7OBlie/9MKHgCtCdqKvLBQcQgOa84GiwQCTsE2agYEGp3/YFkqJWH/M5Z01Y1IWXCcnjso6lBBol6bQVJond8vHFAeJSTk71IY+mvU+PxBrtY+OyiMWkXCnHNBEAVjq2F6bJBsPZRjNQDzU+doDwz15wOeJoBdpFeKHp2olibWZGMOZRDGVPQVc3MJ/YQeKvRocr24OxU3KvW3+Iq3erbwVMvl+oyociduCGepbjIFZ2+7H13a68tDhdodO/T0juSzdvfz0ZdDwdUkNf75ditUfzv5VsHKaSbELOOrec5IR8sdp3DtPY7 Devops
    disallow_cluster_termination: 0
              ganglia_monitoring: 1
                      created_at: 2017-04-11 10:18:15
                      updated_at: 2017-04-11 10:44:42
                       is_latest: 1
                  presto_jvm_mem: 6144
                 presto_task_mem: 1000
                    force_tunnel: 1
             node_bootstrap_file: 
               is_presto_enabled: 0
            custom_presto_config: NULL
             stable_spot_timeout: 10
      stable_spot_bid_percentage: 150.0
            stable_spot_fallback: 1
                          vpc_id: vpc-6d9c0109
                       subnet_id: subnet-a2f836d4
                tunnel_server_ip: NULL
                     use_hadoop2: 1
                ebs_volume_count: 0
                 ebs_volume_size: 100
                 ebs_volume_type: standard
           use_stable_spot_nodes: NULL
     use_qubole_placement_policy: 1
            fallback_to_ondemand: 1
                 custom_ec2_tags: {""Owner"":""force"",""Role"":""force"",""Environment"":""production"",""Cluster"":""force""}
                       use_hbase: 0
                       use_spark: 1
                   config_errors: --- []

                compute_role_arn: NULL
             compute_external_id: fU/WpMRfZ7CkimMnOA9npI/qFlw0skgnJvzK/YPUW14=
                   ami_overrides: null
               datadog_api_token: j+oWXDSySCcm/Mr9g9RbXg==
               datadog_app_token: j+oWXDSySCcm/Mr9g9RbXg==
       use_account_compute_creds: 1
             custom_spark_config: spark-defaults.conf:
spark.speculation false
spark.task.maxFailures 10
                   spark_version: 2.0.0
            hadoop_setup_timeout: NULL
           role_instance_profile: NULL
         bastion_node_public_dns: NULL
               master_elastic_ip: NULL
           spark_s3_package_name: NULL
        zeppelin_s3_package_name: NULL
              engine_config_type: NULL
                engine_config_id: NULL
                  presto_version: 0.142
                 cloud_config_id: NULL
               cloud_config_type: NULL
            heterogeneous_config: {}
       zeppelin_interpreter_mode: legacy
                           is_ha: NULL
            ebs_upscaling_config: null
                    enable_rubix: 0
       compute_validation_result: null
            idle_cluster_timeout: 1
             spot_block_duration: NULL
*************************** 2. row ***************************
                              id: 91316
                      cluster_id: 26106
              compute_access_key: aRrZLcpKuCFDTv3nJhX9z9dgcYK1cZn8+Cwgb6WaxNE=
              compute_secret_key: IkIgZbslq1KGTTJEqVuacxgqgaIsHeWYD+l1B5qJhG4f9v46XMTLkmTnXplLMd3V
               compute_validated: 1
                      aws_region: us-east-1
           aws_availability_zone: Any
              hadoop_master_type: m1.xlarge
               hadoop_slave_type: m1.xlarge
            hadoop_initial_nodes: 2
                hadoop_max_nodes: 180
            custom_hadoop_config: yarn.scheduler.maximum-allocation-vcores=8
mapred.hustler.nodes.lease.period=3600000
spark.sql.parquet.output.committer.class=org.apache.spark.sql.execution.datasources.parquet.DirectParquetOutputCommitter
mapred.output.committer.class=org.apache.hadoop.mapred.DirectFileOutputCommitter
hustler.resource.checker.min.resourcerequest.ignore-ms=0
fs.s3n.multipart.uploads.enabled=true
fs.s3n.multipart.streaming.uploads.maxpartsize.mb=10
fs.s3n.multipart.uploads.concurrency.factor=4
fs.s3n.multipart.copy.block.size=3000000000
spark.files.useFetchCache=false
spark.yarn.max.executor.failures=100
yarn.app.mapreduce.am.resource.mb=6720
           fair_scheduler_config: NULL
                    default_pool: NULL
       hadoop_slave_request_type: spot
    maximum_bid_price_percentage: 100.0
        timeout_for_spot_request: 10
maximum_spot_instance_percentage: 100
      persistent_security_groups: NULL
             restrict_ssh_access: 0
            encrypted_ephemerals: 0
                customer_ssh_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDVGWvqdkSPEduLd7tx7DIptG67gS6WQFy7OBlie/9MKHgCtCdqKvLBQcQgOa84GiwQCTsE2agYEGp3/YFkqJWH/M5Z01Y1IWXCcnjso6lBBol6bQVJond8vHFAeJSTk71IY+mvU+PxBrtY+OyiMWkXCnHNBEAVjq2F6bJBsPZRjNQDzU+doDwz15wOeJoBdpFeKHp2olibWZGMOZRDGVPQVc3MJ/YQeKvRocr24OxU3KvW3+Iq3erbwVMvl+oyociduCGepbjIFZ2+7H13a68tDhdodO/T0juSzdvfz0ZdDwdUkNf75ditUfzv5VsHKaSbELOOrec5IR8sdp3DtPY7 Devops
    disallow_cluster_termination: 0
              ganglia_monitoring: 1
                      created_at: 2017-03-22 01:14:42
                      updated_at: 2017-04-11 10:18:15
                       is_latest: 0
                  presto_jvm_mem: 6144
                 presto_task_mem: 1000
                    force_tunnel: 1
             node_bootstrap_file: 
               is_presto_enabled: 0
            custom_presto_config: NULL
             stable_spot_timeout: 10
      stable_spot_bid_percentage: 150.0
            stable_spot_fallback: 1
                          vpc_id: vpc-6d9c0109
                       subnet_id: subnet-b047559b
                tunnel_server_ip: NULL
                     use_hadoop2: 1
                ebs_volume_count: 0
                 ebs_volume_size: 100
                 ebs_volume_type: standard
           use_stable_spot_nodes: NULL
     use_qubole_placement_policy: 1
            fallback_to_ondemand: 1
                 custom_ec2_tags: {""Owner"":""force"",""Role"":""force"",""Environment"":""production"",""Cluster"":""force""}
                       use_hbase: 0
                       use_spark: 1
                   config_errors: --- []

                compute_role_arn: NULL
             compute_external_id: fU/WpMRfZ7CkimMnOA9npI/qFlw0skgnJvzK/YPUW14=
                   ami_overrides: null
               datadog_api_token: j+oWXDSySCcm/Mr9g9RbXg==
               datadog_app_token: j+oWXDSySCcm/Mr9g9RbXg==
       use_account_compute_creds: 1
             custom_spark_config: spark-defaults.conf:
spark.speculation false
spark.task.maxFailures 10
                   spark_version: 2.0.0
            hadoop_setup_timeout: NULL
           role_instance_profile: NULL
         bastion_node_public_dns: NULL
               master_elastic_ip: NULL
           spark_s3_package_name: NULL
        zeppelin_s3_package_name: NULL
              engine_config_type: NULL
                engine_config_id: NULL
                  presto_version: 0.142
                 cloud_config_id: NULL
               cloud_config_type: NULL
            heterogeneous_config: {}
       zeppelin_interpreter_mode: legacy
                           is_ha: NULL
            ebs_upscaling_config: null
                    enable_rubix: 0
       compute_validation_result: null
            idle_cluster_timeout: 1
             spot_block_duration: NULL
{code}",11/Apr/17 5:28 AM;satyavathib;Cloning the cluster resolved the issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tez query is hanging ,SOL-115,60651,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,Ranjana,Ranjana,10/Apr/17 7:59 PM,17/May/17 11:06 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"+General Information +
Zendesk ticket: 13930
Customer: Lyft
Raised by: amalakar@lyft.com
*Command id: 65914687*
easy_hustler --cluster-id 23574 sshmaster -u 'ec2-user' qbol_acc3321_cl23574

+Problem Description: Analyze UI shows job as hanging but job actually finished successfully as per RM UI +
The Tez query on Analyze UI looks like it is hanging at stage ""Starting job submission and split computation"". The resources tab shows no Application UI. The Hive tier logs show last line as ""Submitting dag to TezSession"". The RM logs show the  applicationId=application_1491376526722_17222 as FINISHED. Searched on the RM UI for this application id and obtained AM UI ( https://api.qubole.com/cluster-proxy?clusterInst=428838&encodedUrl=http%3A%2F%2Fec2-54-165-46-71.compute-1.amazonaws.com%3A8088%2Fcluster%2Fapp%2Fapplication_1491376526722_17222) which shows job as finished. 

Customer reports:
{code:java}
qds.py hivecmd check 65914687 | python -mjson.tool | grep status
    ""status"": ""running"",
{code}

+On Analyze UI : Job hanging for over an hour when I checked+
{code:java}
View: 
Wrap text
Errors and Warnings
Running 3 queries in a single run.
set, add, delete and create temporary query commands that are part of this query wont be added to the session.
2017-04-11 00:43:32,838 INFO hivecli.py:161 - __init__ - Default hive version of the account is set as 1.2
2017-04-11 00:43:33,620 INFO hivecli.py:731 - getQHSHostName - Using Hive tier.
mdhist20170411-27791-3jy6q2 100% |||||||||||||||||||| Time: 00:00:00 0.00 B/s
qexec20170411-27791-1e2pk7a 100% |||||||||||||||||||| Time: 00:00:00 0.00 B/s
2017-04-11 00:43:35,181 INFO hivecli.py:456 - getStandaloneCmd - Using hive version 1.2 for hadoop2 cluster
log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/04/11 00:43:37 INFO conf.HiveConf: HiveConf of name hive.io.override.jsonserde does not exist
17/04/11 00:43:37 INFO conf.HiveConf: HiveConf of name hive.io.file.cache.basedir does not exist
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/tez-41.0.0/tez/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hadoop2-41.32.1/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Processing file s3n://lyftqubole-iad/scripts/hadoop/dataplatform/conf/common/bootstrap_hive.hql
Added [/media/ephemeral0/s3ncache/a20345fea085a9444c104d52f6c34482/lyft-udfs-0.1.0-jar-with-dependencies.jar] to class path
Added resources: [s3://lyftqubole-iad/jars/dataplatform/lyft-udfs-0.1.0-jar-with-dependencies.jar]
Processing file s3://lyftqubole-iad/scripts/bootstrap
Processing file /tmp/mdhist20170411-27791-3jy6q2
Clearing Directory :s3n://lyftqubole-iad/warehouse/exp_metric_daily/ds=2017-04-05/mtr=support_tickets_per_ride_dvr
Getting Hadoop cluster information ...
No config file specified - defaulting to hustler/configs/config.default for config file
2017-04-11 00:45:07,026 >>> Cluster id '23574', account id '3321' marked UP in DB. Trying to latch ...
2017-04-11 00:45:07,557 >>> Resource manager on master ec2-54-165-46-71.compute-1.amazonaws.com for cluster qbol_acc3321_cl23574 is accessible.
{""master-ip"": ""ec2-54-165-46-71.compute-1.amazonaws.com"", ""proxy-port"": ""29745"", ""tunnel-server-ip"": ""10.171.122.249"", ""sec-master-ip"": ""None""}
Initializing tez libs...
Query ID = ec2-user_20170411004456_92885d6c-3791-44eb-8dfb-3cd877b8a1a2
Total jobs = 1
Launching Job 1 out of 1
Starting job submission and split computation
{code}


Hive Tier logs(attached) show:
{code:java}
2017-04-11 00:45:19,359 [command:65914687] INFO  log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=TezBuildDag start=1491871518867 end=1491871519359 duration=492 from=org.apache.hadoop.hive.ql.exec.tez.TezTask>
2017-04-11 00:45:19,359 [command:65914687] INFO  exec.Task (SessionState.java:printInfo(1260)) - Starting job submission and split computation
2017-04-11 00:45:19,360 [command:65914687] INFO  log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=TezSubmitDag from=org.apache.hadoop.hive.ql.exec.tez.TezTask>
2017-04-11 00:45:19,360 [command:65914687] INFO  client.TezClient (TezClient.java:submitDAGSession(405)) - Submitting dag to TezSession, sessionName=HIVE-65914687, applicationId=application_1491376526722_17222, dagName=ec2-user_20170411004456_92885d6c-3791-44eb-8dfb-3cd877b8a1a2:1
{code}
This is last mention of this command id in Hive tier logs. 

RM logs: 
Cluster id: 23574
Cluster instance id: 428838
Master node ip address is: ec2-54-165-46-71.compute-1.amazonaws.com
yarn-yarn-resourcemanager-ip-172-31-44-241.log.2017041100.log.gz
{code:java}
2017-04-11 00:55:24,261 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1491376526722_17222,name=HIVE-65914687,user=data-platform,queue=root.data-platform,state=FINISHED,trackingUrl=http://ec2-54-165-46-71.compute-1.amazonaws.com:8088/proxy/application_1491376526722_17222/#/tez-app/application_1491376526722_17222,appMasterHost=ip-172-31-35-159.ec2.internal,startTime=1491871513933,finishTime=1491872123036,finalStatus=SUCCEEDED
2017-04-11 00:55:24,261 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Max number of completed apps kept in state store met: maxCompletedAppsInStateStore = 10000, removing app application_1491376526722_7212 from state store.
2017-04-11 00:55:24,261 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application appattempt_1491376526722_17222_000001 is done. finalState=FINISHED
2017-04-11 00:55:24,261 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing info for app: application_1491376526722_7212
2017-04-11 00:55:24,261 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1491376526722_17222 requests cleared
{code}

The AM URL for this application shows the Tez job as succeeded in 10mins, 9sec: https://api.qubole.com/cluster-proxy?clusterInst=428838&encodedUrl=http%3A%2F%2Fec2-54-165-46-71.compute-1.amazonaws.com%3A8088%2Fcluster%2Fapp%2Fapplication_1491376526722_17222
Diagnostics show: Session stats:submittedDAGs=0, successfulDAGs=0, failedDAGs=0, killedDAGs=0

The AM sys logs show the following:

{code:java}
2017-04-11 00:45:17,901 INFO [ServiceThread:org.apache.tez.dag.app.web.WebUIService] web.WebUIService: Instantiated WebUIService at http://ip-172-31-35-159.ec2.internal:40108/ui/
2017-04-11 00:45:17,923 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] autoscaling.TezAutoScalingManager: App shutdown requested by scheduler
2017-04-11 00:45:17,923 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] autoscaling.TezAutoScalingManager: Initialized TEZAutoscaling service: waitTime: 60000, resourceCheckerUpdateInterval: 60000, jobDesiredLatency: 120000
2017-04-11 00:45:17,924 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] rm.YarnTaskSchedulerService: TaskScheduler initialized with configuration: maxRMHeartbeatInterval: 1000, containerReuseEnabled: true, reuseRackLocal: true, reuseNonLocal: false, localitySchedulingDelay: 250, preemptionPercentage: 10, numHeartbeatsBetweenPreemptions: 3, idleContainerMinTimeout: 60000, idleContainerMaxTimeout: 120000, sessionMinHeldContainers: 3
2017-04-11 00:45:17,957 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] client.RMProxy: Connecting to ResourceManager at ec2-54-165-46-71.compute-1.amazonaws.com/172.31.44.241:8030
2017-04-11 00:45:18,006 INFO [ServiceThread:org.apache.tez.dag.app.rm.TaskSchedulerEventHandler] autoscaling.TezAutoScalingManager: Started autoscaling thread
2017-04-11 00:45:18,007 INFO [main] history.HistoryEventHandler: [HISTORY][DAG:N/A][Event:AM_STARTED]: appAttemptId=appattempt_1491376526722_17222_000001, startTime=1491871518007
2017-04-11 00:45:18,008 INFO [main] app.DAGAppMaster: In Session mode. Waiting for DAG over RPC
2017-04-11 00:45:18,032 INFO [AMRM Callback Handler Thread] rm.YarnTaskSchedulerService: App total resource memory: 120152 cpu: 1218 taskAllocations: 0
2017-04-11 00:45:18,034 INFO [Dispatcher thread: Central] node.AMNodeTracker: Num cluster nodes = 101
2017-04-11 00:46:17,888 ERROR [HistoryEventHandlingThread] impl.TimelineClientImpl: Failed to get the response from the timeline server.
com.sun.jersey.api.client.ClientHandlerException: java.net.SocketTimeoutException: Read timed out
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter$1.run(TimelineClientImpl.java:233)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:169)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter.handle(TimelineClientImpl.java:244)
	at com.sun.jersey.api.client.Client.handle(Client.java:648)
	at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670)
	at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)
	at com.sun.jersey.api.client.WebResource$Builder.post(WebResource.java:563)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPostingObject(TimelineClientImpl.java:537)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPosting(TimelineClientImpl.java:386)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:310)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.handleEvents(ATSHistoryLoggingService.java:342)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.access$700(ATSHistoryLoggingService.java:52)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService$1.run(ATSHistoryLoggingService.java:186)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:240)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:147)
	... 14 more
2017-04-11 00:46:17,891 WARN [HistoryEventHandlingThread] ats.ATSHistoryLoggingService: Could not handle history events
com.sun.jersey.api.client.ClientHandlerException: java.net.SocketTimeoutException: Read timed out
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter$1.run(TimelineClientImpl.java:233)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:169)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter.handle(TimelineClientImpl.java:244)
	at com.sun.jersey.api.client.Client.handle(Client.java:648)
	at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670)
	at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)
	at com.sun.jersey.api.client.WebResource$Builder.post(WebResource.java:563)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPostingObject(TimelineClientImpl.java:537)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPosting(TimelineClientImpl.java:386)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:310)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.handleEvents(ATSHistoryLoggingService.java:342)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.access$700(ATSHistoryLoggingService.java:52)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService$1.run(ATSHistoryLoggingService.java:186)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:240)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:147)
	... 14 more
2017-04-11 00:47:17,935 ERROR [HistoryEventHandlingThread] impl.TimelineClientImpl: Failed to get the response from the timeline server.
com.sun.jersey.api.client.ClientHandlerException: java.net.SocketTimeoutException: Read timed out
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter$1.run(TimelineClientImpl.java:233)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:169)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter.handle(TimelineClientImpl.java:244)
	at com.sun.jersey.api.client.Client.handle(Client.java:648)
	at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670)
	at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)
	at com.sun.jersey.api.client.WebResource$Builder.post(WebResource.java:563)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPostingObject(TimelineClientImpl.java:537)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPosting(TimelineClientImpl.java:386)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:310)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.handleEvents(ATSHistoryLoggingService.java:342)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.access$700(ATSHistoryLoggingService.java:52)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService$1.run(ATSHistoryLoggingService.java:186)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:240)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:147)
	... 14 more
2017-04-11 00:47:17,936 WARN [HistoryEventHandlingThread] ats.ATSHistoryLoggingService: Could not handle history events
com.sun.jersey.api.client.ClientHandlerException: java.net.SocketTimeoutException: Read timed out
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:149)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter$1.run(TimelineClientImpl.java:233)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:169)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter.handle(TimelineClientImpl.java:244)
	at com.sun.jersey.api.client.Client.handle(Client.java:648)
	at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670)
	at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)
	at com.sun.jersey.api.client.WebResource$Builder.post(WebResource.java:563)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPostingObject(TimelineClientImpl.java:537)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPosting(TimelineClientImpl.java:386)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:310)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.handleEvents(ATSHistoryLoggingService.java:342)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.access$700(ATSHistoryLoggingService.java:52)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService$1.run(ATSHistoryLoggingService.java:186)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:240)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:147)
	... 14 more
2017-04-11 00:55:18,008 INFO [Timer-1] app.DAGAppMaster: Session timed out, lastDAGCompletionTime=1491871518007 ms, sessionTimeoutInterval=600000 ms
2017-04-11 00:55:18,008 INFO [Timer-1] rm.TaskSchedulerEventHandler: TaskScheduler notified that it should unregister from RM
2017-04-11 00:55:18,008 INFO [Timer-1] app.DAGAppMaster: No current running DAG, shutting down the AM
2017-04-11 00:55:18,008 INFO [Timer-1] app.DAGAppMaster: DAGAppMasterShutdownHandler invoked
2017-04-11 00:55:18,008 INFO [Timer-1] app.DAGAppMaster: Handling DAGAppMaster shutdown
2017-04-11 00:55:18,027 INFO [AMShutdownThread] app.DAGAppMaster: Sleeping for 5 seconds before shutting down
2017-04-11 00:55:23,028 INFO [AMShutdownThread] app.DAGAppMaster: Calling stop for all the services
2017-04-11 00:55:23,029 INFO [AMShutdownThread] history.HistoryEventHandler: Stopping HistoryEventHandler
2017-04-11 00:55:23,029 INFO [AMShutdownThread] recovery.RecoveryService: Stopping RecoveryService
2017-04-11 00:55:23,029 INFO [AMShutdownThread] recovery.RecoveryService: Handle the remaining events in queue, queue size=0
2017-04-11 00:55:23,029 INFO [RecoveryEventHandlingThread] recovery.RecoveryService: EventQueue take interrupted. Returning
2017-04-11 00:55:23,029 INFO [AMShutdownThread] ats.ATSHistoryLoggingService: Stopping ATSService, eventQueueBacklog=0
2017-04-11 00:55:23,030 INFO [DelayedContainerManager] rm.YarnTaskSchedulerService: AllocatedContainerManager Thread interrupted
2017-04-11 00:55:23,031 INFO [TEZ Autoscaling thread] autoscaling.TezAutoScalingManager: Received stop request. Stopping the thread.
2017-04-11 00:55:23,032 INFO [AMShutdownThread] rm.YarnTaskSchedulerService: Unregistering application from RM, exitStatus=SUCCEEDED, exitMessage=Session stats:submittedDAGs=0, successfulDAGs=0, failedDAGs=0, killedDAGs=0
, trackingURL=http://ec2-54-165-46-71.compute-1.amazonaws.com:10500/#/tez-app/application_1491376526722_17222
2017-04-11 00:55:23,037 INFO [AMShutdownThread] impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
2017-04-11 00:55:23,138 INFO [AMShutdownThread] rm.YarnTaskSchedulerService: Successfully unregistered application from RM
2017-04-11 00:55:23,138 INFO [AMRM Callback Handler Thread] impl.AMRMClientAsyncImpl: Interrupted while waiting for queue
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2052)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:274)
2017-04-11 00:55:23,142 INFO [AMShutdownThread] mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:0
2017-04-11 00:55:23,242 INFO [AMShutdownThread] ipc.Server: Stopping server on 33510
2017-04-11 00:55:23,243 INFO [IPC Server listener on 33510] ipc.Server: Stopping IPC Server listener on 33510
2017-04-11 00:55:23,243 INFO [AMShutdownThread] ipc.Server: Stopping server on 38479
2017-04-11 00:55:23,243 INFO [IPC Server Responder] ipc.Server: Stopping IPC Server Responder
2017-04-11 00:55:23,244 INFO [IPC Server listener on 38479] ipc.Server: Stopping IPC Server listener on 38479
2017-04-11 00:55:23,244 INFO [IPC Server Responder] ipc.Server: Stopping IPC Server Responder
2017-04-11 00:55:23,246 INFO [Thread-2] app.DAGAppMaster: DAGAppMasterShutdownHook invoked
2017-04-11 00:55:23,246 INFO [Thread-2] app.DAGAppMaster: The shutdown handler is still running, waiting for it to complete
2017-04-11 00:55:23,256 INFO [AMShutdownThread] app.DAGAppMaster: Completed deletion of tez scratch data dir, path=hdfs://ec2-54-165-46-71.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/data-platform/_tez_session_dir/65914687/.tez/application_1491376526722_17222
2017-04-11 00:55:23,256 INFO [AMShutdownThread] app.DAGAppMaster: Exiting DAGAppMaster..GoodBye!
2017-04-11 00:55:23,256 INFO [Thread-2] app.DAGAppMaster: The shutdown handler has completed
{code}

Re-ran the select part of this query separately as 65929589, which completed successfully with non-zero number of rows in result. 

",,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Apr/17 8:25 PM;Ranjana;65914687.txt.zip;https://qubole.atlassian.net/secure/attachment/41205/65914687.txt.zip,,,,,,,,,,,,,AWS,,,,,,Lyft,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04f8n:,,,,,,,,,,,,,,,,,,,1.0,13930,,,,10/Apr/17 8:36 PM;Ranjana;There is no mention of this application id application_1491376526722_17222 in timeline server logs. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notebook paragraph stuck,SOL-107,60352,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,megha,megha,05/Apr/17 4:41 PM,21/Jun/17 11:29 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"General Information:
Notebook: Dest Insight - Review 
User: v-pragrawal@expedia.com
Account: Test Adhoc

Problem Description:
Job gets stuck for 2 tasks, and these don't complete for up to 16 hours..
The issue seems to be somewhere within python nlp package being used..
Pystack at that time:
{code:java}
#0  0x00007f12d374f9cf in sre_ucs1_match (state=0x7ffc46e718b0, pattern=<optimized out>, match_all=0) at ./Modules/sre_lib.h:1115
#1  0x00007f12d375944c in sre_match (match_all=0, pattern=<optimized out>, state=0x7ffc46e718b0) at ./Modules/_sre.c:539
#2  _sre_SRE_Pattern_match_impl (pattern=<optimized out>, endpos=9223372036854775807, pos=<optimized out>, string=<optimized out>, self=0x2bad320) at ./Modules/_sre.c:613
#3  _sre_SRE_Pattern_match (self=0x2bad320, args=<optimized out>, kwargs=<optimized out>) at ./Modules/clinic/_sre.c.h:89
{code}


Things Tried so far:
This was narrowed down in notebook: Dest Insight - Review Per Partition Iteration 
Notebook id: 33213

The issue is with file: 000091_0
Paragraph 6 in the notebook is running same code for this file.. Creates 5 tasks and one of these get stuck again..
pystack at that time is attached herewith..


What can we do further? 
cc: [~mpatel]
",,gmargabanthu,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/17 4:38 PM;megha;pystack_file_000091_0;https://qubole.atlassian.net/secure/attachment/41027/pystack_file_000091_0,,,,,,,,,,,,,AWS,,,,,,expedia,,,,,{},NA,Choose from,,,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04dqn:,,,,,,,,,,,,,,,,,,,1.0,13568,,,,"07/Apr/17 12:58 PM;megha;cc: [~gmargabanthu]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X-axis values are went overlapped with next paragraph,ZEP-839,60347,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,beria,sbadam,sbadam,05/Apr/17 2:36 PM,29/May/17 11:26 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,notebook-usability,,,,,,,"The graph are overlapping with next paragraph which make it harder to read X-axis values. 

From customer:

{code:java}
if I want to see the whole x-axis, I have to squish the graph vertically a lot and it pretty much renders it useless
{code}
",,mahuja,mmajithia,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/17 8:05 PM;sbadam;29362-Panel Summary Current (CIO).json;https://qubole.atlassian.net/secure/attachment/41031/29362-Panel+Summary+Current+%28CIO%29.json,12/Apr/17 10:56 AM;sbadam;29362-Panel Summary Current (CIO).json;https://qubole.atlassian.net/secure/attachment/41321/29362-Panel+Summary+Current+%28CIO%29.json,05/Apr/17 2:32 PM;sbadam;Screen Shot 2017-03-27 at 4.19.18 PM.png;https://qubole.atlassian.net/secure/attachment/41025/Screen+Shot+2017-03-27+at+4.19.18+PM.png,05/Apr/17 2:30 PM;sbadam;Screen Shot 2017-04-05 at 2.54.46 PM.png;https://qubole.atlassian.net/secure/attachment/41026/Screen+Shot+2017-04-05+at+2.54.46+PM.png,,,,,,,,,,AWS,,,,,,returnpath,,,,,{},NA,Choose from,,AN-207,,,,,#Empty,No,super-admin,,,,,No,,#Empty,,1|z04dpj:,,,,,,,,,,,,,,,,,,,1.0,13627,2017-04-05 15:29:56.369,,,"05/Apr/17 3:29 PM;mahuja;[~sbadam] please add notebook details (cluster, user and notebook id).

cc - [~mmajithia], [~beria]","05/Apr/17 4:08 PM;sbadam;User: patrick.lawlor@returnpath.com, Notebook#29362 and Cluster: 26871

The problem is customer keeps on changing the data, we won't be able to see attached graphs in her notebook.",05/Apr/17 4:09 PM;mahuja;In that case let's save the notebook JSON showing problem,05/Apr/17 4:35 PM;sbadam;I requested customer. Just wondering how JSON gives a clue about visual glitches. ,"05/Apr/17 6:26 PM;mahuja;Notebook JSON is a snapshot. It contains both code and output. So if we copy the JSON we have the complete representation. We can import the file in one of our accounts to view the problem. It will be felonies from customers original note. 

",05/Apr/17 6:29 PM;mahuja;We do not need users intervention for getting the note. They will not know how to export it. We can get it from cluster master or S3 given a notebook id. ,"05/Apr/17 8:41 PM;sbadam;Thanks Mayank, I attached JSON file of the notebook(through Export option in UI). 
[~beria]: How to identify a folder for a notebook? It seems a random string is assigned to it(e.g., VK7CNBHB1W1472575665)

","10/Apr/17 10:09 AM;sbadam;Hey Anirudh, can you have a initial look at it?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Malformed reply from SOCKS server ioexception from hive query,MW-676,60332,Bug,Open,MW,Middleware,software,sumitm,,,Major,,ksr,ekang,ekang,05/Apr/17 9:39 AM,24/Jun/17 10:39 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"User: virginia giacomini
Account: TDC ETL
Cluster: 23781
Command Id: 64884119
The server autoscaling logs show this:
2017-04-05 12:08:16 INFO - Rebalancer - stable_nodes: 0, volatile_nodes: 0, graceful_shutdown_nodes: 0, cluster_nodes: 3, excess_stable_nodes: 0
2017-04-05 12:04:16 INFO - Current cluster size (excluding master node) is: 3
2017-04-05 12:03:11 INFO - Initialized Qubole's Autoscaling service
The error occurred in the midst of the server startup it seems at 12:06.
2017-04-05 12:06:11,973 >>> Resource manager on master ec2-54-164-85-154.compute-1.amazonaws.com for cluster qbol_acc6184_cl23781 is accessible.
{""master-ip"": ""ec2-54-164-85-154.compute-1.amazonaws.com"", ""proxy-port"": ""29925"", ""tunnel-server-ip"": ""10.171.122.249"", ""sec-master-ip"": ""None""}
Initializing tez libs...
FAILED: Hive Internal Error: java.lang.RuntimeException(java.io.IOException: Failed on local exception: java.net.SocketException: Malformed reply from SOCKS server; Host Details : local host is: ""ip-10-225-37-172.ec2.internal/10.225.37.172""; destination host is: ""ec2-54-164-85-154.compute-1.amazonaws.com"":9000; )

It appears that the servers haven't fully been started up. Are there sanity checks being performed to verify if the tunnel/nodes are working prior to submitting the job? It would be nice to print that to the logs to help identify everything is in working order.","
Firefox
MAC",ashishs,ekang,,,,,,,,,,,,,,,,,,,,,,,,,MW-512,,,,,,,,,,,,,,,,,,,,AWS,,,,,,turner,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,jayapradag,#Empty,,1|z04dm7:,,,,,,,,,,,,,,,,,,,1.0,13828,2017-04-05 23:59:07.117,,,05/Apr/17 11:59 PM;ashishs;seems to be related to MW-512. assigning to [~adeshr],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to use copy command with Redshift when Partition columns aren't needed.,SQOOP-100,59926,Bug,Open,SQOOP,SQOOP,software,sumitm,,,Major,,ksr,jellin,jellin,30/Mar/17 6:49 AM,25/Jul/17 12:29 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer was previously using redshift export with good results.  In the last release (Sqoop-35) introduced a change where partitioned tables used sqoop instead.  This isn't desirable since its so much slower.  This customer is not interested in exporting the columns from the partition.

Suggest we give the customer of still using copy command and ignoring partition columns and warn them in the UI and Doc that selecting partition columns with redshift will result in degraded performance.

In this case the customer was blocked because the sqoop job went from 5mins > 36hrs resulting in the job being killed.

",,jellin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,sessionm,,,,,"{repository={count=1, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":1,""lastUpdated"":""2011-07-22T07:04:34.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Yes - Documentation,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z04bvz:,,,,,,,,,,,,,,,,,,,1.0,13229,,,,30/Mar/17 6:50 AM;jellin;cc [~sbadam] ; [~sam] ; [~aabbas],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues Using SAP Business Objects with qubole odbc,ODBC-131,59791,Bug,Open,ODBC,odbc,software,stagra,,,Major,,udayk,megha,megha,29/Mar/17 4:22 PM,03/Jul/17 2:55 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer is attempting to use SAP businees objects with qubole's odbc drivers.
They see 2 issues:
1. If the column name is too big, the UI ends up showing only part of column name, and then when running select query on this, it fails with ""column doesnt exist"" error. (screenshot image002.png)
2. If the table name consists of ""_"" , UI doesnt display column names and hence when trying to select, it says no columns to select from.

I tried to repro both this on Tableau, but it doesnt repro. I was wondering if we have any specific settings with tableau which we should set with SAP Business objects? 
The customer doesnt see similar issues with other ODBC connectors they use.
",,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Mar/17 4:20 PM;megha;image001.png;https://qubole.atlassian.net/secure/attachment/40543/image001.png,29/Mar/17 4:20 PM;megha;image002.png;https://qubole.atlassian.net/secure/attachment/40542/image002.png,,,,,,,,,,,,AWS,,,,,,expedia,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z04bjr:,,,,,,,,,,,,,,,,,,,1.0,13281,,,,"09/May/17 4:38 PM;megha;[~udayk]
Any inputs on this ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ganglia data not being generated. ,ACM-1060,59709,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Critical,,ajayb,biswajit,biswajit,28/Mar/17 4:11 AM,14/Jun/17 11:57 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Ganglia data not being generated for comcast cluster. 

Account id :- 3
Cluster id :- 216

Below are the things that I checked. 

{code}

[ec2-user@ip-172-23-0-44 conf.d]$ ls -tr /var/lib/ganglia/rrds/
[ec2-user@ip-172-23-0-44 conf.d]$ ps -ef |grep gmetad
ganglia   49705      1 13 10:12 ?        00:03:49 /usr/sbin/gmetad
ec2-user  51218  32903  0 10:40 pts/0    00:00:00 grep --color=auto gmetad
[ec2-user@ip-172-23-0-44 conf.d]$ ps -ef |grep gmond
ganglia   49615      1  4 10:11 ?        00:01:26 /usr/sbin/gmond
ec2-user  51220  32903  0 10:41 pts/0    00:00:00 grep --color=auto gmond
[ec2-user@ip-172-23-0-44 conf.d]$

{code}

User have enabled Aws cloudwatch for this cluster",,ajayb,biswajit,sankets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,comcast,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z04b1r:,,,,,,,,,,,,,,,,,,,1.0,13449,2017-04-06 10:33:45.894,,,06/Apr/17 3:23 AM;biswajit;any update on this?,06/Apr/17 10:33 AM;ajayb;[~sankets] can you sit with [~rohang] tomorrow to look at this? He has worked on Ganglia related issues earlier and can provide good pointers. Btw both of you should open ACL jira to request access to the Comcast private qubole env.,06/Apr/17 11:23 AM;biswajit;[~sankets] and [~rohang] do let me know if i could be any help here. ,"10/Apr/17 4:29 AM;sankets;We ran this command on master node:
ls /media/ephemeral1/ganglia/rrds/qbol_acc3_cl216/

And we could see the ganglia data for slaves:

ip-172-23-0-102.ec2.internal  ip-172-23-0-255.ec2.internal  ip-172-23-1-193.ec2.internal  ip-172-23-2-100.ec2.internal  ip-172-23-2-48.ec2.internal   ip-172-23-3-191.ec2.internal
ip-172-23-0-104.ec2.internal  ip-172-23-0-27.ec2.internal   ip-172-23-1-196.ec2.internal  ip-172-23-2-111.ec2.internal  ip-172-23-2-51.ec2.internal   ip-172-23-3-202.ec2.internal
ip-172-23-0-108.ec2.internal  ip-172-23-0-29.ec2.internal   ip-172-23-1-1.ec2.internal    ip-172-23-2-112.ec2.internal  ip-172-23-2-53.ec2.internal   ip-172-23-3-203.ec2.internal

Above are the folders corresponding to each slave which contains the ganglia data.

Let us know if we can help you with anything else.","10/Apr/17 6:26 PM;biswajit;[~sankets] what is the command you ran ? and why it does not get executed when the cluster starts ?

Regards
Biswa","11/Apr/17 10:03 AM;sankets;[~biswajit]  I just ran an ls command on the master node for /media/ephemeral1/ganglia/rrds/qbol_acc3_cl216/ location.

ls /media/ephemeral1/ganglia/rrds/qbol_acc3_cl216/

Which means that ganglia data is being generated and put in the above location for the cluster 216 for this account. You can try running the above command from the master node(ssh into the master node) and you would see all the folders corresponding to each slave which further contain the ganglia data.


",19/Apr/17 3:33 AM;biswajit;[~sankets] I could see the rrds data on the give path. Then why the ganglia UI page is not being able to pull data and populate it. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Download as CSV says ""File Too Big to Download"" when actual size < 1 mb",MW-610,59516,Bug,Reopened,MW,Middleware,software,sumitm,,,Major,,tarung,p.vasa,p.vasa,24/Mar/17 12:19 PM,29/Jun/17 5:59 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"*General Information ->*
Account Name -> ""Prod-Adhoc""
Account ID -> 5497
Cluster Name -> ""default""
Cluster ID -> 28437
Cluster Type -> hadoop2
Command ID -> 62324715

*Problem Definition ->*
When the results are downloaded using either ""Download as CSV/TSV"", the file gets downloaded but when the file is opened it says ""Result file is too big - download from..""

*Things Observed so far ->*
-> Checked the resultant size and it is < 1mb (approx = 285 kb)
-> Check this command ->
*[ec2-user@ip-10-23-14-254 ~]$ hadoop fs -du -s -h s3://analytics-qubole-prod/prod-adhoc/tmp/2017-03-22/5497/62324715.dir/000/*

log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
17/03/24 18:23:24 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://analytics-qubole-prod.s3.amazonaws.com/prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000
17/03/24 18:23:24 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://analytics-qubole-prod.s3.amazonaws.com/prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000_%24folder%24
17/03/24 18:23:24 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://analytics-qubole-prod.s3.amazonaws.com/?delimiter=%2F&max-keys=1&prefix=prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000%2F
17/03/24 18:23:24 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://analytics-qubole-prod.s3.amazonaws.com/prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000
17/03/24 18:23:24 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://analytics-qubole-prod.s3.amazonaws.com/prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000_%24folder%24
17/03/24 18:23:24 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://analytics-qubole-prod.s3.amazonaws.com/?delimiter=%2F&max-keys=1&prefix=prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000%2F
17/03/24 18:23:24 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://analytics-qubole-prod.s3.amazonaws.com/prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000
17/03/24 18:23:25 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://analytics-qubole-prod.s3.amazonaws.com/?delimiter=%2F&max-keys=1000&prefix=prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000%2F
17/03/24 18:23:25 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://analytics-qubole-prod.s3.amazonaws.com/?delimiter=%2F&max-keys=1000&marker=prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000%2F000999&prefix=prod-adhoc%2Ftmp%2F2017-03-22%2F5497%2F62324715.dir%2F000%2F
*235.8 K  s3://analytics-qubole-prod/prod-adhoc/tmp/2017-03-22/5497/62324715.dir/000*

-> The default file size limit for this account is ""100mb"" and this is the reason why all the 3 downloading options are visible ""Download CSV, Download TSV, Download RAW""

-> Checked other commands in this account and downloading is working fine for other commands

So the question is here is that, even if the output data set size for a specific account is within the ""File Size Limit"" set for that account and the fact that all 3 downloading options are available, does the generation of downloaded file with actual data depend upon any other factors like the number of rows fetched etc?

Please let me know if you need any additional details on this issue.
",,Kulbir,p.vasa,tarung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,Box,expedia,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z04abz:,,,,,,MW-RB45-Sprint-1,,,,,,,,,,,,,1.0,13596,2017-05-02 12:26:02.446,,,24/Mar/17 3:04 PM;p.vasa;Command rerun -> 62713406 showing the same behavior.,"02/May/17 12:26 PM;tarung;The number of objects in the results directory are: 1195. We have a hard limit of 700 for the number of files. Because of this it is recommending to download the results from S3.

The corresponding log line is:
ip-10-47-192-5 [INFO  pid: 15494: 17-05-02 17:14:17 28441a59] [user:8905] [account:5497] [su:11427] [command:62324715] local_path: /media/ephemeral0/tmp/tapp/full_result_62324715.txt.c452f9ea-2f5a-11e7-ba12-22000bf2d262.tmp.dir, *size: 241469, object_count: 1195*, @is_directory: true ",02/May/17 12:27 PM;tarung;The behaviour is expected as the the of objects in the results directory are 1195 which is greater than 700.,"02/May/17 2:03 PM;p.vasa;Thanks a lot [~tarung]
I have updated the customer.

The customer has made 2 suggestions ->
""Ah, ok. That makes sense... I would suggest to change the warning text, and also provide a link to documentation on the limits. Something like ""Result file is too big or there are too many files, link for limitations on downloads is _here_""

1) Regarding the documentation, I have already published a Public Facing Article as specified here:
https://qubole.zendesk.com/hc/en-us/articles/115000844666-Options-to-Download-Results-from-Qubole-UI

^^ However, the (700 files limit) was not included in this documentation. Hence, I will request you to provide me some description on this limit and when do we check this etc. so that I can add these details in the above article.

2) Regarding improving the Logging/Messaging in such situations, should I open up another JIRA for the same (or) what do you suggest?","28/Jun/17 11:22 AM;Kulbir;[~sumitm] [~tarung] i am reopening this issue(hit this at Box as well) as we still need to provide proper message back to end user w.r.t file # limit of 700, in addition can we ensure that pre signed S3 URL's get generated for this scenario similar to how we do it for result size >file size limit use case vs. showing other links(for e.g Download CSV) which still only gives S3 path info and don't add much to UI experience ?

Data Analysts using Qubole UI mostly won't have S3 CLI/UI access so simply providing a S3 path won't really help much.

cc [~aswina] [~mmajithia]","29/Jun/17 5:59 PM;p.vasa;Adding some more information to this requirement ->

Currently, the various options available for downloading the query results depend upon the various factors like ->
a) ""File Size Limit"" configured for an account
b) Number of files containing the output data being generated in S3


Current behavior of Qubole Platform ->
*Case 1 ->*
When the size of the query output is within the ""File Size Limit"" configured for that account and if the query has SUCCEEDED and if the number of output files generated < 700, then the following options are available for download ->
a) Download as CSV
b) Download as TSV
c) Download RAW (CTRL+A separated)

*Case 2 ->*
When the size of the query output is beyond the ""File Size Limit"" configured for an account and if the query has SUCCEEDED and the number of output files generated is < 700, then the following options are available for Download ->
a) Download Complete RAW Results

*Case 3 ->*
When the size of the query output is within the ""File Size Limit"" configured for an account and the query has SUCCEEDED and the number of files generated > 700, then following options are available for Download ->
a) Download as CSV
b) Download as TSV
c) Download RAW

*Case 4 ->*
When the size of the query output is beyond the ""File Size Limit"" configured for an account and the query has SUCCEEDED and the number of files generated > 700, then following options are available for Download ->
a) Download Complete RAW Results



However, for -> *CASE 3* and *CASE 4* (*when number of files generated > 700*), when the user clicks any DOWNLOAD option like *""Download CSV""*, the file gets downloaded but there are no contents in that file and that file only has a link to S3 where the actual data is stored.

Hence, as [~Kulbir] mentioned, it would be great if we can reduce the customer efforts to download an almost empty file and then go to S3 to check results instead of showing a proper message on the UI.

Example Command ID -> *79736822*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Download results to CSV- apostrophes are being rendered as ""â€tm""",MW-598,59421,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,snamburu,snamburu,23/Mar/17 8:48 AM,23/Mar/17 10:18 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"When the results download to CSV, there is a character replacement. Apostrophes are being rendered as ""â€tm"". Refer to the screenshots attached.",,aswina,nimitk,snamburu,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Mar/17 8:48 AM;snamburu;Screen Shot 2017-03-23 at 14.41.17.png;https://qubole.atlassian.net/secure/attachment/40409/Screen+Shot+2017-03-23+at+14.41.17.png,23/Mar/17 8:48 AM;snamburu;Screen Shot 2017-03-23 at 14.42.19.png;https://qubole.atlassian.net/secure/attachment/40408/Screen+Shot+2017-03-23+at+14.42.19.png,,,,,,,,,,,,All,,,,,,salescycle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z049qv:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyze Search: Inconsistent results,UI-5553,59184,Bug,Open,UI,UI,software,aswina,,,Major,,aswina,aabbas,aabbas,21/Mar/17 8:55 PM,21/May/17 9:36 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,nps,,,,,,,"Analyze search results are inconsistent when searching tablename vs. dbname.tablename. 

*Email from Will at EpicGames elaborating:*
I believe the problem arises from the limitations of the search function.  Specifically, being limited to searching 1 month when searching multiple users.  To spell out the use case, I know someone at Epic ran a query that started with INSERT OVERWRITE TABLE dbnameX.tablenameY but I don't know who or when they did this.  I'd like to see the query for how dbnameX.tablenameY was written, so I search for tablenameY but nothing comes up because it was more than one month ago.  The search is successful if it's filtered on a single user but even including a second user causes the search to fail.  

",,aabbas,aswina,drose@qubole.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,adobe,epicgames,mediamath,scripps,,{},NA,Choose from,,AN-8,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0492n:,,,,,,,,,,,,,,,,,,,1.0,14329,2017-05-15 15:24:18.431,,,"15/May/17 3:24 PM;drose@qubole.com;note from Adobe, ""Tom, Russ and I just spent 20 min trying to find a query that was run by our scheduler this morning. We had unique keywords and were searching on the specific user for a specific date even. It wasn't showing. Not until we finally added a cluster in, were we able to find it.

It shouldn't take this long to find a query.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyze Search: No results without dates and responsiveness with dates,UI-5551,59180,Bug,Open,UI,UI,software,aswina,,,Critical,,aswina,aabbas,aabbas,21/Mar/17 5:15 PM,11/Jun/17 11:16 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,nps,,,,,,,"From Eric Tutlys:
I was talking to Sam and Amer today about some issues that we've had with search.

I am currently trying to query for 'currently running queries' of our reporting user and I just get a spinning icon waiting for results.

The search params are as follows: 
status:InProgress 
interface:API,UI,TEMPLATE,ODBC,SMART_QUERY 
qbol_user_id:reports@mediamath.com

When I added a start and end date of today I got results after… perhaps 40 seconds? Without the dates I never got results",,aabbas,aswina,monikak,p.vasa,,,,,,,,,,,,,,,,,,,,,,,,,,UI-6222,,,,22/Mar/17 11:05 AM;p.vasa;Screen Shot 2017-03-22 at 11.02.50 AM.png;https://qubole.atlassian.net/secure/attachment/40306/Screen+Shot+2017-03-22+at+11.02.50+AM.png,22/Mar/17 11:05 AM;p.vasa;Screen Shot 2017-03-22 at 11.04.14 AM.png;https://qubole.atlassian.net/secure/attachment/40307/Screen+Shot+2017-03-22+at+11.04.14+AM.png,,,,,,,,,,,,AWS,,,,,,mediamath,,,,,{},NA,Choose from,,AN-8,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0491r:,,,,,,,,,,,,,,,,,,,1.0,13524,2017-03-22 11:06:06.62,,,"22/Mar/17 11:06 AM;p.vasa;More from the customer ->
""When I added a start and end date of today I got results after… perhaps 40 seconds? Without the dates I never got results.""

General Information ->
Account ID ->  39
Account Name -> mediamath.com

PFA the screen shots of the ""Developer Console"" while the search was being performed. !Screen Shot 2017-03-22 at 11.02.50 AM.png|thumbnail!  !Screen Shot 2017-03-22 at 11.04.14 AM.png|thumbnail! 

Please let me know if you need any additional details.

","05/Jun/17 1:13 PM;p.vasa;[~aswina] Can you please provide an update on this one?
Let me know if you need any additional information.","06/Jun/17 12:01 AM;aswina;[~p.vasa] - As part of UI-6155, we are trying to find out whether we can offer keyword search with at least reasonable performance before moving to elastic search.  We'll update here once we have final numbers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Job fails with java.io.FileNotFoundException on hdfs part file -mr-xxxxx ,expected to be produced by a previous stage",SOL-77,59163,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,Ranjana,Ranjana,21/Mar/17 10:23 AM,01/Aug/17 11:40 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"+General Information+
Customer Affected: Expedia
Account id: 5507
Cluster id: 28441 (p-omni)
Cluster_type: Hadoop 2
Submitted by: rqiao@expedia.com, mjha@expedia.com, vpoosapati@expedia.com

+Problem Description+
Symptom so far reported for two command ids: 60801307, 61507878
Hive jobs involving several stages. The folders of the part files seem written out in a previous successful stage, but this exact part file is not there in the logs of the previous stage which wrote into that directory. The current stage fails throwing out this error to UI (AM logs or mapper/reducer logs of this failed stage is not available)
{code:java}
java.io.FileNotFoundException: File does not exist: hdfs://ec2-54-225-41-181.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/hive_2017-03-15_01-01-47_660_8245551694809834735-1/-mr-10003
at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1122)
at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
at org.apache.hadoop.fs.FileSystem.resolvePath(FileSystem.java:756)
at org.apache.hadoop.hdfs.DistributedFileSystem$16.(DistributedFileSystem.java:779)
at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:770)
at org.apache.hadoop.mapred.InputPathProcessor.perPathComputation(InputPathProcessor.java:251)
at org.apache.hadoop.mapred.InputPathProcessor.access$000(InputPathProcessor.java:28)
at org.apache.hadoop.mapred.InputPathProcessor$2.run(InputPathProcessor.java:354)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Job Submission failed with exception 'java.io.FileNotFoundException(File does not exist: hdfs://ec2-54-225-41-181.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/hive_2017-03-15_01-01-47_660_8245551694809834735-1/-mr-10003)'
java.io.FileNotFoundException: File does not exist: hdfs://ec2-54-225-41-181.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/hive_2017-03-15_01-01-47_660_8245551694809834735-1/-mr-10003
at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1122)
at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
at org.apache.hadoop.fs.FileSystem.resolvePath(FileSystem.java:756)
at org.apache.hadoop.hdfs.DistributedFileSystem$16.(DistributedFileSystem.java:779)
at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:770)
at org.apache.hadoop.mapred.InputPathProcessor.perPathComputation(InputPathProcessor.java:251)
at org.apache.hadoop.mapred.InputPathProcessor.access$000(InputPathProcessor.java:28)
at org.apache.hadoop.mapred.InputPathProcessor$2.run(InputPathProcessor.java:354)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
{code}

+Things seen in logs+
For the part file path mentioned above, other part files are written by a previous stage
JOb job_1488584293030_5657 is the one which is writing to this directory. 
2017-03-15 01:02:10,843 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: Final Path: FS hdfs://ec2-54-225-41-181.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/hive_2017-03-15_01-01-47_660_8245551694809834735-1/_tmp.-mr-10007/000000_0

But I don't see mention of this exact part file being written. 
Looked at namenode logs during the time. There is no mention of hdfs://ec2-54-225-41-181.compute-1.amazonaws.com:9000/tmp/hive-ec2-user/hive_2017-03-15_01-01-47_660_8245551694809834735-1/-mr-10003

+My ASK+
Anyone seen this issue before ? Any hunch on what could be cause and resolution ?
I have asked customer for more command ids showing this issue. I searched for past jira/tickets, they seem to be resolved by the customer re-writing pipeline. I can provide more logs as I see more recent command ids showing this issue. ",,adubey,mpatel,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Mar/17 10:54 AM;mpatel;hdp_load_chnnl_mktg_code_tbl.hql;https://qubole.atlassian.net/secure/attachment/40259/hdp_load_chnnl_mktg_code_tbl.hql,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z048xz:,,,,,,,,,,,,,,,,,,,1.0,13427,2017-03-21 10:39:00.417,,,"21/Mar/17 10:39 AM;adubey;I think i have seen something similar ( it might not be 100% same ) but if you are allowed to try this - can you try this query with :

SET hive.optimize.skewjoin=false; 

Within the sql file they have explicitly marked as true and IIRC there is some issue ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"OSS Jira SPARK-18125 occurrence in Qubole Spark 2.0.2, not in 2.0.0",SPAR-1519,59027,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Minor,,mahuja,Ranjana,Ranjana,18/Mar/17 1:27 PM,31/May/17 7:05 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster id: 9432
Account id: 2553
easy_hustler --cluster-id 9432 sshmaster qbol_acc2553_cl9432
Business impact: Low. The customer was unblocked after moving to Spark 2.1.0. {color:#59afe1}This jira is raised for documentation purpose only {color}as this bug is newly found in Spark 2.0.2 and did not occur in Spark 2.0.0 and fixed in OSS for Spark 2.1.0, after consultation with  [~mahuja]. {color:#59afe1}No action required{color}. Please feel free to close.
{panel:title=Command ids:}
61131093 (failed with below error, submitted as Scala code on Analyze, on 2.0.2)
61132300 (successful, submitted as Scala code on Analyze, on 2.1.0)
60550579 (successful, submitted as jar on 2.0.0)
{panel}
Spark version on cluster at time of job submission: 2.0.2

The tasks failed with following error:
{code:java}
App > 17/03/16 11:22:12 task-result-getter-2 INFO TaskSetManager: Lost task 3.0 in stage 1.0 (TID 4) on executor ip-10-108-2-40.ec2.internal: java.util.concurrent.ExecutionException (java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 72, Column 65: Unknown variable or type ""value4""
App > /* 001 */ public java.lang.Object generate(Object[] references) {
App > /* 002 */   return new SpecificMutableProjection(references);
App > /* 003 */ }
App > /* 004 */
App > /* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
App > /* 006 */
App > /* 007 */   private Object[] references;
App > /* 008 */   private MutableRow mutableRow;
App > /* 009 */   private Object[] values;
App > /* 010 */   private java.lang.String errMsg;
App > /* 011 */   private java.lang.String errMsg1;
App > /* 012 */   private java.lang.String errMsg2;
...
{code}
Full error log file is attached.
/usr/lib/hive_scripts/storagecli.py --account-id=2553 --cmd=""-get s3://dlx-prod-core-consumer/qubole/prod-tech-core-consumer/tmp/2017-03-16/2553/61131093.err /tmp""

The code submitted as Spark command -> Scala, is as follows:

{code:java}
import java.security.MessageDigest


import scala.collection.mutable.{ListBuffer, WrappedArray}
import org.apache.spark.sql.SparkSession

import scala.collection.Map
import org.apache.spark.sql.functions._
import org.apache.spark.sql.Dataset
import org.apache.spark.sql._

case class ActivityProp(providerId: String, date: Int, count30: Int, count90: Int, lastSeenDate: Int)

case class VertexProp(vertexId: Long,
                      realId: Option[String],
                      vType: String,
                      hashes: Seq[String],
                      providers: Map[String, Map[String, Seq[Int]]],
                      updateStatus: Int,
                      suppress: Int,
                      optOut: Int,
                      deviceIdType: Int,
                      activity: Seq[ActivityProp],
                      mergeHistory: Seq[(Long, Int)],
                      emailSourceId: Int,
                      cookieLastSeenDate: Int,
                      customAttributes: Map[String, Map[String, String]])
case class SimpleDate(value: Int)
case class ProviderAndDate(label: String)
case class DupFactor(label: String, factor: Double)


trait GraphConstant extends Serializable {

  val NOSUBPROVIDER = ""na""
  val DLXPROVIDERID = ""0""

  /* Update Status Flag */
  val FLAG_NO_CHANGE = 0
  val FLAG_ID_NEW = 1
  val FLAG_PROP_UPDATE = 2
  val FLAG_HASH_UPDATE = 3

  /* Vertex and Edge Types and Id Ranges and Associated Functions */
  val VERTEX_EMAIL = ""email""
  val VERTEX_HH = ""hh""
  val VERTEX_IND = ""ind""
  val VERTEX_DEVICE = ""device""
  val VERTEX_COOKIE = ""cookie""
  val VERTEX_GUID = ""guid""
  val VERTEX_BKID = ""bkid""

  val EDGE_EMAIL_DEVICE = ""email-device""
  val EDGE_IND_EMAIL = ""ind-email""
  val EDGE_HH_EMAIL = ""hh-email""
  val EDGE_HH_IND = ""hh-ind""
  val EDGE_EMAIL_GUID = ""email-guid""
  val EDGE_HH_DEVICE = ""hh-device""
  val EDGE_IND_DEVICE = ""ind-device""

  val EDGE_GUID_BKID = ""guid-bkid""
  val EDGE_BKID_BKID = ""bkid-bkid""
  val EDGE_BKID_DEVICE = ""bkid-device""
  val EDGE_COOKIE_DEVICE = ""cookie-device""
  val EDGE_GUID_DEVICE = ""guid-device""
  val EDGE_GUID_COOKIE = ""guid-cookie""
  val EDGE_IND_GUID = ""ind-guid""
  val EDGE_HH_GUID = ""hh-guid""
  val EDGE_COOKIE_COOKIE = ""cookie-cookie""
  val EDGE_DEVICE_DEVICE = ""device-device""
  val EDGE_GUID_GUID = ""guid-guid""
  val EDGE_BAD_EDGES = ""bad-edges""

  val IND_ID_PREFIX = ""IN""
  val HH_ID_PREFIX = ""HH""
  val COOKIE_ID_PREFIX = ""CK""
  val EMAIL_HASH_PREFIX = ""EM""
  val BKID_PREFIX = ""BK""

  val VERTEX_TYPES = Array(VERTEX_EMAIL, VERTEX_HH, VERTEX_IND, VERTEX_DEVICE, VERTEX_COOKIE, VERTEX_GUID, VERTEX_BKID)
  val EDGE_TYPES = Array(EDGE_HH_DEVICE, EDGE_EMAIL_DEVICE, EDGE_IND_EMAIL, EDGE_HH_EMAIL, EDGE_HH_IND, EDGE_COOKIE_DEVICE,
    EDGE_GUID_DEVICE, EDGE_GUID_COOKIE, EDGE_IND_GUID, EDGE_HH_GUID, EDGE_COOKIE_COOKIE, EDGE_DEVICE_DEVICE, EDGE_GUID_GUID,
    EDGE_EMAIL_GUID, EDGE_GUID_BKID, EDGE_BKID_DEVICE, EDGE_BKID_BKID , EDGE_IND_DEVICE)
  val EDGE_TYPES_TO_WRITE = Set(EDGE_HH_DEVICE, EDGE_EMAIL_DEVICE, EDGE_COOKIE_DEVICE,
    EDGE_GUID_DEVICE, EDGE_GUID_COOKIE, EDGE_IND_GUID, EDGE_HH_GUID, EDGE_COOKIE_COOKIE, EDGE_DEVICE_DEVICE, EDGE_GUID_GUID,
    EDGE_EMAIL_GUID, EDGE_GUID_BKID, EDGE_BKID_DEVICE, EDGE_BKID_BKID , EDGE_IND_DEVICE)
  val EDGE_TYPES_RETENTION = Set(EDGE_COOKIE_COOKIE, EDGE_DEVICE_DEVICE, EDGE_GUID_GUID)



  /* Device ID Types*/
  val DEVICEID_BKID = 1020
  val DEVICEID_BKID_STAT = 1021
  val DEVICEID_BKID_MOBILECOOKIE = 1022
  val DEVICEID_BKID_DESKTOPCOOKIE = 1023

  val DEVICEID_BKID_STAT_PREFIX = ""FHz99""
  val DEVICEID_BKID_MOBILECOOKIE_PREFIX = ""ylT99""

  /**
    * Both base and limit are INCLUSIVE
    */
  case class IdRange(base : Long, limit : Long) {
    def isInRange(id : Long) : Boolean = {
      id >= base && id <= limit
    }
  }

  lazy val ID_RANGES = Map[String, IdRange] (
    VERTEX_HH -> IdRange(1L, 100000000000000L),
    VERTEX_IND -> IdRange(100000000000001L, 200000000000000L),
    VERTEX_EMAIL -> IdRange(200000000000001L, 300000000000000L),
    VERTEX_DEVICE -> IdRange(300000000000001L, 400000000000000L),
    VERTEX_GUID -> IdRange(400000000000001L, 500000000000000L),
    VERTEX_COOKIE -> IdRange(500000000000001L, 600000000000000L),
    VERTEX_BKID -> IdRange(600000000000001L, 700000000000000L)
  )

  /**
    * Given a vertex Type, get the IdRange object
    * @param vType must be a valid vertex type string
    * @return IdRange, IdRange(0,0) if invalid vType is passed in
    */
  def getIdRange(vType : String) : IdRange = {
    ID_RANGES.getOrElse(vType, IdRange(0,0))
  }

  /**
    * Given an id, returns the vType
    * @param id the vertex id, if not found in ID_RANGES, return None
    */
  def getVertexType(id : Long) : Option[String] = {
    val found = ID_RANGES.filter(p => p._2.isInRange(id))
    if (found.isEmpty) {
      None
    } else {
      Some(found.head._1)
    }
  }

  /**
    * Check if an id belongs to a vertex type
    * @return true if found the id inrange for the vertex, false otherwise
    */
  def isVertexType(vType : String, id : Long) : Boolean = {
    val range = getIdRange(vType)
    range match {
      case IdRange(0, 0) => false
      case _ => range.isInRange(id)
    }
  }

  val isIndDevice = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_IND, srcId) && isVertexType(VERTEX_DEVICE, dstId)
  }

  val isHhDevice = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_HH, srcId) && isVertexType(VERTEX_DEVICE, dstId)
  }

  val isEmailDevice = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_EMAIL, srcId) && isVertexType(VERTEX_DEVICE, dstId)
  }

  val isIndEmail = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_IND, srcId) && isVertexType(VERTEX_EMAIL, dstId)
  }

  val isHhEmail = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_EMAIL, srcId) && isVertexType(VERTEX_HH, dstId)
  }

  val isHhInd = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_HH, srcId) && isVertexType(VERTEX_IND, dstId)
  }

  val isCookieDevice = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_COOKIE, srcId) && isVertexType(VERTEX_DEVICE, dstId)
  }

  val isGuidDevice = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_GUID, srcId) && isVertexType(VERTEX_DEVICE, dstId)
  }

  val isGuidCookie = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_GUID, srcId) && isVertexType(VERTEX_COOKIE, dstId)
  }

  val isIndGuid = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_IND, srcId) && isVertexType(VERTEX_GUID, dstId)
  }

  val isHhGuid = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_HH, srcId) && isVertexType(VERTEX_GUID, dstId)
  }

  val isCookieCookie = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_COOKIE, srcId) && isVertexType(VERTEX_COOKIE, dstId)
  }

  val isDeviceDevice = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_DEVICE, srcId) && isVertexType(VERTEX_DEVICE, dstId)
  }

  val isGuidGuid = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_GUID, srcId) && isVertexType(VERTEX_GUID, dstId)
  }

  val isEmailGuid = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_EMAIL, srcId) && isVertexType(VERTEX_GUID, dstId)
  }

  val isGuidBkid = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_GUID, srcId) && isVertexType(VERTEX_BKID, dstId)
  }

  val isBkidDevice = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_BKID, srcId) && isVertexType(VERTEX_DEVICE, dstId)
  }

  val isBkidBkid = (srcId: Long, dstId: Long) => {
    isVertexType(VERTEX_BKID, srcId) && isVertexType(VERTEX_BKID, dstId)
  }

  def isEdgeType(eType: String, srcId: Long, dstId: Long): Boolean = eType match {
    case EDGE_HH_DEVICE => isHhDevice(srcId, dstId)
    case EDGE_EMAIL_DEVICE => isEmailDevice(srcId, dstId)
    case EDGE_IND_EMAIL => isIndEmail(srcId, dstId)
    case EDGE_HH_EMAIL => isHhEmail(srcId, dstId)
    case EDGE_HH_IND => isHhInd(srcId, dstId)
    case EDGE_COOKIE_DEVICE => isCookieDevice(srcId, dstId)
    case EDGE_GUID_DEVICE => isGuidDevice(srcId, dstId)
    case EDGE_GUID_COOKIE => isGuidCookie(srcId, dstId)
    case EDGE_IND_GUID => isIndGuid(srcId, dstId)
    case EDGE_HH_GUID => isHhGuid(srcId, dstId)
    case EDGE_COOKIE_COOKIE => isCookieCookie(srcId, dstId)
    case EDGE_DEVICE_DEVICE => isDeviceDevice(srcId, dstId)
    case EDGE_GUID_GUID => isGuidGuid(srcId, dstId)
    case EDGE_EMAIL_GUID => isEmailGuid(srcId, dstId)
    case EDGE_GUID_BKID => isGuidBkid(srcId, dstId)
    case EDGE_BKID_DEVICE => isBkidDevice(srcId, dstId)
    case EDGE_BKID_BKID => isBkidBkid(srcId, dstId)
    case EDGE_IND_DEVICE => isIndDevice(srcId, dstId)
    case EDGE_BAD_EDGES if getEdgeType(srcId, dstId) == EDGE_BAD_EDGES => true
    case _ => false
  }

  def getEdgeType(srcId: Long, dstId: Long): String = {
    if (isEmailDevice(srcId, dstId))  EDGE_EMAIL_DEVICE
    else if (isHhDevice(srcId, dstId)) EDGE_HH_DEVICE
    else if (isIndEmail(srcId, dstId))  EDGE_IND_EMAIL
    else if (isHhEmail(srcId, dstId))  EDGE_HH_EMAIL
    else if (isHhInd(srcId, dstId))  EDGE_HH_IND
    else if (isCookieDevice(srcId, dstId))  EDGE_COOKIE_DEVICE
    else if (isGuidDevice(srcId, dstId))  EDGE_GUID_DEVICE
    else if (isGuidCookie(srcId, dstId))  EDGE_GUID_COOKIE
    else if (isIndGuid(srcId, dstId))  EDGE_IND_GUID
    else if (isHhGuid(srcId, dstId))  EDGE_HH_GUID
    else if (isCookieCookie(srcId, dstId))  EDGE_COOKIE_COOKIE
    else if (isDeviceDevice(srcId, dstId))  EDGE_DEVICE_DEVICE
    else if (isGuidGuid(srcId, dstId)) EDGE_GUID_GUID
    else if (isEmailGuid(srcId, dstId)) EDGE_EMAIL_GUID
    else if (isGuidBkid(srcId, dstId)) EDGE_GUID_BKID
    else if (isBkidDevice(srcId, dstId)) EDGE_BKID_DEVICE
    else if (isBkidBkid(srcId, dstId)) EDGE_BKID_BKID
    else if (isIndDevice(srcId, dstId)) EDGE_IND_DEVICE
    else EDGE_BAD_EDGES
  }

}
trait HashGenerator extends java.io.Serializable {

  //we need this type list so we know which hash is what type in the return list
  val HASH_TYPES = Array(""SHA-1"", ""MD5"")

  def getHashes(deviceId: String): List[String] = {

    val convertToHex = (buf: Array[Byte]) => buf.map(""%02X"" format _).mkString.toLowerCase

    var hashes = new ListBuffer[String]()
    HASH_TYPES.foreach(x => hashes += convertToHex(MessageDigest.getInstance(x).digest(deviceId.getBytes)))
    hashes.toList

    //List(convertToHex(hashSHA1(deviceId)), convertToHex(hashSHA256(deviceId)), convertToHex(hashMD5(deviceId)))
  }

  def getHashesSeq(deviceId: String): Seq[String] = {

    val convertToHex = (buf: Array[Byte]) => buf.map(""%02X"" format _).mkString.toLowerCase
    HASH_TYPES.map(x => convertToHex(MessageDigest.getInstance(x).digest(deviceId.getBytes)))
    //List(convertToHex(hashSHA1(deviceId)), convertToHex(hashSHA256(deviceId)), convertToHex(hashMD5(deviceId)))
  }
}


object ProviderQuery extends GraphConstant with HashGenerator {
  val spark1 = SparkSession
    .builder()
    .enableHiveSupport()
    .getOrCreate()

  import spark1.implicits._


  private val combineProviders = (m1: Map[String, Map[String, Seq[Int]]], m2: Map[String, Map[String, Seq[Int]]]) => {
    val keys = m1.keySet & m2.keySet
    val intersection = keys.map { x => (x, combineSubProviders(m1.get(x).get, m2.get(x).get)) }.toMap
    (m1 -- m2.keySet) ++ intersection ++ (m2 -- m1.keySet)
  }

  private val combineSubProviders = (in: Map[String, Seq[Int]], in2: Map[String, Seq[Int]]) => {
    (in.keySet ++ in2.keySet).map(i =>
      (i, (in.getOrElse(i, Seq[Int]()) ++ in2.getOrElse(i, Seq[Int]())).distinct))
      .toMap
  }

  val combineActivity = (in: Seq[ActivityProp], in2: Seq[ActivityProp]) => {
    in ++ in2
  }

  val combineMergeHistory = (v1: Long, v2: Long, realId1: Option[String], realId2: Option[String],
                             m: Seq[(Long, Int)]) => (realId1, realId2) match {
    case (Some(x), None) => m ++ List((v2, 20170316))
    case (None, Some(x)) => m ++ List((v1, 20170316))
    case _ => m
  }

  private def combineRealId(in: Option[String], in2: Option[String]): Option[String] = (in, in2) match {
    case (Some(x), Some(y)) => Some(x)
    case (Some(x), None) => Some(x)
    case (None, Some(x)) => Some(x)
    case _ => None
  }
  //ATTENTION: if two vertexIds are available, this is a merged existing vertice pair
  //NEED TO TRACK ON THOSE
  val combineVertexId = (in: Long, in2: Long, realId1: Option[String], realId2: Option[String]) => (realId1, realId2) match {
    case (Some(x), None) => in
    case (None, Some(x)) => in2
    case _ => in
  }

  val combineUpdateStatus = (status1: Int, status2: Int) => (status1, status2) match {
    case (FLAG_ID_NEW, FLAG_ID_NEW) => FLAG_ID_NEW
    case _ => FLAG_PROP_UPDATE
  }

  val pickDeviceIdType = (s1: Int, s2: Int) => (s1, s2) match {
    case (1 | 2 | 7, _) => s1
    case (_, 1 | 2 | 7) => s2
    case _ => s1
  }

  val pickSuppressFlag = (s1: Int, s2: Int) => {
    math.max(s1, s2)
  }

  val pickOptOutFlag = (s1: Int, s2: Int) => {
    math.max(s1, s2)
  }

  val mergeMap = (in: Map[String, String], in2: Map[String, String]) => {
    (in.keySet ++ in2.keySet).map(i =>
      (i, (in.get(i), in2.get(i)) match {
        case (Some(x), Some(y)) => x + ""|"" + y
        case (Some(x), None) => x
        case (None, Some(y)) => y
        case _ => """"
      }))
      .toMap
  }

  val mergeMapMap = (in: Map[String, Map[String, String]], in2: Map[String, Map[String, String]]) => {
    (in.keySet ++ in2.keySet).map(i =>
      (i, mergeMap(in.getOrElse(i, Map[String, String]()), in2.getOrElse(i, Map[String, String]()))))
      .toMap
  }

  def zipAndObject(dataset: Dataset[VertexProp], path:String): Dataset[(Long, VertexProp)] = {
    val rddSets = dataset.rdd.zipWithUniqueId().map(x => (x._2, x._1))
    rddSets.toDF().write.mode(SaveMode.Overwrite).parquet(path)
    spark1.read.parquet(path).as[(Long, VertexProp)].repartition(992).persist()
  }

  def generateVertexHashes(vertices: Dataset[VertexProp]): Dataset[VertexProp] = {
    vertices.map { v =>
      v.realId match {
        case Some(id) => v.copy(hashes = getHashes(id.toLowerCase) ++ getHashes(id.toUpperCase))
        case None => v
      }
    }
  }

  def RemoveVertexHashes(vertices: Dataset[VertexProp]): Dataset[VertexProp] = {
    vertices.map { v =>
      v.realId match {
        case Some(id) => v.copy(hashes = List[String]())
        case None => v
      }
    }
  }



  def main(args: Array[String]): Unit = {


    val prevCookies = spark1.read.parquet(""s3://dlx-prod-core-consumer/prod/data/dlx/idgraph/device_processing/data_graph/20170313/vertices/cookie"").as[VertexProp]
    val dCookies = spark1.createDataset(spark1.sparkContext.emptyRDD[VertexProp]).map(x => x)

    val unioned = prevCookies.union(dCookies)

    val groupByRealID = unioned.groupByKey(v => v.realId.getOrElse(v.hashes.head))
      .reduceGroups((s1, s2) =>
        VertexProp(combineVertexId(s1.vertexId, s2.vertexId, s1.realId, s2.realId),
          combineRealId(s1.realId, s2.realId),
          s1.vType,
          (s1.hashes ++ s2.hashes).distinct,
          combineProviders(s1.providers, s2.providers),
          combineUpdateStatus(s1.updateStatus, s2.updateStatus),
          pickSuppressFlag(s1.suppress, s2.suppress),
          pickOptOutFlag(s1.optOut, s2.optOut),
          pickDeviceIdType(s1.deviceIdType, s2.deviceIdType),
          combineActivity(s1.activity, s2.activity),
          combineMergeHistory(s1.vertexId, s2.vertexId, s1.realId, s2.realId, s1.mergeHistory ++ s2.mergeHistory),
          s1.emailSourceId,
          Math.max(s1.cookieLastSeenDate, s2.cookieLastSeenDate),
          mergeMapMap(s1.customAttributes, s2.customAttributes))).map(_._2)
    println(s""count = ${groupByRealID.count()}"")
  }
}
{code}
 
PLEASE LET ME KNOW IF ANY MORE INFORMATION IS REQUIRED. In a few days from now, the ticket will be closed and I would have lost context about this ticket. So better let me know ASAP if anything else is required, so that I can get this before I lose context. The cluster has since been moved to Spark 2.1.0 using bootstrap (as 2.1.0 is not GA yet) and customer is happy.",,Ranjana,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Mar/17 1:37 PM;Ranjana;61131093.err.zip;https://qubole.atlassian.net/secure/attachment/40225/61131093.err.zip,,,,,,,,,,,,,AWS,,,,,,Oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z04853:,,,,,,,,,,,,,,,,,,,1.0,13307,2017-03-20 11:42:30.876,,,"20/Mar/17 11:42 AM;rohitk;Thanks [~Ranjana]
I have seen some issues in this part of the code, typically when someone does a select * on deeply nested json tables. The WholeStage codegen create dynamic java code to deserialise and serialise data, but runs into creating a very large function. Seems like JVM puts a limit of 64KB on compiled function size. This is mostly harmless, because if it fails, Spark will avoid WholeStage Codegen path.  I saw some emails in the spark mailing thread, which talk about a fix and a further issues related to another JVM limitation on constant pool size. 

Lets keep this open. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI taking Too long too load ,SPAR-1517,58504,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Critical,,puneetg,satyavathib,satyavathib,17/Mar/17 2:49 AM,18/May/17 2:25 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"We got this report from multiple customers stating that spark application URL's are taking too long to load.

Sample Application Id : application_1489643051270_0008

Delay is more in case of  running Jobs where as the completed jobs are also observing the delay in loading the URL.

Account Id : 3910

Cluster_ID : 9313

Please let me know If any further details required.

",,athusoo,biswajit,gayathrym,mahuja,puneetg,rohitk,satyavathib,sureshr,umangd,venkatak,,,,,,,,,,,,,,,,,,,,,,,,03/May/17 6:23 AM;satyavathib;Screen Shot 2017-05-03 at 6.52.49 PM.png;https://qubole.atlassian.net/secure/attachment/41962/Screen+Shot+2017-05-03+at+6.52.49+PM.png,03/May/17 6:22 AM;satyavathib;cluster_proxy_log.rtf;https://qubole.atlassian.net/secure/attachment/41961/cluster_proxy_log.rtf,,,,,,,,,,,,AWS,,,,,,indix,insideview,thomsonreuters,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z044wv:,,,,,,,,,,,,,,,,,,,4.0,"13080,13375,13383,13419",2017-03-23 02:54:10.97,,,23/Mar/17 2:54 AM;biswajit;It works fine when the job is completed. ,27/Mar/17 1:54 AM;gayathrym;[~biswajit]  [~satyavathib] : Is this still a blocker? ,"27/Mar/17 8:10 PM;mahuja;[~biswajit], [~satyavathib] please add reason when marking a JIRA blocker. It will help if we know the business impact or if the customer production pipelines are not working because of this issue. cc [~Jtrail]","28/Mar/17 12:02 AM;puneetg;I tried accessing sparkUI in production from dev account, I couldn't see much delay in loading it.

Application UI and Spark UI has totally different workflow and customers has complained that both are loading slow. So my guess is its not the UI which is taking time to load, its the redirection(cluster_proxy) which is taking time. Just a hypothesis, I would need to check it on cusomter's live cluster.

[~biswajit], [~satyavathib] Can we please ask one of the customer to run the job or if we can run jobs on their cluster to see where are we actually facing this issue. Since they all are facing this issue only for running jobs we need to check it real time.",28/Mar/17 12:10 AM;mahuja;Is SPAR-1514 a duplicate for this issue ?,"28/Mar/17 12:15 AM;puneetg;Yes it looks like. In SPAR-1514 also, customer is facing same issue.","28/Mar/17 3:21 AM;biswajit;I have marked this ticket as blocker, since spark UI is loads very slow and sometimes get timedout. And since it has been observed by multiple clients. [~puneetg] [~mahuja]. ","28/Mar/17 8:07 AM;athusoo;Folks, any updates on this? Also is my understanding correct that this blocker has been open for 11 days now? Do we understand the fundamental issue?","28/Mar/17 11:39 PM;mahuja;[~athusoo] - [~puneetg] is investigating this. For escalations we do not use blocker priority as there are ~28 blockers in our system and many of them are old. 

The process is to either report in #escalation-spar (urgent ask) or mark the JIRA SOLSUP for weekly cadence (important ask). The JIRA does not carry these labels. [~biswajit] please mark the JIRA accordingly.

","29/Mar/17 11:35 PM;gayathrym;[~mahuja] : Thanks for letting us know.  I checked today, and there are 19 blockers, some of them are very old as you mentioned, I have left a note on the tickets asking the owners to close them. 

We are trying to monitor the progress of the JIRA escalated blocker and the critical tickets. Is the process for escalations documented somewhere? I will update my queries accordingly. ",30/Mar/17 12:24 AM;biswajit;[~mahuja] Apology. I missed it marking the label correctly.  have added Solsup now. I have marked this as blocker as this has been faced by multiple clients. ,30/Mar/17 5:38 AM;mahuja;[~puneetg] any updates,"30/Mar/17 10:44 AM;puneetg;I tried reproducing same scenario on dev envs and on api.qubole.com, I couldn't see much delay in loading the UI in case of both running and completed spark jobs. We have to check in customer's cluster what is wrong.

[~biswajit] as discussed, can we please ask one of the customer to inform us, before they are running the job. So that we can deep dive into the issue on their cluster itself.","30/Mar/17 11:25 AM;mahuja;[~gayathrym] - the escalation process is documented here: https://qubole.atlassian.net/wiki/display/CS/Customer+Issue+Escalation+Process

I am not sure if there is a standard practice to track escalations in JIRA. For Spark and notebooks, I add 'escalation' and 'escalation-spar'/'escalation-notebooks' labels on these JIRAs.","05/Apr/17 7:36 AM;mahuja;[~biswajit], [~puneetg] - any update on this.","19/Apr/17 8:40 AM;sureshr;[~biswajit]: Have we gotten more information from the customer on this? Since we are not able to reproduce this internally, we need to work with the customer to debug the issue when they run one of their commands.","19/Apr/17 12:27 PM;rohitk;Few suspects:
# *Tunnel Service* - When the cluster is live these requests are served from cluster and when the application is live, it is served from the driver. Both cases Tunnel service will be involved. When the cluster is restarted or down, web tier takes care of Spark UI which doesn't goes through tunnel service.  
# *Synchronisation in driver code* - when rendered via History Server (in cluster or web tier) the rendering code runs without any contention. In case the UI is served from driver, the UI code competes for locks with steady flow of events as well as autoscaling code kicking in fairly often.  CC: [~venkats]
# *Driver cores* - UI rendering is could be CPU intensive for large jobs. I am not even sure if we have a mechanism right now to control driver core for spark analyse jobs. It is work checking if this issue is more prominent in analyse jobs only or does it also impacts Notebook jobs. Drivers in notebooks typically have much more resources available as they don't go via YARN. ",26/Apr/17 7:40 AM;mahuja;[~biswajit] any update?,"02/May/17 8:12 PM;biswajit;[~sureshr] [~mahuja] apology for the delay. 

Few other command-id [ 60783006, 60782942 and 60773370 ] provided by oracle. I had asked one of the user to provide the new command-id that they face this issue. I did not get back any command-id.  They did not face this for all the command-id but randomly. 

If we are looking anything specific information from user, we could ask them.","02/May/17 8:29 PM;venkatak;For such issues, I would suggest that we set aside some time to reproduce the issue. These intermittent issues can cause a lot of pain to customer. Can we take this as a challenge to reproduce such an issue in-house [~biswajit][~satyavathib]? I have started my investigation - will update if I get to some sort of steps. We can discuss when we meet. 

I would start with identifying common patterns of the command ids that we already have. This will go a long way in giving the first aid that a customer needs when such an issue is faced. I think we need to be a little bit more sure on what we need to fix for this specific issue. 

We are trying to catch many things that we could do to improve Spark debugging in general via another JIRA SPAR-1561. For anything that we could do to improve debugging aspects of Spark application, lets pool in our ideas there. cc [~venkats][~rohitk][~adityak]. SPAR-1561 is based on the feedbacks we have from Expedia and Autodesk in the past. We have followup meetings with Expedia this week..




","02/May/17 10:08 PM;venkatak;The only thing I see in common between all these issues is the tunnel server in use: 23.21.156.210
Can we check if these issues correlate with some issues with this tunnel host? cc [~biswajit][~satyavathib]

I suspect the first of the 3 possible issues that [~rohitk] had listed:

*Tunnel Service* - When the cluster is live these requests are served from cluster and when the application is live, it is served from the driver. Both cases Tunnel service will be involved. When the cluster is restarted or down, web tier takes care of Spark UI which doesn't goes through tunnel service.

Hi [~umangd][~sajant] - Has there been issues with this tunnel host in the last 2 months. We see this less often, however when this happens, it is annoying to customer. Can we have some details on how this machine has been doing on March 2nd and March 14th and 15th in particular. Do we have the data for this? Has there been high CPU or some issues on the specified tunnel?
","03/May/17 12:05 AM;umangd;we don't have data for March, but this node is under heavy stress right now as well.. Average CPU is > 95%. We should consider moving customers to other tunnel servers.","03/May/17 12:33 AM;venkatak;Hi [~umangd] Since this involves disruption on customer's side and lot of communication, is it possible to change this machine over to a machine with more cores or some resolution of the likes?

","03/May/17 12:38 AM;umangd;We can launch a new bigger machine and switch the elastic IP, but that would also cause a brief glitch during the switch, which we'll need to communicate to the customers.","03/May/17 2:34 AM;sureshr;[~umangd]: Can you please file a separate QDSOP or MW ticket for the tunnel server upgrade? Has this customer (not clear from the JIRA description who the customer is) moved over to new tunnel service? If yes, could this be a good reason to do the switch?",03/May/17 2:41 AM;umangd;[~sureshr] I've created https://qubole.atlassian.net/browse/QDSOP-1936,"03/May/17 2:43 AM;venkatak;Customers affected as I could see them on this JIRA: indix, Thomsonreuters and insideview
Just seeing [~ajithr] debugging issues for Traveloka with the tunnel tier, not sure of the specific issue yet, and we faced issues with Ola in some specific scenarios - there was a custom metastore connectivity issue i think cc [~biswajit]. I will let them chime in. If we solve those issues, we can move these customers as well, should not be a problem. 

","03/May/17 6:25 AM;satyavathib;A current running application from Thomson reutors to reproduce the issue :

Application id: application_1493196502430_0162<https://api.qubole.com/cluster-proxy?clusterInst=449512&encodedUrl=http%3A%2F%2Fec2-35-166-0-28.us-west-2.compute.amazonaws.com%3A8088%2Fcluster%2Fapp%2Fapplication_1493196502430_0162>

Showed this live to [~puneetg]...
Tunnel Server IP : 23.21.156.210
Cluster ID : 28547
Account :3910
Attaching the cluster proxy logs for the period :

 [^cluster_proxy_log.rtf] 

Also the URL continuosly throws 504 gateway error  !Screen Shot 2017-05-03 at 6.52.49 PM.png|thumbnail! 

Attaching the screenshot for the same. Also Checked the tunnel server logs during the period. I could see nothing suspectable.

{code}
ec2-user@ip-10-171-122-249 ~]$ free -m
            total       used       free     shared    buffers     cached
Mem:          7483       7265        217        154        200       3380
-/+ buffers/cache:       3683       3799
Swap:            0          0          0
{code}

Please let me know If any details required.",03/May/17 6:28 AM;satyavathib;Also Thomson reuters does not have a command ID as they run from their edge node.And any command run on this cluster is experiencing the same delay. The delay is observed in running and completed commands both but in running commands we could not get the UI itself(504 error).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Oracle cluster(9432), instance 407787, never started, returning self._nodes = []",ACM-1020,58329,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,Ranjana,Ranjana,16/Mar/17 12:03 PM,18/Mar/17 12:46 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer affected: Oracle
Cluster id: 9432
Account 2553

The hustler logs were collected from web tier node 10.65.66.169. The hustler logs are attached. 

Cluster 9432, instance 407787 never came up.
The EBS volume count for each node of this cluster was 0. The bid price was at 900%.
On restart the cluster came up normally.

The cluster instance 407787 never came up. It remained in pending state and then was shut down.


{code:java}
production-replica> select * from cluster_insts where cluster_id = '9432' and id='407787' order by id desc limit 2 \G;
*************************** 1. row ***************************
                             id: 407787
                     cluster_id: 9432
              cluster_config_id: 90137
                  cluster_state: DOWN
                       start_at: NULL
                        down_at: 2017-03-16 18:27:13
               terminate_reason: USER-INITIATED
    last_health_check_action_at: NULL
       last_health_check_action: NULL
                     created_at: 2017-03-16 17:38:29
                     updated_at: 2017-03-16 18:27:13
start_cluster_manage_command_id: 98680
                 public_ssh_key: NULL
                private_ssh_key: NULL
          starting_process_host: 10.65.66.169
    starting_process_updated_at: 2017-03-16 18:33:32
1 row in set (0.00 sec)
{code}

In the hustler.logs we see that a few instances were allocated as follows, but cluster_nodes for this instance id is empty set. 
{code:java}
PID: 629  2017-03-16 17:39:44,160 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:3455 - get_spots_allotted - Spot request id: sir-zhri5rjk, allotted instance id: i-08fb8966d29fe75ad
PID: 629  2017-03-16 17:39:44,161 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:3456 - get_spots_allotted - UPSCALE: Spot request sir-zhri5rjk allotted
PID: 629  2017-03-16 17:39:44,161 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:3455 - get_spots_allotted - Spot request id: sir-eakr6c6g, allotted instance id: i-0fe235bd878eebe95
PID: 629  2017-03-16 17:39:44,161 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:3456 - get_spots_allotted - UPSCALE: Spot request sir-eakr6c6g allotted
PID: 629  2017-03-16 17:39:44,161 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:3455 - get_spots_allotted - Spot request id: sir-8jxi7hmk, allotted instance id: i-0bd512ef79e0f6e72
PID: 629  2017-03-16 17:39:44,161 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:3456 - get_spots_allotted - UPSCALE: Spot request sir-8jxi7hmk allotted
PID: 629  2017-03-16 17:39:54,348 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:3500 - wait_for_spots - UPSCALE: Spot instances granted/requested: 20/20
PID: 629  2017-03-16 17:39:54,348 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:4284 - wait_for_cluster - Waiting for cluster to come up... (updating every 10s)
{code}

{code:java}
production-replica> select * from cluster_nodes where cluster_inst_id=407787\G;
Empty set (0.00 sec)
{code}

The rest of the logs on hustler.logs showed 

{code:java}
PID: 629  2017-03-16 17:40:24,425 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1317 - get_nodes - existing nodes: {}
PID: 629  2017-03-16 17:40:24,426 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1356 - get_nodes - returning self._nodes = []
PID: 629  2017-03-16 17:40:24,426 INFO  [cluster:9432] [cluster_instance:407787]  cluster.py:3308 - get_spinner - Waiting for instances to activate...
PID: 629  2017-03-16 17:40:44,747 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1317 - get_nodes - existing nodes: {}
PID: 629  2017-03-16 17:40:44,747 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1356 - get_nodes - returning self._nodes = []
PID: 629  2017-03-16 17:41:04,151 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1317 - get_nodes - existing nodes: {}
PID: 629  2017-03-16 17:41:04,151 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1356 - get_nodes - returning self._nodes = []
PID: 17576  2017-03-16 17:41:15,248 DEBUG  cli.py:310 - main - Exception: SystemExit(0,)
PID: 629  2017-03-16 17:41:26,029 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1317 - get_nodes - existing nodes: {}
PID: 629  2017-03-16 17:41:26,030 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1356 - get_nodes - returning self._nodes = []
PID: 629  2017-03-16 17:41:48,370 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1317 - get_nodes - existing nodes: {}
PID: 629  2017-03-16 17:41:48,370 DEBUG [cluster:9432] [cluster_instance:407787]  cluster.py:1356 - get_nodes - returning self._nodes = []
{code}

We asked the customer to check for a couple of instances on his Ec2 account , which appeared as alloted on the hustler.logs, he said 
{panel:title=From EC2 account of customer as reported by him}
i-0bd512ef79e0f6e72
User initiated (2017-03-16 17:39:54 GMT)

i-0e290072ec4586bd4 
User initiated (2017-03-16 17:39:54 GMT)
{panel}

Cluster configs for that instance is as follows:

{code:java}
production-replica> select * from cluster_configs where cluster_id = '9432' and id = '90137' order by id desc limit 1 \G;
*************************** 1. row ***************************
                              id: 90137
                      cluster_id: 9432
              compute_access_key: p71a5Y7+X0a2cQHHZBcZjR56u0Ss+QORSaKyRB4Bqzw=
              compute_secret_key: JkXGiYWWksEuSZo8agrfPe1YfQ4dHK3TICFbeNIMcsoONBeqp+Sc1r6AD2LYCKle
               compute_validated: 1
                      aws_region: us-east-1
           aws_availability_zone: Any
              hadoop_master_type: r3.4xlarge
               hadoop_slave_type: r3.4xlarge
            hadoop_initial_nodes: 20
                hadoop_max_nodes: 200
            custom_hadoop_config: fs.s3a.endpoint=s3-external-1.amazonaws.com
fs.s3n.sse=AES256
mapred.compress.map.output=true
mapred.fairscheduler.update.interval=2500
mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec
mapred.output.committer.class=org.apache.hadoop.mapred.DirectFileOutputCommitter
mapreduce.job.refresh.timeout=500
mapreduce.map.java.opts=-Xmx3763m
mapreduce.map.memory.mb=4704
mapreduce.reduce.java.opts=-Xmx3763m
mapreduce.reduce.memory.mb=4704
mapreduce.use.directfileoutputcommitter=true
parquet.enable.summary-metadata=false
qubole.s3.standard.endpoint=s3-external-1.amazonaws.com
spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
yarn.app.mapreduce.am.resource.mb=8096
yarn.nodemanager.resource.memory-mb=119040
yarn.scheduler.maximum-allocation-mb=119040
yarn.scheduler.minimum-allocation-mb=1024
dfs.datanode.max.transfer.threads=12288
dfs.datanode.max.xcievers=12288
mapreduce.map.cpu.vcores=1
mapreduce.reduce.cpu.vcores=1
mapreduce.task.io.sort.mb=1024
yarn.app.mapreduce.am.command-opts=-Xmx3763m
yarn.app.mapreduce.am.resource.cpu-vcores=1
yarn.nodemanager.resource.cpu-vcores=15
yarn.scheduler.maximum-allocation-vcores=15
yarn.scheduler.minimum-allocation-vcores=1
           fair_scheduler_config: NULL
                    default_pool: NULL
       hadoop_slave_request_type: spot
    maximum_bid_price_percentage: 300.0
        timeout_for_spot_request: 10
maximum_spot_instance_percentage: 100
      persistent_security_groups: qubole_vpc_securitygroup
             restrict_ssh_access: 1
            encrypted_ephemerals: 1
                customer_ssh_key: NULL
    disallow_cluster_termination: 0
              ganglia_monitoring: 1
                      created_at: 2017-03-16 17:37:59
                      updated_at: 2017-03-16 17:54:01
                       is_latest: 1
                  presto_jvm_mem: 6144
                 presto_task_mem: 1000
                    force_tunnel: 1
             node_bootstrap_file: prod_core_cons_spark_idg_spark21_bootstrap.sh
               is_presto_enabled: 0
            custom_presto_config: NULL
             stable_spot_timeout: 10
      stable_spot_bid_percentage: 900.0
            stable_spot_fallback: 1
                          vpc_id: vpc-088e266d
                       subnet_id: subnet-beee34c9
                tunnel_server_ip: 23.21.156.210
                     use_hadoop2: 1
                ebs_volume_count: 0
                 ebs_volume_size: 100
                 ebs_volume_type: gp2
           use_stable_spot_nodes: NULL
     use_qubole_placement_policy: 1
            fallback_to_ondemand: 1
                 custom_ec2_tags: {""Capability"":""Data"",""Business"":""Shared"",""Team"":""Id Graph"",""Environment"":""Production"",""Component"":""Production""}
                       use_hbase: 0
                       use_spark: 1
                   config_errors: --- []

                compute_role_arn: 9QXJM1ANQoDfsQI9QM/SET+5135EpJD6ozFHAl/oJuednznncQuT5OlRaszs4q4hkBrNf8Hfxk8a+hWsRqDNTg==
             compute_external_id: rjd0gS6bhzEis7K027qSFI/qFlw0skgnJvzK/YPUW14=
                   ami_overrides: null
               datadog_api_token: j+oWXDSySCcm/Mr9g9RbXg==
               datadog_app_token: j+oWXDSySCcm/Mr9g9RbXg==
       use_account_compute_creds: NULL
             custom_spark_config: spark-defaults.conf:
spark.driver.extraJavaOptions -Djava.net.preferIPv4Stack=true
spark.driver.extraLibraryPath /usr/lib/hadoop2/lib/native
spark.eventLog.compress true
spark.eventLog.enabled true
spark.executor.cores 6
spark.executor.extraJavaOptions -Djava.net.preferIPv4Stack=true
spark.executor.memory 47981
spark.logConf true
spark.speculation false
spark.sql.qubole.split.computation false
spark.ui.retainedJobs 33
spark.ui.retainedStages 100
spark.yarn.executor.memoryOverhead 8467
spark.yarn.maxAppAttempts 1
spark.master yarn
spark.submit.deployMode client
spark.rpc.askTimeout 300
                   spark_version: 2.0-latest
            hadoop_setup_timeout: NULL
           role_instance_profile: NULL
         bastion_node_public_dns: NULL
               master_elastic_ip: NULL
           spark_s3_package_name: NULL
        zeppelin_s3_package_name: NULL
              engine_config_type: NULL
                engine_config_id: NULL
                  presto_version: 0.119
                 cloud_config_id: NULL
               cloud_config_type: NULL
            heterogeneous_config: {}
       zeppelin_interpreter_mode: legacy
                           is_ha: NULL
            ebs_upscaling_config: null
                    enable_rubix: 0
       compute_validation_result: null
            idle_cluster_timeout: NULL
             spot_block_duration: NULL
1 row in set (0.00 sec)

ERROR: 
No query specified
{code}




",,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Mar/17 12:17 PM;Ranjana;hustlerlogclustinst407787.txt;https://qubole.atlassian.net/secure/attachment/40115/hustlerlogclustinst407787.txt,,,,,,,,,,,,,AWS,,,,,,Oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0449z:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster should not upscale if requested container size cannot be accommodated in any node,HADTWO-837,58308,Bug,Open,HADTWO,Hadoop2,software,ajayb,,,Critical,,sourabhg,hiyer,hiyer,16/Mar/17 6:58 AM,20/May/17 11:19 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"1. Customer was running cluster with m4.4xlarge (16 vcores) workers  
2. He had set the value of yarn.nodemanager.resource.cpu-vcores to 8, but not modified the value of max-allocation-vcores  
3. There was a spark application running which requested containers with 16 vcores (/03/16 12:11:50 main INFO YarnAllocator: Will request 500 executor containers, each with 16 cores and 52394 MB memory including 1194 MB overhead). These could not be accommodated on any of the workers because the workers were configured with only 8 vcores  
4. YASM added 98 nodes to the cluster seeing that there were pending containers
{code}
2017-03-16 12:12:37,474 INFO  autoscalingLogger (SchedulerApplicationAttempt.java:getTotalPendingRequests(663)) - ResourceRequest Details: {Priority: 1, Capability: <memory:52416, vCores:16>, # Containers: 500, # Containers for AS: 500, Location: *, Relax Locality: true, Init time: 1489666310668}
...
2017-03-16 12:12:37,477 INFO  autoscalingLogger (YarnAutoScalingManager.java:addRemoveNodes(1334)) - UPSCALE: Required nodes for autoscaling: 98 nodes.
2017-03-16 12:12:37,477 INFO  autoscalingLogger (YarnAutoScalingManager.java:addRemoveNodes(1338)) - UPSCALE: Request to provision new nodes: 98
{code}
Note that from the start of the cluster, this was the only application running on it, so the pending requests could have come from this application only.  

At the end of the day this is a configuration issue - max-allocation-vcores should have been <= 8 in this case. But the fact is we wasted 100 nodes of the user, which should have been avoided.",,abhishekmodi,hiyer,Kulbir,sourabhg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,All,,,,,,bloomreach,expedia,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0445b:,,,,,,,,,,,,,,,,,,,1.0,13425,2017-03-22 09:49:13.22,,,"22/Mar/17 9:49 AM;sourabhg;Discussed it with [~hiyer]. 

_yarn.scheduler.maximum-allocation-vcores_ is the property set on the master node. It is used by scheduler to cap vcores in a container resource request. The default value of this property is high (i.e 32) for higher instance types . 

_yarn.nodemanager.resource.cpu-vcores_ is the property set at each slave node. When a slave node registers with RM, it reports this many vcores as the total vcores available on that node. 

Now it can happen that vcores specified in a container request from AM  are more that what any node manager has reported in registration.  Also those vcores can not be capped because _yarn.scheduler.maximum-allocation-vcores_ is set to a value higher than the container request. This would result in container not getting scheduled on any of the slave node.  

I think we should fix the value of yarn.scheduler.maximum-allocation-vcores (so that the container request from AM can be capped appropriately). Ideally it should be the max of vcores of all the slave nodes at the time of registration . We could think of two ways of solving it: 

* Make change in the hustler script which sets these value of the fly for a node. The problem with this is since we have to set this property on master node and because the bring up time of slave and master nodes is different, we have no way of knowing what type of slave nodes are being brought up unless we maintain a static mapping between node type and resources available. This static mapping is what we want to avoid. It gets more complicated in case of heterogeneous cluster.
* Adjust this value dynamically in RM whenever a slave node registers/deregisters.  We won't change this property in case user overrides it. 

I think approach 2 is better unless there is any problem with it. 

Please let me know if there is any other simpler and better approach to solve it. 

cc - [~abhishekmodi] [~ajayb] ",22/Mar/17 9:57 AM;abhishekmodi;Simplest solution is to set it to much higher value. We have done same for `yarn.scheduler.maximum-allocation-mb` in yarn_utils.py in Hadoop2 for cloudman.,"22/Mar/17 10:11 PM;hiyer;[~abhishekmodi] that would not work. For example, if we set max-allocation-vcores to 100, but resource-vcores is only 8, we would hit the same problem. If a user submits an application which requests containers with, say, 16 vcores, it would neither be allocated to a node nor be capped to an usable value.","22/Mar/17 10:15 PM;abhishekmodi;Yes, I synced with Sourabh and I think one way out is to make these properties as final so that users can't override these properties. I could not figure out the rationale behind user modifying these properties.","22/Mar/17 10:21 PM;hiyer;For memory, I agree - we should make it final. There's definitely a use-case for changing the value of vcores to increase (or decrease) concurrency. Even we set the value of vcores to 2X actual number of cores, so it's strictly a logical value rather than a physical one.","22/Mar/17 10:26 PM;sourabhg;[~abhishekmodi]: Yes we can make these properties final unless customer purposefully changed it. 

[~biswajit]: Can we check with the customer why they set the property _yarn.nodemanager.resource.cpu-vcores_ ? ",22/Mar/17 10:28 PM;hiyer;[~sourabhg] this particular customer had changed it by mistake for this cluster. But last I checked there were 50+ customers who had changed the value of resource-vcores for their clusters.,"22/Mar/17 10:43 PM;abhishekmodi;[~hiyer] instead of changing vcores for the node, can we suggest customers to override vcores required by their application(map/reduce/executors) to control parallelism.","22/Mar/17 10:54 PM;hiyer;Won't work if I want to increase concurrency right? e.g. if I say that resource-vcores is 8, there is no way I can run 16 containers on the node concurrently since each container will take at least one vcore. Is it possible to request a container with 0 vcores and just memory requirement? If so, then we can try this approach.","28/Mar/17 11:24 PM;sourabhg;I am removing Jira escalated tag since the ticket has been resolved (customer has removed property yarn.nodemanager.resource.cpu-vcores from hadoop overrides, which was causing the issue ). Nevertheless we will fix it from our end as well.  ","03/Apr/17 12:47 PM;Kulbir;[~sourabhg] we ran into this exact use case but on diff. resource, memory.
In this case(https://qubole.zendesk.com/agent/tickets/13702) customer had configured executor-memory to be 12G (translates to ~13G with yarn overhead added) however NM had published capacity of only 12G
{code}
2017-04-03 18:21:06,018 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node ip-10-190-26-15.ec2.internal(cmPort: 45454 httpPort: 8042) registered with capability: <memory:12288, vCores:8>, assigned nodeId ip-10-190-26-15.ec2.internal:45454
{code}

On Yarn FS side though max-allocation-mb was higher, yarn.scheduler.maximum-allocation-mb=13440, which allowed these AM container requests to be queued and subsequently trigger AS code.

This is a critical bug and we should handle both use cases (CPU\memory config) correctly in code.

cc [~snamburu]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix DBTap bugs,SPAR-1502,58014,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,shridhar,shridhar,shridhar,13/Mar/17 5:06 PM,30/Jun/17 2:52 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"In DOC-708 and SPAR-1500, we discovered a couple of bugs in DBTap

1. DBtap doesn't support tables with dots in them:
{code}
import com.qubole.QuboleDBTap
val catalogName = ""airlineclean""
val hostName = ""trainingdb.cpdokpsr83jx.us-east-1.rds.amazonaws.com""
val databaseType = ""mysql""
val jdbcUrl = s""jdbc:$databaseType://$hostName/""
val username = ""root""
val password = ""quboletraining""
val quboleDBTap = new QuboleDBTap(catalogName, jdbcUrl, username, password, sqlContext)
//list of tables included, supports regex pattern matching
val includes = List()
//list of tables excluded, supports regex pattern matching
val excludes = List()
val databaseName = ""trainingstore""
quboleDBTap.registerTables(s""$databaseName"", includes, excludes)

val tableName = ""airline_clean""
sqlContext.sql(s""select * from `$catalogName.$databaseName.$tableName`"").collect().foreach(println)

//On completion of using the quboleDBTap object
quboleDBTap.unregister()
{code}

will result in {noformat} com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'complete.airline_clean' doesn't exist{noformat}


2. In the REST API flow, the connectionUrl in `spark-dbtap.conf` already has the database appended to it. So the `databaseName` you include in the registerTables method makes it error out.
{code}
import com.qubole.QuboleDBTap
val catalogName = ""airlineclean""
val databaseName = ""trainingstore"" //See step 2 above
val quboleDBTap = QuboleDBTap.get(s""$catalogName"",sqlContext)
//list of tables included, supports regex pattern matching
val includes = List()
//list of tables excluded, supports regex pattern matching
val excludes = List()
quboleDBTap.registerTables(s""$databaseName"")

{code}
ERROR:
{noformat}
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown database 'trainingstore/trainingstore'
{noformat}

{noformat}
[ec2-user@ip-10-171-91-118 conf]$ cat spark-dbtap.conf

{""dbtap"":[{""connectionName"":""mysql"",""connectionUrl"":""jdbc:mysql://trainingdb.cpdokpsr83jx.us-east-1.rds.amazonaws.com:3306/trainingstore"",""connectionUser"":""root"",""connectionPassword"":""quboletraining"",""connectionCatalog"":""airlineclean""}]}[ec2-user@ip-10-171-9

{noformat}

",,mahuja,mpatel,shridhar,,,,,,,,,,,,,,,,,,,,,,,,,,,SPAR-1500,,,,,,,,,,,,,,,,,All,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z042u7:,,,,,,,,,,,,,,,,,,,1.0,13291,2017-03-13 17:33:10.655,,,13/Mar/17 5:15 PM;shridhar;Also need to add info about enabling the `tapp.export_dbtap_to_cluster` account feature to KB-315 once bug#2 is fixed,13/Mar/17 5:33 PM;mahuja;Adding [~jayapradag] for visibility,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose zeppelin error logs in notebooks,ZEP-798,57932,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,vipulm,ekang,ekang,10/Mar/17 12:21 PM,25/May/17 4:12 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"For Scripps, when I looked at notebook 30501 in Ad Marketing Sales, an error msg that's found in zeppelin-interpreter-spark--user_dtreece_scrippsnetworks-ip-10-49-230-233.log shows up in the UI. For example,

ERROR [2017-03-10 20:08:59,133] ({pool-1-thread-3} TThreadPoolServer.java[run]:296) - Error occurred during processing of message.
org.apache.zeppelin.interpreter.InterpreterException: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

But this error message is not very prescriptive about how to resolve it. I had to cross reference another zeppelin log zeppelin--ip-10-49-230-233.log and see a better error msg.

ERROR [2017-03-10 20:08:59,134] ({qtp1293618474-141} NotebookServer.java[onMessage]:286) - Can't handle message
org.apache.zeppelin.interpreter.InterpreterException: Interpreter JVM has stopped responding. This generally happens if spark driver has run out of memory.
Try rerunning paragraph after increasing value of spark.driver.memory in interpreter settings page.
Drop us a mail at help@qubole.com with notebook link for root cause analysis.

Can we expose both the error msgs from both logs in the notebook paragraph? Otherwise, the client will always come back to us asking what the issue is. Thanks.",,ekang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,scripps,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z042fr:,,,,,,,,,,,,,,,,,,,1.0,12983,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notebooks were not migrated after enabling foldering,MW-540,57783,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sumitm,snamburu,snamburu,09/Mar/17 6:12 PM,23/Mar/17 7:31 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Enabled notebook foldering and intelligent loading feature for account:6583. After cluster re-start, some notebooks were not loading because they were not migrated to new s3 location. It observed that the file sizes were being shown as 0 bytes for these files in their previous s3 location. 

Attached the relevant logs. [~mahuja] can provide more context on this as he manually made s3 migration to make them work. 

CC [~vipulm] [~karuppayyar] ",,mahuja,mmajithia,snamburu,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Mar/17 6:06 PM;snamburu;File size with 0 bytes;https://qubole.atlassian.net/secure/attachment/39821/File+size+with+0+bytes,09/Mar/17 6:03 PM;snamburu;Notebook migration issue;https://qubole.atlassian.net/secure/attachment/39823/Notebook+migration+issue,09/Mar/17 6:06 PM;snamburu;Relevant error in Zeppelin server logs;https://qubole.atlassian.net/secure/attachment/39822/Relevant+error+in+Zeppelin+server+logs,,,,,,,,,,,All,,,,,,turner,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z04247:,,,,,,,,,,,,,,,,,,,,,2017-03-09 19:24:09.015,,,"09/Mar/17 7:24 PM;mahuja;[~sumitm], [~mmajithia] we moved  Turner to folder today. The migrations were successful.  Certain notebooks stopped working (this was seen for both online and offline notebooks). The size of note JSON was zero in the new location unlike the original location. I have replaced the offending S3 files with the ones at original note location. 

The migration logs did not have any error message. We need to investigate why it happened? Also do we have retries when we are copying notes from one location to another?

cc [~vipulm], [~karuppayyar]","13/Mar/17 3:36 AM;sumitm;[~mahuja] we tried to look for the errors, but could not found any. Only probable cause could be S3 EC thing, which might happened if at same time Zeppelin is  updating the json and tapp is trying to copy it. Though why it happened to 6 notes making above theory highly unlikely.","13/Mar/17 9:13 AM;mahuja;This might not be due to EC as the note size continued to be 0 bytes after several minutes (may be more than an hour) of the migration. 

Few questions:

- How to we copy note from one location to other? Do we use s3cmd, hadoop command line etc?
- Do we print error message in case of non-zero status of the copy mechanism used?
- Do we retry on failures?","23/Mar/17 7:31 AM;sumitm;[~mahuja], plz find the answers below:

1. We use a rails lib (fog) to download and upload the note json to new destination. 
2. I dont think that we treat 0 file size a failure.
3. There would be 3 retires done for any failure. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Namenode failing to start for Oracle ,SOL-51,57637,Bug,To Do,SOL,Solutions,software,Jtrail,,,Major,,Jtrail,megha,megha,08/Mar/17 4:45 PM,05/Jun/17 1:07 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Oracle is trying to setup a cluster with vpc that uses custom dns.
The cluster startup fails, and when looking at hustler_user_data.log, it seems its not able to connect to hdfs. 
Attaching part of hustler_user_data.log, and full namenode log.
Also attaching resolv.conf.

All this is on canary environment.. (canary.qubole.com)
Cluster id: 22247 user: justin.wainwright@oracle.com account: dev-tech-canary-shared


I also tried setting dfs.namenode.datanode.registration.ip-hostname-check=false.. cluster id: 22428
but that failed as well.
",,adubey,drose@qubole.com,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Mar/17 4:45 PM;megha;canary-hustler-user-data.log;https://qubole.atlassian.net/secure/attachment/39811/canary-hustler-user-data.log,08/Mar/17 4:45 PM;megha;canary-namenode.log;https://qubole.atlassian.net/secure/attachment/39810/canary-namenode.log,08/Mar/17 4:45 PM;megha;canary-resolv.conf;https://qubole.atlassian.net/secure/attachment/39809/canary-resolv.conf,,,,,,,,,,,AWS,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0417r:,,,,,,,,,,,,,,,,,,,1.0,12959,2017-03-08 17:30:58.664,,,"08/Mar/17 4:58 PM;megha;From Justin:
AmazonProvidedDNS  is listed second in DNS options in this vpc..

Options: 
domain-name = ec2.internal 
domain-name-servers = 10.105.2.99, AmazonProvidedDNS

The handle_vpc_custom_domain.sh isn't satisfied with just seeing AmazonProvidedDNS listed as a name server; it has to be primary or the script replaces the resolv.conf completely.",08/Mar/17 5:30 PM;adubey;[~megha] NN log does not have any failure or did i miss it?,02/May/17 10:25 AM;drose@qubole.com;[~adubey][~Jtrail][~wvaldez] this was discussed today in the Oracle Sync.,08/May/17 1:45 PM;drose@qubole.com;[~adubey]do you have any notes or comments from conversations with Justin?,08/May/17 2:06 PM;adubey;I told him that we can try this with private_ip flag where hadoop communication does not rely on DNS.. basically we need more control on this cluster in canary to play around.,08/May/17 2:28 PM;adubey;hadoop.private_ip   - account_features.,"08/May/17 2:42 PM;megha;Thanks [~adubey], have asked Justin to put back original settings on the cluster, and will retry cluster start with above parameter..will update here after.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Templates incorrectly parses parameter values,EAM-86,57633,Bug,Reopened,EAM,ETL App MW,software,sumitm,,,Critical,,rangasayeec,mpatel,mpatel,08/Mar/17 3:17 PM,31/Jul/17 3:54 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Using https://api.qubole.com/command_templates?template_id=330, pass '2016-01-01' for the year value. 

Click on confirm, you see the right query.

However, once you run the query, it changes the value.

Hotels.com reported the same. After moving the quotes around the variable from inside of the template def to the input text box, it seems to work. However, that's not right...

",,aswina,gmargabanthu,mpatel,raghunandan,rangasayeec,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,All,,,,,,hotels.com,,,,,{},NA,Choose from,,UI-4236,,,,,#Empty,No,,,,,,No,,#Empty,,1|xzz13s:,,,,,,,,,,,,,,,,,,,1.0,13232,2017-03-08 18:08:58.267,,,"08/Mar/17 6:08 PM;aswina;This seems very close to existing jiras where we have discussed similar issues before.  Some parts of this cannot be fixed because of the way ruby is processing the macro values.

We can however build an API which displays what the server actually sees so that user gets a better idea of what is getting executed.  There's a separate jira for this (will find and link it). ","08/Mar/17 9:20 PM;mpatel;[~aswina] I think this might be similar, but its different. If you run the ""Confirm Query"" it produces the correct, expected query. Once you run it, you get something else.

Please let me know what you think after checking the repro case...",14/Mar/17 9:45 PM;mpatel;[~aswina] let me know about this one.,"14/Mar/17 10:22 PM;aswina;[~mpatel] - We'll be adding an API to display the exact query that hive backend would execute after substituting the values.  We'll get to it this sprint and it'll reach production by R43.

While this would not solve the substitution problem, it will at least show the same query that the backend executes.

Assigning this to [~bhargavis].  cc: [~raghunandan]","15/Mar/17 10:43 PM;mpatel;But, why not go the other way? The query displayed in ""Confirm Query"" is actually correct, it's the one we run which is invalid. ","16/Mar/17 1:12 AM;aswina;This is the subtle difference in the way JS runs in browser vs the way it's invoked in our envs.  Hence, we have to resort to that API.","22/Mar/17 7:50 PM;mpatel;[~aswina] I don't think we're approaching this correctly.

I have another reprocase: https://api.qubole.com/command_templates/?template_id=2796

I tried the workarounds prescribed in the other jiras with no avail. The only option seems to be moving the single quotes into the value of the template field. This is not a good way for a customer to build such templates.

Can we fix this properly?

In the meantime, any other workarounds?","25/Apr/17 11:03 PM;sureshr;Is this a priority, [~mpatel]? I don't see the SOLSUP label on this ticket...","26/Apr/17 10:49 AM;mpatel;[~sureshr] .. it's not a show stopper. It is preventing Hotels.com from taking advantage of templates.

I'd like some clarity on when we will look into this.
",27/Apr/17 7:41 AM;aswina;[~mpatel] - I have added it to 'UI Sprint 32 (10-May - 23-May)'.  I'll investigate during that sprint and get back.,27/Jul/17 9:54 AM;mpatel;cc [~rangasayeec] - this is one of the JIRAs we discussed...,"31/Jul/17 3:10 PM;rangasayeec;[~mpatel] how do I repro / view this problem? Link doesn't work, think I need to login via super admin into a specific account+user - which one?","31/Jul/17 3:41 PM;mpatel;[~rangasayeec] super admin as mpatel@qubole.com, and click on https://api.qubole.com/command_templates/?template_id=2796","31/Jul/17 3:53 PM;rangasayeec;As far as UX goes, we need to do what we say we will which in this case is honoring the date value user enters from a verification (confirm query) and run (run query) standpoint. Let's figure out how to do this and work past limitations if we have to.

[~mpatel] not sure of prioritization right now given there's an easy workaround. We'll get to this when we look at templates from an analyst / data engineer lens.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commands fail during the duration of cluster startup,ACM-1007,57600,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,sankets,venkatak,venkatak,08/Mar/17 3:30 AM,23/Mar/17 4:14 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,RequiresDebugging,,,,,,,"Failed Command ids:
57080945,57080947,57082782

User: rosie.stevenson@tubemogul.com

Presto command seems to fail whilst the cluster took 3 mins to start-up:

https://api.qubole.com/clusters#/start-logs/25239?instance=56394840

Seen this a few times in the past, where the initial commands that launched the cluster, ends up resulting in failure.

",,ajayb,drose@qubole.com,sankets,stagra,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,tubemogul,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z040zj:,,,,,,,,,,,,,,,,,,,,,2017-03-08 09:57:32.087,,,"08/Mar/17 9:57 AM;stagra;{code}
2017-02-22 18:20:08,562 ERROR prestocli.py:204 - - Traceback (most recent call last):
File ""/usr/lib/hive_scripts/prestocli.py"", line 201, in
sys.exit(main(None))
File ""/usr/lib/hive_scripts/prestocli.py"", line 197, in main
return PrestoCmdHandler().main(qconf)
File ""/usr/lib/hive_scripts/prestocli.py"", line 192, in main
status = self.execute_cmd(qconf)
File ""/usr/lib/hive_scripts/prestocli.py"", line 87, in execute_cmd
master_ip = self._cmdutils_obj._must_get_master_ip(qconf)
File ""/usr/lib/qubole/packages/hive_scripts-40.2.6/hive_scripts/utils/cmd_utils.py"", line 90, in _must_get_master_ip
raise exception.QMasterNotFound(acid, cluster_id, output)
QMasterNotFound: Master node for (account id 648, cluster id 25239) could not be determined.
{code}

It is failing in the path where we talk to hustler to get master ip.
[~venkatak]'s triage is also all about acm code path. Passing it to ACM team for a look","08/Mar/17 5:38 PM;stagra;Just saw that the ticket is couple of weeks old. There was some issue in acm in getProxyPort, wonder if that was the cause for this and is already fixed now!",09/Mar/17 9:26 PM;ajayb;[~sankets] can you take a look?,10/Mar/17 2:30 AM;venkatak;[~ajayb][~sankets] - Any update on this? I see this one more time.,10/Mar/17 6:27 AM;sankets;[~venkatak] Can you give a little more details on today's issue? Like command id/query id which can help us debug it further. ,10/Mar/17 6:36 AM;venkatak;https://api.qubole.com/v2/analyze?command_id=59313298 [~sankets],"10/Mar/17 7:33 AM;drose@qubole.com;To give a little business impact, tubemogul now Adobe is shift gears and wanting to rely even further on presto generated reporting.  they are moving away from an entire installation of vertical to S3 and Qubole.  They share concerns about the stability of these types of cluster start issues in our on-site sessions earlier this week. I am moving this to critical to increase the priority. [~sankets]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IOW..SELECT producing more number of files than the number of buckets specified,HIVE-2022,57539,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,adeshr,p.vasa,p.vasa,07/Mar/17 11:51 AM,07/Aug/17 9:37 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Account Name -> data
Account ID -> 247
Cluster Name -> default
Cluster ID -> 28174
Cluster Type -> Hadoop 1
Command IDS -> 
a) 59339635 (Table Creation)
b) 59342832 (IOW..SELECT)

Hive 0.13 used for Hadoop 1 clusters

Problem Definition ->
The bucketed table is created while specifying 256 buckets, but the number of files inside each partition is around 511

Things tried so far ->

-> When the table is created (Command ID -> 59339635), the number of buckets is clearly specified as 256. However, after the Insert Overwrite..Select query is run, the s3 location of the target table has 511 files in each partition folder.

-> S3 location of the target table -> s3://datausers/baohua/baohua_pin_views_256buckets/

-> Total number of reducer tasks/reducers for the IOW..SELECT query = 256

-> It can be seen that the ‘dt’ partition has only one folder (for one value) and that the ‘eventtype’ partition has 7 folders (for 7 possible values)

-> After the IOW..SELECT query is run you can see that inside each inner partition folder 
(example -> /dt=2017-02-15/eventtype=PIN_CLOSEUP), there are around 511 files generated which should have been 256 only depending upon the number of buckets specified. 511 files are generated for all the 7 different ‘eventtype’ partition folders.

-> Found a couple of JIRAs which might be related ->
a) https://qubole.atlassian.net/browse/HIVE-1504
(In this case, dynamic prefix is enabled too)

b) https://qubole.atlassian.net/browse/HIVE-1954

-> If you check any specific reducer task log 
(example -> https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fip-10-1-1-117.ec2.internal%3A50060%2Ftasklog%3Ftaskid%3Dattempt_247.201702240442_17140_r_000000_0%26all%3Dtrue&clusterInst=386778), at the end you can see that the “CREATED_FILES” = 7 (which explains that the reducer is creating 1 file for each inner partition folder ‘eventtype’ for its 7 values)

-> The names of the files generated by the above reducer are as follows :
dd1a22c4c7f84a8d8765d50c64f35b25_000000

The same name file is generated for each inner partition folder ‘eventtype’ value.

-> The customer is using lots of properties in Hive Bootstrap (can’t say if a specific property is causing this issue)

-> Attaching the file (listFiles.txt) which shows the time of all the files generated under the location -> 
“s3://datausers/baohua/baohua_pin_views_256buckets/dt=2017-02-15/eventtype=PIN_CLOSEUP”

-> The query/command (IOW..SELECT) was run on March 06 at 14.19 PST (March 06 at 22.19 UTC)


Things to check further ->

-> Who is responsible for generating the extra files named -> 00, 01, 02 etc inside each inner partition folder ‘eventtype’
-> Was not able to check the contents of the files as “Download” option is not available from the UI",,asomani,Kulbir,p.vasa,psrinivas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Mar/17 11:52 AM;p.vasa;listFiles.txt;https://qubole.atlassian.net/secure/attachment/39702/listFiles.txt,,,,,,,,,,,,,AWS,,,,,,expedia,pinterest,,,,"{repository={count=2, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":2,""lastUpdated"":""2011-03-03T15:43:09.000-0800""},""byInstanceType"":{""bitbucket"":{""count"":2,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,HIVE-1502,,,,,#Empty,No,,,,,,No,,#Empty,,1|z040ov:,,,,,,Hive Sprint 21,,,,,,,,,,,,,2.0,"13100,15135",2017-03-07 18:15:00.318,,,"07/Mar/17 11:52 AM;p.vasa; [^listFiles.txt] 
Attaching the file here.","07/Mar/17 6:15 PM;Kulbir;[~p.vasa] Couple of observations:
-Extra files created for e.g /baohua/baohua_pin_views_256buckets/dt=2017-02-15/eventtype=PIN_CLOSEUP/103 are all 0 bytes and getting created after MR Job is done
-As such issue is not happening at reducer level but at hive client level, reducer creating 7 files is expected
-For bucketing use case at Hive Client level ,it tries to determine whether required # of files= number of buckets, got created, if not and hive.enforce.bucketing=true is set then it will create empty buckets.
-If you look at hive client logs, above check is creating empty files
-Now why it's creating those empty files is due to reasons outlined in HIVE-1504 , basically while reducer is writing data due to hive.qubole.dynpart.use.prefix being set to true, it appends the file name with a prefix. However later on while trying to check existence of files to enforce bucketing, we are not taking prefix into account and as such incorrectly assume no files are present and hence extra files.

Unless we fix the logic via HIVE-1504 , can you set hive.qubole.dynpart.use.prefix to false and rerun the job to validate above thesis ?","22/Mar/17 3:58 PM;p.vasa;[~Kulbir] Changing the property ""hive.qubole.dynpart.use.prefix=false"" did not help and the IOW statement still creates 512 files instead of 256 in each of the target location partitioning folders.

Latest command ID -> 61903210

I am linking this issue with  https://qubole.atlassian.net/browse/HIVE-1504 for further investigation.","22/Mar/17 4:00 PM;p.vasa;[~asomani] Assigning this to you. 
Please let me know if I need to assign this under a different group.

Please let me know if you need any additional information.","26/Mar/17 10:25 PM;asomani;[~p.vasa] I am surprised that setting dynamic prefix to false didn't work. Do you have the file list that was created by 61903210? The table has been overwritten so was not able to get it myself. Also maybe I missed it in the logs of 61903210, but I could not find the logs saying it is creating empty files for buckets in this case.","28/Mar/17 12:44 AM;p.vasa;[~asomani] Unfortunately the file listing was not taken down.
However, customer again ran the same command (Command ID -> 62706869) with the property -> ""hive.qubole.dynpart.use.prefix=false"" and this time there are only 108 files created under each nested partitioning folder (this may be because of the fact that the job spawned 108 reducers)

However, while creating this target table, the customer had set the number of buckets as 256 so the question is that doesn't hive always generate number of files = number of buckets specified and if the data cannot distribute evenly across all the 256 files (or) if there are less number of reducers then Hive should still create 256 files (with some empty files) according to the table definition (or) is this the way Hive will behave and not create number of files = number of buckets specified in target table definition?

Let me know if you need any additional information. 
","08/Jun/17 6:26 PM;p.vasa;[~asomani]
Another customer is facing this. Sharing the details here ->

*General Information ->*
User -> tiroche@expedia.com
Account ID -> 5497
Account Name -> Prod-Adhoc
Cluster ID -> 22025
Cluster Name -> discovery_services_team_use_only
Cluster Type and Version -> Hadoop2 

*Command IDs* -> 77634926, 77639498

*Problem Description ->*
The MR job to Insert Overwrite..Select a query gets completed. But the command is still running in the background and eventually completes after a couple of hours. When I checked the Hive client logs, it looks like the Hive client is spending some time in creating empty bucket files in some of the partitions.

*Things Tried so far ->*

1) The property ""*hive.qubole.dynpart.use.prefix*"" is not used for any of the above 2 specified commands. (Hence default value will be FALSE)

2) Both the tables are doing IOW into the same target table.
*Target Database* -> project_ede_tdr
*Target Table* -> RBG_PERS_TRANS

Command ID -> 77634926 is inserting into the following partitions ('*trans_date*' between '*01-01-2016*' and '*12-31-2016*')
Command ID -> 77639498 is inserting into the following partitions ('*trans_date*' between '*01-01-2017*' and '*06-30-2017*')

Target Table Location on S3 -> *s3n://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans*

Target Table is clustered by '*email_addr*' column and has *256* buckets specified (SHOW CREATE TABLE *Command ID -> 77654207*)

3) 
Hive Tier node for Command ID -> 77634926 is *10.63.147.223*
Hive Tier node for Command ID -> 77639498 is *10.229.28.103*

4)
 I SSHed into the master node to check the time when the files under the specified partitioning folders (in WHERE condition) were written ->

*Command ID -> 77634926
Command Started at -> June 08, 2017 at 16.06 pm UTC
MR Job Finished at -> Jun 08, 2017 16:17:39 pm UTC*

If you consider one partitioning folder which was in the WHERE clause for this query, example -> (*s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01*), when I do a listing of the files under this location, I get the following output (only a snippet)->
{code:java}
-rw-rw-rw-   1         49 2017-06-08 16:20 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/0252
-rw-rw-rw-   1         49 2017-06-08 16:20 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/0253
-rw-rw-rw-   1         49 2017-06-08 16:20 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/0254
-rw-rw-rw-   1         49 2017-06-08 16:20 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/0255
-rw-rw-rw-   1       4848 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000000
-rw-rw-rw-   1       4555 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000001
-rw-rw-rw-   1       4223 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000002
-rw-rw-rw-   1       4743 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000003
-rw-rw-rw-   1       4694 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000004
-rw-rw-rw-   1       4444 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000005
-rw-rw-rw-   1       4405 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000006
-rw-rw-rw-   1       4839 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000007
-rw-rw-rw-   1       4292 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000008
-rw-rw-rw-   1       4453 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000009
-rw-rw-rw-   1       4972 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000010
-rw-rw-rw-   1       4688 2017-06-08 16:13 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2016-01-01/5ac46ed1bc72487e8fa708be612e1878_000011
{code}

This clearly means 2 things ->
a) The original files have been copied at around *16.13-16.17 pm UTC* just before the MR job got finished while files named ""000"" and others have been written at around *16.19-16.20 pm UTC* which is after the MR job got finished.

b) There are 256 almost empty files (""0000"", ""0255"") of about 49 bytes each created under this partitioning folder

c) This behavior is seen across all the partitioning folders ""*2016-01-01*"" to ""*2016-12-31*"" specified in the WHERE clause of this query  
 
*Command ID -> 77639498
Command Started at -> June 08, 2017 at 16.48 pm UTC
MR Job Finished at -> Jun 08, 2017 16:52:02 pm UTC*

Now, if I consider one partitioning folder involved in the WHERE clause of this query (Eg -> *s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01*), when I do a listing of files under this location, it gives the following output (snippet) ->
{code:java}
17/06/09 00:27:43 INFO s3.S3ServiceFactory: Setting s3 endpoint to s3-external-1.amazonaws.com
17/06/09 00:27:43 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://analytics-qubole-prod.s3-external-1.amazonaws.com/prod-adhoc%2Fwarehouse%2Fproject_ede_tdr.db%2Frbg_pers_trans%2Ftrans_date%3D2017-01-01
17/06/09 00:27:43 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://analytics-qubole-prod.s3-external-1.amazonaws.com/prod-adhoc%2Fwarehouse%2Fproject_ede_tdr.db%2Frbg_pers_trans%2Ftrans_date%3D2017-01-01_%24folder%24
17/06/09 00:27:43 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://analytics-qubole-prod.s3-external-1.amazonaws.com/?delimiter=%2F&max-keys=1&prefix=prod-adhoc%2Fwarehouse%2Fproject_ede_tdr.db%2Frbg_pers_trans%2Ftrans_date%3D2017-01-01%2F
17/06/09 00:27:44 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://analytics-qubole-prod.s3-external-1.amazonaws.com/prod-adhoc%2Fwarehouse%2Fproject_ede_tdr.db%2Frbg_pers_trans%2Ftrans_date%3D2017-01-01
17/06/09 00:27:44 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://analytics-qubole-prod.s3-external-1.amazonaws.com/?delimiter=%2F&max-keys=1000&prefix=prod-adhoc%2Fwarehouse%2Fproject_ede_tdr.db%2Frbg_pers_trans%2Ftrans_date%3D2017-01-01%2F
*Found 256 items*

5060 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000241
-rw-rw-rw-   1       5249 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000242
-rw-rw-rw-   1       5630 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000243
-rw-rw-rw-   1       4638 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000244
-rw-rw-rw-   1       6018 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000245
-rw-rw-rw-   1       5826 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000246
-rw-rw-rw-   1       4846 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000247
-rw-rw-rw-   1       5656 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000248
-rw-rw-rw-   1       6100 2017-06-08 16:51 s3://analytics-qubole-prod/prod-adhoc/warehouse/project_ede_tdr.db/rbg_pers_trans/trans_date=2017-01-01/4058db4090274e5d9969b55bc73d3fdf_000249
{code}

This command is behaving different vs the previous command as in this case only 256 files have been produced and all of them are produced before the MR job has been completed.

5) I have shared the Hive client logs for both the commands on Google Drive with your email for further reference.

6) *I observed the logs for both the commands and here are my observations->*
a) *Observations for log of Command ID -> 77634926 ->*
{code:java}
-> After the MR job is completed at 16.17 pm UTC, from that time until 16.18 pm, it looks like Hive is scanning other partitioning folders('2015-01-01' to '2015-12-31') (which are not a part of the WHERE clause in the query) for the function -> ""*FileSinkOperator.java:findEmptyBuckets()*""

-> From 16.18 pm UTC to 16.19 pm UTC, it is scanning the partitioning folders involved in the WHERE clause of the query ('2016-01-01' to '2016-12-31') and also adding path for empty bucket files (""0000"" to ""0255"") in each partitioning folder

-> *From 16.19 pm UTC to 19.32 pm UTC*, it is creating 256 empty (49 bytes) bucketing files under each partitioning folder involved in the WHERE clause of the query 

-> In the last 4-5 minutes it loads the partitions.
{code}

7) Questions pertaining to this JIRA ->
a) Why is one command producing 256 extra empty files while the other one is not producing any extra files?
b) Where is most of the time spent in the 2 queries after the MR job has been finished (MR job is not taking more than 10 minutes)?
c) Is the MR job responsible for doing the actual IOW operation and the Hive Engine responsible for creating those extra bucketing files?
d) Is this the normal time that will be taken by such queries in this case (the number of partitioning files and data is also large -> 6 months to 1 year worth of data)
","12/Jun/17 2:20 PM;p.vasa;[~adeshr] Please let me know if you need any additional information regarding the above latest issue.
Also let me know if you want me to try to repro this same situation on my account.
Thanks.","15/Jun/17 2:51 PM;p.vasa;[~adeshr] The customer pinged about this issue.
Is there any temporary workaround we can give to the customer so that we can prevent the creation of the empty files and thereby reduce the runtime of the queries?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix s3 jars classpath issue, for spark version >= 2.0 ",ZEP-787,57390,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,karuppayyar,karuppayyar,06/Mar/17 5:46 AM,18/May/17 8:33 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"With ZEP-433, Tthe ability to add jars from s3 from notebook UI was added.
This is not working for sparkversions greater tahn equal to 2.0.

Initail investigation:
1. spark.jars is empty
2. spark.yarn.dist.jars is set with the local absolute path of the jar downloaded from s3(prior to 2.0  spark.jars was set)

cc: [~bharatb] ",,karuppayyar,mahuja,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Apr/17 7:48 PM;Ranjana;NBansal-zepellin-interpreter-29388.rtf;https://qubole.atlassian.net/secure/attachment/41427/NBansal-zepellin-interpreter-29388.rtf,,,,,,,,,,,,,All,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0405z:,,,,,,,,,,,,,,,,,,,1.0,13865,2017-04-14 19:46:48.001,,,06/Mar/17 5:47 AM;karuppayyar;[~vipulm],"14/Apr/17 7:46 PM;Ranjana;I faced this today (April 14, 2017), (Rb 42 already out on April 12) but looks like it works for first jar in list for spark.jars. Let me know if comma separated list does not work.  
+General Information+
Customer: Expedia
Zendesk ticket: 13865
Submitted by: v-nebansal@expedia.com
User login: v-nebansal@expedia.com
Account name: Prod-Adhoc (5497)
Notebook id: 33579
Cluster id: 29388
Spark version : spark-2.0-latest which is 2.0.2 now. 
easy_hustler --cluster-id 29388 sshmaster -u 'ec2-user' qbol_acc5497_cl29388
Interpreter logs attached. NBansal-zepellin-interpreter-29388 attached.
{code:java}
Spark Version: 2.0-latest
Zeppelin Interpreter Mode: user
Master Node Type: m4.4xlarge
Slave Node Type: m4.4xlarge
Minimum Slave Nodes: 4
Maximum Slave Nodes: 200
{code}
Using no custom back end package
{code:java}
production-replica> select aws_region,aws_availability_zone,spark_s3_package_name,ami_overrides,zeppelin_s3_package_name,spark_version,enable_rubix from cluster_configs where cluster_id = '29388' and id = '93137' order by id desc limit 1\G
*************************** 1. row ***************************
              aws_region: us-east-1
   aws_availability_zone: Any
   spark_s3_package_name: NULL
           ami_overrides: null
zeppelin_s3_package_name: NULL
           spark_version: 2.0-latest
            enable_rubix: 0
1 row in set (0.00 sec)
{code}


+Problem description:+ 
In interpreter ""user_vnebansal_expedia""
spark.jars  s3n://occ-decisionengine/spark-redshift_2.11-3.0.0-preview1.jar,s3n://occ-decisionengine/RedshiftJDBC4-1.1.17.1017.jar
zeppelin.dep.additionalRemoteRepository spark-packages,http://redshift-maven-repository.s3-website-us-east-1.amazonaws.com/release,http://dl.bintray.com/spark-packages/maven,false;
Interpreter restarted.
The 1st jar in list seem to be imported but second jar is not. There is no space before or after comma.
The same in Analyze window passed successfully. Command ids: 65778070, 66724938

*First paragraph* in notebook is as follows
{code:java}
%dep
z.reset()
z.load(""com.databricks:spark-redshift_2.11:3.0.0-preview1"")
z.load(""com.amazon.redshift:redshift-jdbc4:1.1.17.1017"")
{code}
Ran, it gave
{code:java}
org.sonatype.aether.resolution.DependencyResolutionException: Could not find artifact com.amazon.redshift:redshift-jdbc4:jar:1.1.17.1017 in central (http://repo1.maven.org/maven2/)
{code}

*Second paragraph* is as follows
{code:java}
import com.databricks.spark.redshift
import com.amazon.redshift.jdbc4.Driver
{code}
Ran, it gave, so problem is for 2nd line.
{code:java}
import com.databricks.spark.redshift
<console>:26: error: object amazon is not a member of package com
       import com.amazon.redshift.jdbc4.Driver
{code}

*Third paragraph* is as follows:
{code:java}
%spark

import com.databricks.spark.redshift
import com.amazon.redshift.jdbc4.Driver

Class.forName(""com.databricks.spark.redshift.RedshiftWriter"")
Class.forName(""com.amazon.redshift.jdbc4.Driver"")
{code}
Ran, it gave
{code:java}
import com.databricks.spark.redshift
<console>:27: error: object amazon is not a member of package com
       import com.amazon.redshift.jdbc4.Driver
{code}

*Fourth paragraph*
{code:java}
%pyspark 

df = sqlContext.read \
    .format(""com.databricks.spark.redshift"") \
    .option(""url"", ""jdbc:redshift://bexg.cdzdl6bkwykk.us-east-1.redshift.amazonaws.com:5439/prod"") \
	.option(""user"", ""occprod_etl"") \
	.option(""password"", ""XXXXXXXXXXXXXX"") \
    .option(""dbtable"",""ods.aso_location"")\
	.option(""schema"", ""ods"") \
    .option(""tempdir"", ""s3n://occ-decisionengine/redshift_data/"") \
	.option(""aws_iam_role"", ""arn:aws:iam::040764703330:role/redshift-bexg-0"") \
    .load()
	
df.show(20,False)
{code}
Ran, it gave,
{code:java}
Py4JJavaError: An error occurred while calling o108.load.
: java.lang.ClassNotFoundException: Could not load an Amazon Redshift JDBC driver; see the README for instructions on downloading and configuring the official Amazon driver.
	at com.databricks.spark.redshift.JDBCWrapper$$anonfun$getDriverClass$1.apply(RedshiftJDBCWrapper.scala:85)
	at com.databricks.spark.redshift.JDBCWrapper$$anonfun$getDriverClass$1.apply(RedshiftJDBCWrapper.scala:71)
	at scala.Option.getOrElse(Option.scala:121)
	at com.databricks.spark.redshift.JDBCWrapper.getDriverClass(RedshiftJDBCWrapper.scala:70)
	at com.databricks.spark.redshift.JDBCWrapper.getConnector(RedshiftJDBCWrapper.scala:190)
	at com.databricks.spark.redshift.RedshiftRelation$$anonfun$schema$1.apply(RedshiftRelation.scala:64)
	at com.databricks.spark.redshift.RedshiftRelation$$anonfun$schema$1.apply(RedshiftRelation.scala:61)
	at scala.Option.getOrElse(Option.scala:121)
	at com.databricks.spark.redshift.RedshiftRelation.schema$lzycompute(RedshiftRelation.scala:61)
	at com.databricks.spark.redshift.RedshiftRelation.schema(RedshiftRelation.scala:60)
	at org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:40)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:122)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: com.amazon.redshift.jdbc4.Driver
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at com.databricks.spark.redshift.Utils$.classForName(Utils.scala:42)
	at com.databricks.spark.redshift.JDBCWrapper$$anonfun$getDriverClass$1.apply(RedshiftJDBCWrapper.scala:82)
	... 24 more
(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o108.load.\n', JavaObject id=o109), <traceback object at 0x7f962199cbd8>)
ERROR   
{code}






","14/Apr/17 8:06 PM;karuppayyar;[~Ranjana]

{code:java}
import com.databricks.spark.redshift
{code}
code worked(in para2) because the first jar u have loaded(com.databricks:spark-redshift_2.11:3.0.0-preview1) is from central maven repo.
Any other imports will not work because the s3  jar support doesnot work for version >=2.0.

Also(not related to this jira).
once we specify in spark.jars or in dependency section, we dont need to use *z.load*


","14/Apr/17 9:12 PM;Ranjana;Thanks [~karuppayyar]
We were able to make the notebook run without depending on spark.jars for now. 
We added the repository URL mentioned in http://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection-with-maven.html
<id>redshift</id>
 <url>http://redshift-maven-repository.s3-website-us-east-1.amazonaws.com/release</url>
and then added the following dependencies in dependency section of notebook
com.databricks:spark-redshift_2.11:3.0.0-preview1	
com.amazon.redshift:redshift-jdbc4:1.1.17.1017
Now all the paragraphs working fine. 
1st jar loaded from cental maven repo and the 2nd jar loaded from redshift-maven-repository. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Canonical reports not working,MW-730,57228,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,megha,megha,02/Mar/17 4:28 PM,15/Jun/17 11:20 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Attaching the screenshot.
Reports for Canonical hive commands hang and dont return anything.. 
It also hangs when API is used..

This is however intermittent, as it was working fine a few days back..


",,aswina,megha,raghunandan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Mar/17 4:27 PM;megha;Screen Shot 2017-03-02 at 4.26.58 PM.png;https://qubole.atlassian.net/secure/attachment/39506/Screen+Shot+2017-03-02+at+4.26.58+PM.png,15/Mar/17 3:59 AM;raghunandan;Screen Shot 2017-03-15 at 4.26.55 PM.png;https://qubole.atlassian.net/secure/attachment/40001/Screen+Shot+2017-03-15+at+4.26.55+PM.png,,,,,,,,,,,,AWS,,,,,,,,,,,{},NA,Choose from,,AN-8,,,,,#Empty,No,,,,,,No,,#Empty,,1|xzz1pk:,,,,,,,,,,,,,,,,,,,1.0,12756,2017-03-02 17:53:27.972,,,"02/Mar/17 5:53 PM;aswina;[~raghunandan] - Could you do a first level triage please?

[~megha] - Can you share the account id and user email in which this problem is occurring? ","02/Mar/17 7:57 PM;megha;Hi Aswin,
Customer reported this: tim.isenhart@oracle.com, I tried from UI, and it was hanging.. Also, customer tried using api.
api call: https://api.qubole.com/api/v1.2/reports/canonical_hive_commands?end_date=2017-02-28T23%3A59%3A59Z&show_ast=true&sort_column=cpu&offset=0&limit=50&start_date=2017-01-29T00%3A00%3A00Z 
I could reproduce this in my account as well.. Megha@qubole.com, account name: megha


",13/Mar/17 11:12 PM;aswina;[~raghunandan] - Can you take look at [~megha]'s comment and get back please?,"15/Mar/17 3:59 AM;raghunandan;[~aswina] - From the network tab i can see that the api request for canonical hive commands takes a very long time. It tried for first couple of times and timed out. Third time it served the request in 1+ mins.

 !Screen Shot 2017-03-15 at 4.26.55 PM.png|thumbnail! ",15/Mar/17 4:06 AM;aswina;[~raghunandan] - Do you think we may have to add indexes to canonical_hive_commands and canonical_hive_command_weeks tables?,"15/Mar/17 4:41 AM;raghunandan;[~aswina] - looks like the two tables already have indexes on the rows with which we retrieve data

{noformat}
production-replica> show indexes from canonical_hive_commands;
+-------------------------+------------+---------------------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| Table                   | Non_unique | Key_name                                    | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |
+-------------------------+------------+---------------------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| canonical_hive_commands |          0 | PRIMARY                                     |            1 | sha2hash    | A         |     3759681 |     NULL | NULL   |      | BTREE      |         |               |
| canonical_hive_commands |          1 | index_canonical_hive_commands_on_account_id |            1 | account_id  | A         |       19787 |     NULL | NULL   | YES  | BTREE      |         |               |
+-------------------------+------------+---------------------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
2 rows in set (0.00 sec)

production-replica> show indexes from canonical_hive_command_weeks;
+------------------------------+------------+---------------------------------+--------------+----------------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| Table                        | Non_unique | Key_name                        | Seq_in_index | Column_name                      | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |
+------------------------------+------------+---------------------------------+--------------+----------------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| canonical_hive_command_weeks |          0 | PRIMARY                         |            1 | id                               | A         |     4751264 |     NULL | NULL   |      | BTREE      |         |               |
| canonical_hive_command_weeks |          0 | index_chcw_on_week_and_sha2hash |            1 | week                             | A         |       28281 |     NULL | NULL   | YES  | BTREE      |         |               |
| canonical_hive_command_weeks |          0 | index_chcw_on_week_and_sha2hash |            2 | canonical_hive_commands_sha2hash | A         |     4751264 |     NULL | NULL   | YES  | BTREE      |         |               |
+------------------------------+------------+---------------------------------+--------------+----------------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
3 rows in set (0.01 sec)
{noformat}","15/Mar/17 5:07 AM;aswina;Okay.  What are the next steps here?  What can we do to improve performance?

On Wed, Mar 15, 2017 at 5:11 PM, Raghunandan Balachandran (JIRA) <

",16/Mar/17 4:19 AM;raghunandan;The queries to retrieve this data seem simple. One option is to see if we can add more indexes in these tables. ,02/May/17 2:41 PM;megha;Any updates on performance improvements for this [~sumitm] [~raghunandan][~aswina],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UGI information sent from Hive Server2. It seems the same user id is sent from multiple client logins,SPAR-1475,57132,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Critical,,shridhar,drose@qubole.com,drose@qubole.com,01/Mar/17 11:59 AM,24/Jul/17 9:55 PM,09/Aug/17 6:03 AM,,,,,0,escalation,escalation-spar,jira_escalated,,,,,,"This is a security concern that if unchecked, will cause them to migrate this workload from Qubole to the Mesos cluster.

Autodesk uses SQL workbench as multiple users to connect to their Fast access cluster via hiveserver2 --A month or 1.5 months - they are using authorization custom code to connect to ldap --Also with respect to object privs and stuff, they use the user to validate access to objects in metastore --ugi -> user identification from JDBC -> user id is sent to metastore and accordingly authorization is passed to the user --Some times, users id is sent differently to the metastore server they host (previous logged in user id is sent for the next one). --They are able to reproduce this scenario in their dev setup: For example, they have a deamon that checks the JDBC connectivity periodically (running some ""show database"" type command). The deamon runs as root, so the next user who logs in using the JDBC client gets access to the metastore as root, who is exempt from authorization process. So the user ends up getting a complete access. This is one part of the problem The other serious concern; If the first user is less privileged than the second user who logs in, the second user, though is entitled to access the table, gets access denial, which they see as a serious problem, blocking their work completely --They are able to consistently reproduce this issue with dev.adp account -> fastaccessdev1 cluster (id: 22304) --After about 10-15 mins, the cached user problem goes away and the actual user gets passed - but still they are not able to suggest this to the users as workaround --They run health check system that checks and a simple command to the cached table - every 5 mins or so. This can be seen on their dev cluster above --I had suggested an action plan yesterday, but that seemed to have not helped",,drose@qubole.com,Jtrail,rohitk,shridhar,vagrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Mar/17 2:19 AM;shridhar;ANANDR_ANANDRADMIN.out;https://qubole.atlassian.net/secure/attachment/39600/ANANDR_ANANDRADMIN.out,,,,,,,,,,,,,AWS,,,,,,autodesk,,,,,"{repository={count=4, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":4,""lastUpdated"":""2017-06-26T18:16:16.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":4,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03yt3:,,,,,,,,,,,,,,,,,,,2.0,"11550,12998",2017-03-03 02:18:37.191,,,"03/Mar/17 2:18 AM;shridhar;Pushed a custom package with a bunch of logs added to autodesk's cluster. Got the help of roshan to reproduce the issue on their end.

Excerpted from slack:
I believe I have narrowed down the issue. There is no bug in the spark thrift server - no evidence of thread caching/race conditions. However, it looks like there's only a single connection between the metastore client and server, which is getting used for multiple queries that are within a 20 second time frame (socket time out period). So, the UGI of the query that opens the metastore client-server connection is the one that gets used. After 20 s, the connection times out and hence a new connection is made for the next query, and the UGI of this query passes through fine.

[2:06]  
While I dig further into why there is only a single connection, could you try setting hive.metastore.client.socket.timeout to a smaller value - say 5 - and please try reproducing this again, to confirm my hypothesis

Attached the logs of 2 users - anandradmin and anandr opening sessions and firing queries. Timeline:
Anandradmin logs in
Anandr logs in
[Query1] Anandradmin fires select query --> ugi=anandradmin on metastoreserver
[Query2] (within 10 seconds) Anandr fires select query --> ugi=anandradmin on metastoreserver
[Query3] (after 1 min) Anandr fires select query --> ugi=anandr on metastoreserver

In Query1 and Query3 a connection is opened between metastore client and server. Query 2 doesn't open a connection.
",06/Mar/17 6:08 PM;Jtrail;[~drose@qubole.com] any updates on this one? Can you please link the Zendesk ticket? Thx!,"06/Mar/17 9:52 PM;drose@qubole.com;[~Jtrail]link has been made to Zendesk, our initial solution was not the answer and [~shridhar]is digging in further.","08/Mar/17 6:00 PM;shridhar;Digged further and found that the issue is that Spark doesn't support multiple hive metastore server connections for a single spark application. In addition, the hive metastore server caches the UGI of the client that opened the connection, and uses the same UGI for all subsequent RPCs on that connection. Fixing this will require adding a feature (something like a UGI-HMS client connection mapping), and hence we aren't planning to roll this out as an immediate hot fix. @drose and myself have communicated all this to the customer.

Moving this to critical.","27/Jun/17 12:35 AM;shridhar;Provided a custom package with the fix (spark_s3_package_name = '1.6.2-SPAR-1475') to Ashwini from Autodesk. They first evaluated it on their dev and staging accounts and it seemed to work fine, so I also pushed it to their prod clusters.
Here are the details of clusters to which the custom package has been passed:
account_id: 6152, cluster_id = 22304, 22305
account_id: 4793, cluster_id = 14006 and 25995
account_id: 6459, cluster_id = 24062 and 26000",19/Jul/17 11:50 PM;vagrawal;[~shridhar] - is there any other action item on this. Otherwise shall we close the jira?,"24/Jul/17 1:56 PM;shridhar;[~vagrawal] this is a weird case actually. We were able to fix the issue locally but it didn't work with Autodesk's setup (custom metastore). Worked with Ashwini from Autodesk and got a fix working on their setup, but strangely this didn't work on our setup. So I am not sure which version to push to our master.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Frequent Tunnel issues with Activision ,ACM-978,57131,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ashishs,adubey,adubey,01/Mar/17 11:29 AM,23/Mar/17 4:23 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,tunnels,,,,,,,"cc [~Harsh] [~prakharj] [~gmargabanthu] [~p.vasa] [~sureshr]

Activiosn is seeing this issue - https://api.qubole.com/v2/analyze?command_id=58438209

https://api.qubole.com/v2/analyze?command_id=58438457

Can we investigate the health of their tunnel gateway - marking it critical as we have had sequence of issues with them today",,adubey,Harsh,p.vasa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Mar/17 2:13 PM;p.vasa;tunnelingLogs.txt;https://qubole.atlassian.net/secure/attachment/39422/tunnelingLogs.txt,,,,,,,,,,,,,AWS,,,,,,activision,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03ysv:,,,,,,,,,,,,,,,,,,,,,2017-03-01 14:13:18.285,,,"01/Mar/17 2:13 PM;p.vasa;Some other information ->

* Cluster Name -> ""ds-hadoop--batch""
* Cluster ID -> 23629
* Account Name -> ds-prod
* Account ID -> 5165

* The customer is running all the queries with Hive On Master
* Some other sample commands -> 58438457, 58438449, 58438393, 58438391
* As you can see some of these commands passed while some other failed but all these commands show the same StackTrace
* For this account -> 5165, they have setup a tunnel to connect to their metastore (private metastore)
* On super admin page, we have also enabled direct communication from the master node to their metastore by setting the following two properties ->
 
1) metastore.url
2) metastore.port

* Tried to check the direct connection from the master node to the metastore ->
* [ec2-user@ip-172-31-40-105 ~]$ nc -vz ds-qubole-meta-prod.c3ltqaduz7er.us-west-2.rds.amazonaws.com 3306
Connection to ds-qubole-meta-prod.c3ltqaduz7er.us-west-2.rds.amazonaws.com 3306 port [tcp/mysql] succeeded!

* Checked the RM logs which appear to be working well.  

* Checked the ""server.log"" on the tunnel server -> ""23.21.156.210"" and attached the logs for the time when the above specified commands were run.


Please let me know if you need any additional details.","01/Mar/17 2:17 PM;Harsh;Tunnel server:
[ec2-user@ip-10-171-122-249 tunneling]$ grep -ir ""19:2.*getProxyPort - Call to getProxyPort(23629, qbol_acc5165_cl23629"" server.log | wc -l
339

[ec2-user@ip-10-171-122-249 tunneling]$ grep -ir ""19:2.*getProxyPort - Call to getProxyPort(23629, qbol_acc5165_cl23629.*ec2-54-201-255-178.us-west-2.compute.amazonaws.com"" server.log | wc -l
339

[ec2-user@ip-10-171-122-249 tunneling]$ grep -ir ""19:2.*23629.*Validation Passed"" server.log  | wc -l
180

[ec2-user@ip-10-171-122-249 tunneling]$ grep -ir ""19:2.*23629.*Validation failed"" server.log  | wc -l
0

There were lock contentions which caused (339-180) failures for fetching the proxy port in 10 mins. The request were timeout eventually.
",03/Mar/17 11:56 AM;Harsh;Enabling tunnel discovery will mitigate the problem here but that would mean whitelisting multiple ips. Not sure if we have a solution when such lock contention happens frequently. Assigning it to [~ashishs].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[foldering] 403 while trying to create new notebook,MW-539,57127,Bug,Open,MW,Middleware,software,sumitm,,,Minor,,tanishg,mpatel,mpatel,01/Mar/17 10:54 AM,12/Jun/17 6:11 AM,09/Aug/17 6:03 AM,,,,,0,escalation,escalation-notebooks,jira_escalated,,,,,,"After moving to foldering, trying to create a new notebook fails with 403. disabling foldering does not help:

{code}
INFO [2017-03-01 18:33:07,842] ({qtp1394926134-50} QuboleACLHelper.java[fetchACLFromTapp]:264) - Fetching ACLs in batch from tapp
 INFO [2017-03-01 18:33:07,858] ({qtp1394926134-50} QuboleUtil.java[sendRequestToQuboleRails]:272) - java.net.ConnectException: Connection refused
ERROR [2017-03-01 18:33:07,859] ({qtp1394926134-50} QuboleACLHelper.java[fetchACLs]:363) - Error while getting permission from tapp
java.lang.NullPointerException
	at org.apache.zeppelin.util.QuboleUtil.getResponseFromConnection(QuboleUtil.java:617)
	at org.apache.zeppelin.socket.QuboleACLHelper.fetchACLs(QuboleACLHelper.java:361)
	at org.apache.zeppelin.socket.QuboleACLHelper.fetchACLFromTapp(QuboleACLHelper.java:265)
	at org.apache.zeppelin.socket.QuboleACLHelper.fetchACLBatch(QuboleACLHelper.java:238)
	at org.apache.zeppelin.socket.QuboleACLHelper.addNotebookACL(QuboleACLHelper.java:208)
	at org.apache.zeppelin.socket.QuboleACLHelper.addSingleNotebookACL(QuboleACLHelper.java:172)
	at org.apache.zeppelin.socket.QuboleACLHelper.checkAndAddNotebookACL(QuboleACLHelper.java:160)
	at org.apache.zeppelin.socket.QuboleACLHelper.isOperationAllowed(QuboleACLHelper.java:312)
	at org.apache.zeppelin.rest.NotebookRestApi.associateNote(NotebookRestApi.java:1096)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.cxf.service.invoker.AbstractInvoker.performInvocation(AbstractInvoker.java:180)
	at org.apache.cxf.service.invoker.AbstractInvoker.invoke(AbstractInvoker.java:96)
	at org.apache.cxf.jaxrs.JAXRSInvoker.invoke(JAXRSInvoker.java:192)
	at org.apache.cxf.jaxrs.JAXRSInvoker.invoke(JAXRSInvoker.java:100)
	at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:57)
	at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:93)
	at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:272)
	at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:121)
	at org.apache.cxf.transport.http.AbstractHTTPDestination.invoke(AbstractHTTPDestination.java:239)
	at org.apache.cxf.transport.servlet.ServletController.invokeDestination(ServletController.java:248)
	at org.apache.cxf.transport.servlet.ServletController.invoke(ServletController.java:222)
	at org.apache.cxf.transport.servlet.ServletController.invoke(ServletController.java:153)
	at org.apache.cxf.transport.servlet.CXFNonSpringServlet.invoke(CXFNonSpringServlet.java:167)
	at org.apache.cxf.transport.servlet.AbstractHTTPServlet.handleRequest(AbstractHTTPServlet.java:286)
	at org.apache.cxf.transport.servlet.AbstractHTTPServlet.doPut(AbstractHTTPServlet.java:223)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:758)
	at org.apache.cxf.transport.servlet.AbstractHTTPServlet.service(AbstractHTTPServlet.java:262)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)
	at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)
	at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.apache.zeppelin.server.CorsFilter.doFilter(CorsFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:229)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{code}",,mahuja,mpatel,sajant,surendranm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,hotels.com,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|xzz1u6:,,,,,,MW Sprint (15Mar-28Mar),,,,,,,,,,,,,1.0,13055,2017-03-01 12:13:03.987,,,"01/Mar/17 11:27 AM;mpatel;This problem is related to ZEP-771..

It is happening because hotels.com whitelists outbound access from the cluster subnet. api.qubole.com is not whitelisted. Per discussion with [~vipulm] and [~mahuja] we use ""ops-api"" now which is an api on our side...

need to figure out how Hotels.com can whitelist outbound access to this.","01/Mar/17 12:13 PM;mahuja;[~surendranm], [~umangd], [~Amit Umarye], [~skurian], [~sajant], [~sureshr] - need your inputs.

We make REST API call from a cluster in private subnet. The endpoint is api.qubole.com. With tight control on outbound traffic, the API calls fail. The possible solution is whitelist Qubole. The IP address of ELB can change internally, so we will like to have a way by which we can whitelist Qubole for such accounts.

cc - [~karuppayyar], [~vipulm], [~beria], [~namanm]",01/Mar/17 1:08 PM;surendranm;How do they limit outbound traffic [~mahuja] [~mpatel]? ,"01/Mar/17 1:13 PM;mpatel;[~surendranm] they are running clusters in a private subnet, and use network acls and/or sg rules to whitelist outbound TCP (and other) connections.","01/Mar/17 7:38 PM;sajant;[~mahuja] [~mpatel] slightly confused,

From [~mpatel]  api.qubole.com is not whitelisted to hotels.com - I would guess out bound connections happening from api.qubole.com not whitelisted

from [~mahuja] We make REST API call from a cluster in private subnet.The endpoint is api.qubole.com - which looks like api.qubole.com is the api we are hitting. Could you clarify please?","01/Mar/17 7:50 PM;surendranm;[~sajant] yes, the opsapi is hitting api.qubole.com and their outbound access to api.qubole.com is blocked. 

[~mahuja]: We dont have a ready solution for this but i can think a couple of ways out. Not sure what is preferable:

1. Implement a pass through proxy (squid/varnish) in our tunnel servers to api.qubole.com and whitelist the tunnel server ips

2. Implement a simple webapp with opsapi endpoints alone that just internally forwards it to api.qubole.com

3. Keep one of the webservers always up and with a static EIP and out of regular api.qubole.com traffic. Use that as ops api endpoint for all outbound restricted clusters.

But they could always whitelist all outbound 443 access right which is simplest of them all from effort point of view.

cc [~jssarma] for additional inputs. 


 ","06/Mar/17 10:05 AM;mpatel;[~mahuja] let me know if we have any concrete plan worked out here. Hotels.com wants to open up Spark to their users, but also wants access to notebook ACLs, Foldering, etc.","07/Mar/17 5:23 PM;mahuja;hotels.com figured out a way to whitelist based on DNS name api.qubole.com.

We are waiting on more inputs from them on how they achieved it.","09/Mar/17 6:47 AM;mpatel;Spoke with hotels.com - 

After whitelisting 'api.qubole.com' in their setup, this is working for them.

They basically have a custom outbound proxy. So it's not something they configure in AWS. In their proxy they can use pattern matching to allow in/out communication.

I'm reducing the priority of this JIRA. It seems it may be good to try to solve in case some customer in the future has a similar requirement, but uses AWS network ACL / SG rules for whitelisting.",09/Mar/17 9:17 AM;mahuja;Moving it MW/infrastructure since ops-api access will need support for whitelisting.,09/Mar/17 9:19 AM;mahuja;[~surendranm] parking with you. It might need Ops support. Can you please assign it to the right owner? Thanks!,"09/Mar/17 9:45 AM;surendranm;4. adding one more way of doing it: Just to set up a reverse tunnel for our api endpoint too

Out of the 3 mentioned earlier and this one, [~mahuja] [~mpatel] what do you think would be better for customers? ","09/Mar/17 12:29 PM;mahuja;[~surendranm] - my thoughts:

1. I was thinking of basic reverse proxy as solution but then it has HA concerns. I quickly looked up varnish/squid and they (can) act as reverse proxy. Not sure about HA part.

2. In the web app solution, will we have multiple nodes for HA. In that case we need ELB and run into same issue. I might have missed the complete solution

3. Hard to do HA in one node setup

4. I am not sure how reverse tunnel can work with REST APIs. Don't like reverse tunnel - they are security nightmare :). Although REST API clients have support for proxy. So one can proxy API requests through our tunnel servers. Client will need to put some load distribution e.g. which tunnel server to pick. Works but not a neat solution - may be better than others.
","06/Apr/17 7:52 AM;mpatel;BTW - how would this work in us.qubole.com ? Since we run in VPC there, Is there a CIDR or IP they can whitelist for that tier?","06/Apr/17 9:51 AM;mahuja;I am not sure but my uneducated guess will be that it will have the same problem. For the APIs we need an endpoint e.g. us.qubole.com and that needs to hit the load balancer. 

I am not sure if NAT gateway can accept connections originating from outside world. Even if it does (and after mapping its IP to a special endpoint/domain), we need controllers/web service/tapp to handle the incoming REST APIs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column values are truncated when multiline or single line comments are present along with query,PRES-957,57087,Bug,Open,PRES,Presto,software,stagra,,,Major,,vamship,sbadam,sbadam,28/Feb/17 4:29 PM,01/Jun/17 2:23 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,qubot-master,,,,,,,"Column values are truncated when multiline or single line comments are present along with query.

Please see below of instances when it occurred:

https://api.qubole.com/v2/analyze?command_id=58283322 
https://api.qubole.com/v2/analyze?command_id=58283444
https://api.qubole.com/v2/analyze?command_id=58284936 
https://api.qubole.com/v2/analyze?command_id=58292781

After removing comments, they are returning correct results:

https://api.qubole.com/v2/analyze?command_id=58292634
https://api.qubole.com/v2/analyze?command_id=58291462

I thought there could be problem with aliases but it seems it's not. Please see last link. We have permission to re-run queries. In case cluster is down, just give a heads-up in wikia-general slack channel to charlieb.

",,qubot,sbadam,stagra,vamship,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,All,,,,,,activision,wikia,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2017-03-20T21:33:12.000-0700"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03yjb:,,,,,,,,,,,,,,,,,,,2.0,"13025,14395",2017-03-09 19:52:32.241,,,09/Mar/17 7:52 PM;stagra;[~vamship] any progress on this? Or you still occupied with 0.168 merge?,"09/Mar/17 9:36 PM;vamship;Yes, did not get a chance. We are almost close to fixing the merge, will pick this JIRA after that.","17/Mar/17 1:49 AM;vamship;Looked at this issue today. This happens when a comment is present at the very beginning of the query. Internally scripts try to parse this SQL command and determine whether to append IOD or not. Discarding comments at the beginning should work.

On a related note, in the case of a composite queries we have a processing bug in tapp where the comments after the semicolon are treated as an another query. ex: https://qa.qubole.net/v2/analyze?command_id=1049282

I will open a PR for both of these in sometime.","17/Mar/17 2:12 AM;qubot;`Vamshi P <vamship@qubole.com>` commited to `master in qbol`
 Msg: `fix: usr: PRES-957: Preprocess SQL comments at the beginning of query.
The change removes comments only at the very beginning of a query. Comments within SQL query will be processed as before.` 
 Link: https://bitbucket.org/qubole/qbol/commits/a1a4ec59dc51d49e56ce2603c747cc0c710b7c5f","17/Mar/17 2:19 AM;qubot;`Vamshi P <vamship@qubole.com>` commited to `master in qbol`
 Msg: `Pushed this mistakenly, haven't got the PR reviewed.

Revert ""fix: usr: PRES-957: Preprocess SQL comments at the beginning of query.""

This reverts commit a1a4ec59dc51d49e56ce2603c747cc0c710b7c5f.` 
 Link: https://bitbucket.org/qubole/qbol/commits/e3989767f38dc83b4a21b3fbc0fbaa44f17ae2e1","17/Mar/17 10:15 AM;stagra;[~vamship] from the looks of it, Hive should also be impacted right?","19/Mar/17 6:51 PM;vamship;Yes, common code. Will add hive devs in PR.","24/May/17 9:51 PM;stagra;[~vamship] whats the state of this?
","24/May/17 9:56 PM;vamship;I had a PR for this long back, we can push this once we have account level feature to enable/disable comment removal flag. I will take this to completion some time soon",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data Import Query running after completion and no UI option to stop it.,MW-502,57016,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,sumitm,navdeepp,navdeepp,28/Feb/17 5:59 AM,03/Mar/17 7:46 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"The Data Import type command (58200146) is running even after the processing has been completed, there no option to stop the query from the UI.",,aswina,navdeepp,nimitk,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Feb/17 5:57 AM;navdeepp;Screen Shot 2017-02-28 at 7.26.26 PM.png;https://qubole.atlassian.net/secure/attachment/39406/Screen+Shot+2017-02-28+at+7.26.26+PM.png,28/Feb/17 5:57 AM;navdeepp;Screen Shot 2017-02-28 at 7.27.20 PM.png;https://qubole.atlassian.net/secure/attachment/39405/Screen+Shot+2017-02-28+at+7.27.20+PM.png,,,,,,,,,,,,AWS,,,,,,tubemogul,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03y3j:,,,,,,,,,,,,,,,,,,,,,2017-03-03 06:58:44.395,,,03/Mar/17 6:58 AM;aswina;[~navdeepp] - Are you still seeing this?  The UI will automatically update in few seconds once command is complete.,"03/Mar/17 7:04 AM;navdeepp;(CREATED AT	February 28, 2017 09:36
UPDATED AT	March 01, 2017 21:32)

These are the last logs when command terminated for timeout:

The command resulted in timeout.
2017-03-01 21:32:01,032 WARNING dbimportcli.py:273 - signal_handler - Waiting for JVM to terminate ...
Traceback (most recent call last):
File ""/usr/lib/hive_scripts/dbimportcli.py"", line 302, in
sys.exit(main())
File ""/usr/lib/hive_scripts/dbimportcli.py"", line 282, in main
retcode = importcli.execute()
File ""/usr/lib/hive_scripts/dbimportcli.py"", line 216, in execute
for line in iter(phive.stderr.readline, ''):
IOError: [Errno 4] Interrupted system call",03/Mar/17 7:08 AM;navdeepp;Similar command finishes in around 2 mins but this one was left stray and kept running. Processing was finished but the script dbimportcli.py did not terminate. There is no UI option to cancel DbImport commands types.,03/Mar/17 7:39 AM;aswina;[~navdeepp] - Can you give me a command id where this happens now?,"03/Mar/17 7:44 AM;aswina;This seems like a middleware issue where the process is not getting killed.  [~navdeepp] can add more info.

[~sumitm] - Can someone from MW take a look please?  cc: [~sureshr]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing Exception logs before Error Stack Trace,SPAR-1474,57004,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Minor,,mahuja,navdeepp,navdeepp,28/Feb/17 2:47 AM,10/Jul/17 3:41 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"For a spark submit job, Exception name and some logs are missing from the complete stack trace.
Command ID 57588218",,navdeepp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Feb/17 2:39 AM;navdeepp;Screen Shot 2017-02-27 at 8.35.53 AM.png;https://qubole.atlassian.net/secure/attachment/39402/Screen+Shot+2017-02-27+at+8.35.53+AM.png,28/Feb/17 2:40 AM;navdeepp;log_57588218.txt;https://qubole.atlassian.net/secure/attachment/39401/log_57588218.txt,,,,,,,,,,,,AWS,,,,,,ReturnPath,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03y0v:,,,,,,,,,,,,,,,,,,,1.0,12966,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table rename doesn't update Storage Desc Parameters Path,SPAR-1493,56919,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,ekang,ekang,27/Feb/17 2:08 PM,05/Jul/17 8:33 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"User/account info found in attached screenshot. 

Steps to reproduce bug:
1. From the Analyze UI using Spark R: create a table temp_user_store.jv_rename_test_again_2 (https://api.qubole.com/v2/analyze?command_id=56061430)

2. From the Analyze UI using Spark SQL: select * from temp_user_store.jv_rename_test_again_2 returns correct result (https://api.qubole.com/v2/analyze?command_id=56061766)

3. From the Analyze UI using Hive: rename the table to temp_user_store.jv_rename_test_again (https://api.qubole.com/v2/analyze?command_id=56061947)

4. From the Analyze UI using Spark SQL: select * from temp_user_store.jv_rename_test_again ERRORS (https://api.qubole.com/v2/analyze?command_id=56062355)

5. From the Analyze UI using Hive: however, select * from temp_user_store.jv_rename_test_again returns correct result (https://api.qubole.com/v2/analyze?command_id=56062613)

If I rename table back to the original name, then I can use Spark SQL to query it (job ids: 56063692 and 56063788).

My Notes:
I ran the following command to describe the corrupted table. https://api.qubole.com/v2/analyze?command_id=58100530 You will see that the Storage Desc Parameters:  Path is incorrect hence causing the errors seen when trying to query the table.",,ekang,psrinivas,rvenkatesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Feb/17 2:08 PM;ekang;Screen Shot 2017-02-27 at 5.06.34 PM.png;https://qubole.atlassian.net/secure/attachment/39328/Screen+Shot+2017-02-27+at+5.06.34+PM.png,,,,,,,,,,,,,All,,,,,,turner,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03xqn:,,,,,,,,,,,,,,,,,,,1.0,12782,2017-03-01 15:26:19.294,,,"01/Mar/17 3:26 PM;psrinivas;cc: [~mahuja]

[~ekang]This seems to related to how spark understands the storage desc params. These params can be very generic(like a key=value pairs). Hive will not be able to modify these params as they do not follow any structure. 

I would suggest to pass this one to Spark team to deal with this and also the stacktrace is from Spark land. ",02/Mar/17 5:45 AM;ekang;Spark set the storage desc path correctly upon table creation. Hive rename table is not updating it. It seems like a hive issue. Are you saying it's impossible for hive to fix that storage desc param?,"09/Mar/17 7:09 PM;psrinivas;[~ekang]: In a way, yes. If there is a structure associated with it, then it makes sense to modify it. Here these are a list of random params. Hive would not know the purpose of these params. ",09/Mar/17 9:42 PM;rvenkatesh;[~mahuja] Assigning to you. The problem here is that Spark is doing something spark-specific in metastore and Hive does not respect that. Is there a good reason to change our fork of hive to respect spark-specific features ? ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tunnel Server partition full,MW-482,56853,Bug,Open,MW,Middleware,software,sumitm,,,Major,,ksr,satyavathib,satyavathib,26/Feb/17 2:04 AM,17/Apr/17 4:26 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Account Id : 5507

Effected : All clusters in Prod-ETL account

like 26174, 27589, and 28152.


{code}
[ec2-user@ip-10-143-240-206 ~]$ df -h 
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        3.7G  60K  3.7G  1% /dev
tmpfs          3.7G    0  3.7G  0% /dev/shm
/dev/xvda1      7.8G  3.7G  4.0G  49% /
/dev/xvdb        30G  28G    0 100% /media/ephemeral0
[ec2-user@ip-10-143-240-206 ~]$ 
{code}


{code}
120K    .
[ec2-user@ip-10-143-240-206 ~]$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        3.7G   60K  3.7G   1% /dev
tmpfs           3.7G     0  3.7G   0% /dev/shm
/dev/xvda1      7.8G  3.7G  4.0G  49% /
/dev/xvdb        30G   28G     0 100% /media/ephemeral0
[ec2-user@ip-10-143-240-206 ~]$
{code}


{code}
[ec2-user@ip-10-143-240-206 tunneling]$ sudo du -sh * |grep -i G
4.0K    init.log
16G    server.log
11G    server-stdout.log
[ec2-user@ip-10-143-240-206 tunneling]$ df -h 
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        3.7G  60K  3.7G  1% /dev
tmpfs          3.7G    0  3.7G  0% /dev/shm
/dev/xvda1      7.8G  3.7G  4.0G  49% /
/dev/xvdb        30G  28G    0 100% /media/ephemeral0
[ec2-user@ip-10-143-240-206 tunneling]$ 
{code}",,ajayb,gmargabanthu,mpatel,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,expedia,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03xbz:,,,,,,,,,,,,,,,,,,,,,2017-02-26 06:08:54.877,,,26/Feb/17 6:08 AM;ajayb;[~ashishs] you can use this jira to add monitoring for tunnel servers and/or upgrading the expedia/pinterest tunnel servers.,"27/Feb/17 12:10 PM;gmargabanthu;Also, can we run a scheduled script to delete logs older than x days? Not sure how long of a log history do we need to keep.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
port patch for ZEPPELIN-734,ZEP-757,56834,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,beria,mpatel,mpatel,24/Feb/17 1:32 PM,07/Aug/17 10:43 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,notebook-reliability,,,,,,,"Can we port the patch from ZEPPELIN-734 ? I was running an integration test with pysparkling / H2o and I hit this error.

For now I manually modified zeppelin_pyspark.py with the changes to implement flush and repackaged /usr/lib/zeppelin/interpreter/spark/zeppelin-spark-0.6.0-incubating-SNAPSHOT.jar

",,beria,mahuja,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,All,,,,,,SocialCode,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03x7z:,,,,,,,,,,,,,,,,,,,1.0,12790,2017-02-28 17:59:34.804,,,"28/Feb/17 5:59 PM;mahuja;[~beria] can you confirm the patch is part of 0.6.2 update. It was supposed to be part of 0.6.0, did we cut the branch before that","01/Mar/17 9:05 PM;beria;Sorry missed this. Yes, ZEP-734 is part of new release. Should we give a package to the customer?
No this isn't part of q-zep-0.6.0","01/Mar/17 9:47 PM;mpatel;Hi [~beria] .. yeah, package would be nice :)",02/Apr/17 6:05 PM;mpatel;ping on this? Just following up so we can close the loop. Not asking for change in priority.,"02/Apr/17 9:57 PM;beria;We are planning to make 0.6.2 default from r-43. But we are still doing some-bug bash at our end. I will notify once we are done with it, then we can give a package of 0.6.2 to them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Results are not displaying/downloading in correct alignment with columns,MW-476,56829,Bug,Open,MW,Middleware,software,sumitm,,,Major,,ksr,megha,megha,24/Feb/17 12:04 PM,27/Jun/17 11:53 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"For command 55694740, the column ""disputelog_banknote"" has some spaces, due to which the row data gets misaligned..
When I download as csv or tab seperated the rows are still misaligned
But when I download raw results and replace ctrlA with comma, the alignment is correct. 
So there seems to be some issue with this conversion.

",,aswina,megha,satyavathib,sumitm,sureshr,yogeshg,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Apr/17 4:34 AM;satyavathib;qubole.PNG;https://qubole.atlassian.net/secure/attachment/41203/qubole.PNG,,,,,,,,,,,,,AWS,,,,,,capillary,expedia,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03x6v:,,,,,,,,,,,,,,,,,,,2.0,"12816,13643",2017-04-10 04:35:59.933,,,05/Apr/17 4:22 PM;megha;[~sumitm] Any updates on this?,"10/Apr/17 4:35 AM;satyavathib;A similar issue with this command ID too : 63767225

 !qubole.PNG|thumbnail! 

When opened with open office and set all delimiters available its showing fine , but when opened with excel it jumping across columns.","12/Apr/17 5:05 AM;satyavathib;Also adding to the above, The table has 83 columns whereas the output file shows 85 columns as per the customer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ability to programmatically update the default location of an account using sdk,SDK-184,56824,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,karthikk,ekang,ekang,24/Feb/17 8:37 AM,07/Mar/17 2:31 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"The client is trying to automate cloning of accounts and wants to customize the defloc. There is no documentation in rest api explaining how to do this. As a work around, they are hacking web requests to do this and simulating form posts. What is the programmatic way of doing this?",,aabbas,ekang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,All,,,,,,scripps,ScrippsNetwork,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03x5r:,,,,,,,,,,,,,,,,,,,,,2017-03-07 14:31:52.914,,,"07/Mar/17 2:31 PM;aabbas;Customer requested opening new zendesk to track this question and/or ETA for supporting this feature.
[~karthikk] any info would help.. thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is no way to block certain features from the UI,AD-89,56722,Bug,Open,AD,Admin,software,bmohanam,,,Major,,bmohanam,mmajithia,mmajithia,22/Feb/17 11:45 AM,26/Jul/17 11:04 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"- S3 Explorer
- Usage Dashboard
- Hive Explorer
- Smart Query

cc: [~gmargabanthu], [~sureshr]",,aswina,bmohanam,gmargabanthu,mmajithia,snamburu,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,All,,,,,,box,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03wk7:,,,,,,,,,,,,,,,,,,,1.0,15804,2017-02-23 02:36:16.657,,,23/Feb/17 2:36 AM;aswina;[~gmargabanthu] - Can you add more context to this jira?  Why should these features be blocked?,"23/Feb/17 11:12 AM;sureshr;Can you provide more context? Is this a requirement from Box?
","24/Feb/17 5:00 PM;gmargabanthu;Yes [~sureshr] this is a request from Box.

To satisfy their internal compliance, they would need a way to block access to certain resources. For Example - admins will have access only to cluster management. Similarly they would want to restrict access to Qubole Support as well.",24/Feb/17 7:28 PM;aswina;[~gmargabanthu] - Can you explore the existing roles and groups bit and get back please?  Let me know if any help is required here.  We can get on a call if required.,"26/Feb/17 11:05 PM;mmajithia;[~aswina]: I did an exploration of blocking the features from roles and groups.

For e.g. If I give scheduler only access to a set of users through roles and groups, it allows/shows the list of items mentioned in the JIRA description.

Similarly, Box wanted to have Cluster only users. 

With Dashboards product, we will have Dashboard only users.
cc: [~mohan], [~mahuja], [~kmayank]","03/Mar/17 7:34 AM;aswina;[~gmargabanthu] [~mmajithia] - After restricting access to various resources through Roles and Groups UI, even if various pages load, they won't be able to access various objects.  Do you want to remove the option itself from main navigation menu?","05/Mar/17 5:16 AM;mmajithia;[~aswina]: The list of items mentioned in the descriptions are not blocked by roles and groups functionality.

When I agree that in Smart Query, the user will not able to run the query if user doesn't have command create permission, its good we don't show certain features whenever possible.

General expectation:
- If I don't have notebook read access, it should not show Notebook in the top menu
- If the user doesn't have command create permission, Smart Query should not be in the top menu
- There should be a new permission resource for S3, by which user should be able to hide S3 Explorer or can control download or upload of a file functionality - this was asked by Autodesk and couple of other customers.
- From usage dashboard, user can monitor account level command progress and all the reports are also available in report section, but there is no way to hide usage dashboard at present

I remember the discussion we had when we implemented roles and groups functionality and we used to hide menu items whenever user doesn't have permission on specific objects. At that time we decided to default to user to Usage page if given page permission is denied, I think we should rethink the decision to move to Control Panel My account page.

","05/Mar/17 9:24 PM;aswina;[~gmargabanthu] [~mmajithia] - Hiding main menu items is not in Q1 or Q2 roadmap.  Hence, am lowering the priority.  If this is required, please bump up the priority and leave a message so that this can be prioritized accordingly.","14/Jul/17 10:37 AM;snamburu;[~aswina] [~mmajithia] Box reached out to us again regarding this feature request. Here is the use-case they are looking for:
Use Case: If a user does not have access to specific feature(s), it should be hidden from the Dashboard UI (front-end).
What are our plans on this request?
[~gmargabanthu]","16/Jul/17 10:15 PM;aswina;[~snamburu] - Each page uses a combination of permissions.  Hence, hiding them from main menu may not be feasible.  Can you get us more details on why this is a high priority item?  Also, as mentioned in a comment, it's not in Q2 roadmap.  cc: [~rangasayeec]","17/Jul/17 11:27 AM;snamburu;[~aswina] Customer's response: ""Not a blocker/critical feature request, but it's something that'd be critical for customers who needs better control in their Qubole permission.""

So it is not a current High priority issue for this customer but can we check the feasibility of this request so that we can set their expectations accordingly.
",26/Jul/17 12:02 PM;snamburu;[~aswina] Any update here?,26/Jul/17 10:44 PM;aswina;[~snamburu] - This is not prioritized at the moment.  It may not be prioritized in near future because developers in the Admin vertical are working on Packaging and Editions.  Assigning this to [~bmohanam] for prioritizing this later.,"26/Jul/17 11:04 PM;bmohanam;[~snamburu] We haven't prioritized this in Q3, and too early to provide timelines for Q4. I will update on this JIRA when we decide on just restriction vs. also hiding the features in the UI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions not being thrown for partially invalid paragraphs,ZEP-748,56717,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,vipulm,ekang,ekang,22/Feb/17 9:25 AM,05/Jul/17 8:32 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,notebook-reliability,,,,,,,"1) Create a new spark notebook
2) Make a paragraph with the following and you will see an exception
%r
x <- this_line_causes_an_error
df <- data.frame(col1=c(1,2), col2=c(3,4))
3) Make a new paragraph with the following and you will NOT see an exception
%r
x <- this_line_causes_an_error
df <- data.frame(col1=c(1,2), col2=c(3,4))
df


The same thing works as expected in Analyze(Spark Command - R)/ SparkR shell.",,ekang,mahuja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,All,,,,,,turner,,,,,"{repository={count=1, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":1,""lastUpdated"":""2017-04-28T00:33:59.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03wj3:,,,,,,,,,,,,,,,,,,,1.0,12853,2017-03-13 14:13:13.095,,,"13/Mar/17 2:13 PM;mahuja;The behavior with R interpreter is like bash one but unlike Spark interpreter. 

If there is an error in R paragraph, the execution continues. But in case of Spark interpreter the execution halts on first error. 

[~vipulm] - any idea on this behavior?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI not available when graphframes package is used,SPAR-1443,56016,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,puneetg,satyavathib,satyavathib,14/Feb/17 6:41 AM,04/Aug/17 7:25 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"Related to :

https://qubole.atlassian.net/browse/SPAR-1330

{code}
I see in the logs that ""spark.master"" was set to ""local2"". As far as I know we never set this setting to ""local"". It must have done by customer somewhere. Expected value of this field is ""yarn"".
I have tried similar command on our dev system it went fine and it was able to generate UI Link. Since this spark.master=local it is going through a totally different flow where this doesn't get generated.
I couldn't find this spark default override set anywhere on job level or at cluster config level. Most probably when they got this issue they must have set it at cluster level and now they have removed it. Because I can see spark UI for their latest jobs that they ran.
{code}

As per The customer only change was using  graphframes packages and he has not set anything at the cluster level . And he is able to see the spark UI when he is not using graph frames package.

command Id : 43362040
Spark 1.6
Customer : jaiganesh.prabhakaran@oracle.com

Let me know if I am missing something here.",,puneetg,rohitk,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03u73:,,,,,,,,,,,,,,,,,,,1.0,11075,2017-02-15 23:50:01.245,,,14/Feb/17 6:46 AM;satyavathib;CC : [~puneetg],"15/Feb/17 11:50 PM;puneetg;We have asked customer to run two queries with and without GraphFrames, to make sure that we are targeting correct problem or if it is even really exists.","22/Feb/17 2:19 AM;satyavathib;[~puneetg] without graphframe dependency is: 56973633 

package having graphframe dependency: 
56974794

So as per the customer when they use graphframes package that is running lacally.

Also seems like customer is facing issues with using graph frames with spark 2.0. This part is awaiting for the command ID from customer. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub logic to identify bad nodes need improvement ,ACM-934,55836,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,sankets,adubey,adubey,10/Feb/17 10:37 PM,17/Jun/17 1:05 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"cl: 26213

I manually killed DNs on 9 nodes ( one of them to test : ec2-54-153-119-122.us-west-1.compute.amazonaws.com )
Cluster page shows 31, DFS page shows 22 and RM shows 22. that means hadoop has released those 9 nodes but we are not able to terminate them . Can we force clean them if for some reason scrub does not recognize them ..
Asignign to you hari as you already looked into it.



",,adubey,ajayb,hiyer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,autodesk,oracle,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03tcf:,,,,,,,,,,,,,,,,,,,2.0,"12053,15173",2017-02-10 22:56:56.97,,,10/Feb/17 10:55 PM;adubey;thanks for manually taking care of this . Do you think we need to handle this in our code ?,"10/Feb/17 10:56 PM;hiyer;Problem seems to be that in the get_active_datanodes function we're considering only those nodes for which the decom_status is normal: if (decom_status and (decom_status.lower() == 'normal'))  

whereas these nodes were stuck in ""decommission in progress"" state:
{code}
2017-02-11 06:42:34,865 >>> node ip 10.47.7.39, decom status decommission in progress, dfs used 0 (0 B)
2017-02-11 06:42:34,867 >>> node ip 10.47.7.31, decom status decommission in progress, dfs used 0 (0 B)
2017-02-11 06:42:34,870 >>> node ip 10.47.7.98, decom status decommission in progress, dfs used 0 (0 B)
2017-02-11 06:42:34,873 >>> node ip 10.47.7.124, decom status decommission in progress, dfs used 0 (0 B)
2017-02-11 06:42:34,874 >>> node ip 10.47.7.68, decom status decommission in progress, dfs used 0 (0 B)
2017-02-11 06:42:34,877 >>> node ip 10.47.7.60, decom status decommission in progress, dfs used 0 (0 B)
2017-02-11 06:42:34,883 >>> node ip 10.47.7.13, decom status decommission in progress, dfs used 0 (0 B)
2017-02-11 06:42:34,886 >>> node ip 10.47.7.51, decom status decommission in progress, dfs used 0 (0 B)
2017-02-11 06:42:34,890 >>> node ip 10.47.7.86, decom status decommission in progress, dfs used 0 (0 B)
{code}","10/Feb/17 10:58 PM;hiyer;Yes, we should fix this.",12/Feb/17 11:44 PM;hiyer;Assigning to acm team. Fix is mentioned above and should be simple. Please triage as required.,"23/Feb/17 9:39 AM;ajayb;[~hiyer] is it safe to terminate nodes that are in ""decommission in progress""? I thought this state means hdfs is still replicating blocks out of this node. By not waiting for this to complete i.e. not waiting for node's decommissioning state to become ""normal' wouldn't we reduce hdfs fault tolerance or in worst case cause data loss?","23/Feb/17 11:04 PM;hiyer;The scrub happens only when there has been no heartbeat for 20 minutes, which usually means the DN service is dead. So it's safe to remove the node.","13/Jun/17 7:20 PM;adubey;cc [~p.vasa]

Folks we saw another case today where a cluster ( oracle's) did not get scrubbed because it seemed to have healthy DFS but did not get NMs up for a longer time ( until user manually initiated a shutdown )
Location - s3://dlx-dev-core-consumer/qubole/dev-tech-core-consumer/logs/hadoop/30938/500463/
Cl: 30938
inst: 500463

We dont even have NM logs backed up so definitely something went wrong. 
1. Can we improve our scrub process to take care of such cases. 
2. How can we debug further why NMs did not come up ( when we have no NM logs )

","17/Jun/17 1:04 AM;adubey;After seeing this incident again - i found that bootstrap was stuck due to a bad logic - but ideally we should add some alerting/scrub here .

cc [~bmohanam]",17/Jun/17 1:05 AM;adubey;[~sankets] - are you planning to work on this?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.jets3t.service.impl.rest.HttpException ,SPAR-1441,55834,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,shridhar,Ranjana,Ranjana,10/Feb/17 5:47 PM,26/May/17 4:39 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Certain customer reported this issue transiently , but we were unable to capture logs right then. I saw this just now for a sparkSQL paragraph in notebook belonging to Scripps and thought I will log a jira to document this issue. 

Saw this with notebook : 29192 belonging to Scripps network.
Account id: Ad Marketing Sales, 6654
Cluster id: 27141
Cluster instance id: 372692

Scripps did not raise a ticket. I saw it as I was working on one of their notebooks notebook. Nextdoor had raised a ticket, but then, I couldn't find back-up logs for that cluster id in the expected location in their s3. 

easy_hustler --cluster-id 27141 sshmaster -u 'ec2-user' qbol_acc6654_cl27141

saw for sparkSQL in paragraph

%sql

CREATE TABLE videos
USING com.databricks.spark.csv
OPTIONS (path ""s3n://sni-ddsp/load/MAM/video/"", header ""true"", inferSchema ""true"", quote '?')

org.jets3t.service.impl.rest.HttpException
	at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRequest(RestStorageService.java:537)
	at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRestHead(RestStorageService.java:899)
	at org.jets3t.service.impl.rest.httpclient.RestStorageService.getObjectImpl(RestStorageService.java:2012)
	at org.jets3t.service.impl.rest.httpclient.RestStorageService.getObjectDetailsImpl(RestStorageService.java:1939)
	at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:2471)
	at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:1671)
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:210)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:250)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at org.apache.hadoop.fs.s3native.$Proxy37.retrieveMetadata(Unknown Source)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:964)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(NativeS3FileSystem.java:845)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:185)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:152)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:152)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:152)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:226)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableUtils$.createDataSourceTable(createDataSourceTables.scala:501)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:105)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:186)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:65)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:158)
	at org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:104)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:316)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:182)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:160)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)",,mahuja,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Feb/17 6:03 PM;Ranjana;zeppelin-interpreter-spark--spark-ip-10-49-230-131.log.excerpt.rtf;https://qubole.atlassian.net/secure/attachment/38509/zeppelin-interpreter-spark--spark-ip-10-49-230-131.log.excerpt.rtf,,,,,,,,,,,,,AWS,,,,,,Nextdoor,scripps,ScrippsNetwork,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03tbz:,,,,,,,,,,,,,,,,,,,2.0,"12539,12651",2017-02-13 16:35:07.323,,,"10/Feb/17 6:04 PM;Ranjana;If this is expected behavior, please feel free to close jira, do let me know the explanation. Thank you. ","13/Feb/17 4:35 PM;mahuja;[~Ranjana] looking at the log attached, there are 403/Forbidden access errors. Is this a customer manage account? One weird thing in the error is that it is pointing to prod.qubole.com bucket that belongs to us.

{code}
WARN [2017-02-11 01:45:04,777] ({pool-2-thread-6} RestStorageService.java[performRequest]:434) - Error Response: HEAD '/production_ec2-user_hu_5134%2Fwarehouse%2Fvideos-__PLACEHOLDER__' -- ResponseCode: 403, ResponseStatus: Forbidden, Request Headers: [Content-Type: , x-amz-request-payer: requester, x-amz-security-token: FQoDYXdzEJr//////////wEaDC12O74l2gn+T+KaXyK3A8i68jPRPWOud3Ta2Ux2SDv9CYOx4I9jmlJmQMcYo07FVM/AiFwvN6ri4ZzsB3mtfVoG9fti6uq9/Crc6vMjEXP7Dfb/lurFVT9BCohjjMR2RxhtjjatWNgb7cRxhSbyWBvBHdFHa7M6FqzLdIAhDL+tHRvw0/AR+YId17LDgWdG/7pkRmhH90FwH/IdohyT3ykN8gojGCob50ABMt91a8N52CCa/fMPTFPcDL63MHc8LDXSnN8QZsoqAeOM9AXfAXOl5Fr/emSEyx2emMUlclz46tFvlv0wrHGy43ySw1YnUif3LJgqxunfPDkbhoeQoxZuqiIRswS9YgdxZeF+uARTOZO3HULKXa91GsQjqvSYOJ9QnoaPICrjNcUz9u53OglJ7qiGsE1IP37h1o8H2fI2y1Wj8tqPtJzgFvn5zsGnc3s8hgoXDIV3P1Hmi5U22vxn2N9lb5j4Hjc1oDwcpRwip0fY56NyBCJatDpdlkEvy5L5405I4LLNtuw3GW3zFF07skcY86mCVJGjX/im42xC/oPgu2K0Gbfv8fuf8nL4qr3vU4I1CM+vcSW9/9lUUdhxBqhAAckoyrn5xAU=, Date: Sat, 11 Feb 2017 01:45:04 GMT, Authorization: AWS ASIAJZHVGXNOMLT37IFA:ljW/nGFgkIOn07YLXifjC+6MP2o=, User-Agent: AWS-ElasticMapReduce, Host: prod.qubole.com.s3.amazonaws.com], Response Headers: [x-amz-request-id: CD812EBD154083D2, x-amz-id-2: s6++UowdB47nSZHG/VZZyx2fZTQalL45LDmmbcb0vay/o2J+rxczfXYpyXlnlxz+R4dSdDcMwxI=, Content-Type: application/xml, Transfer-Encoding: chunked, Date: Sat, 11 Feb 2017 01:45:07 GMT, Server: AmazonS3]
ERROR [2017-02-11 01:45:04,781] ({pool-2-thread-6} Job.java[run]:195) - Job failed
{code}

{code}
Caused by: org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: Request Error. HEAD '/production_ec2-user_hu_5134%2Fwarehouse%2Fvideos-__PLACEHOLDER__' on Host 'prod.qubole.com.s3.amazonaws.com' @ 'Sat, 11 Feb 2017 01:45:07 GMT' -- ResponseCode: 403, ResponseStatus: Forbidden
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:234)
{code}

{code}
Caused by: org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: Request Error. HEAD '/production_ec2-user_hu_5134%2Fwarehouse%2Fvideos-__PLACEHOLDER__' on Host 'prod.qubole.com.s3.amazonaws.com' @ 'Sat, 11 Feb 2017 01:45:07 GMT' -- ResponseCode: 403, ResponseStatus: Forbidden
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:234)
{code}",14/Feb/17 12:21 AM;Ranjana;[~mahuja] I checked the account's defloc. It is s3://sni-qubole-ad-marketing-services/defloc. The path specified in the query is also different s3n://sni-ddsp/load/MAM/video/. The path prod.qubole.com... does not seem specified anywhere. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When job is stopped from scheduler UI, rerun throws an error",SCHED-138,55734,Bug,Open,SCHED,Scheduler,software,sumitm,,,Major,,ksr,megha,megha,09/Feb/17 3:42 PM,19/Jun/17 2:16 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"I repro this in my account for scheduler id: 15574
job run id: https://api.qubole.com/v2/analyze?command_id=54897466

The error message is:
Error when rescheduling job: Instance 1 of spark-cancel-repro is processing

steps to repro:
create a scheduler (I tried spark)
When an instance is running, stop it from ""scheduler UI""
Click on re-run on the cancelled instance..
Above error will appear.


",,megha,rahulg,,,,,,,,,,,,,,,,,,,,,,,,,,,,ACL-1041,,,,,,,,,,,,,,,,,AWS,,,,,,scripps,thomsonreuters,,,,{},NA,Choose from,,SCHED-149,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03syn:,,,,,,,,,,,,,,,,,,,2.0,"12619,14540",2017-02-10 06:30:12.751,,,"10/Feb/17 6:30 AM;rahulg;[~megha].. I could not see the issue in your account. After you cancelled the first one and I see a successful rerun command with ID:54964104 =>  https://api.qubole.com/v2/analyze?command_id=54964104

So, I looked into the customer's account and here is what I think happened:

1.  The customer manually killed the a scheduled job instance with query_hist_id = 54482911 which was in processing state meaning DJ worker was executing the command. DJ updated the query_hist status to `cancelled`  but failed to update the periodic_job_units status to `cancelled` (its stll in `processing` state). cc: [~yogeshg] 


{code:java}
production-replica> select id, query_hist_id, periodic_job_id, status, done from periodic_job_units where periodic_job_id = 10602 order by created_at desc limit 1 ;
+---------+---------------+-----------------+------------+------+
| id      | query_hist_id | periodic_job_id | status     | done |
+---------+---------------+-----------------+------------+------+
| 3243370 |      54482911 |           10602 | processing |    0 |
+---------+---------------+-----------------+------------+------+

production-replica> select id, periodic_job_id, status from query_hists where id = 54482911;
+----------+-----------------+-----------+
| id       | periodic_job_id | status    |
+----------+-----------------+-----------+
| 54482911 |           10602 | cancelled |
+----------+-----------------+-----------+
1 row in set (0.00 sec)

{code}

2. We don't allow re-run of the instance which is in processing state.  thats why the user is getting the error: 
* Error when rescheduling job: Instance 4465 of prod1-enrichment-hourly is processing

Action that need to be taken:

1. manually update the status of this periodi_job_units to `cancelled`. And before that, ask the customer if he wants to skip all the missed instances. If yes,  we also need to update the `no_catch_up` flag for that periodic_job to `true`.

{code:java}
update periodic_jobs set no_catch_up  = 1 where id = 10602 limit 1;       # if customer wants
update periodic_job_units set done = 0, statue = ""cancelled"" where id =  3243370 limit 1;
{code}

[~sumitm].. I am surprised that we have not received  any scheduler alert for this scenario.
","10/Feb/17 6:36 AM;rahulg;[~megha]
the customer has also asked the below queries:

{code:java}
Yes, my cloned (15573) of 10602 is running fine. But I would like to use the same name “prod1-enrichment-hourly” for the cloned one but I can’t because it is already used by 10602. Another question is what become of 10602 afterward? “Stopped”? Is it possible to to revive 10602 and remove the cloned one?
{code}

He can edit the name of `10602` to something else and set   `15573` to `prod1-enrichment-hourly`. That should work. 

we can revive the periodic job id `10602` using the action I have mentioned in my previous comment.
",10/Feb/17 6:37 AM;rahulg;[~sumitm] ..we need to further investigate on why DJ failed to update the status in `periodic_job_units` table.,"02/May/17 2:49 PM;megha;[~rahulg]
Any updates on this? 
Even if we don't find cause of why DJ failed to update periodic_job_units table, we should probably have a better way to handle this scenario? 

For example., next run should continue and not get stuck, and reruns should be allowed?","12/May/17 12:02 PM;megha;[~sumitm]
Saw this again, this time for scripps.
scheduler id: 11669

production-replica> select id, query_hist_id, periodic_job_id, status, done from periodic_job_units where periodic_job_id = 11669 order by created_at desc limit 1 ;
+---------+---------------+-----------------+------------+------+
| id      | query_hist_id | periodic_job_id | status     | done |
+---------+---------------+-----------------+------------+------+
| 3243383 |      54482961 |           11669 | processing |    0 |
+---------+---------------+-----------------+------------+------+

production-replica> select id, periodic_job_id, status from query_hists where id = 54482961;
+----------+-----------------+-----------+
| id       | periodic_job_id | status    |
+----------+-----------------+-----------+
| 54482961 |           11669 | cancelled |
+----------+-----------------+-----------+
1 row in set (0.00 sec)

The customer didn't notice that this was failing for a long time, so lot of their reruns were missed. 
I've told them to clone and run the schedule.. 
But can we fix this asap? 

",15/May/17 8:22 AM;rahulg;[~ksr] lets sync up tomorrow on this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to show more logs for metastore based queries.,MW-418,55514,Bug,Open,MW,Middleware,software,sumitm,,,Major,,psrinivas,satyavathib,satyavathib,07/Feb/17 11:32 PM,08/Feb/17 2:04 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"The ALTER table query run from UI page only show the log :

Processing file s3://analytics-qubole-prod/prod-adhoc/scripts/bootstrap
Processing file /tmp/mdhist20170202-29588-1riku4o

1. and the customer does not have any clue about the actual query progress. We need to go to hive logs to confirm on what is actually happening. 

{code}
ip-10-150-48-249 2017-02-03 05:20:19,068 [command:53829247] INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(748)) - 0: get_prtition_with_auth : db=dm tbl=omniture_transactions_hourly[2016-12-26,EBOOKERS.DE,23]
ip-10-150-48-249 2017-02-03 05:20:19,139 [command:53829247] INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(748)) - 0: get_dtabase: dm
ip-10-150-48-249 2017-02-03 05:20:19,265 [command:53829247] INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(748)) - 0: get_prtition_with_auth : db=dm tbl=omniture_transactions_hourly[2016-12-26,EBOOKERS.FI,00]
ip-10-150-48-249 2017-02-03 05:20:19,334 [command:53829247] INFO  metastore.HiveMetaStore (HiveMetaStore.java:logInfo(748)) - 0: get_dtabase: dm
{code}

2. Also the command is killed explicitly. I Think it would be better if we can show some message at the end of the logs  stating the state of the command.

Account ID : 5497

Please let me know if any details required.
Command ID : 53829247

In this case the command was running for more than 5h and the customer was in an impression that the command was not progressing at all looking at the logs. And he killed the command in that impression.

",,satyavathib,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,expedia,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03sbz:,,,,,,,,,,,,,,,,,,,,,2017-02-08 02:04:17.836,,,08/Feb/17 2:04 AM;sumitm;Looks like a hivecli thing to me.. [~psrinivas] can you check?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dbexportcli.py failing - add better logging for end user,QBOL-6069,55141,Bug,Open,QBOL,qbol,software,sumitm,,,Critical,,ksr,ekang,ekang,01/Feb/17 2:38 PM,09/Feb/17 9:02 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Scripps is having a critical issue where they can't export data from hive to an aurora db. They are using IAM roles (I see other closed tickets related to this) so I'm bringing it up. The hive table has all the necessary partitions. We see one of 2 errors based on the partitions we specify.

CommandId: 53655435
File ""/usr/lib/hive_scripts/dbexportcli.py"", line 369, in get_export_location_for_orthodox_cases
raise RuntimeError(""Failed to run the hive query that fetches the data"")

or

CommandId: 53654403 
File ""/usr/lib/hive_scripts/dbexportcli.py"", line 138, in __check_hive_partition_and_get_location
raise RuntimeError(""Failed to find specified partition for the hive table"")

I also attached a screenshot of the logs found on web node.

",,aabbas,adubey,ekang,ksr,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Feb/17 2:37 PM;ekang;screen_shot_2017-02-01_at_2.08.40_pm.png;https://qubole.atlassian.net/secure/attachment/37907/screen_shot_2017-02-01_at_2.08.40_pm.png,,,,,,,,,,,,,AWS,,,,,,scripps,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03qpj:,,,,,,,,,,,,,,,,,,,,,2017-02-07 11:36:26.727,,,07/Feb/17 11:26 AM;ekang;[~sureshr] I tried looking into this issue but I couldn't make any progress. Will you be able to address this ticket? Thanks.,07/Feb/17 11:36 AM;sureshr;[~ksr]: could you please take a look at this? cc:[~sumitm] and [~yogeshg],"07/Feb/17 9:03 PM;ksr;Yes, taking this up immediately.","07/Feb/17 9:57 PM;ksr;[~ekang] During export data, we create a hivecli object to run few hive queries. These commands are failing in executing those queries. I'll need to check the hive logs for the failed commands. I tried to do that, but the web nodes on which the commands were run do not exist anymore. So, we'll have to rerun the commands and check the logs at the same time to see what the issue is. Do you think we could do that?","08/Feb/17 7:09 AM;ekang;[~ksr] Yes, the command Id is above in description. Please rerun it.","08/Feb/17 7:16 AM;adubey;[~ksr] i also looked into this issue but problem was i could not figure the location of hive error logs and that was a big issue.
We should actually add more logging and bubble up the info on the UI - this is important for our ongoing NPS efforts",08/Feb/17 11:13 AM;aabbas;Scripps are asking for the ETA to plan their onboarding activities. You mind providing ETA for fix and release of the fix?,08/Feb/17 7:46 PM;ksr;I still have to figure out the cause for this issue. I think I should be able to find a fix by early next week.,"09/Feb/17 1:39 AM;ksr;[~ekang] [~aabbas]
The issue here is that while creating the hive table they have used org.apache.hadoop.hive.serde2.OpenCSVSerde serde, which is supported in hive 0.14 and later versions.They used hadoop2 cluster for this, which used hive version 1.2. (Please refer to this doc for this: https://cwiki.apache.org/confluence/display/Hive/CSV+Serde)

I see that they are running the export command on our pixie cluster which is a hadoop1 cluster, using hive 0.13. Hive 1.2 is not supported with hadoop1. So, the only way I think export on this table can work for them is to use their own hadoop2 cluster for the job. For this, {noformat}tapp.enable_customer_cluster_dd{noformat} feature needs to be set to 1 for their account. After this is enabled, they'll see a 'Use Hadoop Cluster' checkbox which will enable them to select their hadoop cluster to run the command on. Running it on a hadoop2 cluster should solve their problem. Can one of you enable this feature for their account?

Please let me know if you need any clarification/details.",09/Feb/17 3:19 PM;ekang;[~ksr] That worked. Thank you!,"09/Feb/17 4:25 PM;aabbas;Thank you [~ksr] - if this is not a bug anymore please feel free to close it.

[~adubey] suggested improving logging, up to you if you want to use this ticket or a new one to track that request.

","09/Feb/17 4:30 PM;adubey;Yes, i think this JIRA could be used for that ( i changed the title of the same ). logging is extremely important because it was just impossible to dig in the root cause here which is really a bad experiment for user.","09/Feb/17 9:01 PM;ksr;[~ekang] Was the command successful? I see that you reopened SQOOP-79 because the customer was seeing 'Communications link failure'. Was that resolved?
Also, could you please give me the account id for which the setting was enabled?

[~aabbas] [~adubey] I'll use this JIRA for improving logging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive query throws different result on MR and tez,HIVE-1886,55004,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Critical,,adeshr,biswajit,biswajit,31/Jan/17 10:51 PM,30/May/17 9:16 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"A hive query throws different result on MR and Tez. 

{code}

add file s3://saavnlogs/lib/redSelectRowsJijoCompatible.php;
		explain select count(distinct users_and_regdates.uid, users_and_regdates.platform)
		from
		(
			select uid,platform, regdate, saavn_uid, country
			from
			(
				select all_new_users.uid as uid, all_new_users.platform as platform, users_with_latest_regdates.regdate as regdate, users_with_latest_regdates.saavn_uid as saavn_uid,
					users_with_latest_regdates.country as country
				from
				(
					select uid, platform
					from
					(
						select uid, platform,sum(case when new_user_flag = 'new' then 1 else 0 end) as new
							from 
								daily_active_users_summary_json_partition_dt 
								where 
								not (country in ('IN', 'PK', 'AF', 'BT', 'NP', 'BD', 'LK', 'MM', 'GQ', 'SC', 'LY', 'BW', 'GA', 'NA', 'CV', 'SZ', 'AO', 'NG', 'GH', 'SD', 'ZW', 'MR', 'CM', 'LS', 'GM', 'GN', 'DJ', 'SN', 'UG', 'TD', 'KM', 'ST', 'TG', 'CI', 'RW', 'CD', 'MZ', 'BF', 'CF', 'BJ', 'KE', 'ML', 'ER', 'ZM', 'MG', 'SL', 'LR', 'NE', 'ET', 'GW', 'BI', 'TZ', 'CD', 'SO', 'MW')) and 
								(app_version like '5.4%')  and 
					  			platform in ('iphone','ipad') and
								dt >= '20170126' and dt <= '20170128' and 
								abpc is not NULL 
								group by uid,platform
					)temp_1
		
					where new !=0
				)all_new_users
		
				inner join
				(
					select uid, platform, regdate, saavn_uid, country
					from
					(
						from
						(
							select uid, saavn_uid, platform, regdate,dt, country 
							from  
								daily_active_users_summary_json_partition_dt 
								where 
									not (country in ('IN', 'PK', 'AF', 'BT', 'NP', 'BD', 'LK', 'MM', 'GQ', 'SC', 'LY', 'BW', 'GA', 'NA', 'CV', 'SZ', 'AO', 'NG', 'GH', 'SD', 'ZW', 'MR', 'CM', 'LS', 'GM', 'GN', 'DJ', 'SN', 'UG', 'TD', 'KM', 'ST', 'TG', 'CI', 'RW', 'CD', 'MZ', 'BF', 'CF', 'BJ', 'KE', 'ML', 'ER', 'ZM', 'MG', 'SL', 'LR', 'NE', 'ET', 'GW', 'BI', 'TZ', 'CD', 'SO', 'MW')) and 
									(app_version like '5.4%')  and 
									platform in ('iphone','ipad') and
									dt >= '20170126' and dt <= '20170128' and 
									regdate is not NULL
								order by uid, saavn_uid,platform, dt desc
		
						)temp_2
						select transform('cico', 'top','1','2',uid, platform, regdate, saavn_uid, country) 
					  		using 'redSelectRowsJijoCompatible.php' as 
					  		uid, platform, regdate, saavn_uid, country, flag
					 )top_selected
					 where flag = 1
				)users_with_latest_regdates
		
				on(all_new_users.uid = users_with_latest_regdates.uid and all_new_users.platform = users_with_latest_regdates.platform)
			)joined_table
		
			where (regdate >= '2017-01-26 00:00:00' and regdate <= '2017-01-28 23:59:59')
		
		)users_and_regdates
		
		inner join
		
		(
			select distinct saavn_id, platform
			from
				active_subscriber_per_day_json_partition_dt 
				where 
					dt = '20170128' and 
					username is not null and 
					platform in ('iphone','ipad') and 
		  			create_time >= '2017-01-26 00:00:00' and 
					product = 'PRIME' and 
		  			first_subscriptional_purchase = '20170128'
		
		)pro_users
		
		on (users_and_regdates.saavn_uid = pro_users.saavn_id and users_and_regdates.platform = pro_users.platform);

{code}

Command id :-

MR:- 53294311
Tez:- 53287722


Tried to set auto convert join to false, but it did not helped on much way 

MR:- 53556773
Tez:- 53555761

",,adeshr,asomani,athusoo,biswajit,drose@qubole.com,psrinivas,rvenkatesh,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AWS,,,,,,saavn,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03qef:,,,,,,,,,,,,,,,,,,,1.0,12373,2017-01-31 23:10:20.483,,,"31/Jan/17 11:10 PM;asomani;[~biswajit] Can you help us break down the query into the offending part. For example, can you help us narrow down which subquery is giving the wrong result.",31/Jan/17 11:14 PM;biswajit;Sure [~asomani],"06/Feb/17 10:37 PM;drose@qubole.com;Here is some additional queries with discrepancies.

Adding these two commands also to the list. We saw the same issue with them. 
map reduce : 53584864 
tez: 53583419

","06/Feb/17 10:46 PM;drose@qubole.com;business impact, ""this is reducing customer confidence in our Tex offering"".","27/Feb/17 2:24 AM;venkatak;[~biswajit][~drose@qubole.com][~asomani] I think we have few examples here:

1:

map reduce : 53584864 
tez: 53583419

2:

MR:- 53294311
Tez:- 53287722


3:

Tried to set auto convert join to false, but it did not helped on much way 

MR:- 53556773
Tez:- 53555761


What is required to get this progressing?",01/Mar/17 10:04 PM;rvenkatesh;[~asomani] Can you please look at the latest examples ? ,"10/Mar/17 8:02 AM;drose@qubole.com;[~asomani][~rvenkatesh]had the customer request an update, do we have additional information or expectations to share?","10/Mar/17 8:26 AM;rvenkatesh;Hi Daniel,
This had fallen in priority because of other JIRAs. I have moved the JIRA
to critical and assigned it to Adesh to follow up. We will have an update
soon.
Sorry for the delay.
On Fri, 10 Mar 2017 at 21:33, Daniel Rose (JIRA) <jira@qubole.atlassian.net>

","15/Mar/17 10:53 PM;adeshr;Update:

1:
map reduce : 53584864 
tez: 53583419

set auto convert join to false will produce correct results.

2:
MR:- 53294311
Tez:- 53287722
Queries are different. 

3:
MR:- 53556773
Tez:- 53555761
Queries are different.","22/Mar/17 9:46 PM;drose@qubole.com;[~rvenkatesh][~adeshr] We are meeting with the CTO and CFO on Tuesday to work out a formal contract commitment, do we have some information I can share with them? To be clear they are doing all of their Tez work off Qubole and back on EMR atm.  I believe sales goals will be an attempt to win all of their business.

Please let me know some fair expectations that I can set.","22/Mar/17 9:56 PM;rvenkatesh;[~drose@qubole.com] Is it possible to use the work around suggested by [~adeshr] ? The work around turns off the join but they can continue to use Tez. 

In the meantime [~adeshr] is looking into it. 

[~adeshr] can you check open source for bug fixes ?
Also please try to get reproducer. ","22/Mar/17 10:00 PM;drose@qubole.com;ok, thanks [~rvenkatesh]what is the effect of adding that parameter to be transparent with the customer? I will absolutely volunteer that.",23/Mar/17 2:35 AM;rvenkatesh;Some queries may run slower than before. ,28/Mar/17 8:11 AM;athusoo;Is this an open source bug? If so how would running on EMR work for Saavn?,"28/Mar/17 11:44 AM;rvenkatesh;EMR runs 1.2.1 and 2.1. QDS runs 1.2.0. So it's possible that the issues
are fixed in other distributions or versions.
We have also found that these versions are patched to different extent. For
e.g. HDP has added over 500 commits to what they call Hive 1.2.

The only source of truth is JIRAS. So it is possible that this maybe an
open source bug and other distributions have a fix while we don't.
On Tue, 28 Mar 2017 at 20:41, Ashish Thusoo (JIRA) <

","29/Mar/17 9:31 PM;adeshr;Setting ""hive.ppd.remove.duplicatefilters"" to ""false"" will give correct results. This is a workaround and it will have almost negligible performance impact. 

This flag decides whether to remove the original filter after predicate pushdown in operator tree.","18/Apr/17 4:17 AM;adeshr;Looked deeper into it. The query contains an inner join and is using MapJoin operator for it. Because of this, the small table is stored as a HashMap. This HashMap stores the join key and corresponding value to it.
The issue is, for the customer's query, the ""value"" contains two columns of small table (one after the other and fetched using an offset). One of the column is needed for ""where"" clause filter, whereas the other one is for the ""select"" operator. While fetching the column from ""value"" from the HashMap, wrong offset is used, and hence hive outputs wrong column for the query.","18/Apr/17 6:10 AM;rvenkatesh;For context, [~adeshr] has spent the better part of 3 weeks to investigate why the offsets are off. He will take a break before coming back to the problem. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Presto query from Tableau fails with PrestoException,ODBC-116,54932,Bug,Reopened,ODBC,odbc,software,stagra,,,Minor,,stagra,ajithr,addon_zendesk_for_jira,30/Jan/17 6:05 PM,13/Jul/17 5:55 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Please check the query ID 53121066 failure. Error message:

Query 20170129_063938_00028_7sysc failed: Invalid format: ""2017-01-21T11:01:44.499+0400"" is malformed at ""T11:01:44.499+0400""
com.facebook.presto.spi.PrestoException
com.facebook.presto.type.DateOperators.castFromSlice(DateOperators.java:139)
com_facebook_presto_$gen_PageProcessor_81.project_0(Unknown Source)
com_facebook_presto_$gen_PageProcessor_81.process(Unknown Source)
com.facebook.presto.operator.ScanFilterAndProjectOperator.getOutput(ScanFilterAndProjectOperator.java:259)
com.facebook.presto.operator.Driver.processInternal(Driver.java:380)
com.facebook.presto.operator.Driver.processFor(Driver.java:303)
com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:577)
com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:529)
com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:665)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
java.lang.Thread.run(Thread.java:745)
Time taken: 0.855244841 seconds




Qubole Public User Groups/ KBs: https://qubole.zendesk.com/hc/en-us   Help Docs: http://docs.qubole.com/en/latest/

",,addon_zendesk_for_jira,ajithr,stagra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAF,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03pyf:,,,,,,,,,,,,,,,,,,,1.0,12384,2017-01-30 20:00:14.59,,,"30/Jan/17 8:00 PM;stagra;The query is wrong as the date type is  yyyy-mm-dd. 

This should be fixed in the workbook. The column should be considered as varchar and then interpreted as date in tableau:

DATEPARSE seems to be one answer: https://community.tableau.com/thread/173259
This needs to be tried out but looks like Tableau supports ISO8601 date in text now: https://community.tableau.com/thread/218131

","30/Jan/17 10:59 PM;ajithr;Thanks [~stagra], I am checking this with the customer.","31/Jan/17 3:03 AM;ajithr;[~stagra], I think what we need to do as a workaround here is the following:

1. Create an additional column, apart from the ISO column in MAF's ETL to hold UTC timestamp:

- mafsampledata_orc_new.lastseen = ISO
- mafsampledata_orc_new.lastseen_utc = UTC

2. As part of the ETL, convert the ISO date to UTC and store in the new column. Use Hive to do this. Something like this:

{code:java}
TO_UTC_TIMESTAMP(UNIX_TIMESTAMP(mafsampledata_orc_new.lastseen, ""yyyy-MM-dd'T'hh:mm:ss.SSS'Z'"") * 1000, '<timezone>')

{code}

3. Run the Presto query and include this in the query:

{code:java}
CAST(mafsampledata_orc_new.lastseen_utc AS DATE)
{code}

4. Check if #3 above fails or not.","01/Feb/17 1:19 AM;ajithr;We decided not to pursue the above method, but instead use `LEFT` function to prune the first 10 characters of the ISO date timestamp.

For this, we edited the tdc file to include the following line:

{code:java}
<customization name='SQL_STRING_FUNCTIONS' value='3353' />
{code}

in the Presto tdc file. This worked for LEFT, MID and RIGHT functions. Raised ODBC-118 for this to be added in to the tdc file.

Leaving this JIRA open for investigation into DATEPARSE and other options to handle ISO type timestamps. I know that with changes in ODBC-118, we can use LEFT, MID and RIGHT functions to format DATE in the query, but, still, a better and cleaner way would be preferred and should be checked. 

Lower priority though.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIMIT query on partition with large number of small files fails with weird error,HIVE-1872,54798,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Minor,,psrinivas,Ranjana,Ranjana,27/Jan/17 1:41 PM,06/Jun/17 11:06 PM,09/Aug/17 6:03 AM,,,,,0,bootcamp,jira_escalated,,,,,,," Command id: 52811125 
select *
from bid_opportunities
where random_hash = '9'
and date = '2017-01-22'
and hour = '23'
limit 100;

and 

52806865
set hive.fetch.task.conversion=more;
select *
from bid_opportunities
where random_hash = '9'
and date = '2017-01-22'
and hour = '23'
limit 100;

Error Message: ""Failed with exception java.io.IOException:Time limit exceeded for local fetch task with SimpleFetchOptimization. Please rerun with hive.fetch.task.conversion=minimal""

setting this did not help. Then we also tried hive.fetch.task.conversion=none, which failed immediately.

Jstack of Hive client attached. Looks like stuck with s3 read and Lzo compression, because it seems to be trying to read index for each file, without index files for performance. 

This partition has 32k files each of size 5 MB. 

Finally, AD suggested this work around which will kick off an MR job and this went through. 

set hive.on.master=true;
set hive.serialization.extend.nesting.levels=true;

select *
from bid_opportunities
where random_hash = '9'
and date = '2017-01-22'
and hour = '23'
limit 1001;
Command id: 52924039
limit > 1000, will create MR job. 

",,adubey,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Jan/17 1:44 PM;Ranjana;52915091jstack.txt;https://qubole.atlassian.net/secure/attachment/37738/52915091jstack.txt,,,,,,,,,,,,,,,,,,,MediaMath,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03plj:,,,,,,,,,,,,,,,,,,,1.0,12276,2017-01-27 14:10:06.351,,,"27/Jan/17 2:10 PM;adubey;just to add little bit more - customer is okay with the workaround so there is really not anything urgent on this one, but the purpose of this ticket is more of a quick review of this scenario and if we can potently improve this flow in case of direct fetch queries... Scenario is definitely not very common ( LZO, per partition 37k files etc. )",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Presto cluster : start failures and execution failures,PRES-919,54742,Bug,Open,PRES,Presto,software,stagra,,,Major,,stagra,Ranjana,Ranjana,25/Jan/17 10:00 PM,17/May/17 9:44 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,rubix,,,,,,,"Cluster id: PrestoSmall 20180

START FAILURE 
=============

https://api.qubole.com/clusters#/view/20180
warnings, ""*** 2017-01-26 02:20:07,319 WARNING - Couldn't create encrypted channel to cluster: qbol_acc648_cl20180. Retrying.
*** 2017-01-26 02:22:58,733 WARNING - Couldn't create encrypted channel to cluster: qbol_acc648_cl20180. Retrying.
*** 2017-01-26 02:25:56,559 WARNING - Couldn't create encrypted channel to cluster: qbol_acc648_cl20180. Retrying.
2017-01-26 02:26:03,566 >>> Creating encrypted channel to master node: ec2-54-197-183-18.compute-1.amazonaws.com
*** 2017-01-26 02:29:15,861 WARNING - Couldn't create encrypted channel to cluster: qbol_acc648_cl20180. Retrying.

[9:01]  
Starting cluster took 22.394 mins

[9:02]  
these error and start times are consistent with the last 3 cluster starts


EXECUTION FAILURE
==================
https://api.qubole.com/v2/analyze?command_id=52621870

There are no logs except this line :  ""Caused by: java.io.IOException: No such file or directory""

Further information he put on Slack channel is:

s3n://emr-test-output/Qubole/Engineering/warehouse/stats.db/pr_timezone_idf4_report_orc/timezone=Europe_London/activity_date=2016-12-12/a3a9adb8-d3d3-45b8-a251-6cf981f812b6_87c6c2e1-6bfb-43a6-8982-f33ec2990515_0  (offset=0, length=7967): No such file or directory
at com.facebook.presto.hive.orc.OrcPageSourceFactory.createOrcPageSource(OrcPageSourceFactory.java:150)
at com.facebook.presto.hive.orc.OrcPageSourceFactory.createPageSource(OrcPageSourceFactory.java:106)
at com.facebook.presto.hive.QuboleHivePageSourceProvider.createPageSource(QuboleHivePageSourceProvider.java:82)
at com.facebook.presto.spi.classloader.ClassLoaderSafeConnectorPageSourceProvider.createPageSource(ClassLoaderSafeConnectorPageSourceProvider.java:43)
at com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:48)
at com.facebook.presto.operator.TableScanOperator.createSourceIfNecessary(TableScanOperator.java:258)
at com.facebook.presto.operator.TableScanOperator.isFinished(TableScanOperator.java:206)
at com.facebook.presto.operator.Driver.processInternal(Driver.java:377)
at com.facebook.presto.operator.Driver.processFor(Driver.java:303)
at com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:587)
at com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:505)
at com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:640)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: No such file or directory
at java.io.UnixFileSystem.createFileExclusively(Native Method)
at java.io.File.createNewFile(File.java:1012)
at com.qubole.rubix.core.CachingInputStream.initialize(CachingInputStream.java:116)
at com.qubole.rubix.core.CachingInputStream.(CachingInputStream.java:76)
at com.qubole.rubix.core.CachingFileSystem.open(CachingFileSystem.java:117)
at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:763)
at com.facebook.presto.hive.orc.OrcPageSourceFactory.createOrcPageSource(OrcPageSourceFactory.java:142)
... 14 more
2017-01-26 01:51:19,079 ERROR ssh.py:679 - execute - command ' echo $$ > /media/ephemeral0/pids/52621870.pid ; /usr/lib/presto/bin/presto client --output-format TSV_JSON --show-progress --execute ""INSERT OVERWRITE DIRECTORY 's3://emr-test-output/Qubole/Engineering/tmp/2017-01-26/648/52621870.dir/000' select count(*) from stats.pr_timezone_idf4_report_orc where activity_date='2016-12-12';
"" --user geoffrey.li; ret=$?; rm -f /media/ephemeral0/pids/52621870.pid 1>/dev/null; exit $ret;' failed with status 255
xargs: /usr/lib/hive_scripts/prestocli.py: exited with status 255; aborting
",,Ranjana,stagra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jan/17 10:05 PM;Ranjana;log_52621870.txt;https://qubole.atlassian.net/secure/attachment/37723/log_52621870.txt,,,,,,,,,,,,,,,,,,,TubeMogul,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03p9b:,,,,,,,,,,,,,,,,,,,1.0,12312,2017-01-29 20:39:00.88,,,"29/Jan/17 8:39 PM;stagra;[~Ranjana] you have added two issues in this jira. The first one about the cluster startup problem should be triaged by acm team, we will look at the second problem about exception.","29/Jan/17 8:45 PM;stagra;The second problem is happening because they enabled rubix but did not configure it properly. Rubix by default caches data on /media/ephemeralN directories but in this clusters there is only /media/ebs available hence rubix threw IOException.

Check the documentation about `hadoop.cache.data.dirprefix.list` at http://docs.qubole.com/en/latest/user-guide/presto/configuring-presto-cluster.html. This value should be configured correctly in this case. We should have `hadoop.cache.data.dirprefix.list=/media/ebs` in hadoop configs.","29/Jan/17 8:47 PM;stagra;We will use this jira to add fix to automatically configure hadoop.cache.data.dirprefix.list according to ebs or ephemeral. Decreasing priority.

[~Ranjana] you should open a new jira for cluster startup issue against acm.","19/Mar/17 6:43 PM;Ranjana;Thanks [~stagra]. I will coordinate with them to find out current status of Cluster start problem and will raise ACM accordingly. At the time this jira was raised, I did not have much clue about Presto clusters and did not know if these 2 could be related. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sqoop import fails for tables specified as ""dbname.tablename""",SQOOP-90,54736,Bug,Reopened,SQOOP,SQOOP,software,sumitm,,,Minor,,ksr,megha,megha,25/Jan/17 4:20 PM,13/Jul/17 5:55 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"command: 52436171
attaching mapper log herewith.

The query is converted as : 
 Executing query: SELECT retailer, card, transactiondate, transactiontime, store, terminal, transactionnumber, transactions, units, grosssales, carddiscount, coupon, bonus, netsales, fuel, basketid, basketid2, giftcard, parttransactiondate, transactionmonth, maxloadid FROM dbo.basket_noncard_inc_sqoop_vw *AS dbo.basket_noncard_inc_sqoop_vw* WHERE (  transactiondate between '2016-01-01 00:00:00.000' and '2016-01-31 23:59:59.997' ) AND ( transactiondate IS NULL ) AND ( transactiondate IS NULL )


which is why it fails with : com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near '.'.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:216)
	
This happens as the command itself uses ""db.tablename"" way of specifying tables..
",,abhishekmodi,drose@qubole.com,ksr,megha,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,oracle,,,,,"{repository={count=1, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":1,""lastUpdated"":""2011-07-22T07:04:17.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03p7z:,,,,,,,,,,,,,,,,,,,1.0,12211,2017-01-29 21:05:32.533,,,"29/Jan/17 9:05 PM;sureshr;[~megha]: Can you please explain why this is a blocker for Oracle?
[~ksr]: Could you please investigate this issue today? Thanks!
cc: [~sumitm]","30/Jan/17 12:21 AM;ksr;Sure, taking a look at this immediately.","30/Jan/17 8:36 AM;megha;[~sureshr] Thanks you for the update. I didnt realize ""dbo"" was for default.. I've pinged oracle if they also have a usecase of ""dbname.tablename"" . If they do this will be a blocker still, as they are trying to move to hadoop2 (as a part of moving to oracle cloud), and there doesnt seem to be a way around this for sqoop on hadoop2. 
[~ksr] I've told them to workaround by not using ""dbo"" for now.

Lowering the priority to critical as of now, unless if they have ""dbname.tablename"" use-case. Will change the priority back in that case","30/Jan/17 9:04 AM;ksr;Sorry, I had forgotten to post this update here. Posting it for future reference:

[~megha] The dbname is specified as part of the connect string (it is at the end of the string). This is the schema we are talking about. ""dbo"" is the default schema for sql server. So, even if it is omitted, import should run fine. Though, if they want to use a custom schema, it can be given in the following way.

Schema should be provided using the --schema option of sqoop. So, --table should consist of only the table name and 
{code:java}
-- --schema dbo
{code}
 should be added to the command. Note that there is an extra pair of ""--"".

Also, please ask them not to use the --driver option. We have connector for sql server in the lib folder which is automatically picked up. Alternatively, if they want to use the --driver option, they should also use --connection-manager to specify the connection manager ('org.apache.sqoop.manager.SQLServerManager', in this case).

Please let me know if you need any more details.","30/Jan/17 10:49 AM;megha;Thanks [~ksr], I guess I'll reduce the priority on more. 
Is there any specific reason for not using --driver and using --connectionManager instead?","30/Jan/17 10:58 AM;ksr;What I meant was that either both --driver and --connectiion-manager should be used or both shouldn't be used. If --driver is used without --connection-manager, sqoop falls back to the generic jdbc manager which might not be advisable. Sorry, if I was not clear in the previous comment.

I would suggest not using the --driver option since we have sql server connector which is automatically picked up.","30/Jan/17 11:07 AM;megha;Got it! Thanks [~ksr]
","07/Feb/17 11:19 AM;megha;[~ksr] just an update: turns out ""--schema"" is working and not ""-- --schema"" : ex commands: 54497932 (failed) ,54497857(success)","07/Feb/17 8:57 PM;ksr;[~megha] Sorry, I forgot to mention that the schema option should be added at the end because of the extra ""--"". Also, the command that succeeded did not take the schema parameter. It worked because it assumed that the table was in the default schema which is true. So, this was like schema was omitted. I would suggest to try it again with the schema option in the end.

You should see something like this in the command logs:
{code:java}
App > 17/01/30 14:15:24 INFO manager.SQLServerManager: We will use schema dbo
{code}
","08/Feb/17 9:01 AM;drose@qubole.com;""When will the avro-tools jar file be included with the Qubole sqoop?"" [~ksr] When can we add support for this in Hadoop2? this is blocking their developement.","09/Feb/17 2:42 AM;ksr;[~drose@qubole.com] Does this need to be added to Sqoop or Hadoop2? I am not sure.

cc: [~abhishekmodi]","09/Feb/17 3:02 AM;abhishekmodi;In Hadoop2, we never heard any requirement from customers for avro-tools jar. If it's required by Sqoop, in my opinion  we should add this in Sqoop.","08/Mar/17 10:25 PM;ksr;[~megha] I am closing this since we had figured out a way to specify the schema. Please let me know if this is still not resolved for the customer.
There is another JIRA tracking inclusion of avro-tools jar, SQOOP-88.","09/Mar/17 11:26 AM;megha;[~ksr]
I havent heard from customer if -- --schema worked for them. But besides that, shouldn't ""dbo.tablename"" format also work? It used to work in previous version of sqoop.. 
If its a bug, it would be good to fix it, although it would be on a low priority..",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutofMemoryError due to large array object,ZEP-705,54727,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,vipulm,ajithr,ajithr,25/Jan/17 12:03 PM,17/May/17 7:35 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Account ID: 6313, user - bgdadmin@upgrad.com, notebook ID - 27349

Paragraph in question:

{noformat}
%sparkr
S3URL = ""s3://ughdfsdemo/aadhar/aadhaar_small_data/""  
uidai_new <- read.df(sqlContext, S3URL, ""com.databricks.spark.csv"", header=""true"", inferSchema=""true"")
uidai_c <-  collect(uidai_new)

{noformat}

This is happening due to a large dataFrame being ""collect()ed"" by the customer. However, an important thing to observe here is that the same works with PySpark. We'd like to understand the differences with SparkR. Why is there a difference?

What I found was that this OOM was not due to memory exhaustion (native / Java), but, the application requesting memory for a very large array object. I saw errors like this in the logs:

{noformat}
++ key=_qubole_base_url

++ echo https://api.qubole.com

#

# java.lang.OutOfMemoryError: Requested array size exceeds VM limit

# -XX:OnOutOfMemoryError=""kill -9 %p""

#   Executing /bin/sh -c ""kill -9 22607""...

/usr/lib/zeppelin/bin/interpreter.sh: line 194: 22607 Killed                  ${SPARK_SUBMIT} --class ${ZEPPELIN_SERVER} ${extra_args} --driver-class-path ""${ZEPPELIN_CLASSPATH_OVERRIDES}:${CLASSPATH}"" --driver-java-options ""${JAVA_INTP_OPTS}"" ${SPARK_SUBMIT_OPTIONS} ${SPARK_APP_JAR} ${PORT}
{noformat}

So, the JVM obviously cannot go beyond Integer.MAX_VALUE and hence throws this error. We can perhaps resolve this by:

a) Try to make use of a different datastructure instead of arrays.

b) Use Java's compressed object references and hope that'll do some magic. Check out Java's 
""-XX:+UseCompressedOops""

c) Combination of both a) and b). Can't think of any other ideas.",,ajithr,mahuja,rohitk,venkatak,vipulm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,upgrad,,,,,{},NA,Choose from,,ZEP-688,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03p5z:,,,,,,,,,,,,,,,,,,,1.0,12342,2017-01-26 08:24:01.719,,,25/Jan/17 12:04 PM;ajithr;This should perhaps go to Spark Team. Please re-assign if so.,"26/Jan/17 8:24 AM;vipulm;It is the spark process which is getting killed during because of OutOfMemory issue. Looks related to SparkR. The above error happens when Java is not able to find a huge chunk of continuos memory in the heap space.
[~puneetg][~bharatb][~rohitk] Any suggestions?","26/Jan/17 6:44 PM;mahuja;Without looking at code, it seems to be the difference in which SparkR allocates memory. Comparison to pyspark does not look valid.

The other question would be - how big is the dataframe being collect()ed?","26/Jan/17 8:20 PM;ajithr;[~mahuja], thanks. We have permission from customer to use the S3 files (used to create the DF) for diagnosis purposes.","26/Jan/17 8:25 PM;mahuja;[~ajithr] I believe this analysis can be best done on field. The count of rows and size of each row will give a good idea. If the data is small then it is a problem. But if the data is big, no matter what the configuration is, it will go OOM. One possibility could be to generate a warning but not sure how helpful it will be since the operation will fail. If the logs currently highlight OOM then it shall be fine","26/Jan/17 8:40 PM;ajithr;This is not a regular OOM. It is because of use of arrays and we are hitting a JVM limitation on size. We are not sure what this array is and why / how it grows. If it is some ArrayList, then we could use other data-structures.

From field, I can think of testing with compressed references whenever the size of the heap is less than 32 GB. But, I will still need help from dev to assess this aspect and suggest if there are better ways to handle this.","26/Jan/17 9:46 PM;rohitk;[~ajithr] Are they using shared interpreter mode or per user interpreter?
","26/Jan/17 10:12 PM;vipulm;Here are the few details I know:
# They are using per user interpreter.
# Data size getting read is around 1GB. 
# When I tried with driver memory as 8 GB. It gave be container failed exception (I wasn't able to copy exception as cluster went down)
# Data is copied at location: s3://dev.canopydata.com/vipulm/upgrad/uidai/

cc [~rohitk] [~mahuja][~ajithr]",30/Jan/17 10:53 PM;ajithr;Any update on this?,"31/Jan/17 10:13 PM;rohitk;{code:java}
We'd like to understand the differences with SparkR. Why is there a difference?
{code}

I looked at the sparkr implementation. An inefficient copy of data from java to R might double the memory requirements. Discussed this with [~vipulm]. 

1) We need to test this with larger memory for driver to check if at some point it starts working or not. It this works once we give enough memory, this could be just an issue with how SparkR uses memory.  The only workaround here is increasing the memory proportionately.  
2) SparkR has some memory leak. If we see the same test failing in the second or third run even after increasing the memory, we might be hitting this issue. https://issues.apache.org/jira/browse/SPARK-17822
3) I believe pyspark and sparkR handle communication between the driver and the ""language specific shell"" differently. I think pyspark uses files whereas sparkR does socket communication. This could be the main reason for why it works in pyspark but not it sparkR. 

Assigning to [~vipulm] for validating points 1 and 2. 





","15/Feb/17 2:45 PM;mahuja;[~ajithr] is it still a priority (came up in escalation/SOLSUP meeting)? As per [~rohitk] comment above, this is related to SparkR implementation and one of the suggestions is to use higher memory.

At present it is not on high priority list but we can revisit based on business impact. Thoughts?","15/Feb/17 5:52 PM;ajithr;I've not heard back from the customer. [~venkatak], is that a correct assessment? Removed SOLSUP on this.",15/Feb/17 8:43 PM;venkatak;I think they have moved on [~ajithr][~mahuja]It did not make sense for them to collect all that big data set.,"16/Feb/17 12:58 AM;ajithr;Thanks [~venkatak]. We can deprioritize then. 

But, when you guys take it up, and as discussed on slack, [~rohitk], I think we have to check if this issue is because of the points mentioned in this post:

https://plumbr.eu/outofmemoryerror/requested-array-size-exceeds-vm-limit.

I think it may have something to do with the issues mentioned in that link because of this message in the log:

* java.lang.OutOfMemoryError: Requested array size exceeds VM limit*
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception in thread Thread-4 (most likely raised during interpreter shutdown):,HIVE-1869,54711,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,psrinivas,satyavathib,satyavathib,25/Jan/17 5:51 AM,14/Jun/17 9:16 PM,09/Aug/17 6:03 AM,,,,,0,bootcamp,jira_escalated,,,,,,,"seeing this issue in a INSERT INTO query.

I am not sure why interpreter exception is visible in hive query.

Also I could not get a clue if the insert is successful or not.

Unable to fetch the hive logs from the log server for 22nd jan.

Command ID : 52015690

{code}Exception in thread Thread-4 (most likely raised during interpreter shutdown):
{code}

Please let me know if I am missing something here.

Account ID : 865",,goden,mpatel,psrinivas,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dataxu,hotels.com,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,HIVE-1623,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03p2f:,,,,,,,,,,,,,,,,,,,2.0,"12216,12750",2017-02-15 16:53:17.705,,,"15/Feb/17 4:53 PM;mpatel;I've seen this occasionally myself.

Also, Hotels.com reported it via support: 55831684

It seems safe to ignore, right? Can we get some info about the error? Also, can we clean it up?","15/Feb/17 5:11 PM;psrinivas;This is coming from the python layer(hive_scripts), probably it is safe to ignore for now. ",14/Jun/17 9:16 PM;goden;we should hide it or do not highlight it. clean up.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark jobs hanging due to unknown reason on 2.0.0, simply re-run goes through",SPAR-1407,54600,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,Ranjana,Ranjana,23/Jan/17 3:16 PM,31/May/17 7:08 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Seen this twice till now. The job simply looks like hanging with messages such as following, the exector and driver logs do not seem to specify what is going on. Not sure if some deadlock has happened, but simply terminating and re-running job with randomly larger executer, driver memory and extended timeouts worked so far. Happened once with Expedia and once with Oracle. 

On the job logs the following is what appears and the job hangs:

App > 17/01/18 21:19:36 SparkUI-3781 INFO JettyUtils: GET http://10.23.14.61:4040/
App > 17/01/18 21:19:36 SparkUI-3775 INFO JettyUtils: GET http://10.23.14.61:4040/jobs/
App > 17/01/18 21:19:39 SparkUI-3781 INFO JettyUtils: GET http://10.23.14.61:4040/executors/
App > 17/01/18 21:19:39 SparkUI-3781 INFO ExecutorsPage: logsExist: true
App > 17/01/18 21:24:48 SparkUI-3794 INFO JettyUtils: GET http://10.23.14.61:4040/jobs/
App > 17/01/18 21:30:16 SparkUI-3775 INFO JettyUtils: GET http://10.23.14.61:4040/jobs/
App > 17/01/18 21:30:26 SparkUI-3824 INFO JettyUtils: GET http://10.23.14.61:4040/jobs/job/
Qubole > 2017-01-19 02:44:43,827 WARNING shellcli.py:354 - signal_handler - Waiting for JVM to terminate ...


Customer: Expedia
Command id : 51222729

Simply rerun, (command id : 51765322), worked with following parameters: --driver-memory 10G --conf spark.sql.broadcastTimeout=10000 --conf spark.network.timeout=10000 --conf spark.rpc.message.maxSize=512 --conf spark.executor.extraJavaOptions=-XX:MaxPermSize=512m  --conf spark.driver.extraJavaOptions=-XX:MaxPermSize=512m 

Customer: Oracle
Command id : 51770704

we looked at jmap - heap of driver

[ec2-user@ip-10-108-2-106 java-1.7.0]$ sudo /usr/lib/jvm/java-1.7.0/bin/jmap -heap 69260
Attaching to process ID 69260, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 24.65-b04

using thread-local object allocation.
Parallel GC with 13 thread(s)

Heap Configuration:
   MinHeapFreeRatio = 0
   MaxHeapFreeRatio = 100
   MaxHeapSize      = 69793218560 (66560.0MB)
   NewSize          = 1310720 (1.25MB)
   MaxNewSize       = 17592186044415 MB
   OldSize          = 5439488 (5.1875MB)
   NewRatio         = 2
   SurvivorRatio    = 8
   PermSize         = 21757952 (20.75MB)
   MaxPermSize      = 536870912 (512.0MB)
   G1HeapRegionSize = 0 (0.0MB)

Heap Usage:
PS Young Generation
Eden Space:
   capacity = 378535936 (361.0MB)
   used     = 28784864 (27.451385498046875MB)
   free     = 349751072 (333.5486145019531MB)
   7.604261910816309% used
From Space:
   capacity = 1048576 (1.0MB)
   used     = 0 (0.0MB)
   free     = 1048576 (1.0MB)
   0.0% used
To Space:
   capacity = 6815744 (6.5MB)
   used     = 0 (0.0MB)
   free     = 6815744 (6.5MB)
   0.0% used
PS Old Generation
   capacity = 1782579200 (1700.0MB)
   used     = 1386821840 (1322.5763702392578MB)
   free     = 395757360 (377.4236297607422MB)
   77.79861001407399% used
PS Perm Generation
   capacity = 119537664 (114.0MB)
   used     = 118833328 (113.32829284667969MB)
   free     = 704336 (0.6717071533203125MB)
   99.41078319884183% used

We took thread dump of driver, saw the following :
=====================================

venkata.yerubandi [12:47 PM] 
""main"" prio=10 tid=0x00007f386401e800 nid=0x10ebd waiting on condition [0x00007f386d8b9000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00007f2873663730> (a scala.concurrent.impl.Promise$CompletionLatch)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
    at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
    at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
    at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1994)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2007)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:58)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
    - locked <0x00007f286b4f7730> (a org.apache.spark.sql.execution.command.ExecutedCommandExec)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
    - locked <0x00007f286b535148> (a org.apache.spark.sql.execution.QueryExecution)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
    at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)
    at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:478)
    at com.oracle.idgraph.devicegraphprocessing.CookieDeviceGraphDelta.checkPointEdges(CookieDeviceGraphDelta.scala:48)
    at com.oracle.idgraph.devicegraphprocessing.CookieDeviceGraphDelta.processCanonical(CookieDeviceGraphDelta.scala:170)
    at com.oracle.idgraph.devicegraphprocessing.GraphDriver$.processProbabilisticCanonical(GraphDriver.scala:43)
    at com.oracle.idgraph.devicegraphprocessing.GraphDriver$.processBothCanonical(GraphDriver.scala:90)
    at

We re-ran same job by just bumping up two timeout to 10000, spark.network.timeout and spark.sql.broadcastTimeout, (command id : 51931332),  it passed successfully. 

This looks like some kind of thread situation that the jobs get stuck in. 

",,drose@qubole.com,mahuja,Ranjana,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jan/17 3:29 PM;Ranjana;Screen Shot 2017-01-19 at 5.46.53 PM.png;https://qubole.atlassian.net/secure/attachment/37615/Screen+Shot+2017-01-19+at+5.46.53+PM.png,,,,,,,,,,,,,,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03olb:,,,,,,,,,,,,,,,,,,,3.0,"11974,12083,12308",2017-01-23 15:19:39.02,,,23/Jan/17 3:17 PM;Ranjana;So far both customers just wantd to terminate job and re-run and get it done with. I will keep an eye out for more occurances of this and update this jira. ,23/Jan/17 3:19 PM;mahuja;[~Ranjana] can you attach the thread dumps of executors and driver.,"23/Jan/17 3:27 PM;Ranjana;[~mahuja], yes, next time I see this. These 2 jobs above have been killed.  So you need, executer thread dump, driver thread dump and let me know anything more. I will keep an eye out for more occurrences of this issue. Partial thread dump of driver of one of the jobs is above. The Oracle issue happened last Friday, I thought about transfering the thread dump over to my laptop , but I was caught up in a flat tire and by the time I could get to my computer the node on which the driver was running was gone. 

","23/Jan/17 3:30 PM;Ranjana;Attached is a screenshot I took while the Oracle job , 51770704 was running. ",01/Feb/17 4:09 PM;Ranjana; [~mahuja]   [~venkats]  This issue is happening right now with command id 53665421 .  I will be collecting the required thread dumps. have asked customer to keep it running for 10 minutes.,"02/Feb/17 3:22 AM;rohitk;For 30G RAM machines, here is my recommended config.

*Driver Settings*
spark.driver.maxResultSize 10g
spark.driver.memory 16g
spark.driver.cores 4

*Executor Settings*
spark.executor.memory 18000M
spark.yarn.executor.memoryOverhead 7000
spark.executor.cores 4

*Executor count should be based on cluster size*
spark.qubole.max.executors 16 
spark.executor.instances 16

*Hive Settings*
spark.sql.hive.metastore.jars builtin 
spark.sql.hive.metastore.version 1.2.1
Note that these assume that node bootstrap is used for running hive 1.2 metastore server on master


*SQL Settings*
spark.sql.autoBroadcastJoinThreshold 2000000000

*GC settings *
spark.driver.extraJavaOptions -Djava.net.preferIPv4Stack=true -XX:+UseG1GC -XX:+AlwaysPreTouch -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=4 -XX:+ParallelRefProcEnabled -XX:ParallelGCThreads=4 -XX:G1HeapWastePercent=20
spark.executor.extraJavaOptions -Djava.net.preferIPv4Stack=true -XX:+UseG1GC -XX:+AlwaysPreTouch -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=4 -XX:+ParallelRefProcEnabled -XX:ParallelGCThreads=4 -XX:G1HeapWastePercent=20

For larger clusters, we probably also need to look memory settings for data node and node manager. 
","03/Feb/17 11:31 AM;mahuja;Worked with [~Ranjana] to go over the seemingly hung jobs. They were spending time in GC. There are couple of things that can be tried out to reduce GC affect: increase size of executor memory and overhead memory, or decrease the number of tasks per executor (decreases mem requirement and hence GC), and/or try out alternate GC algorithms (e.g. G1GC) [details: https://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning]. 

Config provided by [~rohitk] is a good example.

[~Ranjana] - was the issue resolved post the proposed changed? Can this issue be closed?","03/Feb/17 1:30 PM;Ranjana;[~mahuja], increasing size of executors and reducing number of tasks per executers definitely helped, but this did not reduce the GC time too much, it is in the range of 20-24 min now. The customer has given permission to re-run with GC tuning etc. I will try out what Rohit suggested. Is it ok to keep this open a bit longer, just so that I can be sure, we have collected and documented all collected information in this JIRA ? 

Btw, just to document here, the messages we saw on logs as follows are just our clicks on the Spark UI and nothing else. 

App > 17/01/18 21:19:36 SparkUI-3781 INFO JettyUtils: GET http://10.23.14.61:4040/
App > 17/01/18 21:19:36 SparkUI-3775 INFO JettyUtils: GET http://10.23.14.61:4040/jobs/
App > 17/01/18 21:19:39 SparkUI-3781 INFO JettyUtils: GET http://10.23.14.61:4040/executors/
App > 17/01/18 21:19:39 SparkUI-3781 INFO ExecutorsPage: logsExist: true
App > 17/01/18 21:24:48 SparkUI-3794 INFO JettyUtils: GET http://10.23.14.61:4040/jobs/...
","03/Feb/17 1:41 PM;mahuja;Sure, please use this JIRA for logging the observations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster not terminated even with zero nodes.,ACM-896,54565,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,satyavathib,satyavathib,23/Jan/17 12:08 AM,28/Jun/17 12:00 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster ID : 21636

Account ID: 4019

Sample Command ID : 52013531

The cluster is 30% spot node and the UI showing 0 nodes for more than 3 hours without terminating the cluster . Eventually all the commands failed with no master found. The user have to manually restart the cluster.

Could not fetch the logs from log server after 19th jan:

{code}
[ec2-user@ip-10-182-127-68 ebs]$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        3.7G   68K  3.7G   1% /dev
tmpfs           3.7G     0  3.7G   0% /dev/shm
/dev/xvda1      7.8G  7.5G  235M  98% /
/dev/xvdb        40G  2.4G   35G   7% /media/ephemeral0
/dev/xvdc        40G   27G   11G  72% /usr/tmp
/dev/xvdf       296G  296G     0 100% /media/ebs
{code}

Could only get the command logs from the UI :

{code}
AttributeError: 'NoneType' object has no attribute 'get_public_dns_name_or_private_ip'
!!! 2017-01-21 18:37:41,163 ERROR - An error occured while running hustler script
Retrying to connect to the cluster ...
No config file specified - defaulting to hustler/configs/config.default for config file
2017-01-21 18:38:03,515 >>> Cluster id '21636', account id '4019' marked UP in DB. Trying to latch ...
Traceback (most recent call last):
File ""/usr/lib/qubole/packages/hustler-40.0.1/hustler/lib/py/starcluster/cli.py"", line 251, in main
sc.execute(args)
File ""/usr/lib/qubole/packages/hustler-40.0.1/hustler/lib/py/starcluster/commands/clusterinfo.py"", line 233, in execute
self.perform_curl_check_and_print_master(master_instance.get_public_dns_name_or_private_ip(), scluster, clname,
AttributeError: 'NoneType' object has no attribute 'get_public_dns_name_or_private_ip'
!!! 2017-01-21 18:38:03,515 ERROR - An error occured while running hustler script
Retrying to connect to the cluster ...
No config file specified - defaulting to hustler/configs/config.default for config file
2017-01-21 18:38:26,205 >>> Cluster id '21636', account id '4019' marked UP in DB. Trying to latch ...
Traceback (most recent call last):
File ""/usr/lib/qubole/packages/hustler-40.0.1/hustler/lib/py/starcluster/cli.py"", line 251, in main
sc.execute(args)
File ""/usr/lib/qubole/packages/hustler-40.0.1/hustler/lib/py/starcluster/commands/clusterinfo.py"", line 233, in execute
self.perform_curl_check_and_print_master(master_instance.get_public_dns_name_or_private_ip(), scluster, clname,
AttributeError: 'NoneType' object has no attribute 'get_public_dns_name_or_private_ip'
!!! 2017-01-21 18:38:26,206 ERROR - An error occured while running hustler script
2017-01-21 18:38:46,264 ERROR cmd_utils.py:88 - _must_get_master_ip - Failed to connect to Hadoop cluster.
Qubole > 2017-01-21 18:38:46,267 ERROR shellcli.py:697 - - Traceback (most recent call last):
File ""/usr/lib/hive_scripts/shellcli.py"", line 673, in
status = main(None)
File ""/usr/lib/hive_scripts/shellcli.py"", line 336, in main
master_ip = cmd_utils_obj._must_get_master_ip(qconf)
File ""/usr/lib/qubole/packages/hive_scripts-40.0.1/hive_scripts/utils/cmd_utils.py"", line 90, in _must_get_master_ip
raise exception.QMasterNotFound(acid, cluster_id, output)
QMasterNotFound: Master node for (account id 4019, cluster id 21636) could not be determined.
{code}


Screenshot from cluster UI page showing 0 nodes :

",,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jan/17 12:07 AM;satyavathib;inline500634744.png;https://qubole.atlassian.net/secure/attachment/37608/inline500634744.png,,,,,,,,,,,,,,,,,,,fanatics,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03odj:,,,,,,,,,,,,,,,,,,,1.0,12203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ipc.Client: Address change detected. Old: 10.23.13.170:37032 New: 10.23.13.170/10.23.13.170:37032 ,SPAR-1506,54516,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,satyavathib,satyavathib,20/Jan/17 3:52 AM,30/May/17 2:23 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,," ipc.Client: Address change detected. Old: 10.23.13.170:37032 New: 10.23.13.170/10.23.13.170:37032 

seeing this error in a spark job.

Command ID : 51782703

Cluster ID : 25149

Account ID : 5497

Could someone plz throw some light on this.",,anum,biswajit,mahuja,satyavathib,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,expedia,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03o2n:,,,,,,,,,,,,,,,,,,,2.0,"12170,14923",2017-01-24 19:34:14.136,,,"24/Jan/17 7:34 PM;mahuja;Reading the ticket, looks related to folders (creation of notebook fails). Moving it to MW project.

[~satyavathib]  - the error in the description though will not be related to this issue.","24/Jan/17 10:13 PM;satyavathib;[~mahuja] He is actually referring to 2 issues in the ticket.

1. with the command ID : 51782703

2. with the notebooks creation.

In this command(51782703) I could see that all the spark jobs are successful and at the last the command failed with this message. Could you please let me know If I am missing something here.
",25/Jan/17 2:02 AM;sumitm;[~mahuja] the notebook thing (which I wasn't aware) must be some simple permission thing.. I moved this jira to spark queue cause of the errors in command 51782703.  ,"14/Mar/17 12:14 AM;anum;The same issue seems to be happening in azure.qubole.com also. The query https://azure.qubole.com/v2/analyze?command_id=148 is failing due to this. It seems to have been reported in MW-537 also.


{noformat}
Qubole > log4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.
Qubole > Shell Launcher Begin...
Qubole > 17/03/13 16:07:12 WARN ipc.Client: Address change detected. Old: 10.10.0.5:8032 New: 10.10.0.5/10.10.0.5:8032
Qubole > Tracking URL: Application UI
Qubole > Exception: java.lang.NullPointerException, Message: null
Qubole > java.lang.NullPointerException
Qubole > at com.qubole.shell.executor.ShellLauncher.monitorAndPrintJob(ShellLauncher.java:305)
Qubole > at com.qubole.shell.executor.ShellLauncher.run(ShellLauncher.java:271)
Qubole > at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
Qubole > at com.qubole.shell.executor.ShellLauncher.main(ShellLauncher.java:172)
Qubole > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Qubole > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
Qubole > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Qubole > at java.lang.reflect.Method.invoke(Method.java:606)
Qubole > at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
Qubole > at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Qubole > Shell Command Launcher failed
Qubole > 17/03/13 16:08:01 ERROR mapreduce.Job: Exception while killing job:
Qubole > java.io.IOException: Job status not available
Qubole > at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:356)
Qubole > at org.apache.hadoop.mapreduce.Job.getJobState(Job.java:392)
Qubole > at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1369)
Qubole > at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
{noformat}
","14/Mar/17 12:32 PM;mahuja;[~satyavathib] for command id 51782703, there are following errors at end

{code}
Qubole > 2017-01-20 04:20:11,216 ERROR shellcli.py:231 - run - Retrying exception reading mapper output: (22, 'The requested URL returned error: 404 Not Found')
{code}

This indicates that Spark command was unable to talk to driver (Shell command metaAM) to fetch logs. It is possible that driver went OOM. Is there a new instance of this command if this is still an issue?

[~anum] - the specific instance looks like MR job. Please follow-up with Hadoop JIRA.",30/May/17 2:22 AM;biswajit;Here is a fresh command-id(75553648) which faced the same error.  Please do let us know if any info is required. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can datadog metrics be filling up disk on cluster master,ACM-883,54282,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,hiyer,megha,megha,18/Jan/17 3:10 PM,22/Jun/17 1:20 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Can datadog push be filling up disk on cluster master, if it is enabled. 
I saw this cron:
*/4 * * * * python /usr/lib/hustler/lib/py/hadoop_setup/push_datadog_metrics.py edecb953edf9a69f570e3eee84a87d53 6c719592598bae04eae4dfbbde2149ddf7a3a490 qbol_acc5507_cl25609 25609 ganglia_metrics

Also, looks like ganglia is cleaned up every six hours...

For example cluster id: 24611
These are disk metrics on master:
{code:java}
Filesystem    1M-blocks  Used Available Use% Mounted on
/dev/xvda1        32126 14302    17727  45% /
devtmpfs            7514    1      7514  1% /dev
tmpfs              7522    0      7522  0% /dev/shm
/dev/xvdab        37662 35742        0 100% /media/ephemeral1
/dev/xvdaa        37662 19045    16698  54% /media/ephemeral0
/dev/xvdp        201459  1881    197531  1% /media/ebs2
{code}

as ephemeral1 was full, the ganglia metrics UI gave following error:
There was an error collecting ganglia data (127.0.0.1:8652): fsockopen error: Connection refused


cc: [~sriramg]
",,hiyer,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03njr:,,,,,,,,,,,,,,,,,,,1.0,11686,2017-05-03 21:51:28.675,,,"03/May/17 9:51 PM;hiyer;Datadog push can't fill up the disk, but ganglia metrics certainly can. Ganglia data doesn't get cleaned up completely, but it does get rolled up. One problem that we have seen in the past is that because of autoscaling we have a lot of data lying around for dead hosts, and this does not get removed.","08/May/17 11:04 AM;megha;Thanks [~hiyer]
Is there something we can do about older data? Don't we clean up ganglia data every six hours? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Usage Report for All Commands sorting is incorrect,AD-147,53534,Bug,Open,AD,Admin,software,bmohanam,,,Minor,,tabraiz,ekang,ekang,05/Jan/17 1:51 PM,08/Aug/17 6:59 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"1) Go into Cluster Usage
2) Report Type: All Commands
3) Specify a custom date range
4) Hit submit
5) Notice that the top of the report starts with the oldest created commands. 
6) Scroll down to the pagination tabs at the bottom of the report and click Older. You will be taken to the next page which actually contains the newer commands rather than older.",,aswina,ekang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03ktr:,,,,,,,,,,,,,,,,,,,1.0,11862,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Managing Zendesk access,SEC-610,53525,Bug,Open,SEC,Security,software,drew,,,Critical,,drew,mpatel,mpatel,05/Jan/17 8:51 AM,23/Jun/17 9:13 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"CC [~Jtrail], [~drew]

Let's assume a Qubole user leaves her organization. The customer takes care of disabling that user from all of the necessary Qubole accounts.

Since user management in Qubole is not tied to Zendesk, that user can still log in to Zendesk and potentially view all of the former organization tickets.

Zendesk provides a way to ""suspend"" user access. Maybe we can come up with a short term solution to run a nightly job to look up any users which do not have any active accounts and suspend their zendesk access...

Longer term, it would be great if we can enforce user authentication in one place (SSO between QDS and Zendesk...)

",,beng,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,hotels.com,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03krr:,,,,,,,,,,,,,,,,,,,1.0,9216,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command was stuck for about 2 hours in hustler,ACM-847,53081,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,megha,megha,28/Dec/16 5:25 PM,30/Jan/17 11:45 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command id: 45775524
This was stuck for 2 hours as depicted in following log messages:
2016-12-07 15:14:29,435 >>> Job tracker on master ec2-54-175-99-206.compute-1.amazonaws.com for cluster qbol_acc4911_cl15717 is accessible.
2016-12-07 17:25:51,519 ERROR dbinterface.py:56 - connect - Attempt No: 1. Operational Error while trying to establish MySQL connection! Message: '(2013, ""Lost connection to MySQL server at 'reading initial communication packet', system error: 110"")'. 

This seems to be in hustler. 
The log for hustler for this cluster around above time:
https://drive.google.com/open?id=0B8ud5Y6FWFzvRlFRMDdvZS1RdkU

It is not evident why it was stuck. Can we get some insight? ",,ajayaa,ajayb,megha,mpatel,yogeshg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03izr:,,,,,,,,,,,,,,,,,,,,,2017-01-20 08:11:32.64,,,20/Jan/17 8:11 AM;mpatel;[~ajayb] did you get a chance to review this? Do you think it could be related to the AWS issues which Pinterest is facing?,27/Jan/17 2:52 AM;ajayb;[~mpatel] based on above pasted log snippet this does not look related to issues seen at Pinterest. Primarily because by this time we have already connected to the cluster (as job tracker check has passed). cc [~ajayaa],"29/Jan/17 10:32 PM;ajayaa;[~mpatel] [~ajayb]

Looks like this query was stuck while retrying to connect to DB.

When a hive on master query is run, there are two commands in Hustler which get executed. The first is clusterinfo which gives the master IP and proxy information to hivecli.py. The presence of below line suggests that, in this case clusterinfo ran successfully and returned master ip to hivecli.

{code:java}
2016-12-07 15:14:29,435 >>> Job tracker on master ec2-54-175-99-206.compute-1.amazonaws.com for cluster qbol_acc4911_cl15717 is accessible.
{code}

It seems that hivecli tries to connect to rstore and get other info. This is where the query got stuck for more than two hours. Looks like the query was hitting QBOL-5906

{code:java}
2016-12-07 15:14:29,435 >>> Job tracker on master ec2-54-175-99-206.compute-1.amazonaws.com for cluster qbol_acc4911_cl15717 is accessible.
2016-12-07 17:25:51,519 ERROR dbinterface.py:56 - connect - Attempt No: 1. Operational Error while trying to establish MySQL connection! Message: '(2013, ""Lost connection to MySQL server at 'reading initial communication packet', system error: 110"")'.
2016-12-07 17:25:51,519 ERROR dbinterface.py:58 - connect - Trying to establish connection again after 1 secs..
2016-12-07 17:25:54,540 WARNING dbinterface.py:70 - connect - Connection to database established after '1' retries
2016-12-07 17:25:55,007 INFO hivecli.py:598 - sshPopenHiveCmd - Master ip is : ec2-54-175-99-206.compute-1.amazonaws.com
mdhist20161207-10634-ef7za0 100% |||||||||||||||||||| Time: 00:00:00 0.00 B/s
qexec20161207-10634-1chaowo 100% |||||||||||||||||||| Time: 00:00:00 0.00 B/s
2016-12-07 17:25:57,919 INFO hivecli.py:406 - getStandaloneCmd - Using hive 0.13 for hadoop1 cluster
{code}

The second Hustler command is sshmaster which in this case was run at 15:26 and finished quickly.

{code:java}
2016-12-07 17:25:57,920 INFO cmd_utils.py:426 - _ssh_and_execute - Launching Map Reduce Job(s)
*** 2016-12-07 17:27:05,545 WARNING - Skipping keypair fingerprint validation...
2016-12-07 17:28:04,255 >>> Connected (version 2.0, client OpenSSH_6.6.1)
2016-12-07 17:28:04,928 >>> Authentication (publickey) successful!
{code}

cc [~yogeshg] [~psrinivas] [~nitink]
",30/Jan/17 11:45 PM;yogeshg;The query ran on one of the bad web nodes on Dec 7 that is available in our records - {{ip-10-79-132-8.ec2.internal}} which was impacted due to bad m3 instance class nodes from AWS. So it could be possible that parts of the code was stuck due to these networking issues.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Server side encryption for S3 issues with Presto,PRES-879,52933,Bug,Reopened,PRES,Presto,software,stagra,,,Major,,ankitd,ajithr,addon_zendesk_for_jira,25/Dec/16 5:52 AM,04/Jun/17 11:03 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Server side encryption for S3 does not work fine with Presto cluster. EU Central account 73, cluster id 420.

Command *22319* fails where bucket has the encryption constraint enabled, whereas *22310* passes for a different bucket/path without this constraint.

Stack trace of failure:

{noformat}
Query 20161221_140120_00000_h2i8y failed: Error committing write to Hive
com.facebook.presto.spi.PrestoException: Error committing write to Hive
at com.facebook.presto.hive.HivePageSink$HiveRecordWriter.commit(HivePageSink.java:602)
at com.facebook.presto.hive.HivePageSink.finish(HivePageSink.java:236)
at com.facebook.presto.spi.classloader.ClassLoaderSafeConnectorPageSink.finish(ClassLoaderSafeConnectorPageSink.java:50)
at com.facebook.presto.operator.TableWriterOperator.getOutput(TableWriterOperator.java:222)
at com.facebook.presto.operator.Driver.processInternal(Driver.java:380)
at com.facebook.presto.operator.Driver.processFor(Driver.java:303)
at com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:577)
at com.facebook.presto.execution.TaskExecutor$PrioritizedSplitRunner.process(TaskExecutor.java:529)
at com.facebook.presto.execution.TaskExecutor$Runner.run(TaskExecutor.java:665)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 933854DD8B8789CF), S3 Extended Request ID: jL7hrrY9ycBiwMOdkKdtXecCNaHWMSyODrAuA1+W2TqhgdNY9OMJqiA8Br8HjdxKUjinW5xvc/Q=
at com.facebook.presto.hive.PrestoS3FileSystem$PrestoS3OutputStream.uploadObject(PrestoS3FileSystem.java:1014)
at com.facebook.presto.hive.PrestoS3FileSystem$PrestoS3OutputStream.close(PrestoS3FileSystem.java:977)
at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
at org.apache.hadoop.hive.ql.io.orc.WriterImpl.close(WriterImpl.java:2429)
at org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat$OrcRecordWriter.close(OrcOutputFormat.java:106)
at com.facebook.presto.hive.HivePageSink$HiveRecordWriter.commit(HivePageSink.java:599)
... 11 more
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 933854DD8B8789CF)
at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1377)
at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:923)
at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:701)
at com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:453)
at com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:415)
at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:364)
at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3964)
at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1538)
at com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:131)
at com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:123)
at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:139)
at com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:47)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
... 3 more
2016-12-21 14:02:06,335 ERROR ssh.py:679 - execute - command ' echo $$ > /media/ephemeral0/pids/22319.pid ; /usr/lib/presto/bin/presto client --output-format TSV_JSON --show-progress --execute ""INSERT INTO dougtest.employees_orc (name, role, gender, hair_colour) VALUES ('Ian McAlpine', 'Cloud Engineer', 'Male', 'Brown');
"" --user doug.johnson; ret=$?; rm -f /media/ephemeral0/pids/22319.pid 1>/dev/null; exit $ret;' failed with status 255
xargs: /usr/lib/hive_scripts/prestocli.py: exited with status 255; aborting
{noformat}
",,addon_zendesk_for_jira,ajithr,ankitd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,salecycle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03id3:,,,,,,Presto Sprint 14,,,,,,,,,,,,,1.0,11712,2016-12-26 02:21:12.64,,,"25/Dec/16 6:16 AM;ajithr;The settings mentioned in this [link|https://goo.gl/vCELUk] are already applied. I had discussed this on Presto channel and [~ankitd] had confirmed that we won't need this setting:

{noformat}
set fs.s3n.sse=AES256
{noformat}

And just the one below is needed:

{noformat}
catalog/hive.properties:
hive.s3.serverside-encryption-algorithm=AES256
{noformat}

I checked the Presto source file: com.facebook.presto.hive.PrestoS3FileSystem.java

          try {
                log.debug(""Starting upload for host: %s, key: %s, file: %s, size: %s"", host, key, tempFile, tempFile.length());
                STATS.uploadStarted();

                PutObjectRequest request = new PutObjectRequest(host, key, tempFile);
              {color:red}  if (sseEnabled || (s3ServerSideEncryptionAlgorithm != null)) {{color}
                    ObjectMetadata metadata = new ObjectMetadata();
                    metadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
                    setS3EncryptionAlgorithm(metadata);
                    request.setMetadata(metadata);
                }
                Upload upload = transferManager.upload(request);

                if (log.isDebugEnabled()) {
                    upload.addProgressListener(createProgressListener(upload));
                }
                upload.waitForCompletion();
                STATS.uploadSuccessful();
                log.debug(""Completed upload for host: %s, key: %s"", host, key);
            }

Not sure which log to check to track the source line marked in {color:red}red{color} above. Need some help from engineering team to diagnose this further and any other suggestions to get this working.","26/Dec/16 2:21 AM;ankitd;[~ajithr] I tried bringing up a clone cluster ""presto-debug"" with cluster-id:440 and re-ran the same query to insert into dougtest.employees_orc table. Can you please check with them if they changed something, otherwise it seems working to me.",28/Dec/16 4:07 AM;ankitd;[~ajithr] Did you get time to check this ?,02/Jan/17 8:12 AM;ajithr;Verified that this is no longer recreatable in the customer's env. Have written to the customer about the same.,05/Jan/17 11:00 AM;ajithr;Customer says it's still recreatable in his Presto-Test cluster. Can you please check commands *22852* and *22530* in customer account *operations* in EU-central. User facing the issue is doug.johnson@salecycle.com.,"05/Jan/17 11:13 AM;ajithr;Latest updates in the ticket - https://qubole.zendesk.com/agent/tickets/11712

The difference between the 2 clusters appears to be the SSE/SSL property settings. presto-debug cluster in customer account has the following property set:

{noformat}
catalog/hive.properties:
hive.s3.serverside-encryption-algorithm=AES256
hive.s3.sse.enabled=true
{noformat}

However, the other cluster where it's failing has the following:

{noformat}
catalog/hive.properties:
hive.s3.serverside-encryption-algorithm=AES256
hive.s3.ssl.enabled=true
{noformat}

I understand from [~stagra] that we can have either ""hive.s3.sse.enabled=true"" or  ""hive.s3.serverside-encryption-algorithm"" and get the same intended result - enable SSE for data at rest for S3. 

I guess setting just ""hive.s3.serverside-encryption-algorithm"" as above is not addressing this yet. Can you please throw more light?","05/Jan/17 7:29 PM;ajithr;I've asked customer to try the following settings too:

{noformat}
catalog/hive.properties:
hive.s3.ssl.enabled=true
hive.s3.sse.enabled=true
{noformat}
","06/Jan/17 5:54 AM;ajithr;Customer confirmed that only the option ""hive.s3.sse.enabled=true"" is working and not setting ""hive.s3.serverside-encryption-algorithm=AES256"". Please check the comments in the ticket.","06/Jan/17 6:13 AM;ankitd;Ok, thanks will check and deprecate the other thing,. Thanks a lot for looking at the source code too :) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not bill for QCUH when cluster is terminated due to error,ACM-835,52824,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,xing,Kulbir,Kulbir,21/Dec/16 5:05 PM,17/Feb/17 1:12 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"ACM team,
I am not 100% sure but it seems like we bill\calculate QCUH even if the cluster couldn't be properly started for e.g Hadoop Daemons couldn't be started or network on node is bad etc.

We recognize such use cases in ACM and populate the termination reason properly for e.g ""Forced cleanup"", so ask is that if termination reason is anything other than inactivity or user-initiated we shouldn't be charging customers for such clusters.

Also pls. let me know if my understanding w.r.t QCUH calculation above is incorrect.

cc [~Harsh]",,ajayaa,ajayb,Kulbir,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03i1z:,,,,,,,,,,,,,,,,,,,,,2017-01-09 01:23:37.267,,,"09/Jan/17 1:23 AM;ajayaa;[~Kulbir] If the terminate_reason column in a cluster_insts is FORCED-CLEANUP that means that the cluster could not brought up successfully and forcefully cleaned up during the cluster start process. Even though we log the nodes of such an cluster instance to cluster_nodes table, the customer should not be charged for that cluster instance.

Assigning it to [~xing] to reassign it appropriately.","17/Feb/17 1:12 AM;ajayb;[~ajayaa] I was talking with [~xing] briefly on how the qcuh is calculated. This is done by exporting the cluster_nodes table into S3 and then running hive queries on top of it. Having this calculation be done on top of two tables will mean both need to be exported, but then there will be some skew/gap between their contents due to time taken to export. For the case mentioned above, it should be possible to detect this situation if the termination_reason of such nodes has a special value like say Forced Cleanup. Is that possible?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster API issues in v1.3,ACM-821,52510,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,venkatak,addon_zendesk_for_jira,15/Dec/16 6:28 AM,24/Mar/17 1:30 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,sdk,,,,,,,"Issue reported:
---------------------

Using qds sdk, if you use the v1.3 api, you only get 10 clusters instead of 15 in the list. You must use the v1.3 api in order to get EBS details (which I need).

It looks like neither the qds.py command nor the underlying api currently support passing the paging arguments (judging by the list function in cluster.py https://github.com/qubole/qds-sdk-py/blob/master/qds_sdk/cluster.py#L59).

vagrant@vagrant-ubuntu-wily-64:/vagrant/qubole_cluster_mgmt$ ./bin/setaccount.sh PROD_ADHOC qds.py --vv cluster list | grep label | wc -l 
INFO:qds_connection:[GET] https://api.qubole.com/api/v1.2/clusters 
INFO:qds_connection:Payload: null 
INFO:qds_connection:Params: None 
INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): api.qubole.com 
DEBUG:requests.packages.urllib3.connectionpool:""GET /api/v1.2/clusters HTTP/1.1"" 200 None 
15

vagrant@vagrant-ubuntu-wily-64:/vagrant/qubole_cluster_mgmt$ ./bin/setaccount.sh PROD_ADHOC qds.py --vv --version=v1.3 cluster list | grep label | wc -l 
INFO:qds_connection:[GET] https://api.qubole.com/api/v1.3/clusters 
INFO:qds_connection:Payload: null 
INFO:qds_connection:Params: None 
INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): api.qubole.com 
DEBUG:requests.packages.urllib3.connectionpool:""GET /api/v1.3/clusters HTTP/1.1"" 200 None 
10

To be clear my use-case is using the api directly not via qds.py, but just using it for demonstration.

Customer's Expectation:
------------------------------------

I have no choice but to use the latest api, because I need EBS volume info. But I also need to list the clusters, and qds.py is broken for that in v1.3 because paging is not implemented.
They want to fix Qds.py for older version of APIs as well, which is v1.3 in this case.",,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03gtz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application Logs not backed up on S3,HADTWO-733,52429,Bug,Open,HADTWO,Hadoop2,software,ajayb,,,Critical,,abhishekmodi,adubey,adubey,14/Dec/16 9:07 AM,25/May/17 5:30 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,oracle-bluekai,,,,,,,"46623129 - 

We have seen this in other cases as well, but check this command it failed and there is no way to determine why this one failed. Can we diagnose and fix this type of occurrences. 
this is a new team from Oracle so any early reliability issues can ruin the experience.

cc [~drose@qubole.com]",,abhishekmodi,adubey,jssarma,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,HADTWO-922,,,,,,,,,,,,,,,,,,,,,,,clickagy,comcast,oracle-bluekai,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03gl3:,,,,,,,,,,,,,,,,,,,1.0,14455,2016-12-22 21:37:29.717,,,22/Dec/16 2:44 PM;adubey;[~abhishekmodi] is it a known issue ?,"22/Dec/16 9:37 PM;abhishekmodi;No, we rarely see this issue. I am trying to repro this scenario. Also going through code to figure out any possible reasons.","29/Dec/16 9:41 PM;abhishekmodi;[~adubey]: Just to give an update, we saw this issue with clickagy job too. I think there is some edge case in which log aggregation doesn't happen for failed and killed jobs. I have some possible theory but still need to validate that. I am actively looking into it and will keep you updated with the progress.","30/Dec/16 5:19 AM;jssarma;[~abhishekmodi] - bumping the priority of this. this is critical - we are just not able to debug jobs. i have seen this in comcast and now clickagy. the impact is severe because nothing can be done on old jobs. 

note that the issue is not just with killed commands. Check command https://api.qubole.com/v2/analyze?command_id=48853234 and URL:

https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-54-164-109-62.compute-1.amazonaws.com%3A8088%2Fproxy%2Fapplication_1483011493836_0015%2F&clusterInst=338216",30/Dec/16 5:20 AM;jssarma;also - while i get a <file-not-found> error right now on this URL - for sometime after the command had failed - i got 500 errors on the same URL. we may have a trace of this in our logs.,"30/Dec/16 6:04 AM;abhishekmodi;[~jssarma] : As I mentioned above, it happens with failed or killed queries. Another thing that is common among all the occurrences is that all of them were PIG commands. 

Today I spent couple of hours on this issue and found that log aggregation was successful and they were uploaded to s3 but jhist file was not generated. I will further check whether PIG has some role to play here.","04/Jan/17 1:08 AM;abhishekmodi;Seen it happening with one MR job also. So, I think it's not related to Pig. 
One potential reason could be AM getting killed due to different reasons before writing jhist file.",13/Jan/17 12:10 AM;abhishekmodi;Fixed one of the potential reasons for this issue as HADTWO-762.,"08/May/17 3:40 PM;sbadam;Observed that logs were not synced to S3 for command - 70630615. Both Application and Spark links are not working.
Cluster ID - 30242,  is terminated after ~10mins of command completion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schedule Job was stuck in cancelling state,SCHED-130,52407,Bug,Open,SCHED,Scheduler,software,sumitm,,,Major,,sureshr,biswajit,biswajit,14/Dec/16 3:32 AM,24/May/17 10:09 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"{code}

production-replica> select * from query_hists where id = 43621012 limit 1 \G;
*************************** 1. row ***************************
                        id: 43621012
              qbol_user_id: 12146
                submit_time: 1479639612
                  end_time: NULL
                  progress: 100
                    cube_id: NULL
                created_at: 2016-11-20 11:00:12
                updated_at: 2016-11-20 11:00:13
                      path: /tmp/2016-11-20/5031/43621012
                    status: cancelling
                  host_name: ip-10-79-140-172.ec2.internal
                  user_loc: 1
            qbol_session_id: 2578719
                command_id: 509885
              command_type: CompositeCommand
                      qlog: NULL
            periodic_job_id: 7248
                      wf_id: NULL
            command_source: SCHEDULED
            resolved_macros: {""s3_job_out_location"":""s3://komliuseastprod/crunched-logs/mr-uds-loaders/trackerEvent-loader/output"",""curr_year"":""2016"",""curr_month"":""11"",""curr_day"":""20"",""curr_hour"":""11"",""curr_min"":""00"",""max_files"":""10000"",""trk_table_base_location"":""s3://komliudsprod/hivewarehouse/udsprod.db/trackerevent_new"",""retry_count"":""3"",""wait_time"":""30"",""Qubole_nominal_time_iso"":""2016-11-20 11:00:00+00:00"",""Qubole_nominal_time"":""Sun Nov 20 2016 11:00:00 GMT+0000""}
                status_code: NULL
                        pid: NULL
            editable_pj_id: 25013
                  template: generic
        command_template_id: NULL
command_template_mutable_id: NULL
                can_notify: 0
            num_result_dir: 0
                start_time: 1479639613
                      pool: 
                    timeout: NULL
                        tag: PRODUCTION
                      name: NULL
    saved_query_mutable_id: NULL
                account_id: 5031
1 row in set (0.00 sec)

ERROR: 
No query specified

production-replica> 


{code}


The current job of the schedule workflow was stuck in cancelling state for more than 12 hrs and the concurrency was set to 1. So the next job was not picked up. 

",,biswajit,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,revx,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03gg7:,,,,,,,,,,,,,,,,,,,1.0,11130,2016-12-14 17:55:27.825,,,"14/Dec/16 5:55 PM;sureshr;Is this the same as MW-179?
cc: [~yogeshg]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"json read for file with wildcard (ex: s3://<location>/*.gz) fails with ""Unable to infer schema""",SPAR-1353,52331,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,venkats,megha,addon_zendesk_for_jira,13/Dec/16 2:43 PM,23/May/17 5:44 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Havnt been able to reproduce this in my account, but is consistently reproducible in customer's account.
cmd id:
44512738

The command works fine if following conf is added:
--conf spark.sql.qubole.split.computation=true
ex: 46590751

Also, file with fully qualified name works, for example command: 44027971

",,addon_zendesk_for_jira,rohitk,sbadam,venkats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03g6n:,,,,,,Spark-ZEP Sprint R43,,,,,,,,,,,,,1.0,11455,2016-12-16 21:18:20.434,,,"16/Dec/16 9:18 PM;rohitk;I have seen two kind of problems with wildcards.

1) Use of colon in file names. This causes problem because of how s3 paths are parsed by hadoop classes.  The only place s3 path can have colon is at the start.  
 s3://abc/somefile 
This will fail - s3://abc/some:file.
With wildcards, typically if the file name is based on timestamp, we will see this issue.

2) The second kind of issue is empty path issue. This happens if the s3 directory has a path like this: s3://abc/table//xyz. Notice the double slash. When enumerated, these empty files will cause exceptions.  I believe our split computation code is smart enough to recognise these empty paths and ignore them.

","16/Dec/16 9:24 PM;venkats;Yeah [~rohitk] , with our split computation it works fine. In fact I did some more investigation on this, I could find a pattern where batches of 32 files works fine if it has more it gets this ""Unable to infer schema exception"" . But somehow it works fine, if we have qubole split computation enabled. 

I think I need to spend some more cycles to figure out whats happening underneath which causes the ""Unable to infer schema"" exception.

I tried to reproduce the same behavior in my test cluster with some synthesized json data all gzipped (customer data is also gzipped), but couldn't do it successfully. Also tried to have similar file name as that of customer data for the files.",16/Dec/16 11:09 PM;rohitk;It might be related to https://issues.apache.org/jira/browse/SPARK-16975 also. Which is related to filtering of some of the schema files.,25/Jan/17 5:19 PM;sbadam;Do we have any update or ETA for this issue? Thanks.,27/Jan/17 10:34 AM;venkats;May be we should try the same with Spark-2.0.2 or 2.1.0? Its working with the spark.sql.qubole.split.computation flag set to true.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All Commands Report API  returning only hive query metrics,MW-318,52289,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,satyavathib,satyavathib,12/Dec/16 11:50 PM,28/Jun/17 12:01 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"http://docs.qubole.com/en/latest/rest-api/reports_api/all-commands-report.html

This API is returning reports of hive commands only but the customer wants to get reports of presto queries. Also it returns only non-null values and nulls are ignored.

Can someone look into this. Also Let me know if any details required.",,goden,satyavathib,stagra,surendranm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ensighten,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03fy7:,,,,,,,,,,,,,,,,,,,1.0,11383,2016-12-13 23:08:21.786,,,"13/Dec/16 11:08 PM;stagra;Passing it to MW for a look, this is api specific",14/Dec/16 12:51 PM;surendranm;[~sumitm] can you reassign to the correct person?,04/Jan/17 11:54 AM;goden;I moved this to MW instead of PRES tracker.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The password checking function in the SOC2 environment does not seem to be operating as expected.,UI-4867,52288,Bug,Open,UI,UI,software,aswina,,,Major,,mukundag,drew,drew,12/Dec/16 11:28 PM,29/Jan/17 11:59 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,security_radar,,,,,,,"I was demonstrating to someone how the password function worked.  When I tried to use a password of poor quality (password) it rejected it as expected, but using the Chrome Version 54.0.2840.98 (64-bit), with the LastPass chrome extension when I hit ""Save"" it kicked me out of the password dialog and back out to my profile.  The message displayed only for a second or two, and my password wasn't changed, but I couldn't choose another without going back into my profile and starting over.  Not a huge issue, but would be a poor user experience.

Not sure if this is a UI issue or something else.  [~sureshr] and [~sumitm] maybe you can help classify this into the proper project if this isn't it?

cc: [~beng]


",us.qubole.com,aswina,drew,mukundag,sumitm,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Dec/16 6:30 PM;drew;qu changing my password.png;https://qubole.atlassian.net/secure/attachment/36642/qu+changing+my+password.png,30/Dec/16 6:30 PM;drew;qu password warning too short.png;https://qubole.atlassian.net/secure/attachment/36641/qu+password+warning+too+short.png,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|xzz2b0:,,,,,,,,,,,,,,,,,,,,,2016-12-12 23:36:52.773,,,12/Dec/16 11:36 PM;aswina;[~drew] - Were you trying this out in sign-up page or inside Conrol Panel?,"14/Dec/16 10:01 PM;drew;[~aswina] I was in ""My Profile"" and ""Change Password",17/Dec/16 9:24 PM;aswina;[~mukundag] - Can you take a look please?  This seems to be in us.qubole.com.,29/Dec/16 10:56 PM;mukundag;[~drew] Can you list down the exact steps to reproduce this I lost you in the LastPass chrome extension part.,"30/Dec/16 6:32 PM;drew;[~mukundag] the lastpass browser extension doesn't matter, I just replicated it in an incognito window.  My Profile > Change Password 

Try to change it to a weak password.  I used change.me which was ""Weak"" and hit [Save|https://qubole.atlassian.net/browse/UI-4867#qu changing my password.png], the page submits and for maybe one second the [error message|https://qubole.atlassian.net/browse/UI-4867#qu password warning too short.png] appears and then kicks me back to my account.  There is no chance to enter a better password, and if I blinked I would miss the error message because it is displayed for a much too short period of time.",02/Jan/17 2:45 AM;aswina;cc: [~sumitm],02/Jan/17 2:46 AM;aswina;[~drew] - Which is the SOC2 env?  Is it us.qubole.com?,"02/Jan/17 4:17 AM;sumitm;Yeah, this can be repro on any env.. actually we use {{My Profile}} section for password update as well as name change.. As our UI don't block users to submit weak passwords (it hints only), the request gets submitted and whole control-panel gets reloaded with error message coming from backend..","02/Jan/17 12:10 PM;drew;[~mukundag], [~sumitm], [~aswina] do you have all the information you need now from me, if not let me know if you need something else.","02/Jan/17 7:54 PM;aswina;[~sumitm] [~mukundag] - Should we reject weak passwords?  Secondly, irrespective of whether we reject weak passwords or not, error messages should be displayed properly in its context so that it's meaningful. 

[~mukundag] - Can you add this to next sprint please?",04/Jan/17 12:56 AM;drew;FYI - We must reject weak passwords this is documented as a SOC2 control and will be critical going forward both for SOC2 and for HIPAA and other pending compliance objectives this year.,29/Jan/17 11:59 PM;drew;[~mukundag] any update here?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Sessions class name is still printed as spark context,SPAR-1350,52245,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Critical,,prakharj,satyavathib,satyavathib,12/Dec/16 5:54 AM,07/Aug/17 10:44 PM,09/Aug/17 6:03 AM,,,,,1,bootcamp,in-r47,jira_escalated,,,,,,"Test Command Id : 46404201

In spark 2.0 when class is printed for spark, it throws SparkContext. Could you please have a check on this. 

{code}

println(spark.getClass)

class org.apache.spark.SparkContext

{code}",,drose@qubole.com,gayathrym,mahuja,megha,mpatel,rohitk,satyavathib,xing,,,,,,,,,,,,,,,,,,,,,,,,,,28/Mar/17 10:37 PM;megha;Screen Shot 2017-03-22 at 3.51.19 PM.png;https://qubole.atlassian.net/secure/attachment/40515/Screen+Shot+2017-03-22+at+3.51.19+PM.png,,,,,,,,,,,,,,,,,,,clarivate,oracle,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2017-05-09T23:33:54.000-0700"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Yes - External Release Notes,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03fof:,,,,,,,,,,,,,,,,,,,2.0,"11425,14063",2016-12-12 19:13:32.335,,,12/Dec/16 7:13 PM;rohitk;Is this blocking anything for the customer? What is the impact? ,12/Dec/16 9:51 PM;satyavathib;Customer is asking if  there is a plan to fix this and also that this is kind of a big deal since anyone using Spark 2.0 will need to override this setting.,12/Dec/16 9:57 PM;drose@qubole.com;Per Oracle this is affecting their build out and escalated via a call today. What type of fix is this?,16/Dec/16 1:28 PM;drose@qubole.com;[~rohitk][~sureshr] what is the scope of this fix? It has been identified as a concerns a few times from the the last 2 weeks.,"16/Dec/16 6:39 PM;rohitk;It is a naming issue.
Befor spark 2.0.0 - main object used to interface with spark: SparkContext 
2.0.0 & After         - It is SparkSession, but SparkContext is supported

The ""wrapper code"" or the ""implicit initialisation code"" is common the all versions of spark and has not changed. What this jira suggests is creating different ""wrapper code"" for spark 2.0.0. This isn't much of a problem. 

The problem is, given a single name ""spark"" it can either mean SparkContext or it can mean SparkSession.  For customers starting their spark journey with 2.0.0, spark means SparkSession but those who started their journey with previous versions it means SparkContext.

Once we make spark mean SparkSession in Spark 2.0.0, we will start seeing another flurry of support requests saying it should be SparkContext and not SparkSession. 
----

I had a short discussion with [~bharatb] on this. One of the ways to fix this issues is to expose the  ""spark application template"" to the customers. This way they can make spark mean anything. 


{code:java}
import org.apache.spark._
import org.apache.spark.storage._
import org.apache.spark.SparkContext._
import org.apache.spark.rdd._
object generated {
  def main(args:Array[String]) {
   //autocreation SparkContext so that it works like spark-shell in Analyze
   val sc = new SparkContext(new SparkConf())
   val spark = sc
   ################
   user_script
   ################
  }
}
{code}",03/Jan/17 9:12 AM;drose@qubole.com;[~bharatb][~rohitk] do we have any type of ETA or expectation we can help set for the customer?,"03/Jan/17 8:09 PM;rohitk;I can't think of anything better than wont fix here. 
Assigning to  [~xing]



","04/Jan/17 12:28 PM;xing;Trying to come up to speed here. 

If this is something that Spark 2.0 open source does, I'm not sure why we would want to ""fix"" it. Am I missing something here? This is behavior as expected by Spark 2.0.","28/Mar/17 11:28 PM;megha;[~rohitk]
Looks like this isn't spark 2.0.0 open source behavior.
Attaching screenshot that customer updated: !Screen Shot 2017-03-22 at 3.51.19 PM.png|thumbnail! 


","29/Mar/17 12:58 AM;rohitk;Yes [~megha] this is not. 
It is just a one line change to fix it.

The reason we are hesitating to make this changes is because any user who has used ""spark"" variable as ""SparkContext"" in any of the scala snippets in Analyse (on Spark 2.0.0+), will see that their code doesn't runs any more after we make this change. 

If the user needs a spark session, all he needs to do is to create one: 


{code:java}
val sparkSession = SparkSession
   .builder()
   .appName(""SomeName"")
   .enableHiveSupport()
   .getOrCreate()
{code}

Ideally if we plan to push a change that will break many users code, we should find out a way to find/list all the users/commands who/which will be impacted and communicate it to them before making this change. 
",02/Apr/17 8:51 PM;satyavathib;[~rohitk][~drose@qubole.com] Could you please confirm If we should communicate the above challenges that we are thinking of  to the customer?,"03/Apr/17 2:30 PM;mahuja;I suggest we make the change for R43 where spark is SparkSession. This change will be applicable post Spark 2.0 release. We also need to communicate the change in release notes. 

The idea of using the template while composing Spark commands seems to be the right way to go. For now, we can make it consistent with open source behavior.

[~bharatb], [~rohitk], [~venkats] - thoughts on this?

R43 code freeze is a couple of days away, so we shall be able to absorb this change.","17/Apr/17 11:18 PM;satyavathib;The same issue reported by clarivate today.

{code}
App > script.scala:25: error: method conf in class SparkContext cannot be accessed in org.apache.spark.SparkContext

App > spark.conf.set(s""1pdatamonitoringCassandra-$topenv-$subenv/spark.cassandra.connection.host"", s""1pdatamonitoring.cassandra.us-west-2.$subenv.$topenv.oneplatform.build"")

App > ^

App > script.scala:26: error: value read is not a member of org.apache.spark.SparkContext

App > val pageDefsTable = spark.read.cassandraFormat(""snowplow_page_definitions"", ""datadmin"",s""1pdatamonitoringCassandra-$topenv-$subenv"").load

App > ^

App > script.scala:32: error: value read is not a member of org.apache.spark.SparkContext

App > val pageDefs = spark.read.json(""s3://1p-data-snowplow-dev/snowplow-page-defs/*"")

App > ^

App > script.scala:35: error: value read is not a member of org.apache.spark.SparkContext

App > val eventDefsTable = spark.read.cassandraFormat(""snowplow_events_definitions"", ""datadmin"",s""1pdatamonitoringCassandra-$topenv-$subenv"").load

App > ^

App > script.scala:37: error: value read is not a member of org.apache.spark.SparkContext

App > val eventDefs = spark.read.json(""s3://1p-data-snowplow-dev/snowplow-event-defs/*"")

App > ^

App > 5 errors found

=================================================================================================================

When I tried to create a SparkSession like this,

val spark = SparkSession.builder().appName(""SnowplowEnrichment-v2"").getOrCreate()

The error is,

====================================================================================================================

App > script.scala:20: error: spark is already defined as value spark

App > val spark = SparkSession.builder().appName(""SnowplowEnrichment-v2"").getOrCreate()

App > ^

App > one error found


{code}",18/Apr/17 12:00 AM;rohitk;[~satyavathib] PR is open for this issue.,"27/Jun/17 12:17 PM;mahuja;[~prakharj] lets sync on it to get the background information. [~venkats], [~prachim] and [~rohitk] have the relevant information.","07/Aug/17 10:44 PM;gayathrym;Commit to MASTER :
*  12533a2f0e87f2663ad4e2834ec5e9f4dbcdc219	Mon Aug 7 04:51:55 2017	hive_scripts	master	SPAR-1350	prakharj@qubole.com	fix: dev: SPAR-1350: Pass &quot;spark.set_spark_var_based_on_version&quot; flag to cluster		
								
*  3fe8643fe9347f7924cc38a944cfee95dbed771e	Mon Aug 7 04:50:16 2017	tapp2	master	SPAR-1350	prakharj@qubole.com	fix: dev: SPAR-1350: Set spark as SparkSession for 2.x.x versions		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Users unable to change permissions on HDFS folders\files via chmod\chown commands,HADTWO-727,52216,Bug,Open,HADTWO,Hadoop2,software,ajayb,,,Major,,sourabhg,Kulbir,Kulbir,09/Dec/16 5:28 PM,17/May/17 2:37 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Team,
It seems like we disallow\disable changing permissions on files\folders in HDFS. For e.g
{code}
bash-4.2$ hadoop dfs -mkdir /mpateldir
bash-4.2$ hadoop dfs -ls /
Found 3 items
drwxrwxrwx   - mapred   supergroup          0 2016-12-09 22:40 /mpateldir
drwxr-xr-x   - ec2-user supergroup          0 2016-12-09 22:31 /qubole
drwxr-xr-x   - mapred   supergroup          0 2016-12-09 22:31 /tmp
bash-4.2$ hadoop dfs -chmod 655 /mpateldir
bash-4.2$ hadoop dfs -ls /
Found 3 items
drwxrwxrwx   - mapred   supergroup          0 2016-12-09 22:40 /mpateldir
drwxr-xr-x   - ec2-user supergroup          0 2016-12-09 22:31 /qubole
drwxr-xr-x   - mapred   supergroup          0 2016-12-09 22:31 /tmp
{code}

On checking with [~hiyer] and reviewing code this seems to be happening because we need to have dfs.permissions.enabled flag set to false and as such we simply ignore chmod\chown commands. Is there any reason we are deviating from OSS and not allowing this ?

Another issue I have observed is that with permissions flag set to true, Autoscaling code doesn't work and logs below exception:
{code}
2016-11-17 04:22:46,862 WARN  autoscalingLogger (YarnAutoScalingManager.java:reapShuttingDownNodes(281)) - Got exception in add nodes to excluded hosts: Access denied for user yarn. Superuser privilege is required
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkSuperuserPrivilege(FSPermissionChecker.java:109)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkSuperuserPrivilege(FSNamesystem.java:6481)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.refreshNodes(FSNamesystem.java:5554)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.refreshNodes(NameNodeRpcServer.java:913)
{code}

So effectively if we need to fix this issue, we need to:
-Disable\remove the check in code
OR
-Enable permissions OOB and then modify hdfs-site.xml to include yarn for e.g
{code}

<property>
   <name>dfs.permissions.superusergroup</name>
   <value>hdfs,yarn</value>
 </property>
{code}

Please review and address this bug.

cc [~mpatel]",,abhishekmodi,adubey,gmargabanthu,Jove,Kulbir,mpatel,p.vasa,sbadam,sourabhg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Box,Expedia,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03fi7:,,,,,,,,,,,,,,,,,,,1.0,11526,2016-12-13 10:20:16.953,,,"13/Dec/16 10:20 AM;mpatel;Question: Is dfs.permissions.superusergroup = yarn,hdfs enough? Do we need to have mapred there too?","13/Dec/16 1:08 PM;mpatel;Actually, I take it back [~Kulbir]it looks like you can only assign one group to dfs.permissions.superusergroup ..

so now back to [~abhishekmodi].. any way to safely enable dfs.permissions.enabled ?","13/Dec/16 8:09 PM;abhishekmodi;We previously have dfs permissions enabled but it led to plethora of issues because of different users being used for accessing hdfs. We also saw strange issues, where some commands were intermittently failing because of HDFS permission issues.

I would further investigate it, but it may take some time from our end to safely enable it due to above mentioned issues.","20/Dec/16 3:03 PM;Jove;Hey [~abhishekmodi], we were at box today and just discussed this issue. 

Arani felt that they would need this for sure as a basic functionality of the hadoop file system. I have explained that we don't persist data in HDFS but they have always on clusters and will use HDFS for Oozie job files etc. so they think they would have a use case where they need to set the permission in HDFS.

It seems acceptable though if we have a path to enabling it even with some known limitations... ","25/Jan/17 5:25 PM;sbadam;Hey Sourabh, do we have any update or ETA for this issue? Thanks.","27/Feb/17 12:55 PM;p.vasa;[~sourabhg] Can you please provide an ETA on this one?
Please let me know if you need any additional information.
Thank you.",27/Feb/17 9:55 PM;sourabhg;[~p.vasa] [~sbadam]: We haven't yet prioritized it. Any customers complaining recently? ,"02/Mar/17 11:04 AM;p.vasa;[~sourabhg] we were going through the ON-HOLD tickets queue.
Will update you if the customer asks for it.
Thank you.","16/Mar/17 11:16 AM;adubey;Folks - we are doing in an integration with SnapLogic and they do create some HDFS directories as part of their bootstrap thing. this seems to be hitting there. In the past i have tried a workaround where you can create a directory with umask bits and that works, i will get back here if this becomes an issue. However - it would be better we fix it before it becomes a noticeable issue for some big customer.","05/May/17 4:16 PM;Kulbir;[~sourabhg] [~abhishekmodi] any progress on this one ?
Looks like expedia is also looking for usage of this feature, can we prioritize this ?

cc [~mpatel]
","07/May/17 11:16 PM;sourabhg;[~Kulbir]: We are yet to prioritize this. 

cc - [~ajayb] [~abhishekmodi]","08/May/17 1:38 AM;abhishekmodi;[~sourabhg] could you investigate what is the effort required here. Can we enable permissions for HDFS and do basic sanity testing. If that works, can we put this behind a flag and provide this feature to interested customers?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validation of presto overrides ,UI-4842,52067,Bug,Open,UI,UI,software,aswina,,,Major,,aswina,venkatak,addon_zendesk_for_jira,08/Dec/16 6:33 AM,24/Jul/17 5:21 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer faced an issue where a small mistake in presto overrides caused a lots of troubleshooting and pain in identifying the issue. Customer wants to be able to see some logs from the UI to troubleshoot this.

I would like to see if we can perform some validation of presto overrides while saving the cluster configuration that customer does not have to wait until the command fails because of a poor presto cluster (with wrong configurations passed to it).

For example: like the IAM policy validator we have in AWS",,addon_zendesk_for_jira,aswina,stagra,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,UI-4206,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03f6f:,,,,,,,,,,,,,,,,,,,1.0,11467,2016-12-08 10:55:11.995,,,08/Dec/16 6:33 AM;venkatak;Infact the same thing can be said about all the overrides we do at the cluster level;,"08/Dec/16 10:55 AM;aswina;[~venkatak] - We do validate and show appropriate warnings in ""Override Presto Configuration"" textbox under ""Presto Settings"" in old cluster settings page.  Did the customer use this box or ""Override Hadoop Configuration Variables"" under ""Hadoop Cluster Settings""?  cc: [~stagra] [~udayk]

Please share the configuration that the customer tried to use which errored out, account id or cluster id etc. so that we can debug further.  Kindly assign the jira back to me once sufficient details are added.",08/Dec/16 10:56 AM;aswina;[~venkatak] - Cluster start logs are available in the new clusters page.  It'll be rolled out in Jan.,"08/Dec/16 7:25 PM;stagra;[~venkatak] there is work going on to improve user experience in this part. Check PRES-729
As part of it we will do two things:
1. Do not start cluster if presto configs caused server failure (this was the suggested action by solutions team)
2. Place the contents of presto server logs into Cluster start logs Aswin mentioned above which will make it easy to find out what config caused the failure",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to use ssl with redshift db import/export,SQOOP-81,51914,Bug,Open,SQOOP,SQOOP,software,sumitm,,,Major,,ksr,jellin,jellin,06/Dec/16 9:05 AM,26/Jul/17 1:19 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Scripps has a redshift instance that requires ssl,  there is currently no way to specify specify sqoop options,  therefore I can’t tell db connection to use ssl.

ssl=true&sslfactory=com.amazon.redshift.ssl.NonValidatingFactory

should either allow for ssl connections or allow a way for customer to provide additional options for connection strings in a generic way.",,jellin,ksr,sam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,scripps,ScrippsNetwork,,,,"{repository={count=1, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":1,""lastUpdated"":""2011-07-22T07:04:16.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03enr:,,,,,,,,,,,,,,,,,,,1.0,11297,2016-12-19 05:22:41.4,,,"06/Dec/16 9:06 AM;jellin;cc [~ksr] [~sam]

Sameer I added you as the assignee as there is no default for this project.  Please reassign if needed.","06/Dec/16 1:27 PM;jellin;we did figure this out,  will be adding a kb for this.

the amazon ssl factory does not work; must use postgres one.",15/Dec/16 12:36 PM;jellin;Might be just a doc improvement. ,"19/Dec/16 5:22 AM;ksr;Hi [~jellin]. Did you add a KB for this? Also, can this be closed?","19/Dec/16 7:11 AM;jellin;[~ksr]  https://qubole.atlassian.net/browse/KB-492

I think some of the info on adding extra params should make it into the docs,  but otherwise this jira can be closed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Can not create a Path from an empty string"" error when doing load() on a lot of files",SPAR-1339,51345,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,jayapradag,Jove,Jove,28/Nov/16 7:29 PM,07/Mar/17 5:22 PM,09/Aug/17 6:03 AM,,,sql,,0,jira_escalated,,,,,,,,"Command ID 44646305, error:
{code}
App > Exception in thread ""main"" java.lang.IllegalArgumentException: Can not create a Path from an empty string
App > at org.apache.hadoop.fs.Path.checkPathArg(Path.java:129)
App > at org.apache.hadoop.fs.Path.(Path.java:137)
App > at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:244)
App > at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:437)
App > at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$29.apply(SparkContext.scala:1062)
App > at org.apache.spark.SparkContext$$anonfun$hadoopFile$1$$anonfun$29.apply(SparkContext.scala:1062)
App > at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:304)
App > at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:304)
App > at scala.Option.map(Option.scala:146)
App > at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:304)
App > at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:323)
App > at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)
App > at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)
App > at scala.Option.getOrElse(Option.scala:121)
App > at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)
App > at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
App > at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)
App > at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)
App > at scala.Option.getOrElse(Option.scala:121)
App > at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)
App > at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
App > at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)
App > at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)
App > at scala.Option.getOrElse(Option.scala:121)
App > at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)
App > at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1284)
App > at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
App > at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
App > at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
App > at org.apache.spark.rdd.RDD.take(RDD.scala:1279)
App > at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1319)
App > at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
App > at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
App > at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
App > at org.apache.spark.rdd.RDD.first(RDD.scala:1318)
App > at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.findFirstLine(CSVFileFormat.scala:164)
App > at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:58)
App > at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:392)
App > at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:392)
App > at scala.Option.orElse(Option.scala:289)
{code}

However, this error doesn't happen when we refined the wildcard so it returns less files. See command 44647655.

This error doesn't happen any more once we set:
{code}
--conf spark.sql.qubole.split.computation=true
{code}

Checked with [~venkats].",,aswina,bharatb,gayathrym,Jove,mahuja,rohitk,sumitm,sureshr,,,,,,,,,,,,,,,,,,TES-1791,,,,,,,,,,,,,,,,,,,,,,,,,,,sheknows,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03dfj:,,,,,,,,,,,,,,,,,,,,,2016-12-07 00:49:16.757,,,"07/Dec/16 12:49 AM;bharatb;{code}
production-replica> select * from settings where var = 'spark.qubole_split_computation';
+-----+--------------------------------+-------+----------+------------+---------------------+---------------------+
| id  | var                            | value | thing_id | thing_type | created_at          | updated_at          |
+-----+--------------------------------+-------+----------+------------+---------------------+---------------------+
| 372 | spark.qubole_split_computation | 1     |     NULL | NULL       | 2016-07-06 14:49:45 | 2016-10-20 08:09:14 |
| 382 | spark.qubole_split_computation | 0     |     NULL | NULL       | 2016-07-06 14:49:45 | 2016-07-06 14:49:45 |
| 392 | spark.qubole_split_computation | 0     |     NULL | NULL       | 2016-07-06 14:49:45 | 2016-07-06 14:49:45 |
| 402 | spark.qubole_split_computation | 0     |     NULL | NULL       | 2016-07-06 14:49:45 | 2016-07-06 14:49:45 |
+-----+--------------------------------+-------+----------+------------+---------------------+---------------------+
4 rows in set (0.01 sec)
{code}

I think this is causing the flag to be false in the end, especially in new accounts (where account level feature is not enabled explicitly). This customer does not have the flag turned on at account level and the global level enable is not taking effect.

Solution: we need to delete the three rows of 0 for the flag from the settings table.

Further action: we need to ensure that the same flag does not repeat in settings table.

[~mahuja] who can do the above?","08/Dec/16 9:45 PM;rohitk;Looks like https://issues.apache.org/jira/browse/HADOOP-10218. No fix
","19/Dec/16 11:13 PM;gayathrym;[~aswina] : Can you please advise on how to resolve this?  When is [~tabraiz] back? 
CC [~bharatb]  [~puneetg] [~rohitk] [~mahuja]",19/Dec/16 11:46 PM;aswina;[~gayathrym] [~bharatb] - Please set all occurrences of {{spark.qubole_split_computation}} to {{1}} via super_admin.  This is a known issue in hive_scripts.  cc: [~sumitm] for more info.,"20/Dec/16 12:01 AM;sumitm;instead of setting all to 1, lets delete the duplicates entries directly from db.. this is know issue whenever a new flag gets added to settings.. ","20/Dec/16 12:41 AM;gayathrym;Thanks [~aswina] and [~sumitm] for your responses, +1 to [~sumitm]'s recommendation. Who can do this? We were discussing about doing this in the feature rollout tool - phase II. ",20/Dec/16 2:29 AM;aswina;[~gayathrym] - Can you login to https://api.qubole.com/super_admin/settings and set the values to {{1}}?  I'm not sure I have write access to database.,"20/Dec/16 2:52 AM;rohitk;As far as I understand this, the bug is global settings don't take effect. I believe we sort of depend on this to work. And every time this doesn't work, it causes lots of pain to customers, wastes support and engineering cycles. 

Serious enough to take the following actions:
1) we should fix the product
2) migration script to fix the old stuff
","20/Dec/16 3:36 AM;gayathrym;[~aswina] : I can do it temporarily if required for 2-3 weeks maximum, but that is not scalable, and not the right way to handle this.  The duplicates need to be removed, and that is the permanent solution.  CC [~sureshr] and [~xing] for prioritizing this required.  Agree with [~rohitk]. ","20/Dec/16 3:54 AM;sureshr;Yes, I agree we should fix this properly. Do we have a ticket for the known hive_scripts issue referenced above?","20/Dec/16 4:53 AM;aswina;[~gayathrym] [~sureshr] - I'm unable to find the middleware jira associated
with duplicate entries in settings table.  [~sumitm] may know.



On Tue, Dec 20, 2016 at 5:25 PM, Suresh Ramaswamy (JIRA) <

",20/Dec/16 7:11 AM;aswina;[~bharatb] [~rohitk] - I have set the duplicate entries to {{1}} via super_admin.  So this should now be globally enabled.  ,27/Jan/17 2:53 AM;mahuja;[~bharatb] can we close this ?,27/Jan/17 3:44 AM;bharatb;Would be good to verify that it is really fixed. Maybe good test case for [~jayapradag] to start with.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Workspace python editor indents : editor should fill with spaces not tabs,UI-4809,51343,Bug,Open,UI,UI,software,aswina,,,Major,,mukundag,drose@qubole.com,drose@qubole.com,28/Nov/16 5:24 PM,26/Jan/17 8:00 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"This has to do with editing python Spark application in the Workspace editor. This editor should either :

(a) be robust with respect to tab indent, and not fail with spurious indentation error (i.e., failing on tab indents that appear good to the eye, and requiring replacing each with spaces), or

(b) NOT automatically fill indents with tabs, and fill with spaces instead. There are several operations where the editor automatically indents, but does so with tab, even though the run-time operation always fails due to the inserted tab.",,aswina,drose@qubole.com,gmargabanthu,sbadam,snamburu,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,flipboard,,,,,{},NA,Choose from,,AN-9,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03df3:,,,,,,,,,,,,,,,,,,,,,2016-11-28 20:48:12.762,,,"28/Nov/16 8:48 PM;sureshr;[~drose@qubole.com] Is this in the Notebook editor or the Analyze page?
cc: [~aswina]",01/Dec/16 4:43 PM;snamburu;[~sureshr] [~aswina] it is observed in Workspace editor,"25/Jan/17 4:20 PM;sbadam;Hey Mukunda, do we have any update or ETA for this issue? Thanks.",26/Jan/17 8:00 PM;aswina;[~sbadam] - We haven't prioritized this.  Is this blocking any customer?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected Null value in colums,HIVE-1756,51320,Bug,Reopened,HIVE,qbol hive,software,psrinivas,,,Major,Fixed,psrinivas,biswajit,biswajit,27/Nov/16 9:36 PM,04/Aug/17 2:53 PM,09/Aug/17 6:03 AM,23/Feb/17 3:32 PM,,,,0,in-r41-cp1,jira_escalated,qubot-master,qubot-release-branch-40,qubot-release-branch-41,SOLSUP,,,"Insert Command id :-  44500559

code snapshot :- 

{code}

SUM(action_count) AS total_actions,
SUM(CAST(action_count AS BIGINT) * CAST(action_count AS BIGINT)) AS total_actions_sq,
SUM(CAST(action_count AS BIGINT) * CAST(action_count AS BIGINT) * CAST(action_count AS BIGINT)) AS total_actions_cb

{code}


output :- 

Command id of the test run :- https://api.qubole.com/v2/analyze?command_id=44498514

Ex:
149          	2859	\N

149 * 149 is not 2859...


The inserted data is inconsistent . 

Do let us know if you need more info. ",,abhisheks,adeshr,asomani,biswajit,gayathrym,jssarma,karthikk,Kulbir,mpatel,psrinivas,qubot,rvenkatesh,sam,sureshr,venkatak,,,,,,,,,,,,,,,RM-160,HIVE-1667,RM-138,,05/Dec/16 2:24 PM;mpatel;bad_datareducer_log_r_000069.txt.zip;https://qubole.atlassian.net/secure/attachment/35707/bad_datareducer_log_r_000069.txt.zip,,,,,,,,,,,,,,,,,,,pinterest,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=2}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":2,""lastUpdated"":""2017-02-13T17:13:26.000-0800"",""stateCount"":2,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":2,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Yes-Internal Release Notes,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03da7:,,Commits to print the corrupted values in LazySimpleSerDe.,,41.32.0,,,,,,,,,,,,,,,5.0,"11264,11396,12225,12250,16013",2016-11-27 22:29:52.934,,,27/Nov/16 10:29 PM;venkatak;Could this be prioritized? ,30/Nov/16 8:36 AM;mpatel;Pinterest was unable to reproduce this. I'll put some further comments here...,"05/Dec/16 2:22 PM;mpatel;Happened again on 12/4...

some analysis / notes:

Problematic file on 12/4: s3://pinlogs/pinalytics/segmented_experiment_actions_day_v3/action_dt=2016-12-04/000069
See: 45539772 (data)

I copied the file (copy not move!)
- from s3://pinlogs/pinalytics/segmented_experiment_actions_day_v3/action_dt=2016-12-04/000069
- to: s3://pinlogs/pinalytics/chunyan/mpatel_segmented_experiment_actions_day_v3/action_dt=2016-12-04/000069

Created a dummy table with the problem columns as string instead of int/big int: 45541942

That reveals that semicolon got inserted into the data: 45543738


",05/Dec/16 4:32 PM;mpatel;[~psrinivas] Do you think we can enable some sort of logging / tracing to get more info about what is happening when the problem does happen?,"06/Dec/16 9:45 AM;mpatel;[~psrinivas] re-run did not reproduce the problem. 

What do you think we can do here? 

For now I've asked them to set `set mapred.job.reuse.jvm.num.tasks=1;` in the jobs which are producing the data. It's kind of a shot in the dark.

Ideally, if we have some better way to narrow down the conditions it will help. Do you think there is anything we can log at all?



","06/Dec/16 10:41 PM;psrinivas;
Cc: [~jssarma] , [~rvenkatesh]

[~mpatel] spent some today, but could not conclude anything yet.  Would like to bet on the compress being on, can we disable it for a while to get to the bottom of it. 

Hopeful to get more details by tomorrow eod. 

On the other hand, logs being cleared so aggressively does not give much time to play around. Need to tackle this quickly to debug any pinterest issue. ","08/Dec/16 5:09 PM;Kulbir;[~psrinivas] [~rvenkatesh] We are hitting this same issue with https://qubole.zendesk.com/agent/tickets/11396 as well. Original issue was that Pinterest found some userid's to be missing from their data pipeline on random check for e.g:

Experiment (ios_usm_christmas_board_reminder) started on 11/09. 
For user *349662495977253918*, the current in production hive pipeline only captures records on 11/23, 11/24, 11/25, and 11/30. The pipeline missed 11/26, 11/27, 11/28, 11/29 (query here (https://api.qubole.com/v2/analyze?command_id=45017324) 

I worked with Minesh today and we have narrowed down issue to be data corruption while processing, see :
https://api.qubole.com/v2/analyze?command_id=45959116,45959154

There is somehow a semicolon getting inserted into the userid replacing one character(1) and hence the issue for e.g:
*3496624959772539;8*

This issue is seriously impacting Pinterest's production pipeline\data and is being escalated by their mgmt, so we really need to buckle down on this one and narrow down the root cause ASAP. Let us know what our next steps here are.

cc [~mpatel] [~Jtrail]

","08/Dec/16 6:49 PM;psrinivas;cc: [~Jtrail]
[~Kulbir], [~mpatel]: What is the status on saving logs as of now? ",14/Dec/16 8:27 AM;mpatel;[~psrinivas] we will work on rolling the logging stuff out this week. Will there also be some additional debug fix that we can deploy for pinterest?,14/Dec/16 11:55 AM;psrinivas;That requires a HF deployment or using ExperimentalDJ for Pinterest. ,"17/Dec/16 3:35 PM;psrinivas;cc: [~mpatel], [~Kulbir], [~Jtrail], [~athusoo]

  Here is the plan we are proposing to help us in identifying the data-corruption issue(i.e ';' being part of the BIGINT column). 

   We are dividing the flow in to two parts. 
   1) After the aggregation operator i.e after the SUM() operators, we want to understand if the output of the AGG operator is correct. 
   2) After the serializer() i.e the contents being passed to RecordWriter(that writes the contents to the file). 

We want to insert a log message in the serializer() of the SerDe to verify its inputs that if it is receiving any corrupted data. With this, we can narrow down our debugging further into either halves. Once we have these answers, we can play around with more settings to understand the problem completely. 

This logging will happen under a flag and will not insert any performance penalty. 

I will further update this thread with the ETA of the fix being in production. At the moment, i believe it will be mid next week with a HF deployment. 

",19/Dec/16 9:12 PM;mpatel;Thanks [~psrinivas]. Just to check again (i think we talked about it): This will be gated such that we can enable / disable it per job.,"21/Dec/16 12:59 AM;psrinivas;cc: [~mpatel], [~Kulbir]

Before going further into fixes, I wanted to do few more experiments and validate our theory that 1 is being replaced with "";"". 

Here are the things I did: 

I created two tables out of location ""s3n://pinlogs/pinalytics/chunyan/mpatel_segmented_experiment_actions_day_v3"" 
{code}
CREATE EXTERNAL TABLE IF NOT EXISTS qubole_tmp_seq_hive1756_segmented_experiment_actions_day_v3
    (experiment_hash STRING,
     experiment_group STRING,
     experiment_start_date STRING,
     days_in INT,
     gender STRING,
     country STRING,
     language STRING,
     starting_user_state STRING,
     platform STRING,
     method STRING,
     refer_type STRING,
     action_type INT,
     num_active BIGINT,
     total_actions BIGINT,
     total_actions_sq BIGINT,
     total_actions_cb BIGINT)
PARTITIONED BY (action_dt STRING)
STORED AS SEQUENCEFILE
location 's3://pinlogs/pinalytics/chunyan/mpatel_segmented_experiment_actions_day_v3/';
{code}
and by replacing bigint with strings(to read values): 
{code}
CREATE EXTERNAL TABLE IF NOT EXISTS qubole_tmp_seq_string_hive1756_segmented_experiment_actions_day_v3
    (experiment_hash STRING,
     experiment_group STRING,
     experiment_start_date STRING,
     days_in INT,
     gender STRING,
     country STRING,
     language STRING,
     starting_user_state STRING,
     platform STRING,
     method STRING,
     refer_type STRING,
     action_type INT,
     num_active BIGINT,
     total_actions string,
     total_actions_sq string,
     total_actions_cb string)
PARTITIONED BY (action_dt STRING)
STORED AS SEQUENCEFILE
location 's3://pinlogs/pinalytics/chunyan/mpatel_segmented_experiment_actions_day_v3/';
alter table qubole_tmp_seq_string_hive1756_segmented_experiment_actions_day_v3 recover partitions;
{code}

This location has 2 dirs imporant ones. 
{code}
drwxrwxrwx   -          0 1970-01-01 00:00 /pinalytics/chunyan/mpatel_segmented_experiment_actions_day_v3/action_dt=2016-12-04
drwxrwxrwx   -          0 1970-01-01 00:00 /pinalytics/chunyan/mpatel_segmented_experiment_actions_day_v3/action_dt=retryfull
{code}
action_dt=2016-12-04 has the corrupted data and action_dt=retryfull has the correct data. 


Ran initial set of commands: 
{code}
select * from qubole_tmp_seq_hive1756_segmented_experiment_actions_day_v3 where experiment_hash='61a7fe02b0f3' and experiment_group = 'enabled' and experiment_start_date='2016-11-27' and days_in = 7 and gender = 'F' and country = 'LK' and language = 'en'  and action_dt='2016-12-04';

select * from qubole_tmp_seq_string_hive1756_segmented_experiment_actions_day_v3 where experiment_hash='61a7fe02b0f3' and experiment_group = 'enabled' and experiment_start_date='2016-11-27' and days_in = 7 and gender = 'F' and country = 'LK' and language = 'en'  and action_dt='2016-12-04';


select * from qubole_tmp_seq_hive1756_segmented_experiment_actions_day_v3 where experiment_hash='61a7fe02b0f3' and experiment_group = 'enabled' and experiment_start_date='2016-11-27' and days_in = 7 and gender = 'F' and country = 'LK' and language = 'en'  and action_dt='retryfull';
{code}

and got output as(respectively, though the last query produced huge amount of data, i picked the line which i am interested in): 
{code}
61a7fe02b0f3   enabled   2016-11-27     7    F    LK   en   1    iphone    pinterest NA   1015 1    71   NULL 357911    2016-12-04
61a7fe02b0f3   enabled   2016-11-27     7    F    LK   en   1    iphone    pinterest NA   1015 1    71   504; 357911    2016-12-04
61a7fe02b0f3   enabled   2016-11-27     7    F    LK   en   1    iphone    pinterest NA   1015 1    71   5041 357911    retryfull
{code}


Second set of commands: 
{code}
select * from qubole_tmp_seq_hive1756_segmented_experiment_actions_day_v3 where experiment_hash='61f0ac313d83' and experiment_group = 'control' and experiment_start_date='2016-11-18' and days_in = 16 and gender = 'F' and country = 'US' and language = 'en' and platform='web' and method='facebook' and refer_type='facebook' and action_dt='2016-12-04';


select * from qubole_tmp_seq_string_hive1756_segmented_experiment_actions_day_v3 where experiment_hash='61f0ac313d83' and experiment_group = 'control' and experiment_start_date='2016-11-18' and days_in = 16 and gender = 'F' and country = 'US' and language = 'en' and platform='web' and method='facebook' and refer_type='facebook' and action_dt='2016-12-04';

select * from qubole_tmp_seq_hive1756_segmented_experiment_actions_day_v3 where experiment_hash='61f0ac313d83' and experiment_group = 'control' and experiment_start_date='2016-11-18' and days_in = 16 and gender = 'F' and country = 'US' and language = 'en' and platform='web' and method='facebook' and refer_type='facebook' and action_dt='retryfull';

{code}

Output look like: 
{code}

61f0ac313d83   control   2016-11-18     16   F    US   en   0    web  facebook  facebook  140  66   24881     22567695  NULL 2016-12-04
61f0ac313d83   control   2016-11-18     16   F    US   en   0    web  facebook  facebook  140  66   24881     22567695  27425;03125    2016-12-04
61f0ac313d83   control   2016-11-18     16   F    US   en   0    web  facebook  facebook  140  66   24881     22567695  27425103125    retryfull
{code}

This confirms our theory about 1 is being replaced with "";"". 
","21/Dec/16 2:15 AM;psrinivas;cc: [~mpatel], [~Kulbir], [~Jtrail]

I am ready with my fix: https://bitbucket.org/qubole/hive-0.13.1/pull-requests/193/fix-dev-hive-1756-commits-to-print-the/diff 
I will deploy this to canary tomorrow and will push it as a HF by EOD. Will send out a separate note to the ops-team and Suresh after the canary deployment. ","21/Dec/16 4:16 PM;psrinivas;This is how the logs are going to look when both the params are set true:
Query:
{code}
set qubole.print.corrupted.bigint=true;
set qubole.print.corrupted.bigint.binary.with.perf.impact=true;
insert overwrite table hive_1756 partition(dt=""dummy"") select bar, sum(foo) from test_data group by bar limit 2;
{code}
{code}
2016-12-22 05:43:48,740 -1 ERROR lazy.LazySimpleSerDe:427 (Task Thread for Task: attempt_201612210332_1018_r_000000_0): Printing the corrupted value passed to LazySimpleSerde: 0
2016-12-22 05:43:48,741 -1 ERROR lazy.LazySimpleSerDe:458 (Task Thread for Task: attempt_201612210332_1018_r_000000_0): Printing the byte error stream in: val_00
2016-12-22 05:43:48,741 -1 ERROR lazy.LazySimpleSerDe:427 (Task Thread for Task: attempt_201612210332_1018_r_000000_0): Printing the corrupted value passed to LazySimpleSerde: 10
2016-12-22 05:43:48,741 -1 ERROR lazy.LazySimpleSerDe:458 (Task Thread for Task: attempt_201612210332_1018_r_000000_0): Printing the byte error stream in: val_1010
{code}","21/Dec/16 6:00 PM;psrinivas;Deployed it and verified it on Canary: https://canary.qubole.com/v2/analyze?command_id=220471 

Logs of the reducer look like: https://canary.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fip-10-63-25-56.ec2.internal%3A50060%2Ftasklog%3Ftaskid%3Dattempt_1938.201612220116_0001_r_000001_1%26start%3D-8193&clusterInst=19393
{code}
2016-12-22 01:53:55,432 -1 ERROR lazy.LazySimpleSerDe:452 (Task Thread for Task: attempt_1938.201612220116_0001_r_000001_1): Printing the byte error stream in: d2e2da560f69notification_crp_pin_swap_local_v2enabled13216557357945602312016-11-19
2016-12-22 01:53:55,432 -1 ERROR lazy.LazySimpleSerDe:452 (Task Thread for Task: attempt_1938.201612220116_0001_r_000001_1): Printing the byte error stream in: d2e2da560f69notification_crp_pin_swap_local_v2enabled13216557357945779352016-11-19
2016-12-22 01:53:55,432 -1 ERROR lazy.LazySimpleSerDe:452 (Task Thread for Task: attempt_1938.201612220116_0001_r_000001_1): Printing the byte error stream in: d2e2da560f69notification_crp_pin_swap_local_v2enabled13216557357945902352016-11-22
2016-12-22 01:53:55,432 -1 ERROR lazy.LazySimpleSerDe:452 (Task Thread for Task: attempt_1938.201612220116_0001_r_000001_1): Printing the byte error stream in: d2e2da560f69notification_crp_pin_swap_local_v2enabled13217261045387587752016-11-19
{code}","22/Dec/16 12:07 AM;psrinivas;[~Kulbir], [~mpatel]: HF deployment is done. You can ask Pinterest folks to set this parameter at query level of at hive-bootstrap. 

{code}
set qubole.print.corrupted.bigint=true;
{code}","02/Jan/17 10:03 PM;sureshr;[~psrinivas]: Since the hotfix is complete, can this JIRA be resolved? I assume the fix has also gone into master.
[~abhisheks] and [~karthikk]: Can we accommodate this fix into RB40? If yes, please CP to release-branch-40, [~psrinivas].",02/Jan/17 10:41 PM;abhisheks;sure we can accommodate this fix in rb40 as this is only a logging change and impact is minimal,03/Jan/17 10:42 AM;mpatel;[~sureshr] the fix which [~psrinivas] made is just a debug logging fix to help us troubleshoot. It does not address the cause of the problem. I don't think this can be marked as resolved..,"03/Jan/17 10:50 AM;sureshr;Ah! Ok, I missed that. Yes, in that case, this JIRA should stay open.
[~psrinivas]: Please cp your logging improvements to release-branch-40.","03/Jan/17 12:10 PM;psrinivas;[~sureshr]: I can do that. But, i would not like this fix to be in master. It will be only in the release branches till we get a reproducible or some clue about the issue. ","03/Jan/17 12:18 PM;psrinivas;[~karthikk] FYI, i have CPed the logging changes. ","03/Jan/17 10:24 PM;gayathrym;Commit to R40 :

*  75bb003dbe09a2226fe0c63dba8d16b7a9f478ec    Tue Jan 3 20:17:49 2017    hive2    release-branch-40    UNKNOWN    psrinivas@qubole.com    updated the if condition (cherry picked from commit 229caa1dd8f3b7e69936c5e6c73d0dd4a2b5f30a) (cherry picked from commit fd3fa425e54f89c1946455c3951b7afa25c8d85f)                
                                        
*  75bb003dbe09a2226fe0c63dba8d16b7a9f478ec    Tue Jan 3 20:17:49 2017    hive13    release-branch-40    UNKNOWN    psrinivas@qubole.com    updated the if condition (cherry picked from commit 229caa1dd8f3b7e69936c5e6c73d0dd4a2b5f30a) (cherry picked from commit fd3fa425e54f89c1946455c3951b7afa25c8d85f)","03/Jan/17 10:27 PM;gayathrym;The in-r40 label is to track the log changes for now.  When the fix is ready, we will update the new in-rxx label. ","05/Jan/17 10:50 AM;karthikk;[~gayathrym] the log changes are in, can u update the new label as this came up in ur mail about open/in-progress jiras",05/Jan/17 8:18 PM;gayathrym;Removing the in-r40 label based on [~karthikk]'s confirmation. Will add the new label when the fix is in.  ,11/Jan/17 10:08 PM;asomani;Reduced priority as it hasnt occured. Turning off JVM reuse as helped. No definite proof yet. ,"23/Jan/17 6:52 PM;Kulbir;[~psrinivas] just documenting what we discussed on hangout.
Adding the debug flag in bootstrap has caused job failures: 52317488, 52319747  (have removed it from hive bootstrap)

with below error :
{code}
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""_col0"":""04162ec39cbd"",""_col1"":""ads_shopping_manas_threshold_v2"",""_col2"":""enabled_13"",""_col3"":1,""_col4"":206158434724},""value"":{""_col0"":""2016-12-05""}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:283)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425)
	at org.apache.hadoop.mapred.Child$3.run(Child.java:205)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""_col0"":""04162ec39cbd"",""_col1"":""ads_shopping_manas_threshold_v2"",""_col2"":""enabled_13"",""_col3"":1,""_col4"":206158434724},""value"":{""_col0"":""2016-12-05""}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:271)
	... 3 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:808)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:262)
	... 3 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:425)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:657)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:793)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:86)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:793)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:1064)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:875)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:737)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:803)
	... 4 more
{code}

Keep us posted once you patch this and roll it out as a hotfix.

cc [~mpatel]","23/Jan/17 7:56 PM;psrinivas;cc: [~Kulbir], [~mpatel], [~asomani], [~nitink]

I have added a null check: 
https://bitbucket.org/qubole/hive-0.13.1/commits/2f4ae26f305dce173ea3fb487ec98bbe336ad321

and tested it with 
{code}

set qubole.print.corrupted.bigint=true;

CREATE EXTERNAL TABLE `hive_1756`(
  `bar` string,
  `bar_count` bigint)
PARTITIONED BY (
  `dt` string)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.mapred.SequenceFileInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
LOCATION
  's3n://dev.canopydata.com/psrinivas/hive_1756';

insert overwrite table hive_1756 partition(dt=""npe"") select null , null from test_data limit 1;
{code}

Please review it. 


","24/Jan/17 4:26 AM;abhisheks;For thie hotfix:
verified HIVE-1756 fix on qa2:
https://qa2.qubole.net/v2/analyze?command_id=640501 - passed

failing in prod (without fix):
https://api.qubole.com/v2/analyze?command_id=52361060 - failed",25/Jan/17 8:56 AM;sam;Was this one of the fixes pushed to prod on 1/24?,25/Jan/17 1:55 PM;psrinivas;[~sam]: We don't know the issue yet. The HF went on 1/24 is only to improve the debuggability. ,"27/Jan/17 12:46 PM;qubot;`Pavan Srinivas B <psrinivas@qubole.com>` commited to `release-branch-40 in hive-0.13.1`
 Msg: `fix: dev: HIVE-1756: Added a null pointer check
(cherry picked from commit 2f4ae26f305dce173ea3fb487ec98bbe336ad321)` 
 Link: https://bitbucket.org/qubole/hive-0.13.1/commits/8dfb85d74b5ea5351aaa8c00b6455c540e8668e5",29/Jan/17 9:31 PM;gayathrym;Commit to R40-cP1 above. ,"29/Jan/17 9:34 PM;gayathrym;[~psrinivas]  : From the comments above, it looks like this is for Pinterest, and the comments say that this issue (the debug logs) was hot fixed on 1/24. But I don't see the release-branch-40.0 label in the labels section. Who deployed this hotfix and what was the release version? ","30/Jan/17 11:35 AM;psrinivas;[~gayathrym]: As per [~sureshr] suggestion, I have created a separate jira HIVE-1866 for the hot-fix. It has the required details. ","30/Jan/17 6:22 PM;gayathrym;[~psrinivas] : Thanks for the info, is the hot fix HIVE-1866 different from the fix that you merged into release-branch-40 via this ticket, or are they the same? ","31/Jan/17 12:26 AM;gayathrym;The above mentioned hot fix for the debug logs was released to api.qubole.com on Dec 22 2016, by [~sajant]. Ref : RM-138. Release version : 38.16.0","31/Jan/17 5:22 AM;gayathrym;[~psrinivas] : I am removing the R40-CP1 label from this ticket, please let me know in case I am missing something.","22/Feb/17 8:57 AM;sureshr;[~Kulbir] updated that the problem has not reoccurred since the debug logs were added, so we are waiting...","23/Feb/17 3:30 PM;qubot;`Pavan Srinivas B <psrinivas@qubole.com>` commited to `master in hive-0.13.1`
 Msg: `Squashed commit of the following:

commit b478e0c11ca8c8064779f81d5013d357c532e291
Author: Pavan Srinivas B <psrinivas@qubole.com>
Date:   Tue Jan 24 09:20:49 2017 +0530

    fix: dev: HIVE-1756: Added a null pointer check
    (cherry picked from commit 2f4ae26f305dce173ea3fb487ec98bbe336ad321)
    (cherry picked from commit 8dfb85d74b5ea5351aaa8c00b6455c540e8668e5)

commit d7a124c40ba09d72333283bbd01b22671c294539
Author: Pavan Srinivas B <psrinivas@qubole.com>
Date:   Thu Dec 22 05:48:24 2016 +0530

    updated the if condition
    (cherry picked from commit 229caa1dd8f3b7e69936c5e6c73d0dd4a2b5f30a)
    (cherry picked from commit fd3fa425e54f89c1946455c3951b7afa25c8d85f)
    (cherry picked from commit 75bb003dbe09a2226fe0c63dba8d16b7a9f478ec)

commit f1e81c5f7b55d703dfca886bd275cbb676dbcc0c
Author: Pavan Srinivas B <psrinivas@qubole.com>
Date:   Wed Dec 21 15:40:38 2016 +0530

    fix: dev: HIVE-1756: Commits to print the corrupted values in LazySimpleSerDe(will not commit this to master
    (cherry picked from commit d7ef4cd804b6d2f095c79614e824411e0bc25c64)
    (cherry picked from commit 1dcb36ee1808944529022a40edbe6011c94d2e86)
    (cherry picked from commit 45e8e1f028e11d836f045f5cf5fa6ca884a4ed0e)` 
 Link: https://bitbucket.org/qubole/hive-0.13.1/commits/bd2b21985157e8e130fa1617eb183625601489e1","26/Feb/17 9:44 PM;gayathrym;Commit to MASTER :  [~psrinivas] confirmed on slack that the below commit belongs to this JIRA.
*  bd2b21985157e8e130fa1617eb183625601489e1    Thu Feb 23 23:29:52 2017    hive2    master    UNKNOWN    psrinivas@qubole.com    Squashed commit of the following:            
                                    
*  bd2b21985157e8e130fa1617eb183625601489e1    Thu Feb 23 23:29:52 2017    hive13    master    UNKNOWN    psrinivas@qubole.com    Squashed commit of the following:","27/Feb/17 2:21 PM;qubot;`Pavan Srinivas B <psrinivas@qubole.com>` commited to `release-branch-41 in hive-0.13.1`
 Msg: `Squashed commit of the following:

commit b478e0c11ca8c8064779f81d5013d357c532e291
Author: Pavan Srinivas B <psrinivas@qubole.com>
Date:   Tue Jan 24 09:20:49 2017 +0530

    fix: dev: HIVE-1756: Added a null pointer check
    (cherry picked from commit 2f4ae26f305dce173ea3fb487ec98bbe336ad321)
    (cherry picked from commit 8dfb85d74b5ea5351aaa8c00b6455c540e8668e5)

commit d7a124c40ba09d72333283bbd01b22671c294539
Author: Pavan Srinivas B <psrinivas@qubole.com>
Date:   Thu Dec 22 05:48:24 2016 +0530

    updated the if condition
    (cherry picked from commit 229caa1dd8f3b7e69936c5e6c73d0dd4a2b5f30a)
    (cherry picked from commit fd3fa425e54f89c1946455c3951b7afa25c8d85f)
    (cherry picked from commit 75bb003dbe09a2226fe0c63dba8d16b7a9f478ec)

commit f1e81c5f7b55d703dfca886bd275cbb676dbcc0c
Author: Pavan Srinivas B <psrinivas@qubole.com>
Date:   Wed Dec 21 15:40:38 2016 +0530

    fix: dev: HIVE-1756: Commits to print the corrupted values in LazySimpleSerDe(will not commit this to master
    (cherry picked from commit d7ef4cd804b6d2f095c79614e824411e0bc25c64)
    (cherry picked from commit 1dcb36ee1808944529022a40edbe6011c94d2e86)
    (cherry picked from commit 45e8e1f028e11d836f045f5cf5fa6ca884a4ed0e)
(cherry picked from commit bd2b21985157e8e130fa1617eb183625601489e1)` 
 Link: https://bitbucket.org/qubole/hive-0.13.1/commits/eac76f951c80272d20c44307bf260838bef975a2","27/Feb/17 10:52 PM;gayathrym;Commit to R41-CP1 :
*  eac76f951c80272d20c44307bf260838bef975a2    Mon Feb 27 22:21:02 2017    hive2    release-branch-41    UNKNOWN    psrinivas@qubole.com    Squashed commit of the following:    
                           
*  eac76f951c80272d20c44307bf260838bef975a2    Mon Feb 27 22:21:02 2017    hive13    release-branch-41    UNKNOWN    psrinivas@qubole.com    Squashed commit of the following:","14/Mar/17 1:18 AM;gayathrym;This fix was a part of R41-CP1 and this release was deployed to api.qubole.com on Mar 14 2017, around 1:15pm IST. Release version : 41.32.0.","25/Jul/17 1:00 PM;Kulbir;[~psrinivas] Pinterest had another occurrence of this issue, see https://qubole.zendesk.com/agent/tickets/16013 for details.
Basically cascading job failed with a NumberFormatException:
{code}
2017-07-24 08:46:33,239 -1 WARN mapred.Child:246 (main): Error running child
cascading.tuple.TupleException: unable to sink into output identifier: s3://pinlogs/pinalytics/xd7_users_by_app_sa/snapshot_dt=2017-07-23
    at cascading.tuple.TupleEntrySchemeCollector.collect(TupleEntrySchemeCollector.java:160)
    at cascading.tuple.TupleEntryCollector.safeCollect(TupleEntryCollector.java:145)
    at cascading.tuple.TupleEntryCollector.add(TupleEntryCollector.java:133)
    at cascading.tuple.TupleEntrySchemeCollector.add(TupleEntrySchemeCollector.java:143)
    at cascading.tap.BaseTemplateTap$TemplateCollector.collect(BaseTemplateTap.java:172)
    at cascading.tuple.TupleEntryCollector.safeCollect(TupleEntryCollector.java:145)
    at cascading.tuple.TupleEntryCollector.add(TupleEntryCollector.java:95)
    at cascading.flow.stream.SinkStage.receive(SinkStage.java:90)
    at cascading.flow.stream.SinkStage.receive(SinkStage.java:37)
    at cascading.flow.stream.OpenDuct.receive(OpenDuct.java:45)
    at cascading.flow.stream.OpenDuct.receive(OpenDuct.java:28)
    at cascading.flow.hadoop.stream.HadoopGroupGate.run(HadoopGroupGate.java:93)
    at cascading.flow.hadoop.FlowReducer.reduce(FlowReducer.java:136)
    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509)
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425)
    at org.apache.hadoop.mapred.Child$3.run(Child.java:205)
Caused by: java.lang.NumberFormatException: For input string: ""51911043;957107793""
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Long.parseLong(Long.java:441)
    at java.lang.Long.parseLong(Long.java:483)
    at cascading.tuple.coerce.LongCoerce.coerce(LongCoerce.java:50)
    at cascading.tuple.coerce.LongCoerce.coerce(LongCoerce.java:29)
{code}

and on further investigation it seems like bad data was generated by upstream Hive job:
https://api.qubole.com/v2/analyze?command_id=86040814

The corrupted data was found on s3a://pinlogs/pinalytics/spam_users/snapshot_dt=2017-07-23/source=data_science/000113 output by PopulateSpamUsersFromPossibleSpammersJob. This is the task which output this file https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fec2-52-91-140-48.compute-1.amazonaws.com%3A50030%2Ftaskdetailshistory.jsp%3Fjobid%3Djob_3508.201706021938_73875%26logFile%3Ds3n%3A%2F%2Fqubole-pinterest%2Fdata-core13%2Flogs%2Fhadoop%2F201706021938%2Fhistory%2Fec2-52-91-140-48.compute-1.amazonaws.com_0_job_3508.201706021938_73875_qubole-prod%252540pinterest.com_prod%25253APopulateSpamUsersFromPossibleSpammersJob.lzo%26taskid%3Dtask_3508.201706021938_73875_r_000113&clusterInst=490663

We back up task logs to S3 in this cluster so I checked those:

Reducer log location on S3:
s3://pinterest-gilroy/data-core13/logs/hadoop/201706021938/ec2-54-172-40-108.compute-1.amazonaws.com.node16264/userlogs/attempt_3508.201706021938_73875_r_000113_0/syslog

but couldn't find anything specific to debug patch .

Any further thoughts ? Are we missing some code path in debug patch to capture above scenario ?

cc [~mpatel]","25/Jul/17 11:28 PM;rvenkatesh;[~Kulbir] [~psrinivas] is on vacation today. He should be online tomorrow.

In the mean time, [~asomani] or [~adas] (on call), can you help [~Kulbir] make sure the debug flag is turned on correctly ?","26/Jul/17 10:03 AM;Kulbir;[~rvenkatesh] Debug flag was turned on properly and I verified that already, it seems like there are gaps in debug patch based on what we have seen transpire with this issue, as such we will need to do another iteration with additional code changes to ensure we can capture more details if this happens again. In addition it will also help if someone else can review data from current occurence as well, ensuring I didn't miss anything.

cc [~adas] [~asomani]","27/Jul/17 12:07 AM;rvenkatesh;[~adeshr] since you are on call next week ? Can you look into this ?
[~psrinivas] has a lot of context. He should be online today and tomorrow. Please reach out to him as well. ","01/Aug/17 10:03 AM;mpatel;Hi [~psrinivas], [~rvenkatesh] just checking on this. The problem has not reoccurred, but would be good to prescribe some next steps here. 

Thanks!","01/Aug/17 10:35 PM;rvenkatesh;[~adeshr] can you please provide a status update here ? 

The main action item I can think of is to make sure the debug flag is working fine. ","02/Aug/17 7:45 AM;adeshr;Previously log statements were added in ""LazySimpleSerDe"" to check if the SerDe is getting corrupted value, but in this occurrence of the issue, debug flag was turned on and there were no log statements. This implies that SerDe is receiving correct values.
So, next step is to check/add log statements to the output committer for corrupted values that are written to the file.

cc [~psrinivas]



","03/Aug/17 5:58 AM;adeshr;[~psrinivas] added two flags in the previous commit, one was to check the value received by the SerDe and second was to check the serialized value from SerDe for corrupted data. 

For pinterest only the first flag was enabled, and the second one was disabled (which checks the serialized value returned from SerDe).

[~Kulbir] [~mpatel] Can we set the second flag to true for the query which is causing the issue. Once enabled, this flag will print the serialized data to logs which can then be looked for corrupted data. *Note*: this flag will impact the performance as it will generate a lot of logs and print data in the logs too. So this flag should be turned on for *that* query only. If the log size is too much, the machine can go out of disk space.

{code:java}
set qubole.print.corrupted.bigint.binary.with.perf.impact=true;
{code}

Sample statements in the logs:

{code:java}
2017-08-03 17:41:34,231 INFO [uber-SubtaskRunner] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: ExecMapper: processing 1 rows: used memory = 213607064
2017-08-03 17:41:34,231 ERROR [uber-SubtaskRunner] org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe: Printing the byte error stream in: 2
2017-08-03 17:41:34,231 ERROR [uber-SubtaskRunner] org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe: Printing the byte error stream in: 3
2017-08-03 17:41:34,231 ERROR [uber-SubtaskRunner] org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe: Printing the byte error stream in: 4
{code}","04/Aug/17 2:52 PM;Kulbir;[~adeshr] thanks for the note, we were aware of this flag but I didn't realize it was for output from serde.
That being said, clearly the reason we couldn't enable it in past was due to sporadic/random nature of the issue, and exceptionally large overhead associated for every hive job running in the cluster. Think we will need to find a more creative way to debug this while at the same time ensure that the additional overhead is acceptable. [~psrinivas] if you will be back next week can we ([~mpatel]) get together to discuss gameplan for this ?",,,,,,,,
2 out of 3 calls to s3 fails with 404 ,QBOL-5961,51230,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,sureshr,abhimanyuc,addon_zendesk_for_jira,24/Nov/16 5:04 AM,18/Jan/17 4:05 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"For every url there are 3 calls, 2(HEAD) of them fail with 404 response code, where one(GET) is successful. 

Command id: 42367548

App > 16/11/10 00:21:48 Executor task launch worker-0 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://lyftqubole-iad.s3.amazonaws.com/staging/hive/public_dimension_bills_processed/_temporary/0/task_201611100021_0000_m_000001

App > 16/11/10 00:21:48 Executor task launch worker-0 INFO s3OperationsLog: Method=HEAD ResponseCode=404 URI=http://lyftqubole-iad.s3.amazonaws.com/staging/hive/public_dimension_bills_processed/_temporary/0/task_201611100021_0000_m_000001_$folder$

App > 16/11/10 00:21:48 Executor task launch worker-0 INFO s3OperationsLog: Method=GET ResponseCode=200 URI=http://lyftqubole-iad.s3.amazonaws.com/?delimiter=/&max-keys=1&prefix=staging/hive/public_dimension_bills_processed/_temporary/0/task_201611100021_0000_m_000001/",,abhimanyuc,addon_zendesk_for_jira,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03d27:,,,,,,,,,,,,,,,,,,,,,2017-01-18 16:05:22.127,,,"29/Nov/16 2:26 AM;abhimanyuc;Hi,
Any insights on this?",18/Jan/17 4:05 PM;sbadam;Can we have any update or ETA for this issue? Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running Pig query on Spark cluster,UI-4759,50990,Bug,Open,UI,UI,software,aswina,,,Major,,laliti,abhimanyuc,addon_zendesk_for_jira,21/Nov/16 4:05 AM,21/Nov/16 9:48 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"For the given command id the user used a spark cluster, but spark clusters are not showing for pig queries now, which is how it should be, since pig queries cannot be run on spark clusters from UI I believe.

Command id: 42543728 

Confirmed with user that there were no changes in labels.",,addon_zendesk_for_jira,aswina,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03c2n:,,,,,,,,,,,,,,,,,,,,,2016-11-21 21:48:40.656,,,21/Nov/16 9:48 PM;aswina;[~laliti] [~raghunandan] - This looks similar to the cluster label caching fix that you guys did.  Can one of you take a look please?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Packing code being called after the command has finished execution,MW-230,50985,Bug,Open,MW,Middleware,software,sumitm,,,Major,,yogeshg,satyavathib,satyavathib,21/Nov/16 1:20 AM,28/Nov/16 4:12 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Command ID : 43349379

Account ID : 2555

Cluster ID : 24192

 !image-2016-11-21-14-45-12-119.png|thumbnail! 

From the cluster  usage report there is a cluster restart at 	2016-11-18 11:05:14.

And the query start time is : 18 nov 10: 34

Ran for 13h around.

1. The command was not terminated when the cluster is down.

INFO [command:43349379] [account:2555] [cluster:24192] [cluster_instance:307757]  hivecli.py:163 - __init__ - Default hive version of the account is set as 1.2

Starting Job = job_1479447288851_0002, Tracking URL =  <a href='https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-54-164-225-206.compute-1.amazonaws.com%3A8088%2Fproxy%2Fapplication_1479447288851_0002%2F&amp;clusterInst=*307783*' target='_blank'> Application UI </a> 

A different cluster instance Id recorded .

Also the command failing with exception :

org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-657743718-10.108.62.98-1479447277559:blk_1073745544_4733 file=/tmp/hive-ec2-user/hive_2016-11-18_14-06-54_983_4697913171139297769-16/-mr-10003/fbf98c08-3a9f-46f8-bc3f-af953a0f264f/map.xml 

and 

2016-11-18T18:26:19.000Z         dj ERROR [command:43349379] [account:2555] dj_wait_time exceeded..



",,nitink,psrinivas,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Nov/16 1:15 AM;satyavathib;image-2016-11-21-14-45-12-119.png;https://qubole.atlassian.net/secure/attachment/35017/image-2016-11-21-14-45-12-119.png,,,,,,,,,,,,,,,,,,,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03c1j:,,,,,,,,,,,,,,,,,,,,,2016-11-21 06:38:25.973,,,21/Nov/16 1:20 AM;satyavathib;Cc : [~venkatak],"21/Nov/16 6:38 AM;nitink;The command ran on only one cluster as it submitted on Fri Nov 18 05:35:26 UTC 2016. Refer First job link for details https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-54-164-225-206.compute-1.amazonaws.com%3A19888%2Fjobhistory%2Fjob%2Fjob_1479447288851_0002%2F&clusterInst=307783

The command failed because of s3 exception while creating the temporary folder which is an intermittent issue. It should succeed in retry.

Regarding the last log 
2016-11-18T18:26:19.000Z dj ERROR command:43349379 account:2555 dj_wait_time exceeded..
it seems that it is unrelated to the command running. Had a discussion with [~yogeshg] [~ashishs] as this code path is unrelated to the command running. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cluster start failed - due to ""!"" in metastore password",ACM-764,50934,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,sankets,megha,megha,17/Nov/16 6:45 PM,24/Mar/17 1:43 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"As mentioned above, cluster start was failing (id: 21175)
 (attached are the RM logs, hustler_user_data and RM-timelineserver logs - all are partial as  i couldnt copy whole thing - but they pretty much sum up)

[~mahuja], [~prakharj] and [~Harsh] debugged this to find it was to do with ""!"" mark in their metastore password...",,ajayb,megha,psrinivas,sriramg,,,,,,,,,,,,,,,,,,,,,,,,,,UI-4753,HIVE-1740,,,17/Nov/16 6:43 PM;megha;RMlog.txt;https://qubole.atlassian.net/secure/attachment/35008/RMlog.txt,17/Nov/16 6:43 PM;megha;hustler-user-data.log;https://qubole.atlassian.net/secure/attachment/35009/hustler-user-data.log,17/Nov/16 6:43 PM;megha;rm-timeline.log;https://qubole.atlassian.net/secure/attachment/35010/rm-timeline.log,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03bqn:,,,,,,,,,,,,,,,,,,,,,2016-11-20 21:36:57.797,,,20/Nov/16 9:36 PM;ajayb;The specific issue described in this jira about metastore password is being fixed by [~nitink] as HIVE-1740. I am going to repurpose this jira to be about better handling of any special characters/delimiters by encoding the values before passing to cluster.,24/Mar/17 1:43 PM;sriramg;I think the right solution is to pass the entire cluster config as a whole rather than doing the piecemeal thing we do today.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent Cluster Bring up failures,ACM-759,50903,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,satyavathib,addon_zendesk_for_jira,17/Nov/16 1:48 AM,24/Mar/17 1:45 PM,09/Aug/17 6:03 AM,,,,,0,clusterbringupfailed,jira_escalated,,,,,,,"Expedia is intermittently facing cluster bring up failures with "" NO MASTER NODE FOUND  ""

From my initial analysis of the issue I could see that we are getting the nodes but Unable to latch up to RM with the status.
 
Also I could see that almost 174 clusters on IP '23.21.142.142' .

From the hustler logs :

2016-11-17 07:06:17,721 DEBUG  cluster.py:1260 - get_nodes - returning self._nodes = [<Node: master (i-0f15cbb309dd32528)>, <Node: node0001 (i-0bcd1d61148b90d26)>, <Node: node0003 (i-03308be1e2678b180)>, <Node: node0004 (i-0626d10b0e81b79b4)>]
2016-11-17 07:06:19,400 DEBUG  cluster.py:1218 - get_nodes - existing nodes: {u'i-0f15cbb309dd32528': <Node: master (i-0f15cbb309dd32528)>, u'i-03308be1e2678b180': <Node: node0003 (i-03308be1e2678b180)>, u'i-0bcd1d61148b90d26': <Node: node0001 (i-0bcd1d61148b90d26)>, u'i-0626d10b0e81b79b4': <Node: node0004 (i-0626d10b0e81b79b4)>}

But the cluster bring up failed.

Account Id : 5497

Customer : Expedia.

Reported Clusters : 23618, 23612.",,addon_zendesk_for_jira,mpatel,satyavathib,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03bjr:,,,,,,,,,,,,,,,,,,,,,,,,17/Nov/16 1:49 AM;satyavathib;CC FYI : [~venkatak] [~biswajit],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
job hangs in hive tier when connectivity between datanode and namenode is broken,HADTWO-709,50687,Bug,Reopened,HADTWO,Hadoop2,software,ajayb,,,Critical,,sourabhg,megha,megha,15/Nov/16 6:21 PM,24/May/17 1:10 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Have put the namenode, bad datanode logs and jstack at:
https://drive.google.com/open?id=0B8ud5Y6FWFzvd3BSU1Uzam9peGM

Hive job was run, and according to jstack it was trying to put a block to the cluster..
The namenode shows that it tried to put this block on the bad datanode
However, the baddatanode threw exception:
60000 millis timeout while waiting for channel to be ready for read

Now the hive command process on hivetier node is stuck waiting on Connection to namenode (you can see in jstack file above)

Is there any way to avoid this situation...

",,abhishekmodi,drose@qubole.com,ekang,hiyer,megha,psrinivas,sourabhg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,oracle,turner,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03b3z:,,,,,,,,,,,,,,,,,,,3.0,"10841,11611,14848",2016-11-16 09:30:38.777,,,"16/Nov/16 9:30 AM;sourabhg;Tried to debug the issue today. Following are my observations so far: 

The customers' tez jobs have set a replication factor of 20 (and only 15 datanodes were live). This means a block will be replicated on all the live nodes. 
There was one bad datanode `10.108.57.13` on which block replication request failed everytime. Because of this failure, the block replication probably got stalled on other nodes as well. I verified this by running hadoop dfs -put command. I then reduced the replication factor to 4 and whenever datanode 10.108.57.13 (bad one) was not selected for replication, the command worked fine. 

For my command I was seeing the following logs on the bad datanode: 
{code}
2016-11-16 06:26:46,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1745725929-10.108.63.135-1479139552301:blk_1073743935_3222, type=HAS_DOWNSTREAM_IN_PIPELINE
java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2209)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1164)
        at java.lang.Thread.run(Thread.java:745)
{code}

Also there was hardly any disk space used by hdfs on this bad node
{code}
[ec2-user@ip-10-108-57-13 ~]$ df -h
Filesystem         Size  Used Avail Use% Mounted on
/dev/xvda1          32G   16G   17G  49% /
devtmpfs            32G   68K   32G   1% /dev
tmpfs               32G     0   32G   0% /dev/shm
/dev/mapper/mebs2  985G  121M  975G   1% /media/ebs2

{code}

hdfsadmin also reported the node as normal:
{code}
""10.108.57.13"" : {
      ""hostname"" : ""ip-10-108-57-13.ec2.internal"",
      ""decommission-status"" : ""normal"",
      ""configured-capacity"" : ""1055684034560 (983.18 GB)"",
      ""dfs-used"" : ""2703360 (2.58 MB)"",
      ""non-dfs-used"" : ""12211992576 (11.37 GB)"",
      ""dfs-remaining"" : ""1043469338624(971.81 GB)"",
      ""dfs-used-percentage"" : ""0.00"",
      ""dfs-remaining-percentage"" : ""98.84"",
      ""last-contact"" : ""Wed Nov 16 06:42:30 MST 2016""
    },
{code}

I am not sure what went wrong with the node `10.108.57.13`.  Also googled the Sockettimeout errors and people have suggested to increase the timeout of dfs.client.socket-timeout (default: 60000) and dfs.datanode.socket.write.timeout (default: 480000) and make it as large as 3000000 ms. Not sure if it is going to help. 

The customer had terminated the cluster before I could debug further. 

[~megha]: Can we ask the customer to keep the cluster alive if they face the issue again ? 
","16/Nov/16 4:58 PM;megha;This is happening again [~sourabhg]
I have told them to leave the cluster up this time.. The cluster id is 18914

In order to unblock them Ive told them to move over label ""hadoop2"" to new cloned cluster..

Current bad datanode seems to be ip-10-144-241-133.ec2.internal/10.144.241.133

","05/Dec/16 3:37 PM;megha;[~sourabhg] 
I see this reoccurring many times. This time the cluster is at 51 nodes, so node count is not less then 15.
The cluster is not heterogenous any more.
New failure ids:
45541689, 45517239

What else can we check here? ",05/Dec/16 8:23 PM;sourabhg;[~megha]: I will look into it today. ,"08/Dec/16 3:10 AM;sourabhg;[~megha]: I tried to look at the logs. 

ip-10-108-60-179 was the bad datanode we saw in failed queries. The cluster was down so I tried to fetch the logs from s3. Looks like the name node logs were not synced properly because name node logs does not contain any info about datanode ip-10-108-60-179 (not even registration info)

hadoop dfs -ls on s3 files shows : 
{code}
-rwxrwxrwx   1    1403380 2016-12-06 01:21 /qubole/prod-operations-consumer/logs/hadoop/24192/320771/ec2-54-174-83-214.compute-1.amazonaws.com.node0087/hdfs/hadoop-hdfs-datanode-ip-10-108-60-179.log

-rwxrwxrwx   1     459416 2016-12-05 12:21 /qubole/prod-operations-consumer/logs/hadoop/24192/320771/ec2-54-175-219-204.compute-1.amazonaws.com.master/hdfs/hadoop-hdfs-namenode-ip-10-108-52-196.log
{code}

Note that name node log file was last updated on  *2016-12-05 12:21* whereas bad datanode log file was last updated on *2016-12-06 01:21*

Looked at the data node log file, The only thing I could see was all the block writes on it failed. 

Can we request the customer to *not* terminate the cluster when they start hitting issues ? ",21/Dec/16 4:04 AM;abhishekmodi;Increasing priority of this as we are seeing it again.,"03/Jan/17 9:01 AM;drose@qubole.com;[~abhishekmodi][~sourabhg] I wanted to reiterate the urgency on this issue, do we have the root cause understood?","04/Jan/17 4:00 AM;sourabhg;[~drose@qubole.com]: No, we have not been able to figure out the root cause so far. HDFS logs haven't helped much. 

If we see this issue again, then it would be great if we keep the cluster UP, so that we can debug it live. 
I have requested to keep the cluster UP many times. I think we have communicated this to customer as well but their cluster is down by the time the issue is reported and we start looking into it. 

","16/Feb/17 1:26 PM;megha;HI [~sourabhg]
I think it goes down due to ""automatic termination"". 
I've pinged again to find out if they saw this lately. If they do, is there something that I can check or some logs that I can take a backup of..? 
I'll also tell them to ""disable automatic termination"" 

","16/Feb/17 1:27 PM;megha;Actually customer just solved the ticket saying they didnt see it anymore..
I'm closing this jira as well..
But it would still be good to know if there are any logs, and resource etc that can be checked in case the issue is seen again.

","24/May/17 1:09 PM;ekang;Account: TDC ETL
User: virginia
Command: https://api.qubole.com/v2/analyze?command_id=74242818

Turner is seeing this error. Cluster is not set to auto terminate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Workspace : Saves and runs a query, even when there is no change in query",UI-4713,50640,Bug,Reopened,UI,UI,software,aswina,,,Critical,Fixed,tabraiz,monikak,monikak,14/Nov/16 10:19 PM,08/May/17 2:55 AM,09/Aug/17 6:03 AM,28/Mar/17 12:04 AM,,Analyze v2,,0,jira_escalated,qubot-master,,,,,,,"1. Log in to qa
2. Go to Analyze -> Workspace
3. Click on a Saved Query
4. Change something in query and click on Save. It will save the changes with a new version
5. Now, click on Run. 

Observation : This time also it saves another version of query, which is not required.","qa
Firefox
MAC",aswina,gayathrym,megha,monikak,nimitk,qubot,sureshr,veenamj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,scripps,ScrippsNetwork,,,,"{pullrequest={dataType=pullrequest, state=MERGED, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2017-03-28T00:03:37.000-0700"",""stateCount"":1,""state"":""MERGED"",""open"":false},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Yes - External Release Notes,,UI-4372,,,,,#Empty,No,,,,,,No,nimitk,#Empty,,1|z03atj:,,Save and run in workspace saves another version of the query even when there is no change from the previous one.,,,,UI Sprint 26 (15-Feb - 28-Feb),UI Sprint 27 (1-Mar - 14-Mar),UI Sprint 28 (15-Mar - 28-Mar),UI Sprint 34 (7-Jun - 20-Jun),,,,,,,,,,,,2017-03-28 00:03:47.162,,,"28/Mar/17 12:03 AM;qubot;`tabraiz ali <tabraiz@qubole.com>` commited to `master in qbol`
 Msg: `fix: usr: UI-4713: Saves and runs a query, even when there is no change in query

Approved-by: Lalit Indoria <laliti@qubole.com>
Approved-by: Aswin Anand <aswina@qubole.com>
Approved-by: Mukunda Gogoi <mukundag@qubole.com>` 
 Link: https://bitbucket.org/qubole/qbol/commits/0d8b16baf518de566ca84f0905e166a125b5d632","29/Mar/17 6:45 PM;gayathrym;Commit to MASTER :
*  0d8b16baf518de566ca84f0905e166a125b5d632	Tue Mar 28 07:03:31 2017	hive_scripts	master	UI-4713	tabraiz@qubole.com	fix: usr: UI-4713: Saves and runs a query	even when there is no change in query		
									
*  0d8b16baf518de566ca84f0905e166a125b5d632	Tue Mar 28 07:03:31 2017	tapp2	master	UI-4713	tabraiz@qubole.com	fix: usr: UI-4713: Saves and runs a query	even when there is no change in query		","30/Mar/17 5:39 AM;monikak;Few scenarios to verify :

- scenario1 
1. Go to Workspace
2. Click on create. Select Hive Query and enter show tables;
3. Click on Shift+Enter.
4. It opens a dialog. Save with query name

- scenario 2
1. Go to Workspace
2. Click on any existing query. Edit the query and click on Save. It saves another version
3. Now, without editing anything, click on Shift+Enter.","18/Apr/17 6:50 AM;nimitk;verified on qa-RB43, working fine

On Clicking ""Run"" for a saved query the new version gets created only when there is unsaved change in query or configuration.

scenario 1, step 4 mentioned in previous comment is not applicable now. We can run queries from workspace also without save

scenario 2, is working fine now after the fix.","24/Apr/17 1:59 AM;nimitk;Reopening it as this is still reproducible on qa-RB43 for data import,data export, redshift query commands

Not sure but general observation: it is reproducible for the command types where we have drop-down selections
",02/May/17 11:34 PM;sureshr;Discussed in the UI team meeting and decided that this was not a blocker for R43. We will keep this bug reopened and Tabraiz will work on it after the data modeling work. Most likely after mid-May.,02/May/17 11:34 PM;sureshr;[~tabraiz]: Please have this JIRA ticket removed from the release notes.,03/May/17 1:39 AM;veenamj;Removed it in the [R43 Release Notes|https://docs.google.com/document/d/1VA65QweOY0NGC0uXxXxE1wcPTCBiA-p0nnlcfyWed-c/edit#]. But [~gayathrym] - this jira still has the in-r43 label. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Old cluster master is cached in our Middletier even after the cluster is restarted,QBOL-5927,50572,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,sureshr,venkatak,venkatak,13/Nov/16 10:46 PM,13/Nov/16 10:47 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Check command id:
42414518

Command log is attached.

This seems to be an issue with our middle tier, where the old cluster master ip is cached. Actually the current cluster master is not "" ec2-52-221-226-253.ap-southeast-1.compute.amazonaws.com"" (this was an r3.2xlarge instance that went down at 2016-11-10 06:32:25 UTC time) as you can see. 

The command has run at 2016-11-10 08:33:26,068 UTC

Can we check why there are such caching issues?",,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Nov/16 10:45 PM;venkatak;HikeCommandlog.txt;https://qubole.atlassian.net/secure/attachment/34712/HikeCommandlog.txt,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03ajj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster ID 21921 under Content Cloud account could not start for several minutes,ACM-746,50558,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,ajithr,addon_zendesk_for_jira,12/Nov/16 5:20 AM,10/Jul/17 3:53 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster ID 21921 under ""Content Cloud"" account could not be brought up recently.

Wonder what could explain this - The cluster_instance is 'UP', however all it's nodes are down. This looks like a race condition to me. Hence opening this JIRA. The DB states at the time of the issue:

{noformat}
production-replica> select * from cluster_insts where cluster_id=21921 and cluster_state='UP'\G;
*************************** 1. row ***************************
                             id: 300608
                     cluster_id: 21921
              cluster_config_id: 62470
                  cluster_state: UP
                       start_at: 2016-11-09 19:03:52
                        down_at: NULL
               terminate_reason: NULL
    last_health_check_action_at: NULL
       last_health_check_action: NULL
                     created_at: 2016-11-09 18:58:38
                     updated_at: 2016-11-09 19:03:52
start_cluster_manage_command_id: 0
                 public_ssh_key: NULL
                private_ssh_key: NULL
1 row in set (0.00 sec)

production-replica> select * from cluster_nodes where cluster_inst_id=300608\G;
*************************** 1. row ***************************
                  id: 1264834369
     ec2_instance_id: i-812cfade
            hostname: ec2-35-163-73-117.us-west-2.compute.amazonaws.com
                role: master
amazon_instance_type: r3.2xlarge
       spot_instance: 0
             up_time: 2016-11-09 18:58:49
           down_time: 2016-11-09 19:06:20
      last_seen_time: 2016-11-09 19:06:20
          account_id: 3910
          private_ip: ip-172-17-18-166.us-west-2.compute.internal
     cluster_inst_id: 300608
              status: NULL
  private_ip_address: 172.17.18.166
  termination_reason: Cluster Terminated.
*************************** 2. row ***************************
                  id: 1264834370
     ec2_instance_id: i-1c2cfa43
            hostname: ec2-35-163-98-91.us-west-2.compute.amazonaws.com
                role: node0001
amazon_instance_type: r3.2xlarge
       spot_instance: 0
             up_time: 2016-11-09 18:58:51
           down_time: 2016-11-09 19:06:20
      last_seen_time: 2016-11-09 19:06:20
          account_id: 3910
          private_ip: ip-172-17-43-148.us-west-2.compute.internal
     cluster_inst_id: 300608
              status: NULL
  private_ip_address: 172.17.43.148
  termination_reason: Cluster Terminated.
*************************** 3. row ***************************
                  id: 1264834371
     ec2_instance_id: i-1d2cfa42
            hostname: ec2-35-163-90-156.us-west-2.compute.amazonaws.com
                role: node0002
amazon_instance_type: r3.2xlarge
       spot_instance: 0
             up_time: 2016-11-09 18:58:51
           down_time: 2016-11-09 19:06:20
      last_seen_time: 2016-11-09 19:06:20
          account_id: 3910
          private_ip: ip-172-17-43-147.us-west-2.compute.internal
     cluster_inst_id: 300608
              status: NULL
  private_ip_address: 172.17.43.147
  termination_reason: Cluster Terminated.
3 rows in set (0.01 sec)
{noformat}

Unlike ACM-740, in this case, the customer was able to shut down the cluster and a restart fixed the issue. If this is a DUP of ACM-740, please feel free to merge.",,addon_zendesk_for_jira,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03agf:,,,,,,,,,,,,,,,,,,,1.0,10845,2017-01-18 16:01:09.156,,,"18/Jan/17 4:01 PM;sbadam;Hey Ajay, can we have any update or ETA for this issue? Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S3n Cache timeout should be same as Query timeout,MW-200,50518,Bug,Open,MW,Middleware,software,sumitm,,,Major,,vagrawal,Kulbir,Kulbir,10/Nov/16 6:25 PM,15/Nov/16 3:37 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Per Hive-1101 we changed the s3n cache timeout to 24 hours however Pinterest is still issuing same problem with long running queries for e.g 41496956.

Given the query timeout is 36 hours at TAPP layer we should change S3N cache timeout to match same.

cc [~mahuja] [~mpatel]",,Kulbir,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03a7r:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFErrors for too big DAGs,QBOL-5922,50501,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,sumitm,Ranjana,addon_zendesk_for_jira,10/Nov/16 1:52 AM,17/Nov/16 3:37 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Diana Falkowska
Tuesday 03:01 pm
When I create quite big dag and I try to run backfill on it I get EOFError:

```/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py:469: Warning: (1265L, ""Data truncated for column 'pickle' at row 1"") 
cursor.execute(statement, parameters) 
Traceback (most recent call last): 
File ""/home/vagrant/de_ve/bin/airflow"", line 15, in <module> 
args.func(args) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/airflow/bin/cli.py"", line 117, in backfill 
pool=args.pool) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/airflow/models.py"", line 3023, in run 
job.run() 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/airflow/jobs.py"", line 173, in run 
self._execute() 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/airflow/jobs.py"", line 819, in _execute 
pickle_id = pickle.id 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/attributes.py"", line 237, in __get__ 
return self.impl.get(instance_state(instance), dict_) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/attributes.py"", line 579, in get 
value = state._load_expired(state, passive) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/state.py"", line 592, in _load_expired 
self.manager.deferred_scalar_loader(self, toload) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 674, in load_scalar_attributes 
only_load_props=attribute_names) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 223, in load_on_ident
return q.one() 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2754, in one 
ret = self.one_or_none() 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/query.py"", line 2724, in one_or_none 
ret = list(self) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 90, in instances 
util.raise_from_cause(err) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 202, in raise_from_cause 
reraise(type(exception), exception, tb=exc_tb, cause=cause) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 75, in instances 
rows = [proc(row) for row in fetch] 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 435, in _instance 
loaded_instance, populate_existing, populators) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/orm/loading.py"", line 496, in _populate_full 
dict_[key] = getter(row) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py"", line 1462, in process 
return loads(value) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/dill/dill.py"", line 260, in loads 
return load(file) 
File ""/home/vagrant/de_ve/local/lib/python2.7/site-packages/dill/dill.py"", line 250, in load 
obj = pik.load() 
File ""/usr/lib/python2.7/pickle.py"", line 858, in load 
dispatch[key](self) 
File ""/usr/lib/python2.7/pickle.py"", line 880, in load_eof 
raise EOFError 
EOFError```

If I cancel some branches of my dag (any of them, just make it smaller) then error disappears. This column in `dag_pickle` table has type `blob`: 
```describe dag_pickle; 
+--------------+------------+------+-----+---------+----------------+ 
| Field | Type | Null | Key | Default | Extra | 
+--------------+------------+------+-----+---------+----------------+ 
| id | int(11) | NO | PRI | NULL | auto_increment | 
| pickle | blob | YES | | NULL | | 
| created_dttm | datetime | YES | | NULL | | 
| pickle_hash | bigint(20) | YES | | NULL | | 
+--------------+------------+------+-----+---------+----------------+```

so has the maximum length of 65,535 bytes. Is that possible that it got extended? How to deal with this error?

",,addon_zendesk_for_jira,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,wikia,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-11-15T12:08:09.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Yes - External Release Notes,,AIR-1,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03a3z:,,Upgrading *dag_pickle* table column *pickle* to mediumblob from blob. ,,,,,,,,,,,,,,,,,,,2016-11-10 01:53:51.007,,,"10/Nov/16 1:53 AM;sumitm;Response on qubole.slack.com 

from Sumit Maheshwari: 
Hi @diana as u've correctly identified the issue, we've to run a migration to update the `blob` to `mediumblob` or `longblob`.. as doing that via code would've to wait till next release, u can consider doing that directly on rds instance..

from Diana:
Yes, I've tried my local env - I've changed `pickle` type to `mediumblob` and works perfectly. We didn't test it on our dev/staging env because we're blocked with other issue, but it seems to work. So we've found a workaround, but in the core product still this functionality can be problematic. Thanks for advices!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Same code that runs in Analyze does not work in Notebook,ZEP-621,50500,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,mahuja,abhimanyuc,abhimanyuc,10/Nov/16 1:45 AM,22/Nov/16 12:02 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Spark version: 1.6.1

Using sc.addPyFile in the notebook gives error:

Py4JJavaError: An error occurred while calling o229.addFile. 
: java.lang.NullPointerException 
at org.apache.spark.SparkFiles$.getRootDirectory(SparkFiles.scala:37) 
at org.apache.spark.SparkContext.addFile(SparkContext.scala:1458) 
at org.apache.spark.SparkContext.addFile(SparkContext.scala:1412) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:606) 
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231) 
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381) 
at py4j.Gateway.invoke(Gateway.java:259) 
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133) 
at py4j.commands.CallCommand.execute(CallCommand.java:79) 
at py4j.GatewayConnection.run(GatewayConnection.java:209) 
at java.lang.Thread.run(Thread.java:745) 
(, Py4JJavaError(u'An error occurred while calling o229.addFile.\n', JavaObject id=o230), )


The only difference in the code is that the user is commenting out this line in the Notebook since notebook already gives a spark context:

#	sc = SparkContext(appName=""Pythonctr"") 


Successful command id in Analyze: 40516910

Notebook id: 22264

Attaching a screenshot of the successful Analyze command and the erroneous para from Notebook.",,abhimanyuc,mahuja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Nov/16 1:43 AM;abhimanyuc;saavn1.png;https://qubole.atlassian.net/secure/attachment/34701/saavn1.png,10/Nov/16 1:44 AM;abhimanyuc;saavn2.png;https://qubole.atlassian.net/secure/attachment/34700/saavn2.png,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03a3r:,,,,,,,,,,,,,,,,,,,,,2016-11-22 00:02:18.737,,,13/Nov/16 10:03 PM;abhimanyuc;Any insights?,21/Nov/16 10:16 PM;abhimanyuc;[~mahuja] updates on this one?,"22/Nov/16 12:02 AM;mahuja;[~abhimanyuc] - we could not schedule it till now. Can we rerun the notebook paragraph to generate logs.

Generally, it is helpful to add a note if there are any errors in driver log or failed executors with a possible cause.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Hive API calls fails most of the time when running from airflow,QBOL-5920,50422,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,tanishg,svstaden,addon_zendesk_for_jira,09/Nov/16 10:40 AM,18/Jan/17 3:34 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Hooklogic are encountering a recurring job failure when run through airflow.

From the Zendesk ticket:

I made a cleaned up version to illustrate the problem. It failed here:

https://api.qubole.com/v2/analyze?command_id=42099875

The complete code for this failed run is attached. Please note that Qubole API was used in the code in augment_transaction_data.py in four places:

line 184: worked. It is just 'show tables' 
line 222: worked. It is create table statement 
line 246: failed. It is longer and more complex 
line 250: did not even get there.

Its able to run successfully as a Hive command:

https://api.qubole.com/v2/analyze?command_id=41720609

h3. Zendesk Support Attachments


[Attachment 1|https://qubole.zendesk.com/attachments/token/WPMciYpovom1nYVMYjWUjprk4/?name=ap-attribution.zip]

[Attachment 2|https://qubole.zendesk.com/attachments/token/hGDBR5ePyg2K4eO0NNZtuA1Pv/?name=attribution_dag.py]
",,addon_zendesk_for_jira,psrinivas,sbadam,yogeshg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z039xj:,,,,,,,,,,,,,,,,,,,,,2016-11-11 01:12:23.77,,,11/Nov/16 1:12 AM;yogeshg;[~svstaden] This issue is linked with MW-127. Not related to Airflow. From the jira it looks like there is problem with sessions in python qds_sdk. Assigning it to [~tanishg] to look at it further.,"18/Jan/17 3:34 PM;sbadam;Hey Tanish, can we have any update or ETA for this issue? Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler Using Cron Runs Job immediately,SCHED-126,50321,Bug,Open,SCHED,Scheduler,software,sumitm,,,Major,,ksr,jellin,jellin,08/Nov/16 3:25 PM,19/Jun/17 2:08 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,r44-candidate,,,,,,,"I have scheduled a job using the following cron expression.

22 23 * * ?

I have also ticked of “skip missed instances” as soon as I save the job it runs.  I am expecting this job to not run at all until 23:22 GMT.

You can see this behavior in my account schedule 13356, account 6110 

See attached screenshot




",,aabbas,aswina,jellin,kmayank,mukundag,rahulg,rvenkatesh,sbadam,sumitm,sureshr,tabraiz,,,,,,,,,,,,,,,,,,,UI-5158,,,,08/Nov/16 3:24 PM;jellin;Scheduler_ __Qubole_Data_Services__QDS_.png;https://qubole.atlassian.net/secure/attachment/34501/Scheduler_%C2%A0__Qubole_Data_Services__QDS_.png,,,,,,,,,,,,,,,,,,,turner,,,,,{},NA,Choose from,,SCHED-149,,,,,#Empty,No,,,,,,No,,#Empty,,1|z039nb:,,,,,,,,,,,,,,,,,,,1.0,10812,2016-11-09 02:56:52.035,,,"09/Nov/16 2:56 AM;rahulg;This was a trick case to debug.
I wanted to create a scheduler which will get executed at 23:22PM everyday starting today.
So, When I create a scheduler using cron expression from UI, the payload thats being sent to server has values:

{noformat}
- start_time:2016-11-09 00:00
- end_time:2019-11-09 00:00
- time_zone: Chennai
- cron_expression:0 22 23 * * ?
{noformat}

and these gets converted to UTC and are saved to DB as
{noformat}
created_at: 2016-11-09 09:59:54
start_time: 2016-11-08 18:30:00
end_time: 2019-11-08 18:30:00 
time_zone: Asia/Kolkata
cron_expression: 0 22 23 * * ? 
next_materialized_time: 2016-11-08 23:22:00
{noformat}

The issue here is, 
1. `start_time` and `end_time` were not selected by user as user does not have an option to select in UI. The value were chosen based on todays date.  cc: [~aswina] [~tabraiz]

2. Only `start_time` and `end_time` were converted to UTC time zone where as the `cron_expression` remained same.  This lead to an extra execution.

The same happened when Jeffrey created a scheduled job with cron expression.

The `skip_missed_instances` is working fine. 

cc: [~sumitm] [~sureshr]","09/Nov/16 3:21 AM;aswina;I think this has to do with calculation of ""next materialized time"".  If 'minute' part of cron expression is filled in, then next materialized time should be based on that instead of scheduling it immediately.

cc: [~rvenkatesh]","09/Nov/16 3:24 AM;aswina;On UI, I think we should display start and end date even if a cron expression has to be selected.  cc: [~kmayank] [~mukundag] [~tabraiz]","09/Nov/16 4:26 AM;jellin;two questions, 

1. Can an api call be used to get around this as a temporary work around?  I don’t see an example of using a cron expression in the docs.

2. why the inconsistency of when “skip_missed_instances” appears.  I only see it with cron.  

Also According to the docs.

{{Set this parameter to true if you want to skip instances supposed to have run in the past and run only the latest instances. By default, this parameter is set to false. When a new schedule is created, the scheduler runs instances from start time to the current time. For example, if a daily schedule is created from Jun 1, 2015 on Dec 1, 2015, jobs are run for Jun 1, 2015, Jun 2, 2015, and so on. If you do not want the scheduler to run the missed instances for months earlier to Dec, set no_catch_up to true.}}

This seems like if I issue a cron command the start date set should be irrelevant?  It most cases the expected behavior is probably that the start date be set to “now""","09/Nov/16 6:41 AM;rahulg;[~jellin]..
1.  can you please make use of `frequency` field rather than using `cron_expression` if its a simple 
scheduled job which will run daily. Even if you provide the correct start time using API, there will one extra job instance if cron_expression is used. This is because the the first run depends on the `start_time` and there after cron_expression kicks in. 

2.There were two run instance in your scheduled job. The first instance with id = 2612274 seems to be a bug. That should not have happened as explained in point 1.
But the second instance with id = 2612278 was a valid run. Let me explain that:
1. you created a job at `2016-11-08 18:19:xx` in America/New_York(GTM-05:00) timezone and expected it to run the first instance on 2016-11-08  23:22:00` in  America/New_York timezone. But that did not happen because

{code:java}
when your created the job, the UTC time in the server was 2016-11-08 23:19 (5 hours ahead of your time) and the next_materialized_time was set to 2016-11-08  23:22:00 which was 3 minutes in future. So, 3 minutes after you had created the job, the second instance was triggered. This can be validated from the created_at time of the second instance which is 2016-11-08 18:22 (- 5 hours from server time). Thus, there was no missed instance run in this case.
{code}

Even though you expect your job to run at `23:22:00`(US timezone) everyday, it will run at  `18:22:00` (US timezone) because we assume cron expression time is provided in UTC.

cc: [~sumitm] [~rvenkatesh]

","18/Jan/17 3:55 PM;sbadam;Hey Rahul, this is our review of backlog tickets. Can we have any update or ETA for this issue? Thanks.",21/Mar/17 9:54 PM;aabbas;[~rahulg] I would like to provide an update. I am not trying to add urgency as much as provide an update given turner has 11 on-hold tickets (jiras),"22/Mar/17 2:54 AM;rahulg;[~aabbas] I had provided a work around, did that not work ?

I had a discussion around this in our spring meeting and currently I  have no bandwidth to pick this up.
But surely I am planning to take this next sprint. making this as R-44 candidate.
",22/Mar/17 9:37 AM;aabbas;[~rahulg] R44 is a good enough answer. No issues with workaround.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop job succeeded but the command-id failed,MW-192,50202,Bug,Open,MW,Middleware,software,sumitm,,,Major,,vagrawal,biswajit,biswajit,07/Nov/16 3:02 AM,18/Jan/17 3:46 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"The hadoop jar job has succeeded but the command-id failed. 

 https://api.qubole.com/v2/analyze?command_id=41998408

Please do let know if you need more information. 

https://qubole.slack.com/archives/middleware-dev/p1478511013000580",,biswajit,gmargabanthu,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bloomreach,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0399z:,,,,,,,,,,,,,,,,,,,,,2017-01-18 15:46:40.228,,,18/Jan/17 3:46 PM;sbadam;[mailto:Biswajit Nayak] - we need to find a new assignee for this issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Command failed to upload results to s3, but still succeeded",QBOL-5911,50165,Bug,Reopened,QBOL,qbol,software,sumitm,,,Major,,surendranm,mpatel,mpatel,05/Nov/16 11:44 AM,04/Jul/17 10:46 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Pinterest ran into an issue where AWS was rate limiting s3 writes. The commands were marked as succeeded, but dj.log was throwing the following and failing to upload the results to s3.

{code}
{ip-10-165-128-144 [ERROR pid: 11300: 16-11-04 03:11:01 ] [command:41652144] [account:4911] Expected(200) <=> Actual(503 Service Unavailable)
ip-10-165-128-144 excon.error.response
ip-10-165-128-144  :body          => ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n<Error><Code>SlowDown</Code><Message>Please reduce your request rate.</Message><RequestId>F614DDA3ADF17A93</RequestId><HostId>vNTfTlUvnnPlS2mqubO7/UF4yWEg43upHPAM3fuX7TfU/mDVIbQ+ydXFE9zPXZH0zwVM0yXsKhM=</HostId></Error>""
ip-10-165-128-144  :headers      => {
ip-10-165-128-144 /usr/lib/ruby/gems/2.1.0/gems/excon-0.45.4/lib/excon/middlewares/base.rb:10:in `error_call'
ip-10-165-128-144 /usr/lib/ruby/gems/2.1.0/gems/excon-0.45.4/lib/excon/connection.rb:256:in `rescue in request'
ip-10-165-128-144 /usr/lib/ruby/gems/2.1.0/gems/excon-0.45.4/lib/excon/connection.rb:204:in `request'
ip-10-165-128-144    ""Connection""      => ""close""
ip-10-165-128-144    ""Content-Type""    => ""application/xml""
ip-10-165-128-144    ""Date""            => ""Fri, 04 Nov 2016 03:11:01 GMT""
ip-10-165-128-144    ""Server""          => ""AmazonS3""
ip-10-165-128-144    ""x-amz-id-2""      => ""vNTfTlUvnnPlS2mqubO7/UF4yWEg43upHPAM3fuX7TfU/mDVIbQ+ydXFE9zPXZH0zwVM0yXsKhM=""
{code}

Shouldn't we fail the command in such cases?

See: 41641401,41644383,41652144,41651263

cc [~Kulbir]",,jssarma,mpatel,p.vasa,surendranm,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Pinterest,,,,,{},NA,Choose from,,MW-1,,,,,#Empty,No,,,,,,No,,#Empty,,1|xzz1aq:k,,,,,,,,,,,,,,,,,,,1.0,10704,2016-11-29 22:42:41.475,,,29/Nov/16 10:42 PM;sureshr;[~surendranm] will look into this as part of the async re-architecture.,"30/Nov/16 8:26 AM;mpatel;[~surendranm], [~sureshr] any rough timelines for this? If we can have a timeframe, SOLSUP can be removed.","30/Nov/16 8:29 AM;sureshr;Will let [~surendranm] confirm, but I think end-Q1 2017 would be a reasonable estimate.","22/Feb/17 11:54 PM;surendranm;[~mpatel] Now that I have gone through our architecture and codebase a lot more, I think the current behaviour is correct: Customers build their pipelines on exit status codes. So if a job exited with 0 then that command must be marked successful. Most often the results are already handled in S3 or some cloud store directly by the engine. Only some type of metadata queries and some select queries end up generating an results file. 

We also have enough retries for results and log uploads in place now so that should mitigate this situation. I have also modelled async behaviour similar to this. Marking this jira as won't fix, let me know if this needs to be addressed in any other way. ",23/Feb/17 8:43 AM;mpatel;[~surendranm] In the case of 'SELECT' queries in Hive/Presto/SparkSQL commands do we fail the command if the upload to s3 fails?,"23/Feb/17 9:14 AM;surendranm;No, we still don't fail them. Success/failure of a command in our system currently only depends upon the exit code of the cli process. ","23/Feb/17 9:28 AM;mpatel;[~surendranm] then I feel we need to look at this differently. The problem is that if the command is marked successful, then it is fair to expect that the results and logs make it to s3. 

The current behavior actually broke Pinterest pipeline jobs. 

","23/Feb/17 9:33 AM;surendranm;[~mpatel] I agree, I think if there is a select command that ends up generating a output file and a pipeline job getting impacted because of that is definitely a problem. 

This error happens very rarely though and especially when S3 is having temporary connectivity/latency issues. We have sufficient retries to mitigate that too. So not sure how to fix it though. What would you recommend that we do?","23/Feb/17 9:41 AM;mpatel;If the retries to upload to s3 fail, can't we mark the command as failed with the error from the dj log?","05/Jun/17 1:21 PM;p.vasa;[~surendranm] Are we planning this behavior sometime in the roadmap?
Is it possible to ""Fail a command if the retries to upload the results/logs to s3 fail, with an appropriate error message?""","05/Jun/17 11:32 PM;surendranm;Hey [~p.vasa] - I think I should have escalated at that point in time itself. It is not clear to me should we fail the command if the exit code is zero but the results are unable to be uploaded. The reason for this is if the query had any data operation and that has succeeded, but QDS wasn't able to upload logs/results, then if we fail the query, the customer might retrigger the command again which might have undesirable consequences. I am not the right person to figure out the right behaviour here.

Tagging [~jssarma] [~hiyer] [~xing] [~sureshr] [~sumitm] [~yogeshg]. Assigning this to Joy for now to determine what ideal behaviour should be.   ","07/Jun/17 7:26 PM;jssarma;Agree with [~mpatel] - if result cannot be uploaded - it's a failure. 

- only a subset of SQL commands (in Hive) are automatically converted into 'insert overwrite' - a whole bunch of other stuff (including all non-SQL commands) produce output in stdout.
- idempotency is an issue outside of this specific Jira. there are lots of failure modes that can produce partial output and clients need to be prepared to deal with retrying under such conditions

[~surendranm] is partly right that there are commands that don't care about output (a common pattern in hadoop world is to signal another job to start by creating DONE files in HDFS/S3 etc. - in this case the output and the signalling are entirely in S3 and the exit status of 0 is marker of success).

what i suggest we can do is that if the result output size is zero - then we don't fail on upload failure. if there's a client who doesn't have any interesting output and they don't like our failure-on-upload-failure policy - then they can generate zero sized output. this will accomodate the DONE file pattern above.

failure in uploading logs should NOT cause command failure obviously.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If spark-type of interpreter is not first in the bindings, %pyspark, etc fails",ZEP-611,50155,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,mahuja,megha,megha,04/Nov/16 10:23 AM,29/Jun/17 11:34 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Tested this for returnpath.. 
If the user changes interpreter bindings, and if the spark interpreter is not first in the list, they get error 
"" pyspark interpreter not found""
Similar things happens for %sql, %dep, %knitr, %r
Please check screenshot, and notebook id:  11831
user: megha@qubole.com


",,mahuja,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Nov/16 10:23 AM;megha;Screen Shot 2016-11-04 at 10.23.47 AM.png;https://qubole.atlassian.net/secure/attachment/34306/Screen+Shot+2016-11-04+at+10.23.47+AM.png,,,,,,,,,,,,,,,,,,,returnpath,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0394n:,,,,,,,,,,,,,,,,,,,1.0,10697,2016-11-04 10:39:47.583,,,"04/Nov/16 10:39 AM;mahuja;[~megha] - this is the expected behavior. One needs to specify the interpreter being used in the paragraph. If not specified, it uses a default interpreter, which is the first interpreter on the binding list.","08/Nov/16 11:46 PM;mahuja;Had a chat with [~megha] and the problem comes when pyspark is not the default interpreter (and we run python code with interpreter specified). In my test, the interpreter was not up already.",02/May/17 5:03 PM;megha;Any updates on this issue? [~mahuja],"02/May/17 6:19 PM;mahuja;[~megha] I confirmed in qa env that is still an issue. Unfortunately it is not on short term roadmap. Is it an important JIRA from customer perspective? We can prioritize accordingly.

Steps to reproduce:
1. Write python code in paragraph with pyspark interpreter specified
2. Change the ordering of interpreters so that pyspark is not the default interpreter
3. Run the paragraph.

Error in Zeppelin logs:
{code}
ERROR [2017-05-03 01:17:09,428] ({qtp1676419999-14} NotebookServer.java[runParagraph]:1171) - Exception from run
java.lang.NullPointerException
	at org.apache.zeppelin.notebook.Note.getSettingFromInterpreter(Note.java:580)
	at org.apache.zeppelin.notebook.Note.run(Note.java:482)
	at org.apache.zeppelin.socket.NotebookServer.runParagraph(NotebookServer.java:1167)
	at org.apache.zeppelin.socket.NotebookServer.onMessage(NotebookServer.java:224)
	at org.apache.zeppelin.socket.NotebookSocket.onWebSocketText(NotebookSocket.java:56)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextMessage(JettyListenerEventDriver.java:128)
	at org.eclipse.jetty.websocket.common.message.SimpleTextMessage.messageComplete(SimpleTextMessage.java:69)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.appendMessage(AbstractEventDriver.java:65)
	at org.eclipse.jetty.websocket.common.events.JettyListenerEventDriver.onTextFrame(JettyListenerEventDriver.java:122)
	at org.eclipse.jetty.websocket.common.events.AbstractEventDriver.incomingFrame(AbstractEventDriver.java:161)
	at org.eclipse.jetty.websocket.common.WebSocketSession.incomingFrame(WebSocketSession.java:309)
	at org.eclipse.jetty.websocket.common.extensions.ExtensionStack.incomingFrame(ExtensionStack.java:214)
	at org.eclipse.jetty.websocket.common.Parser.notifyFrame(Parser.java:220)
	at org.eclipse.jetty.websocket.common.Parser.parse(Parser.java:258)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.readParse(AbstractWebSocketConnection.java:632)
	at org.eclipse.jetty.websocket.common.io.AbstractWebSocketConnection.onFillable(AbstractWebSocketConnection.java:480)
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
	at java.lang.Thread.run(Thread.java:745)
{code}

cc - [~beria], [~karuppayyar], [~namanm], [~vipulm]","02/May/17 10:26 PM;megha;Thanks Mayank, this is not asked by the customer yet.. so priority can remain as it is.. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to retrieve query results with python 3 ,SDK-164,50118,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,sourabhg,jellin,jellin,03/Nov/16 8:54 AM,12/Dec/16 2:51 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer is trying to use Python 3 with qds.py   they are receiving a segmentation fault when retrieving the results.

This does work fine with Python 2.7

I was able to reproduce this in a clean environment. 

{code:bash}
docker run -ti python:3.5-alpine sh
Unable to find image 'python:3.5-alpine' locally
3.5-alpine: Pulling from library/python
3690ec4760f9: Already exists 
8cf4eb2be1b3: Already exists 
9f0a8b480adf: Pull complete 
8cb3ded8f170: Pull complete 
Digest: sha256:43b1fb72ee4034568a2384842ce79d3ea34fb8fb75a11ede929df4b0fe660235
Status: Downloaded newer image for python:3.5-alpine
/ # python --version
Python 3.5.2
/ # pip install qds-sdk
Collecting qds-sdk
  Downloading qds_sdk-1.9.4.tar.gz (43kB)
    100% |████████████████████████████████| 51kB 518kB/s 
Collecting requests>=1.0.3 (from qds-sdk)
  Downloading requests-2.11.1-py2.py3-none-any.whl (514kB)
    100% |████████████████████████████████| 522kB 953kB/s 
Collecting boto>=2.1.1 (from qds-sdk)
  Downloading boto-2.43.0-py2.py3-none-any.whl (1.3MB)
    100% |████████████████████████████████| 1.4MB 879kB/s 
Collecting six>=1.2.0 (from qds-sdk)
  Downloading six-1.10.0-py2.py3-none-any.whl
Collecting urllib3>=1.0.2 (from qds-sdk)
  Downloading urllib3-1.18.1-py2.py3-none-any.whl (103kB)
    100% |████████████████████████████████| 112kB 2.1MB/s 
Collecting inflection>=0.3.1 (from qds-sdk)
  Downloading inflection-0.3.1.tar.gz
Installing collected packages: requests, boto, six, urllib3, inflection, qds-sdk
  Running setup.py install for inflection ... done
  Running setup.py install for qds-sdk ... done
Successfully installed boto-2.43.0 inflection-0.3.1 qds-sdk-1.9.4 requests-2.11.1 six-1.10.0 urllib3-1.18.1
You are using pip version 8.1.2, however version 9.0.0 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
/ # apk update
fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.4/community/x86_64/APKINDEX.tar.gz
v3.4.5-5-g512f81f [http://dl-cdn.alpinelinux.org/alpine/v3.4/main]
v3.4.4-21-g75fc217 [http://dl-cdn.alpinelinux.org/alpine/v3.4/community]
OK: 5976 distinct packages available

/ # apk add git
(1/4) Installing libssh2 (1.7.0-r0)
(2/4) Installing libcurl (7.50.3-r0)
(3/4) Installing pcre (8.38-r1)
(4/4) Installing git (2.8.3-r0)
Executing busybox-1.24.2-r11.trigger
OK: 47 MiB in 37 packages
/ # git clone https://github.com/qubole/qds-sdk-py
Cloning into 'qds-sdk-py'...
remote: Counting objects: 1706, done.
remote: Total 1706 (delta 0), reused 0 (delta 0), pack-reused 1706
Receiving objects: 100% (1706/1706), 522.30 KiB | 556.00 KiB/s, done.
Resolving deltas: 100% (1065/1065), done.
Checking connectivity... done.
/ # cd qds-sdk-py/
/qds-sdk-py/example # ls
mr_1.py
qds.py —token=redacted hivecmd getresult ""41587475"" > output2.txt
Segmentation Fault
{code}",,jellin,sourabhg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DataXu,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z038wf:,,,,,,,,,,,,,,,,,,,,,2016-11-08 09:37:20.824,,,03/Nov/16 8:54 AM;jellin;[~sam],"08/Nov/16 9:37 AM;sourabhg;[~jellin]: Thanks for reporting the issue. 
I will look into it sometime this week. ","11/Nov/16 2:24 AM;sourabhg;[~jellin]: I haven't been able to reproduce the issue so far. I tried fetching results of couple of commands ran in my qubole account and qds-sdk returned the results successfully. I then tried fetching results of command id:_41587475_ 
and qds-sdk did not return results for a long time and ultimately failed with error: 
{code}
Error: Status code 449 (RetryWithDelay) from url https://api.qubole.com/api/v1.2/commands/41587475/results?inline=True
{code}

I also checked customer's s3 location where we store results for commands. The command _41587475_ was run on Nov 3 but there are no logs present on s3 for that:
{code}
[ec2-user@ip-10-236-21-232 ~]$ hadoop dfs -ls s3://rts-logs/tmp
Found 5 items
drwxrwxrwx   -          0 1970-01-01 00:00 /tmp/2016-11-07
drwxrwxrwx   -          0 1970-01-01 00:00 /tmp/2016-11-08
drwxrwxrwx   -          0 1970-01-01 00:00 /tmp/2016-11-09
drwxrwxrwx   -          0 1970-01-01 00:00 /tmp/2016-11-10
drwxrwxrwx   -          0 1970-01-01 00:00 /tmp/2016-11-11
{code}

Note that there is no entry for  /tmp/2016-11-03 and this also explains why I was getting Status code 449 (RetryWithDelay). 

Can you tell me any other command id for which we are getting same segmentation fault error ?  ","15/Nov/16 5:38 PM;jellin;here are two new command ids.

* https://api.qubole.com/v2/analyze?command_id=43044403 
* https://api.qubole.com/v2/analyze?command_id=43044413

","20/Nov/16 10:01 PM;sourabhg;[~jellin]: [~hiyer] and I looked at the issue. We were also getting Segmentation fault when using Python3. However the size of the results file returned in python2  and python3 was same.  

Regarding segmentation fault issue, we ran the python process in gdb and following is the stack we saw: 

{code}
Program received signal SIGSEGV, Segmentation fault.
PyObject_Hash (v=0x0) at /usr/src/debug/Python-3.5.1/Objects/object.c:761
761    	    PyTypeObject *tp = Py_TYPE(v);
Missing separate debuginfos, use: debuginfo-install libuuid-2.23.2-16.22.amzn1.x86_64
(gdb) bt


#77 0x00007ffff79b53ce in PyDict_GetItemWithError (op={},
    key=((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((0x0, '-', None, 'rb', 'wb', 438, 'begin %o %s\n', 511, 'ascii', 45, 0, b' \nend\n'), None, True, 1, '%s.%s', 2, 7, '10.3', 8, '10.4', False, 3), 'sys', 'stdout', 'stderr', 'FieldStorage', 'print_directory', 'print_arguments', 'print_form', 'print_environ', 'print_environ_usage', 'print_exception', 'maxlen'), 'cgi.escape is deprecated, use html.escape instead', 'stacklevel', 2, '&', '&amp;', '<', '&lt;', '>', '&gt;', '""', '&quot;'), 'port', 'family', 'type', 'proto', 'flags', 'addrlist', 'res', 'af', 'socktype', 'canonname', 'sa'), 'match_number', 'memo', 'object_hook', 'object_pairs_hook', 'parse_array', 'parse_constant', 'p---Type <return> to continue, or q <return> to quit---
arse_float', 'parse_int', 'parse_object', 'parse_string', 'strict'), 'parse_array', 'parse_string', 'NUMBER_RE', 'match', 'strict', 'parse_float', 'parse_int', 'parse_constant', 'object_hook', 'object_pairs_hook', 'memo'), 'match_number', 'memo', 'object_hook', 'object_pairs_hook', 'parse_array', 'parse_constant', ...(truncated))
    at /usr/src/debug/Python-3.5.1/Objects/dictobject.c:1146
#78 0x00007ffff79b5487 in _PyDict_LoadGlobal (globals=<optimized out>, builtins=0x7fffebae1c08,
    key=key@entry=((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((0x0, '-', None, 'rb', 'wb', 438, 'begin %o %s\n', 511, 'ascii', 45, 0, b' \nend\n'), None, True, 1, '%s.%s', 2, 7, '10.3', 8, '10.4', False, 3), 'sys', 'stdout', 'stderr', 'FieldStorage', 'print_directory', 'print_arguments', 'print_form', 'print_environ', 'print_environ_usage', 'print_exception', 'maxlen'), 'cgi.escape is deprecated, use html.escape instead', 'stacklevel', 2, '&', '&amp;', '<', '&lt;', '>', '&gt;', '""', '&quot;'), 'port', 'family', 'type', 'proto', 'flags', 'addrlist', 'res', 'af', 'socktype', 'canonname', 'sa'), 'match_number', 'memo', 'object_hook', 'object_pairs_hook', 'parse_array', 'parse_constant', 'parse_float', 'parse_int', 'parse_object', 'parse_string', 'strict'), 'parse_array', 'parse_string', 'NUMBER_RE', 'match', 'strict', 'parse_float', 'parse_int', 'parse_constant', 'object_hook', 'object_pairs_hook', 'memo'), 'match_number', 'memo', 'object_hook', 'object_pairs_hook', 'parse_array', 'parse_constant', ...(truncated))
    at /usr/src/debug/Python-3.5.1/Objects/dictobject.c:1195
Python Exception <type 'exceptions.RuntimeError'> maximum recursion depth exceeded while calling a Python object:
#79 0x00007ffff7a33d1a in PyEval_EvalFrameEx (f=f@entry=, throwflag=throwflag@entry=0) at /usr/src/debug/Python-3.5.1/Python/ceval.c:2375
{code}

Basically we are seeing _maximum recursion depth exceeded_ error. Not sure what has caused this but we suspect it might be because of boto version we are using. Right now we are using boto-2.43.0. We will try to upgrade boto version to 3 and see if the issue goes away. ","30/Nov/16 10:48 PM;sourabhg;[~sam] [~jellin]: Can we check with DataXu to get a sense of priority on this issue? There was one more utf-8 related issue ([#175|https://github.com/qubole/qds-sdk-py/issues/175])  recently reported with Python3.5. I suspect it was reported by DataXu. 

Can we check both the issues with DataXu and get a sense of urgency so that we can prioritize them accordingly ? 
",12/Dec/16 2:51 AM;sourabhg;[~sam] [~jellin]: Any updates ? ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark application ui not showing up in logs,SPAR-1313,50018,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,puneetg,megha,megha,31/Oct/16 6:05 PM,25/Jul/17 11:19 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,"For the command ids: 
https://api.qubole.com/v2/analyze?command_id=41276804 
https://api.qubole.com/v2/analyze?command_id=41274557
https://api.qubole.com/v2/analyze?command_id=41279590

It's a re-run of same job, so seems to be some kind of parsing issue.. The spark job launches, but the ""Spark Application UI"" is not showing up.. Logs indicate that the stages start execute, and some stages fail...

cc: [~adubey]",,mahuja,megha,puneetg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,fanatics,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z038f3:,,,,,,,,,,,,,,,,,,,1.0,10502,2016-11-01 19:11:19.711,,,"01/Nov/16 7:11 PM;mahuja;[~puneetg] - can you please do an initial triage. cc - [~swatis]
","03/Nov/16 5:23 AM;puneetg;[~megha]

I am seeing following exception in logs of all the jobs:
{{java.lang.ClassCastException: org.apache.spark.unsafe.types.UTF8String cannot be cast to org.apache.spark.sql.types.Decimal}}

Looks like they are doing wrongs cast conversion somewhere in the code. Since they are running job from jar, I am not able to see actual job code.

From the logs on Qubole page ask them to download complete logs( by clicking on ""Click here to download complete logs."") , you will be able to see this exception there.","15/Nov/16 10:08 PM;puneetg;[~megha]
Any updates on this exception. Was it fixed?","27/Jan/17 3:08 PM;megha;Hi [~puneetg]
The error is fine, however the main issue is that ""spark Application UI"" link is not being created on the logs.
I do see that the job stages progress and fail. But there is no ""Spark UI"" link to go to from logs to analyze the issue.. ",20/Mar/17 4:33 PM;megha;[~puneetg] Any updates? ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QDS SDK command failure returning Qubole information,SDK-162,49945,Bug,Open,SDK,SDK,software,abhijitj,,,Critical,,sourabhg,abhimanyuc,addon_zendesk_for_jira,28/Oct/16 6:23 AM,07/Jun/17 1:38 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"cc: [~sureshr]
A command failure from the Python SDK returned a lot of information about the setup of Quboles webserver.

h3. Zendesk Attachments


[Attachment 1|https://qubole.zendesk.com/attachments/token/Jt4N23XcEf3nnZVyGQWDM8FZC/?name=qubole_output]
",,addon_zendesk_for_jira,sbadam,sourabhg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DataXu,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0385j:,,,,,,,,,,,,,,,,,,,1.0,10572,2017-01-18 14:33:47.602,,,"18/Jan/17 2:33 PM;sbadam;Hey Sourabh, can we have any update or ETA for this issue? Thanks.","19/Jan/17 12:06 AM;sourabhg;[~sbadam]: I haven't looked into the issue. 
By the way, the customer is not impacted with this issue. Will look into it. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Copying and viewing example notebooks for system users throws access denied,QBOL-5895,49907,Bug,Reopened,QBOL,qbol,software,sumitm,,,Major,Fixed,ashishs,karthikb,karthikb,27/Oct/16 2:46 AM,27/Jan/17 1:54 AM,09/Aug/17 6:03 AM,27/Oct/16 2:47 AM,,,,0,folderingbugbash,jira_escalated,,,,,,,"When we try to copy or view an eample notebook as system user.

steps to reproduce
1) login into as user with system-user access
2) change the view to example note books
3) try to view or copy example note book

this will throw access denied",,ekang,jellin,karthikb,mmajithia,sam,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z037xb:,,,,,,,,,,,,,,,,,,,,,2017-01-24 13:00:27.08,,,"24/Jan/17 1:00 PM;ekang;this is an issue for scripps so I'm reopening the ticket.

account: 6654 user: dtreece@scrippsnetworks.com

Go into notebooks and try to copy an Examples notebook. You will see access denied.","24/Jan/17 2:26 PM;sam;Is this related to the lack of permissions to the Qubole bucket that holds examples? Some clients remove those lines from IAM policy, which causes issues with loading Example notebooks.

This could also be something entirely new related to new Notebook features.. Just wanted to note that the above has been the case..","25/Jan/17 1:08 AM;sumitm;Issue being is that customer doesn't have access to paid-qubole bucket, where these example notebooks resides. Why then customer is able to view them, cause we've them in our cache, so our code doesn't need to fetch them via s3 and it just works. 

We'll keep this ticket open to make sure that we show correct error message on UI. ","25/Jan/17 6:48 AM;ekang;What permissions do we need for paid-qubole? I'm following the qubole docs for setting up iam roles. This is the only permission that's listed for paid-qubole. Is there something else we are missing? Document is here: http://docs.qubole.com/en/latest/user-guide/features/control-panel/create-iam-roles.html

{
                  ""Effect"": ""Allow"",
                  ""Action"": [
                              ""s3:GetObject"",
                              ""s3:ListBucket""
                            ],
                  ""Resource"": [
                                ""arn:aws:s3:::paid-qubole/*"",
                                ""arn:aws:s3:::paid-qubole"",
                                ""arn:aws:s3:::<bucketpath1>/*"",
                                ""arn:aws:s3:::<bucketpath1>""
                              ]
                },","27/Jan/17 1:54 AM;sumitm;[~ekang] not an iam role expert, but seems like what you've pasted should work. cc - [~yogeshg]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWS api calls need to be reduced for cluster start,ACM-721,49870,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,megha,megha,26/Oct/16 12:30 PM,19/Dec/16 11:53 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"clusters with min size about 250+ face this issue:
""Service limit Exceeded""..
Saw this with spacetime and Pinterest...
For now, we've told them to use 200 min nodes instead of 250 for the cluster...

Following is the error : <Response><Errors><Error><Code>InsufficientInstanceCapacity</Code><Message>Insufficient capacity.</Message></Error></Errors><RequestID>8be3e8be-8bd9-460b-bd6c-f83dee462062</RequestID></Response>",,ajayb,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,spacetime,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z037rj:,,,,,,,,,,,,,,,,,,,,,2016-11-03 20:57:51.077,,,"26/Oct/16 12:35 PM;megha;This issue seems to have started occurring after the release.. Before that, pinterest had 350 nodes min, which always worked fine, also spacetime had min set to 256 that used to work fine...

cc: [~mpatel]",26/Oct/16 5:55 PM;megha;cc: [~Harsh] as he was looking at this one,"03/Nov/16 8:57 PM;ajayb;[~megha], the error you have pasted in the description above is [InsufficientInstanceCapacity|http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-capacity.html]. This indicates AWS does not have enough capacity in that region and is not dependent on how or what calls we make. Did you paste the wrong error code above?","16/Dec/16 10:25 PM;ajayb;[~megha] any response to my comment above? Else, please resolve the jira as the currently pasted error in description is related to the capacity configured in customer's AWS account and not related to QDS.

cc [~mpatel] if he knows something more about the issue seen by customer.","19/Dec/16 11:52 AM;megha;[~ajayb] The error message is incorrect one., there was another one with Servive limit exceeded , the one for api request needed.
","19/Dec/16 11:53 AM;megha;although, I dont think I've seen this for some time now.. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job stuck in cancelling state for 8+ hours,MW-179,49818,Bug,Open,MW,Middleware,software,sumitm,,,Critical,,surendranm,Kulbir,Kulbir,25/Oct/16 7:48 PM,19/Jun/17 1:03 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Team,
Pinterest reported an issue where one of their jobs got into ""cancelling"" state and is stuck there for last 8+ hours, as a result of which their workflows got disrupted and stuff didn't run properly. 

Command ID:
https://api.qubole.com/v2/analyze?command_id=40564941

Interestingly per JT logs underlying MR Job succeeded in less than 20 seconds:

{code}
2016-10-25 17:09:32,605 INFO -1 org.apache.hadoop.mapred.JobInProgress$JobSummary (IPC Server handler 1 on 9001): jobId=job_4911.201607061550_419921,submitTime=1477415356317,launchTime=1477415356335,finishTime=1477415372605,numMaps=1,numSlotsPerMap=1,numReduces=128,numSlotsPerReduce=1,user=qubole-prod@pinterest.com,queue=default,status=SUCCEEDED,mapSlotSeconds=11,reduceSlotsSeconds=870,clusterMapCapacity=2496,clusterReduceCapacity=1716
2016-10-25 17:09:32,608 INFO -1 org.apache.hadoop.mapred.JobHistory (pool-3-thread-1): Moving file:/usr/lib/hadoop-0.20/logs/history/ec2-54-175-99-206.compute-1.amazonaws.com_0_job_4911.201607061550_419921_qubole-prod%40pinterest.com_prod%3AHourlyDumpAdsConversionTagsV3DatabaseJob to s3n://qubole-pinterest/m10n_hourly/logs/hadoop/201607061550/history
{code}

I reviewed web tier logs and dj logs didn't indicate any issues either:

{code}
/media/ephemeral0/logs/rails_log/qubole_delayed_jobs.log:[INFO  pid: 30504: 16-10-25 17:02:20 ] [command:40564941] [account:4911] qid: 40564941, pid: 5385
/media/ephemeral0/logs/rails_log/qubole_delayed_jobs.log:[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] qid: 40564941, cmd done with status: pid 5385 exit 0
/media/ephemeral0/logs/rails_log/qubole_delayed_jobs.log:[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] qid: 40564941 HadoopCommand execution completed with status: 0
/media/ephemeral0/logs/rails_log/qubole_delayed_jobs.log:[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] log_path: /media/ephemeral0/tmp/tapp/tmp/2016-10-25/4911/40564941, output_file: /media/ephemeral0/tmp/tapp/tmp/2016-10-25/4911/40564941
/media/ephemeral0/logs/rails_log/qubole_delayed_jobs.log:[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] Populating the cache production:command:raw_results:40564941 with the results for queries with no dirs on s3
/media/ephemeral0/logs/rails_log/qubole_delayed_jobs.log:[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] Query not small. Uploading logs to S3 immediately.
{code}

However interestingly in controller logs I can see attempt to kill the job and status being stuck in ""cancelling"":

{code}
ip-10-229-40-250 [INFO  pid: 6065: 16-10-25 18:22:58 ] Started GET ""/api/v1.2/commands/40564941"" for 54.236.1.18 at 2016-10-25 18:22:58 +0000
ip-10-229-40-250 [INFO  pid: 6065: 16-10-25 18:22:58 ]  Parameters: {""id""=>""40564941"", ""command""=>{}}
ip-10-229-40-250 [INFO  pid: 6065: 16-10-25 18:22:58 f246ccb9] [user:11013] [account:4911] [""read"", #<QueryHist id: 40564941, qbol_user_id: 16683, submit_time: 1477414940, end_time: nil, progress: 0, cube_id: nil, created_at: ""2016-10-25 17:02:20"", updated_at: ""2016-10-25 17:02:20"", path: ""/tmp/2016-10-25/4911/40564941"", status: ""running"", host_name: ""ip-10-9-153-11.ec2.internal"", user_loc: true, qbol_session_id: 1922040, command_id: 2811381, command_type: ""HadoopCommand"", qlog: nil, periodic_job_id: nil, wf_id: nil, command_source: ""API"", resolved_macros: nil, status_code: nil, pid: 5389, editable_pj_id: nil, template: ""generic"", command_template_id: nil, command_template_mutable_id: nil, can_notify: false, num_result_dir: 0, start_time: 1477414941, pool: nil, timeout: nil, tag: ""default"", name: nil, saved_query_mutable_id: nil, account_id: 4911>]
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:17 ] Started PUT ""/api/v1.2/commands/40564941"" for 54.236.1.18 at 2016-10-25 18:23:17 +0000
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:17 ]  Parameters: {""status""=>""kill"", ""id""=>""40564941"", ""command""=>{""status""=>""kill""}}
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:17 6f0d738f] [user:11013] [account:4911] [""update"", #<QueryHist id: 40564941, qbol_user_id: 16683, submit_time: 1477414940, end_time: nil, progress: 0, cube_id: nil, created_at: ""2016-10-25 17:02:20"", updated_at: ""2016-10-25 17:02:20"", path: ""/tmp/2016-10-25/4911/40564941"", status: ""running"", host_name: ""ip-10-9-153-11.ec2.internal"", user_loc: true, qbol_session_id: 1922040, command_id: 2811381, command_type: ""HadoopCommand"", qlog: nil, periodic_job_id: nil, wf_id: nil, command_source: ""API"", resolved_macros: nil, status_code: nil, pid: 5389, editable_pj_id: nil, template: ""generic"", command_template_id: nil, command_template_mutable_id: nil, can_notify: false, num_result_dir: 0, start_time: 1477414941, pool: nil, timeout: nil, tag: ""default"", name: nil, saved_query_mutable_id: nil, account_id: 4911>]
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:17 6f0d738f] cancel_adhoc_command status of qid: 40564941 is running
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:17 6f0d738f] do_cancel status of qid: 40564941 is cancelling
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:18 6f0d738f] ssh_on_cluster_and_kill: result = /bin/cat: /media/ephemeral0/pids/40564941.pid: No such file or directory
ip-10-150-46-11  kill_succeeded = true for command id: 40564941
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:18 ] Started GET ""/api/v1.2/commands/40564941"" for 54.236.1.18 at 2016-10-25 18:23:18 +0000
ip-10-150-46-11  kill_succeeded = true for command id: 40564941
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:17 6f0d738f] ssh_on_cluster_and_kill:: host = ec2-54-175-99-206.compute-1.amazonaws.com  cmd = kill -SIGINT -`/bin/cat /media/ephemeral0/pids/40564941.pid` ; rm -f /media/ephemeral0/pids/40564941.pid ;/usr/lib/hadoop/bin/hadoop job -jt ec2-54-175-99-206.compute-1.amazonaws.com:9001 -kill job_4911.201607061550_419921; 
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:18 ]  Parameters: {""id""=>""40564941"", ""command""=>{}}
ip-10-150-46-11 [INFO  pid: 30520: 16-10-25 18:23:18 6bc9800d] [user:11013] [account:4911] [""read"", #<QueryHist id: 40564941, qbol_user_id: 16683, submit_time: 1477414940, end_time: nil, progress: 100, cube_id: nil, created_at: ""2016-10-25 17:02:20"", updated_at: ""2016-10-25 17:02:20"", path: ""/tmp/2016-10-25/4911/40564941"", status: ""cancelling"", host_name: ""ip-10-9-153-11.ec2.internal"", user_loc: true, qbol_session_id: 1922040, command_id: 2811381, command_type: ""HadoopCommand"", qlog: nil, periodic_job_id: nil, wf_id: nil, command_source: ""API"", resolved_macros: nil, status_code: nil, pid: 5389, editable_pj_id: nil, template: ""generic"", command_template_id: nil, command_template_mutable_id: nil, can_notify: false, num_result_dir: 0, start_time: 1477414941, pool: nil, timeout: nil, tag: ""default"", name: nil, saved_query_mutable_id: nil, account_id: 4911>]

but still stuck in cancelling state

ip-10-63-205-136 [INFO  pid: 30918: 16-10-26 01:17:06 704a9844] [user:11013] [account:4911] [su:9454] [command:40564941] qid: 40564941, Fetching results: status: cancelling


{code}

Questions:
-Why would this happen and why are we still stuck in ""cancelling"" state ?
-If user didn't sent a kill command is there any scenario under which this would automatically happen ?

This is a critical issue as customer is unsure if there are other similar jobs that have also gotten into similar state and hence not being detected by their code thus affecting their data pipeline.

Please advise what's the root cause ASAP.

Regards,
Kulbir.

cc [~mpatel]",,biswajit,Kulbir,mpatel,sumitm,surendranm,sureshr,vagrawal,yogeshg,,,,,,,,,,,,,,,,,,,,,,MW-1104,EAM-15,QBOL-5904,,,,,,,,,,,,,,,,,,,,,Expedia,Pinterest,,,,"{branch={count=1, dataType=branch}, json={""cachedValue"":{""errors"":[],""summary"":{""branch"":{""overall"":{""count"":1,""lastUpdated"":null},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|xzz1tk:,,,,,,MW Sprint 17(26Oct-8Nov),MW Sprint 18(9Nov-22Nov),MW Sprint 19(23Nov-6Dec),MW Sprint 20(7Dec-20Dec),MW Sprint 21(21Dec-3Jan),MW Sprint 21(4Jan-17Jan),MW Sprint 22(18Jan-31Jan),MW Sprint 23 (1-Feb - 14-Feb),MW Sprint 15Feb-28Feb,MW Sprint (15Mar-28Mar),,,,2.0,"10499,10659",2016-10-26 03:34:05.743,,,"25/Oct/16 8:08 PM;Kulbir;Some further updates, per Pinterest kill signal actually came from their scheduler:

{code}

jooseong [8:00 PM]  
OK

[8:00]  
I think the kill came from us

[8:00]  
It got aborted

[8:00]  
and the timeline matches when it was aborted.

[8:01]  
I guess the question is then what the state was before it was killed.

[8:01]  
The map reduce job finished already

[8:01]  
and the command was hung for a long time.

kulbir [8:03 PM]  
interesting well the state was running before it got killed

[8:04]  
ip-10-229-40-250 [INFO  pid: 6065: 16-10-25 18:22:58 ]  Parameters: {""id""=>""40564941"", ""command""=>{}}
ip-10-229-40-250 [INFO  pid: 6065: 16-10-25 18:22:58 f246ccb9] [user:11013] [account:4911] [""read"", #<QueryHist id: 40564941, qbol_user_id: 16683, submit_time: 1477414940, end_time: nil, progress: 0, cube_id: nil, created_at: ""2016-10-25 17:02:20"", updated_at: ""2016-10-25 17:02:20"", path: ""/tmp/2016-10-25/4911/40564941"", status: ""running"",

[8:04]  
@jooseong

[8:05]  
but in any case there are still open questions w.r.t why job status was running while MR job succeeded immediately and second why current status is still not propagated

[8:05]  
I will pass this info to engineering though so we don’t chase where kill signal came from

new messages
jooseong [8:06 PM]  
Yes.

[8:06]  
Yeah. The kill was an automated kill from our scheduler.
{code}

So open questions are still:
-Why the command remained in ""running"" state though underlying MR job finished in less than a minute ?
-Why now it's stuck in cancelling state ?

cc [~mpatel]","26/Oct/16 3:34 AM;yogeshg;Sharing the RCA so far:

To answer ques 1:
Our delayed_job received successful exit status from MR. But before we marked the command as ""done"" in database it got stucked while uploading logs(even command logs are not present on s3) and rest of the command flow did not happen. There is no logging activity for that command post that(stage 4).
{noformat}

[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] qid: 40564941, cmd done with status: pid 5385 exit 0
[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] qid: 40564941 HadoopCommand execution completed with status: 0
[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] log_path: /media/ephemeral0/tmp/tapp/tmp/2016-10-25/4911/40564941, output_file: /media/ephemeral0/tmp/tapp/tmp/2016-10-25/4911/40564941
[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] Populating the cache production:command:raw_results:40564941 with the results for queries with no dirs on s3
[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] Query not small. Uploading logs to S3 immediately.
[INFO  pid: 30504: 16-10-25 17:09:33 ] [command:40564941] [account:4911] log_path: /media/ephemeral0/tmp/tapp/tmp/2016-10-25/4911/40564941, err file: /media/ephemeral0/tmp/tapp/tmp/2016-10-25/4911/40564941.err
[INFO  pid: 30504: 16-10-25 17:09:34 ] [command:40564941] [account:4911] stage 3
[INFO  pid: 30504: 16-10-25 17:09:34 ] [command:40564941] [account:4911] Getting job URLs for hadoop1 cluster
[INFO  pid: 30504: 16-10-25 17:09:34 ] [command:40564941] [account:4911] stage 4

{noformat}

To answer ques 2:
Still trying to understand this part, will share once I come up with something tangible.

Is this happening for all queries ?
I checked the commands fired from that account before that command(40564941) till so far and things looks fine:

production-replica> select count(*) from query_hists where id >= 40564800 and account_id = 4911 and status = 'cancelling';
+----------+
| count(*) |
+----------+
|        1 |
+----------+
1 row in set (0.04 sec)

Overall it seems to me that the corresponding delayed_job process went bad somehow (may be memory). Sharing {{top}} stats of the process as of writing. One interesting pointing is this delayed_job process has not taken up any command after that so it is still somehow bind to the underlying command.

{noformat}
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                         
30504 ec2-user  20   0 1250m 650m  10m S  0.0  4.3   0:46.90 ruby
{noformat} 

cc: [~vagrawal] [~Kulbir] [~mpatel] [~biswajit] [~sumitm]","26/Oct/16 3:39 AM;yogeshg;[~vagrawal] shall we terminate the delayed_process as well, this seems to be stuck ?",26/Oct/16 4:06 AM;sumitm;[~yogeshg] can u see if there are memory details available during that time.. using `sar` etc..,"26/Oct/16 4:21 AM;vagrawal;We looked at all relevant stats for the ec2-instance. everything looks fine - memory, cpu, network-in, network-out.  Process was stuck while uploading logs. So we dont even have the logs in S3

Also only one process was stuck. all other workers were able to upload logs etc around the same time.

We have terminated the process for now
","26/Oct/16 4:44 AM;yogeshg;Just to verify if everything looks fine, i ran below query to check the status of the commands submitted so far since that command:

{noformat}
production-replica> select status, count(*) from query_hists where id >= 40564800 and   account_id  = 4911 group by status;
+------------+----------+
| status     | count(*) |
+------------+----------+
| cancelling |        1 |
| done       |     2342 |
| error      |       31 |
| running    |       11 |
+------------+----------+
4 rows in set (0.05 sec)
{noformat}",26/Oct/16 6:58 PM;biswajit;[~yogeshg] Thanks for the detail update. But how to avoid this scenario in future. ,"27/Oct/16 1:48 AM;yogeshg;Update on the cancelling part:

Highlighting the general code flow to help explain:

1) Worker process P1 starts command with child process P2.
2) P1 waits for P2 to finish and after that P1 moves ahead to upload logs & updates the command status. 

There are two possibilities here. P2 can either finish properly or it can be killed externally. If killed externally we first mark the command ""cancelling"", then send kill command for P2, P2 terminates & P1 moves ahead and marks the status to be ""cancelled"" only if the status was set ""cancelling"".

What happened in this scenario:
We sent kill command from UI and set status to ""cancelling"", then send kill command for process P2. But P2 had already died as the command had already got finished successfully earlier and P1 had already moved ahead but got stuck while uploading logs.

So in nutshell we send the kill signal to a non-existing process which had no impact on command flow and just moved the status from ""running"" to ""cancelling"".

cc: [~Kulbir] [~biswajit]","31/Oct/16 4:48 PM;Kulbir;[~yogeshg] Thanks for the update and detailed explanation of the workflow.
Given that we don't have a way to determine why the DJ worker process got into a zombie state, what else could be done to mitigate or have better debugging info next this happens. Could we add some checks\logging in worker code for S3 upload scenario so we can debug it better ?

cc [~mpatel]","02/Nov/16 1:13 PM;Kulbir;[~yogeshg] [~vagrawal] I have another similar issue from Expedia(https://qubole.zendesk.com/agent/tickets/10659) they have https://api.qubole.com/v2/analyze?command_id=41240156 stuck in running state since Oct. 31st though the hive command finished in like few minutes. Relevant logs:
{code}
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] Fetching s3://bex-analytics-softwares/builds/ede-omniture-hit-hourly-qubole-mapreduce.git/f0043197189481d80375f07e8e19967600991601/src/main/scripts/write_lkp_tables_to_files.hql to /media/ephemeral0/tmp/tapp/d20d2a4c-9f7c-11e6-9bb9-22000a4f8d7d.script
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:12 ] [command:41240156] [account:5507] Beginning execution of command...
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:12 ] [command:41240156] [account:5507] Monthly usage limit: {""ODBC""=>0, ""enable_orc_format""=>1.0, ""Hive version""=>1.2, ""Max concurrent commands""=>40.0, ""Max periodic job instances per day""=>48.0, ""port""=>nil, ""url""=>nil, ""PrivilegeLevel""=>-1.0, ""Use Experimental DJ""=>0.0, ""UseHiveTier""=>1.0}
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:12 ] [command:41240156] [account:5507] Monthly usage: {""QCUH""=>0.0, ""CONNECTOR_INVOCATIONS""=>0, ""USERS""=>0, ""SCHEDULER""=>0, ""ODBC""=>0}
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:12 ] [command:41240156] [account:5507] ret_val is: {:plan_expired=>false, :qcuh_exceeded=>false, :connector_invocations_exceeded=>false, :users_exceeded=>false, :alert_message=>""OK"", :message=>""OK""}
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:12 ] [command:41240156] [account:5507] Deferring HiveCommand to thread
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] local_path: /media/ephemeral0/tmp/tapp/d20d2a4c-9f7c-11e6-9bb9-22000a4f8d7d.script, size: 933, object_count: 1, @is_directory: false
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] Reading builds/ede-omniture-hit-hourly-qubole-mapreduce.git/f0043197189481d80375f07e8e19967600991601/src/main/scripts/write_lkp_tables_to_files.hql
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] token[0] is INSERT
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] Original Query after split is: [""\r\nSET hive.on.master=true"", ""\r\n\r\nINSERT OVERWRITE DIRECTORY 's3://big-data-analytics-prod/ODS/ETL-LKP/LKP-TBL/ede-omniture-hit-hourly-qubole-mapreduce/EWW_OMNITURE_SITE_DOMAIN_MAP' SELECT concat_ws('\\t', site_name, domain_name) FROM sync.EWW_OMNITURE_SITE_DOMAIN_MAP"", ""\r\n\r\nINSERT OVERWRITE DIRECTORY 's3://big-data-analytics-prod/ODS/ETL-LKP/LKP-TBL/ede-omniture-hit-hourly-qubole-mapreduce/ETL_JAVATZ_TO_PYTHONTZ' SELECT concat_ws('\\t', java_tm_zone_name, python_tm_zone_name ) FROM sync.ETL_JAVATZ_TO_PYTHONTZ"", ""\r\n\r\nINSERT OVERWRITE DIRECTORY 's3://big-data-analytics-prod/ODS/ETL-LKP/LKP-TBL/ede-omniture-hit-hourly-qubole-mapreduce/ETLDM_OMNITURE_SITE' SELECT concat_ws('\\t', site_name, java_tm_zone_name ) FROM sync.ETLDM_OMNITURE_SITE""]
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] token[0] is SET
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] token[0] is INSERT
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] token[0] is INSERT
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] Num Result dir: 0 and 0 
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] Return Queries: [""INSERT OVERWRITE DIRECTORY 's3://big-data-analytics-prod/ODS/ETL-LKP/LKP-TBL/ede-omniture-hit-hourly-qubole-mapreduce/EWW_OMNITURE_SITE_DOMAIN_MAP' SELECT concat_ws('\\t', site_name, domain_name) FROM sync.EWW_OMNITURE_SITE_DOMAIN_MAP"", ""INSERT OVERWRITE DIRECTORY 's3://big-data-analytics-prod/ODS/ETL-LKP/LKP-TBL/ede-omniture-hit-hourly-qubole-mapreduce/ETL_JAVATZ_TO_PYTHONTZ' SELECT concat_ws('\\t', java_tm_zone_name, python_tm_zone_name ) FROM sync.ETL_JAVATZ_TO_PYTHONTZ"", ""INSERT OVERWRITE DIRECTORY 's3://big-data-analytics-prod/ODS/ETL-LKP/LKP-TBL/ede-omniture-hit-hourly-qubole-mapreduce/ETLDM_OMNITURE_SITE' SELECT concat_ws('\\t', site_name, java_tm_zone_name ) FROM sync.ETLDM_OMNITURE_SITE""], Query Types: [[false, false, true, false, false, false, nil, false], [false, false, true, false, false, false, nil, false], [false, false, true, false, false, false, nil, false]]
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] Complete Query: INSERT OVERWRITE DIRECTORY 's3://big-data-analytics-prod/ODS/ETL-LKP/LKP-TBL/ede-omniture-hit-hourly-qubole-mapreduce/EWW_OMNITURE_SITE_DOMAIN_MAP' SELECT concat_ws('\t', site_name, domain_name) FROM sync.EWW_OMNITURE_SITE_DOMAIN_MAP;
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] running INSERT OVERWRITE DIRECTORY 's3://big-data-analytics-prod/ODS/ETL-LKP/LKP-TBL/ede-omniture-hit-hourly-qubole-mapreduce/EWW_OMNITURE_SITE_DOMAIN_MAP' SELECT concat_ws('\t', site_name, domain_name) FROM sync.EWW_OMNITURE_SITE_DOMAIN_MAP;
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] session is []
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] Using hadoop2 cluster. Disabling QHS
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] use_hive_server = false
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] can_use_hive_server=false
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] Active Record is disconnected
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] qid: 41240156, cmd: /usr/lib/hive_scripts/hivecli.py -f /tmp/qexec20161031-16743-1n2pnod --ugi 's-qubole-ede-pivot@expedia.com,default_group'  --cmdid 34142595 --qid 41240156 --sid 0 -q -s -c hive.session.id=2405072 -i /tmp/mdhist20161031-16743-1iyt2ox 5507 --cluster-id 23295 --qbol-user-id 17478 --retry 0 --use-hive-on-master  --md-path /media/ephemeral0/tmp/tapp/tmp/2016-10-31/5507/41240156.md  > /media/ephemeral0/tmp/tapp/tmp/2016-10-31/5507/41240156.tmp 2>>/media/ephemeral0/tmp/tapp/tmp/2016-10-31/5507/41240156.err.tmp
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:15:13 ] [command:41240156] [account:5507] qid: 41240156, pid: 12837
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] qid: 41240156, cmd done with status: pid 12837 exit 0
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] log_path: /media/ephemeral0/tmp/tapp/tmp/2016-10-31/5507/41240156, output_file: /media/ephemeral0/tmp/tapp/tmp/2016-10-31/5507/41240156
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] Populating the cache production:command:raw_results:41240156 with the results for queries with no dirs on s3
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] Query not small. Uploading logs to S3 immediately.
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] log_path: /media/ephemeral0/tmp/tapp/tmp/2016-10-31/5507/41240156, err file: /media/ephemeral0/tmp/tapp/tmp/2016-10-31/5507/41240156.err
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] stage 3
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] Getting job URLs for hadoop2 cluster
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] stage 4
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] Upload done
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:33 ] [command:41240156] [account:5507] stage 5
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:34 ] [command:41240156] [account:5507] Upload done
ip-10-79-141-125 [INFO  pid: 16743: 16-10-31 15:19:34 ] [command:41240156] [account:5507] stage 6
{code}

I see that the web tier node is still up however pid 16743 is already gone and also saw a email from [~psrinivas] indicating issues with this particular node.
Can you please explain what happened in this case ? Is it still some issue with S3 communication ?

cc [~mpatel]","02/Nov/16 6:56 PM;Kulbir;In addition Expedia is asking if they could cancel the command from UI, per my understanding it will just move to cancelling state and won't really impact anything but please provide direction on that as well.","03/Nov/16 7:22 AM;yogeshg;The process of the command has already been terminated on {{ip-10-79-141-125}} machine, so sending cancel command from UI will have no impact. We should manually update the status of the command in this case. 

In this case also command got stuck while uploading to s3. This time it was trying to upload query. They passed a s3 script to execute query. As part of flow we upload the query contents to s3 after downloading & resolving macros at the same location where command logs are stored. 

The above mentioned node is problematic and was having lot of connectivity issues especially rds and hence this might have got stuck while trying to connect to s3. The issue is tracked here MW-186

We already have exception handling and retries for uploading to s3. I am checking if there is any scope for improvement especially handling timeouts. cc: [~sumitm] [~surendranm]","03/Nov/16 8:28 AM;Kulbir;[~yogeshg] Thanks for the update.
As you mentioned we need additional polling\monitoring\timeout logic for code chunk which deals with S3 as an improvement ASAP, exception handling\retry is good but won't help in scenarios like these. ,These failures are critical because on customer end there could be complex workflows based on command status and this blocks entire pipeline as was the case with Pinterest as well as Expedia.

Appreciate your help.",03/Nov/16 10:29 AM;yogeshg;As discussed we will come up with some action items on this to better handle such failures and/or system should error out reliably so that end user can take some action appropriately.,"18/Nov/16 5:43 PM;Kulbir;Hi [~yogeshg]
Any further updates that I can pass on to Pinterest ?","20/Nov/16 8:50 AM;yogeshg;Let me discuss the solution to both problems here to wider group.

*Problem 1* - command gets stuck indefinitely & user is waiting for his job to finish. In this JIRA commands got stuck while uploading logs.

There can be multiple sources for job to get stuck - db query, file operation, network calls. Since these are not always avoidable so what we can do in our system to reliably error out. To understand this I divided the command flow into different stages -

# Submission of command over POST call - This stage registers the command and do bunch of validations
# Pre-processing of the command by the DJ - performs some database queries and create files
# Command processing by DJ - Actually performs the job
# Post processing of the command by the DJ - Updates the status, upload logs, results.

If some operation get stuck in stage 1 then http request will get timeout and user will get a non-200 status code so this is fine. Now the problem is we have cumulative timeout of 36 hours for stage 2,3,4 whereas 2 takes some milliseconds & 4 can take at most an hour in extreme cases to upload large logs/results. So we should have timeout defined for each stage and error out immediately in that case instead of waiting for 36 hours.

To better handle stage 3, I think we should have concept of sla where user can provide this while submitting the job. If a user knows the time a job will take(in this case Pinterest knew) and it is taking longer than usual then we should error out. This is helpful if a job gets stuck inside cluster somehow. This data will be useful for Tenali project as well(expected time vs time taken)

Another approach to do the same is to have a job monitoring system running on each machine. It can monitor the heartbeats of each job running on it’s machine and take some action based on the current stage of the job. We have a minimal version of this in the form of a cron job to find hung queries[command is in running state but dj worker died]

Although i would prefer second approach, there are some challenges here as we don’t maintain all the stages properly in db e.g if a job has completed stage 3 we keep the status as “running” only. After stage 3 we should have something like “job_finished”, “logs_uploaded”, “results_logs_uploaded” and then “done”. Adding new states is difficult as lot of code has been written inside QDS/customer’s scheduler based on these states. Adding a new column is also tricky in {{query_hists}} as it has tens of millions of rows. May be we can create a new table to monitor job health. Since the first approach works within the running code this information is easily available and can be quickly implemented.

*Problem 2*: User issued a cancel command on the stuck command but it moved to ""cancelling"" state only rather than ""cancelled"".

Our existing kill logic simply send SIGINT to job process id(stored in db) and do not check if the process exists or not. If we start handling this scenario we can get rid of the issue easily.

[~Kulbir] [~sumitm] [~surendranm] Let me know your suggestions on this.

cc: [~sureshr]","20/Nov/16 10:14 PM;sumitm;My take on these problems:

*Problem 1:*

Adding new states would be a long and big job to do. Instead if we can make DJ or its underlying CLI to register the heartbeats and modify the existing cron to work on that heartbeat, that would be better and doable thing. Because in 2nd approach a task can still be stuck in some state, and then that problem would be same like current one. 

*Problem 2:*

Yeah, we should check if that PID exists or not and if not, then update the state of cmd directly. ","21/Nov/16 11:09 AM;Kulbir;+1 on fixing Problem 2, for Problem 1 I concur with [~sumitm], as a customer it will really be hard to define expected times and we definitely need a heartbeat\monitoring mechanism with DJ to identify such scenarios.","24/Nov/16 10:34 AM;yogeshg;[~surendranm] Do we have/planned something like job monitoring system as discussed above for async. This is definitely useful. To do the same in current DJ architecture for all clis is a big effort as far i can think.

cc: [~sureshr]

Filed MW-229 for problem 2","24/Nov/16 9:03 PM;sureshr;Great summary, [~yogeshg]. Really like the way you have clearly broken up the command flow and identified the various phases. For problem #1, I agree with the heartbeat/monitoring approach and that we should not take the effort to fix in in the current DJ architecture, but instead make sure we incorporate this learning into the async architecture.

[~surendranm] Can you confirm on [~yogeshg]'s question above?","25/Nov/16 3:00 AM;surendranm;[~sureshr] Yes, ATC tracks all commands in flight and there are only final state. If the command gets stuck in some intermediate state, an alert email is triggered. This is already implemented as part of ATC. 

Also, Email is just one way for this data point to surfaced up. We trigger emails for a bunch of things in async. Depending upon the severity of this event, we can surface these up in a different ways like pager duty alert etc.

We could do something similar in DJ like [~yogeshg] mentioned but not sure if its worth the effort. ","29/Nov/16 11:16 PM;yogeshg;As discussed in the MW sprint meeting, assigning it to [~surendranm] to take care of these scenarios in Async, I will take up problem 2 in next sprint.","14/Dec/16 2:08 PM;sureshr;[~yogeshg] I know you are dealing with a number of production issues. But, can you give a quick update on fixing problem #2 (check if child PID exists or not and if not, then update the state of cmd directly to cancelled)? Could you please file a separate bug for this and provide a tentative ETA?",14/Dec/16 7:26 PM;yogeshg;[~sureshr] There is already a JIRA - MW-229 for second problem. It is part of my current sprint.,15/Dec/16 10:09 PM;sureshr;Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command ran even when the cluster is down,ACM-711,49611,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Minor,,ajayb,satyavathib,satyavathib,21/Oct/16 4:12 AM,12/Jul/17 11:50 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Referring to command ID: 39746163

Issues are:

1. Cluster Instance in URL :

https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-54-177-207-19.us-west-1.compute.amazonaws.com%3A19888%2Fjobhistory%2Fjob%2Fjob_1476919050284_0001%2F&clusterInst=284140

The cluster instance with the failed host name in database is:

    id: 1232948640
     ec2_instance_id: i-58c20aec
            hostname: ec2-204-236-128-23.us-west-1.compute.amazonaws.com
                role: master
amazon_instance_type: m2.2xlarge
       spot_instance: 0
             up_time: 2016-10-19 19:45:14
           down_time: 2016-10-19 20:50:30
      last_seen_time: 2016-10-19 20:50:28
          account_id: 164
          private_ip: ip-10-198-85-253.us-west-1.compute.internal
     cluster_inst_id: 283945
              status: NULL
  private_ip_address: 10.198.85.253
  termination_reason: User initiated

2.     !image-2016-10-21-16-37-24-889.png|thumbnail! 


This account id is : 164
https://api.qubole.com/v2/reports?report_name=cluster_usage_report&clusters_filter=19721#reports

Cluster ran for 3 hours and then down for 5 mins and was up but the command failed after 5 hours.

3.Could not find MR Logs.
",,nikunjv,satyavathib,sriramg,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Oct/16 4:07 AM;satyavathib;image-2016-10-21-16-37-24-889.png;https://qubole.atlassian.net/secure/attachment/33802/image-2016-10-21-16-37-24-889.png,,,,,,,,,,,,,,,,,,,Saavn,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z036tr:,,,,,,,,,,,,,,,,,,,1.0,9181,2016-10-24 00:36:34.745,,,"24/Oct/16 12:36 AM;venkatak;[~satyavathib] could you please share the links related to the screenshot, so that it is easier for dev folks to hit that directly","25/Oct/16 4:00 AM;satyavathib;This account id is : 164

https://api.qubole.com/v2/reports?report_name=cluster_usage_report&clusters_filter=19721#reports","14/Dec/16 12:20 AM;nikunjv;From the conversations in ticket, seems like we solved this and we are waiting for customer's input. [~satyavathib] please confirm. Lowering the priority ","14/Dec/16 1:20 AM;satyavathib;[~nikunjv]
The ticket  has two issues :

1. No logs present which has a seperate JIRA and was solved by developers by increasing the executors memory.

2. Command ran even when the cluster is down ,Thought this to be abnormal and so raised a JIRA.

It seems the linked ticket is resolved from customer side as we didn't get any further updates from customer.

It would be helpful if we get some analysis for the cause so that we can have some idea in future when faced such issues.",28/Mar/17 11:04 AM;sriramg;Hi [~satyavathib]. There are certain commands such as `show tables` which run without a cluster at all. Would be helpful to know what kind of command it was. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to create workflow on a schedule,QBOL-5863,49436,Bug,Reopened,QBOL,qbol,software,sumitm,,,Minor,,qubole,Jove,Jove,18/Oct/16 1:59 PM,08/Aug/17 6:58 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"We are getting a weird 403 error when trying to create a schedule that's also a workflow command. The token we are using is already a system-admin with full permission in the system. 

This is blocking The Trade Desk to build all the workflow scheduled jobs for their clients. cc [~drose@qubole.com] [~gmargabanthu]

Here's a reproducible case for anyone to try (replace $token with your system-admin token). You can put this in a .sh and run it to reproduce the error message:

{code}
curl -i -X POST -H ""X-AUTH-TOKEN: $token"" -H ""Accept: application/json"" -H ""Content-type: application/json"" -d '{
  ""command_type"":""CompositeCommand"",
  ""name"":""jove4"",
  ""sub_commands"": [
   {
     ""inline"": ""if hadoop fs -ls /user/hduser/terasort-input; then\n  hadoop fs -rmr  /user/hduser/terasort-input\nfi\n\nif hadoop fs -ls /user/hduser/terasort-output; then\n  hadoop fs -rmr  /user/hduser/terasort-output\nfi"",
     ""command_type"": ""ShellCommand""
   },
   {
     ""inline"": ""#!/bin/bash\n\nNUM_MAP_TASKS=20\n\n# Total data generated = (DATA_SIZE * 100) bytes\n# Keep DATA_SIZE at 1000000000 to generate 100GB for terasort\nDATA_SIZE=5000000000\n\nhadoop jar /usr/lib/hadoop/hadoop-0.20.1-dev-examples.jar teragen -Dmapred.map.tasks=${NUM_MAP_TASKS} ${DATA_SIZE} /user/hduser/terasort-input"",
     ""command_type"": ""ShellCommand""
   },
   {
     ""inline"": ""#!/bin/bash\n\nNUM_REDUCE_TASKS=20\nNUM_MAP_TASKS=20\nhadoop jar /usr/lib/hadoop/hadoop-0.20.1-dev-examples.jar terasort -Dmapred.map.tasks=${NUM_MAP_TASKS} -Dmapred.reduce.tasks=${NUM_REDUCE_TASKS} /user/hduser/terasort-input/ /user/hduser/terasort-output"",
     ""command_type"": ""ShellCommand""
   }
 ],
  ""start_time"": ""2012-07-01T02:00Z"",
  ""end_time"": ""2022-07-01T02:00Z"",
  ""frequency"": 1440,
  ""time_unit"": ""days"",
  ""time_out"":""10""
}' ""https://api.qubole.com/api/v1.2/scheduler""
{code}

Error:
{color:red}{""error"":{""error_code"":403,""error_message"":""Permission denied to create Workflow.""}}{color}",,aswina,Jove,sumitm,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,No Doc Impact,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0363b:,,,,,,,,,,,,,,,,,,,1.0,10313,2016-10-19 11:42:15.401,,,"19/Oct/16 11:42 AM;sureshr;[~rahulg] Could you please investigate?
cc: [~sumitm]","19/Oct/16 11:46 AM;sumitm;It looks like coming due to recent command type ACL changes.. 

cc - [~aswina]","19/Oct/16 7:28 PM;aswina;[~Jove] - There is a problem in the way JSON payload is constructed.  I have made some minor changes in the JSON payload below and it works fine:

# There's a {{command_type}} key item in the outermost JSON object.
# {{command_type}}, {{name}}, and {{sub_commands}} have been wrapped inside a {{command}} key.
# These changes were done as described [in documentation|http://docs.qubole.com/en/latest/rest-api/scheduler_api/create-a-schedule.html].

{code:java}
curl -i -X POST -H ""X-AUTH-TOKEN: $token""  -H ""Accept: application/json"" -H ""Content-type: application/json"" -d '{
  ""command_type"":""CompositeCommand"",
  ""command"": {
    ""command_type"":""CompositeCommand"",
    ""name"":""jove4"",
    ""sub_commands"": [
     {
       ""inline"": ""if hadoop fs -ls /user/hduser/terasort-input; then\n  hadoop fs -rmr  /user/hduser/terasort-input\nfi\n\nif hadoop fs -ls /user/hduser/terasort-output; then\n  hadoop fs -rmr  /user/hduser/terasort-output\nfi"",
       ""command_type"": ""ShellCommand""
     },
     {
       ""inline"": ""#!/bin/bash\n\nNUM_MAP_TASKS=20\n\n# Total data generated = (DATA_SIZE * 100) bytes\n# Keep DATA_SIZE at 1000000000 to generate 100GB for terasort\nDATA_SIZE=5000000000\n\nhadoop jar /usr/lib/hadoop/hadoop-0.20.1-dev-examples.jar teragen -Dmapred.map.tasks=${NUM_MAP_TASKS} ${DATA_SIZE} /user/hduser/terasort-input"",
       ""command_type"": ""ShellCommand""
     },
     {
       ""inline"": ""#!/bin/bash\n\nNUM_REDUCE_TASKS=20\nNUM_MAP_TASKS=20\nhadoop jar /usr/lib/hadoop/hadoop-0.20.1-dev-examples.jar terasort -Dmapred.map.tasks=${NUM_MAP_TASKS} -Dmapred.reduce.tasks=${NUM_REDUCE_TASKS} /user/hduser/terasort-input/ /user/hduser/terasort-output"",
       ""command_type"": ""ShellCommand""
     }
    ]
  },
  ""start_time"": ""2012-07-01T02:00Z"",
  ""end_time"": ""2022-07-01T02:00Z"",
  ""frequency"": 1440,
  ""time_unit"": ""days"",
  ""time_out"":""10""
}' 'http://api.qubole.com/api/v1.2/scheduler'
{code}
","19/Oct/16 7:30 PM;aswina;This was an error in the JSON payload in API request.  Hence marking it as ""won't fix"".","19/Oct/16 8:29 PM;Jove;Thanks [~aswina]. There's not an example of this exact use case I copied only the content from off our documentation [here|http://docs.qubole.com/en/latest/rest-api/command_api/submit-a-composite-command.html].

Could we update the error message so that it's not a 403? That's wrong still... We can lower the priority though.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Function lapply doesn't work in Notebook,ZEP-579,49435,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,Jove,Jove,18/Oct/16 1:05 PM,18/Jan/17 12:48 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"This code works in Analyze:

{code}
library(SparkR)
sc <- sparkR.init()
rdd <- SparkR:::parallelize(sc, 1:10)
multiplyByTwo <- SparkR:::lapply(rdd, function(x) { x * 2 })
collect(multiplyByTwo)
{code}

But the equivalent code doesn't work in Notebook:

{code}
%r
print('hi')
rdd <- SparkR:::parallelize(sc, 1:10)
multiplyByTwo <- SparkR:::lapply(rdd, function(x) { x * 2 })
collect(multiplyByTwo)
{code}

Seems like Databricks has this issue too. See https://forums.databricks.com/questions/9049/sparkr-lapply-function-hangs.html.",,Jove,karuppayyar,sbadam,vipulm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,roomdots,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03633:,,,,,,,,,,,,,,,,,,,,,2016-10-18 21:11:05.482,,,"18/Oct/16 9:11 PM;karuppayyar;[~vipulm] do you have any idea on this?
R works with spark-2.0?","19/Oct/16 9:14 PM;vipulm;Is this issue on spark 2.0 cluster?
There is a bug tracking R not working on 2.0.
[#https://qubole.atlassian.net/browse/ZEP-450]
","19/Oct/16 9:31 PM;Jove;No, this is not related to Spark 2.0 since R isn't even available in Notebook for Spark 2.0 yet as far as I know.

The test was done with Spark 1.6.1.","18/Jan/17 12:48 PM;sbadam;Hey Karuppayya, can we have any update or ETA for this? Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notify and Kill buttons don't show up till user re-click into the running command,UI-4563,49394,Bug,Open,UI,UI,software,aswina,,,Major,,tabraiz,Jove,Jove,17/Oct/16 5:02 PM,17/Oct/16 8:50 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Steps to reproduce:

1. Type a long running hive query in Analyze, click ""Run""
2. Do NOT click away after step 1, but wait until the submitter is populated on the left panel under the command ID.
3. When the submitter is populated, ""Notify"" and ""Kill"" buttons are not available still. However, if now I click away from the command, and click back. These 2 buttons will appear.",,aswina,Jove,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Oct/16 5:03 PM;Jove;Screen Shot 2016-10-17 at 4.56.57 PM.png;https://qubole.atlassian.net/secure/attachment/33759/Screen+Shot+2016-10-17+at+4.56.57+PM.png,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z035un:,,,,,,,,,,,,,,,,,,,,,2016-10-17 20:25:57.711,,,"17/Oct/16 8:25 PM;aswina;On a related note, there was a similar email on feedback@qubole.com from Oracle about these 'missing' buttons.  I had responded to them saying those buttons appear when a query is run.","17/Oct/16 8:35 PM;Jove;[~aswina], please note that the issue I filed here is about the buttons NOT appearing even when the query IS RUNNING. I believe it's a bug. Please check.",17/Oct/16 8:50 PM;aswina;[~Jove] - Agreed it's a bug.  My comment was to document a related item that we received on feedback@qubole.com.  Sorry for the confusion.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not start zeppelin server until node bootstrap script finishes,ZEP-568,49180,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,beria,Jove,Jove,10/Oct/16 8:55 PM,28/Jun/17 9:43 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Currently, Zeppelin server starts before node bootstrap script finishes. As a result, bootstrap nodebook runs prior to cluster node bootstrap script finishes. 

This is causing problem for some customers. 

cc [~mahuja] [~karuppayyar] [~vipulm] [~venkats]",,ajithr,beria,Jove,Kulbir,mahuja,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ReturnPath,thomsonreuters,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z034j3:,,,,,,,,,,,,,,,,,,,1.0,10126,2016-10-11 18:30:11.994,,,11/Oct/16 6:30 PM;mahuja;[~beria] - can you do initial effort estimate for this. This is also a requirement from T&R (Ilya),"26/Oct/16 1:27 PM;Kulbir;[~beria] ReturnPath is hitting same problem, do we have any ETA yet ?
This is fairly imp. fix as we normally suggest customers to bootstrap 3rd party libs vs. doing at notebook level and in this case it's not reliable.
Please advise.

cc [~mahuja]",26/Oct/16 1:36 PM;Jove;This could potentially be addressed by ACM-615 depending on what's the solution there... ,"26/Oct/16 2:26 PM;mahuja;Adding few thoughts:

We need a handshake mechanism in which the interpreters do not start till the node bootstrap is complete. But also this is not applicable for all notebooks that will be independent of dependencies being installed.

We can have a Zeppelin property which when set to true, looks for completion of the node bootstrap before starting the interpreter. Persistent interpreters can have this property set by default. The notebook execution with this bit set will start only after completion on node bootstrap. The challenge is to inform the user that this is the case. We can show a warning on top of notebook page if node bootstrap is still under execution. This way we do not need to block the start of Zeppelin server.

Thoughts - [~beria], [~karuppayyar], [~vipulm]
","18/Jan/17 11:59 AM;sbadam;Hey Anirudh, can we have any update or ETA for this? Thanks.","31/Jan/17 2:39 AM;beria;[~sbadam] Zeppelin start is also done by hustler, so I think ideally hustler could decide to start zeppelin when node-bootstrap is complete.

But from the discussion, I gather that we want to start Zeppelin server first because bootstrap is taking long (so long that it might even cross the hour boundary)? In that case, we can come up with the process [~mahuja] is suggesting, i.e. to check whether bootstrap is finished or not, and only then allow some interpreter to start. This part (some way of checking if bootstrap finished) is being done in ACM-615 (currently there is no way to know that), so this Jira will be dependent on it.",01/Feb/17 10:54 AM;sbadam;Thanks [~beria] for the update.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating partition table using pyspark resulted in table not usable in Hive,SPAR-1292,49114,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,Jove,Jove,07/Oct/16 11:01 AM,07/Oct/16 11:10 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"If I create a table using this pyspark code:

{code}
%pyspark
from pyspark import HiveContext
from pyspark.sql.types import *

#hivectx = SparkSession.builder.master(""local"").enableHiveSupport().getOrCreate()

hivectx = HiveContext(sc)
data = hivectx.sql(""select 1 as id, 10 as score, 1 as ds"")
data.show(10)
data.write.mode(""append"").partitionBy(""ds"").saveAsTable(""temp.test_jk_spark161_1"")
{code}

The table is created successfully and can be consumed in Spark. A show create table command (38303378) in Spark SQL shows:
{code}
CREATE TABLE `temp`.`test_jk_spark161_1` (`id` INT, `score` INT, `ds` INT)
USING parquet
OPTIONS (
`serialization.format` '1'
)
PARTITIONED BY (ds)
{code}

However, this table is not usable in Hive. A show create table command (38303304) in Hive shows:
{code}
CREATE TABLE `temp.test_jk_spark161_1`(
  `col` array<string> COMMENT 'from deserializer')
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe' 
WITH SERDEPROPERTIES ( 
  'path'='s3n://jkuang/warehouse/temp.db/test_jk_spark161_1') 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.SequenceFileInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat'
LOCATION
  's3n://jkuang/warehouse/temp.db/test_jk_spark161_1'
TBLPROPERTIES (
  'EXTERNAL'='FALSE', 
  'spark.sql.sources.provider'='org.apache.spark.sql.parquet', 
  'spark.sql.sources.schema.numPartCols'='1', 
  'spark.sql.sources.schema.numParts'='1', 
  'spark.sql.sources.schema.part.0'='{\""type\"":\""struct\"",\""fields\"":[{\""name\"":\""id\"",\""type\"":\""integer\"",\""nullable\"":true,\""metadata\"":{}},{\""name\"":\""score\"",\""type\"":\""integer\"",\""nullable\"":true,\""metadata\"":{}},{\""name\"":\""ds\"",\""type\"":\""integer\"",\""nullable\"":true,\""metadata\"":{}}]}', 
  'spark.sql.sources.schema.partCol.0'='ds', 
  'transient_lastDdlTime'='1475818672')
{code}",,Jove,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0345b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Junk inserted while downloading results from analyse panel.,MW-147,48956,Bug,Open,MW,Middleware,software,sumitm,,,Major,,sumitm,satyavathib,satyavathib,04/Oct/16 11:30 PM,17/Jan/17 3:57 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Referring to:

https://api.qubole.com/v2/analyze?command_id=36330637

While downloading the results in csv/tsv junk characters are being inserted in the empty field. Also the headings are not being copied. ",,satyavathib,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,thomsonreuters,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0336v:,,,,,,,,,,,,,,,,,,,,,2017-01-17 15:57:06.665,,,"17/Jan/17 3:57 PM;sbadam;Hey Sumit, can we have any update or ETA for this issue? Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Save query option is not working as system user,UI-4466,48918,Bug,Open,UI,UI,software,aswina,,,Major,,aswina,venkatak,addon_zendesk_for_jira,04/Oct/16 3:48 AM,13/Oct/16 10:26 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Save query option is not working as system user, it is neither allowing user to edit or delete his own query that is saved into the repo tab

h3. Zendesk Attachments


[Attachment 1|https://qubole.zendesk.com/attachments/token/wiqxwMAfSSDZHM5MF9R4BwODP/?name=image001.png]
",,addon_zendesk_for_jira,aswina,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Oct/16 4:05 AM;aswina;sq.PNG;https://qubole.atlassian.net/secure/attachment/33601/sq.PNG,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z032yf:,,,,,,,,,,,,,,,,,,,,,2016-10-04 04:07:07.479,,,"04/Oct/16 4:07 AM;aswina;[~venkatak] - Create, update, and delete is allowed on Repo for system-users.  Please check the attached screenshot.  Can you tell us what exactly the user is trying to do?

 !sq.PNG|thumbnail! 
",13/Oct/16 10:26 PM;aswina;[~venkatak] - I have provided a workaround in the Zendesk ticket.  Please let me know if it's fine.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"display proper warning message, when cluster start got lesser nodes than minimum",ACM-656,48806,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,megha,megha,03/Oct/16 4:26 PM,02/May/17 5:14 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster 17919 was configured to launch 256 nodes..
cluster inst id: 272686
web node ip: ip-10-9-178-172

Only 147 instances were launched..
logs showed, we launched 256 instances, got instance ids for all.. but when we listed instances, some of these were missing.  For ex, node0007 was not in list instances [in debug.log]

So final node count was 147 whereas, requested min nodes was 256..
We should be displaying a warning message on the UI, that minimum number of nodes were not met..



",,megha,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,spacetime,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z032pb:,,,,,,,,,,,,,,,,,,,,,2017-01-17 17:11:26.25,,,"17/Jan/17 5:11 PM;sbadam;Hey Ajay, can we have any update or ETA for this issue? Thanks.","02/May/17 5:14 PM;megha;Hi [~ajayb]
Any updates on this? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when account creation fails with aws keys error or s3 location error, page is reloaded, account is not created and no error message is shown",UI-4618,48804,Bug,Open,UI,UI,software,aswina,,,Major,,tabraiz,megha,megha,03/Oct/16 4:18 PM,25/Oct/16 11:23 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"A user tried to create a new qubole account using aws keys. 
On pressing ""create Account"" button, nothing happened..
I found following on the log server for this account:

./webapp/controller/controller.log:ip-10-187-36-254 [INFO  pid: 7011: 16-10-03 18:03:05 c8b1be26] [AWS S3 403 0.130645 1 retries] put_object(:bucket_name=>""mm-iq-acme"",:content_length=>54,:data=>#<StringIO:0x00556e1fa7bc68 @_routes=nil>,:key=>""quoble/a1f8c072-8993-11e6-9242-22000b8e49cb"") AWS::S3::Errors::InvalidAccessKeyId The AWS Access Key Id you provided does not exist in our records.


Probably account creation failed but didnt show up in the UI..",,aswina,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Oct/16 4:18 PM;megha;IQ_Account_Creation_DoesntWork.png;https://qubole.atlassian.net/secure/attachment/33518/IQ_Account_Creation_DoesntWork.png,,,,,,,,,,,,,,,,,,,mediamath,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z032ov:,,,,,,,,,,,,,,,,,,,,,2016-10-25 23:23:53.9,,,"12/Oct/16 1:09 PM;megha;This also happens in case when s3 location is invalid, for example instead of `s3://meghaqubole/test', if I specify 'meghaqubole' 
seems like UI validation fails but error message doesnt appear...","13/Oct/16 4:38 PM;megha;Also in case of aws errors:
for example i saw this error in the logs:
get_bucket_location(:bucket_name=>""mm-iq-test_acme_3"") AWS::S3::Errors::AccessDenied Access Denied
Supplied bucket:mm-iq-test_acme_3
Supplied prefix:
verify_storage failed with message: InvalidBucketName: Hadoop does not support underscores in the bucket names.


",25/Oct/16 11:23 PM;aswina;[~tabraiz] - Can you take a look at this please?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Any recent change broke /usr/lib/sqoop/bin/sqoop on Had2 Cluster,HADTWO-642,48729,Bug,Open,HADTWO,Hadoop2,software,ajayb,,,Major,,ksr,adubey,addon_zendesk_for_jira,30/Sep/16 2:23 PM,11/Jul/17 2:59 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"https://api.qubole.com/v2/analyze?command_id=37355905 is a sample of running job in the past but now the same job fails:
https://api.qubole.com/v2/analyze?command_id=37558022

One difference i see is -
HADOOP_USER_NAME=artem.yankov
vs HADOOP_USER_NAME=artem.yankov@reddit.com

I remember we did this change for using shellcli for hadoop dfs commands across different version of hadoop - i suspect this is the culprit here . i think we just need to identify and summarize the finding here",,abhishekmodi,addon_zendesk_for_jira,adubey,ksr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03287:,,,,,,,,,,,,,,,,,,,1.0,9911,2016-10-03 02:23:04.577,,,"03/Oct/16 2:23 AM;abhishekmodi;Assigning to Sqoop team for initial look. If there is any issue related to Hadoop, please move it back.","04/Oct/16 8:37 PM;adubey;guys - can we close this with a proper summary. there is definitely something which has got changed - [~abhishekmodi] what about the user id having email or non-email do you think that could have caused this. 
Customer is fine with changing the jobs - but they also expect a root cause analysis on this that what has changed and how they can be prepared for something like this. they had several jobs impacted due to this.","04/Oct/16 9:36 PM;abhishekmodi;[~adubey]: This should not have changed anything. I wanted sqoop team to do initial triage with few sqoop commands. If we see issue is on Hadoop side, I can take a look.",04/Oct/16 9:41 PM;ksr;[~adubey] [~abhishekmodi] I'll take a look at this today and see if I can figure something out.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Py spark paragraph fails when line ending has CR LF characters,ZEP-547,48419,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,karuppayyar,karuppayyar,27/Sep/16 12:01 PM,09/Mar/17 2:33 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Example para text:
""text"": ""%pyspark\r\nprint(sc.version)\r\nprint(sc.version)\r\n"",

The above para fails with *invalid syntax*

cc: [~adubey]",,adubey,gmargabanthu,Jove,karuppayyar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,activision,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z03153:,,,,,,,,,,,,,,,,,,,,,2016-09-28 12:20:40.825,,,28/Sep/16 12:20 PM;Jove;Is this a possible dupe with ZEP-350?,"28/Sep/16 12:29 PM;karuppayyar;Pyspark fails. Scala passes.
Checked this yesterday.","19/Oct/16 3:06 PM;adubey;[~karuppayyar] did you get a chance to look into it. or zep-350 is fixed, will cover this ","19/Oct/16 7:10 PM;karuppayyar;ZEP-350 does not cover this. 
ZEP-350 handles cases where the repl name(%sql, %pyspark etc contains CR LF).
In this case, I guess it is happening due to CR LF charcter in the para text. I will check this again today and get back to you.","20/Oct/16 5:59 AM;karuppayyar;I tried to repro this again in-house. But was not able to. 
Did they have custompackages then(I checked their cluster configs, they don have now)

Also tred to fetch the snapshot of the notebopok at the time at which it was reported.
s3://ds-qubole-prod/logs/hadoop/16060/269209/ec2-54-218-68-163.us-west-2.compute.amazonaws.com.master/zeppelin/notebook/3XMZNEXVHN1474571324/note.json
Looks like they have fixed it by removing the carriage return.

[~adubey] can u try once, confirm if its not repro-able?",09/Mar/17 2:33 PM;gmargabanthu;[~karuppayyar][~adubey] - Next steps? Can we have a preliminary ETA? Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default Spark cluster is 1.5.1 while UI defaults to 1.6.1,SPAR-1272,48331,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,Jove,addon_zendesk_for_jira,26/Sep/16 2:05 PM,26/Sep/16 4:40 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,,,addon_zendesk_for_jira,Jove,mahuja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z030xr:,,,,,,,,,,,,,,,,,,,,,2016-09-26 16:32:31.08,,,"26/Sep/16 4:32 PM;mahuja;[~Jove] - to confirm, on restart the version came up fine on the cluster.","26/Sep/16 4:40 PM;Jove;In order to fix, all I needed to do is to click open the cluster config page, then click on ""Save"" without changing anything. Then the value 1.6.1 will be populated into the field spark_version and the cluster comes up as 1.6.1.

I think somewhere (middleware perhaps?) will default to Spark 1.5.1 if spark_version=null, but then the UI defaults it to 1.6.1 now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New lines in results corrupts spark display table,ZEP-543,48302,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,abhimanyuc,abhimanyuc,26/Sep/16 2:47 AM,17/Jan/17 11:32 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Ran the query 'select explode(array('foo','bar','\n','baz','1','2','3'))' in Notebook and Analyze. Analyze works fine. In Notebook, there's no o/p after the '\n'.",,abhimanyuc,drose@qubole.com,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,underarmour,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z030sb:,,,,,,,,,,,,,,,,,,,,,2017-01-17 11:29:06.585,,,17/Jan/17 11:29 AM;sbadam;Hey [mailto:Karuppayya Rajendran ] can we have any update or ETA for this issue? Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No of nodes varies form autolog to the no of nodes in cluster page,ACM-635,48250,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,biswajit,biswajit,25/Sep/16 8:47 PM,25/Sep/16 8:48 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Hi Team, 

The no of active nodes shown in cluster page is different form the no of nodes in autoscalling log. The minimum no of slave nodes is set to 1 in cluster config. The autoscalling log says it is 1 while in control panel says 2. 

{code}

2016-09-26 03:39:15 INFO - Current cluster size: 1
2016-09-26 03:38:14 INFO - Current cluster size: 1
2016-09-26 03:37:13 INFO - Current cluster size: 1
2016-09-26 03:36:12 INFO - Current cluster size: 1
2016-09-26 03:35:11 INFO - Current cluster size: 1
2016-09-26 03:34:10 INFO - Current cluster size: 1
2016-09-26 03:33:08 INFO - Current cluster size: 1
2016-09-26 03:32:07 INFO - Current cluster size: 1
2016-09-26 03:31:06 INFO - Current cluster size: 1
2016-09-26 03:30:04 INFO - Current cluster size: 1
2016-09-26 03:29:00 INFO - Current cluster size: 1
2016-09-26 03:27:59 INFO - Current cluster size: 1
2016-09-26 03:26:57 INFO - Current cluster size: 1
2016-09-26 03:25:56 INFO - Current cluster size: 1
2016-09-26 03:24:55 INFO - Current cluster size: 1
2016-09-26 03:23:54 INFO - Current cluster size: 1
2016-09-26 03:22:53 INFO - Current cluster size: 1
2016-09-26 03:21:52 INFO - Current cluster size: 1
2016-09-26 03:20:51 INFO - Current cluster size: 1

{code}

",,biswajit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Sep/16 8:46 PM;biswajit;Screen Shot 2016-09-26 at 9.12.40 AM.png;https://qubole.atlassian.net/secure/attachment/33104/Screen+Shot+2016-09-26+at+9.12.40+AM.png,,,,,,,,,,,,,,,,,,,thomsonreuters,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z030ob:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Job fails after all MR jobs succeed. Ret code -9,QPIG-64,48213,Bug,Open,QPIG,qbol pig,software,bharatb,,,Critical,,beria,ajithr,addon_zendesk_for_jira,23/Sep/16 5:58 AM,09/Jul/17 11:19 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"The workflow command 'https://api.qubole.com/v2/analyze?command_id=36385909' failed while executing a pig script.

It appears that the hive_script utils code (cmd_utils.py) invoked pig and waited for it to complete. As part of the pig script execution, MR job was launched and that succeeded too (from application / MR logs). But, when pig returned, it exited with an return code of -9. 

In the absence of logs and cluster getting restarted, we would like to understand if there is any other method to get to the bottom of this.

Also, notice that this happened in the night when RB37 CP1 was released. Wanted some expert help to check if this command was also a victim of those release issues?",,addon_zendesk_for_jira,ajithr,beria,gayathrym,sam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,clickagy,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z030g3:,,,,,,,,,,,,,,,,,,,1.0,9656,2016-09-27 22:07:01.3,,,"27/Sep/16 10:07 PM;gayathrym;Thanks [~ajithr] : I think you meant 'RB37' was released.  
RB37-CP1 is going to prod tomorrow. ",28/Sep/16 12:16 AM;ajithr;That's correct. My bad. Meant RB37.,12/Oct/16 11:26 AM;sam;Any finding on this issue?,27/Dec/16 1:31 PM;sam;Status please? Thanks ,"27/Dec/16 11:27 PM;beria;I have not been looking into this because of other assignments. I think [~ajithr] and [~adubey] found that it was linked to AWS network issue or our hadoop issue?
cc [~mahuja] [~bharatb]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop job link missing form the analyse page,SCHED-123,48070,Bug,Open,SCHED,Scheduler,software,sumitm,,,Critical,,sureshr,biswajit,biswajit,22/Sep/16 1:07 AM,25/May/17 7:53 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,rb39-candidate,,,,,,,"Command-id :- 36481501

As per the user hadoop job was launched and MR was completed successfully with proper output. But there is no link for the hadoop job. ",,abhishekmodi,biswajit,gayathrym,hiyer,ksr,rahulg,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,revx,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0303f:,,,,,,MW Sprint 22(18Jan-31Jan),MW Sprint 23 (1-Feb - 14-Feb),,,,,,,,,,,,1.0,9665,2016-09-23 00:36:43.215,,,"23/Sep/16 12:36 AM;abhishekmodi;[~hiyer] and I looked into this jira and found that it was run using scheduler. We can see ip-10-5-164-49.ec2.internal present in default dj logs and this command id also. Somehow this node was not able to access the database.
{code}
""Host 'ip-10-5-164-49.ec2.internal' is not allowed to connect to this MySQL server""
{code}
Moving jira to SCHED for taking a further look.","23/Sep/16 12:40 AM;hiyer;Some logs from defaultDJ for this command:
{noformat}
ip-10-5-164-49 [INFO  pid: 22705: 16-09-21 19:01:22 ] [command:36481501] [periodic_job:7248] [account:5031] Finished execution for PJUnit: #<PeriodicJobUnit id: 2336890, query_hist_id: 36481501, objects_json: nil, created_at: ""2016-09-21 19:00:02"", updated_at: ""2016-09-21 19:00:07"", periodic_job_unit_id: nil, periodic_job_id: 7248, sequence_id: 4037, nominal_time: ""2016-09-21 19:00:00"", is_rerun_of: nil, editable_periodic_job_id: 21156, dependencies: {""found""=>[], ""not_found""=>[]}, rerun_number: 1, status: ""done"", done: true>
ip-10-5-164-49 [INFO  pid: 22705: 16-09-21 19:01:22 ] [command:36481501] [periodic_job:7248] [account:5031] 2016-09-21T19:01:22+0000: [Worker(delayed_job.0 host:ip-10-5-164-49 pid:22705)] PjuWorker#run_pju completed after 77.0613
ip-10-5-164-49 [INFO  pid: 22705: 16-09-21 19:01:22 ] [command:36481501] [periodic_job:7248] [account:5031] 2016-09-21T19:01:22+0000: [Worker(delayed_job.0 host:ip-10-5-164-49 pid:22705)] 1 jobs processed at 0.0129 j/s, 0 failed ...
{noformat}",28/Sep/16 1:46 AM;gayathrym;Thanks [~rahulg] for updating the ETA label. Appreciate it!!,"17/Jan/17 3:55 PM;sbadam;Hey Rahul, can we have any update to this issue? Thanks.",17/Jan/17 8:16 PM;rahulg;[~sbadam]..will pick it in this sprint.,22/Feb/17 2:36 AM;rahulg;[~ksr] can you please look into this as well.,06/Mar/17 9:41 PM;ksr;[~sureshr] I wasn't able to investigate this issue. Assigning it to you. Could you please assign it to the MW dev-on-call?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFound Exception: Spark constructs incorrect Class name,SPAR-1270,48054,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,ajithr,addon_zendesk_for_jira,21/Sep/16 9:29 PM,15/Jun/17 11:06 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"- Command 36525426 fails with ClassNotFoundException.

- But, the class being loaded is incorrect - id_sets.hhid,.VarReduceSparkWide.

- User is running a Spark - Scala program from the Analyze UI. So, it is QDS' responsibility to construct the command line for the same (with spark-submit)

- It appears that we constructed an incorrect cmd line:

{noformat}
App > + jar cvf code.jar script.scala script.scala.tmp 'VarReduceSparkWide$$anonfun$1.class' 'VarReduceSparkWide$$anonfun$2.class' 'VarReduceSparkWide$$anonfun$3.class' 'VarReduceSparkWide$$anonfun$4.class' 'VarReduceSparkWide$$anonfun$importAndUnion$1$1.class' 'VarReduceSparkWide$$anonfun$keepSelectedFeatures$1$1.class' 'VarReduceSparkWide$$anonfun$main$1.class' 'VarReduceSparkWide$$anonfun$main$2.class' 'VarReduceSparkWide$$anonfun$main$3.class' 'VarReduceSparkWide$$anonfun$printMapAsDict$1$1.class' 'VarReduceSparkWide$$anonfun$VarReduceSparkWide$$vecToString$1$1.class' VarReduceSparkWide.class 'VarReduceSparkWide$.class'
App > + /usr/lib/spark/bin/spark-submit --master yarn-client --num-executors 6 --executor-cores 5 --executor-memory 30G --conf spark.yarn.executor.memoryOverhead=3700 --conf spark.rdd.compress=true --conf spark.rdd.compress=true --conf spark.driver.maxResultSize=5G --conf spark.sql.qubole.result.formatted=true --driver-memory 30G --class id_sets.hhid,.VarReduceSparkWide code.jar
{noformat}
",,addon_zendesk_for_jira,ajithr,drose@qubole.com,mahuja,rohitk,sbadam,tsp,,,,,,,,,,,,,,,,,,,,,,,SPAR-1271,,,,,,,,,,,,,,,,,,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0300b:,,,,,,,,,,,,,,,,,,,1.0,9655,2016-09-21 22:52:12.197,,,"21/Sep/16 9:34 PM;ajithr;One other thing to notice here is that the first few chars of incorrect class string - ""id_sets.hhid,.VarReduceSparkWide"" is coming from the Spark SQL code that the user has implemented. See below:

{noformat}
val initial_data = sqlContext.sql(""""""   
    select id_sets.hhid, id_sets.model_set, id_sets.resp,
{noformat}

This could be a good clue to check what may be wrong underneath.","21/Sep/16 9:37 PM;ajithr;Trying to create a reproducible testcase. If successful, I will attach the same here. I have also checked with customer to see if we can rerun the command in his setup until we recreate this in-house.","21/Sep/16 10:52 PM;tsp;Code: https://qubole.slack.com/files/tsp/F2EGGPVS8/classnotfoundexception__36525426_.scala

When this command is run as a scala snippet on analyze our {{spark_command_executor.rb}} will attempt to parse {{classname}} and {{packagename}} for the snippet so as to wrap in in a jar.

The code does the following:
After the {{classname}} is parsed

{code}
307     if language.include? ""scala""
308       if !full_class_provided
309    cmdline = cmdline + "" --class generated ""
310       else
311           packagename = """"
312           program.split(""\n"").each do |line|
313             if line.include? ""package""
314               packagename = line.split("" "")[1]
315             end
316           end
317           if packagename.blank?
318             cmdline = cmdline + "" --class ""+ classname
319           else
320             cmdline = cmdline + "" --class "" + packagename + ""."" + classname
321           end
322       end
323     end
324
325     puts cmdline
{code}

The problem is in the line 313. Without any spaces around {{package}} we won't have a valid package syntax anyway so a hacky way to fix this is to at least look for a space around the string.

[~rohitk] suggests a solution wherein we do not try to parse for package names after the first imports are defined.

","22/Sep/16 2:01 AM;tsp;Couple of solutions discussed to solve parsing text from the script

h1. Jar disassembly at web nodes

# Explodes the jar formed from the snippet of code ie. {{code.jar}}
# Use the exploded jar's dir hierarchy to tell the package name eg: com/qubole/MyQuboleSpark.class is in {{com.qubole}}
# Use the {{javap}} disassembler to print the names of methods and classes 
# Parse the names of classes and find the class that contains the main method
# Con: Falls back to greps again to guess the package and classname 

h2. Jar load and reflection

# Application manager gets a command line without the {{--class}} argument
# Within spark code before launching the application, re-build the command line to guess the class name
# Introspect the classname using {{Class.getSimpleName()}}
# Filter out classes that are anonymous using {{Class.isAnonymousClass()}}
","23/Sep/16 12:05 AM;ajithr;[~tsp] I couldn't understand the 2nd option completely. The Class.getSimpleName() is an instance method and not static. So, how will we have the class object at this point?

*Another suggestion below:*

1. Get the code definition as a byte array.
2. Use ClassLoader.defineClass(...) to create a class object (instance of java.lang.Class). The doc is [here|https://goo.gl/lbLjZn].
3. We can also ignore steps 1 & 2. We could explode the jar, get the files with "".class"" extension. and use the  ClassLoader.defineClass(..) as shown [here|https://goo.gl/b1RjlE]
4. One can use a custom classloader (and not delegate) for #2 and #3 above if we want to create a separate namespace.
5. Use classObject.getDeclaredMethods() (for each instance) and check if there is a method - ""public <type> static main(String)"". Doc is [here|https://goo.gl/ivGoEz].
6. For each method, call getName() and check other details before deciding on a candidate. Doc is [here|https://goo.gl/XLFkxp] 
7. If there are multiple main methods in the JAR, then throw an appropriate exception.",04/Oct/16 9:10 PM;drose@qubole.com;[~ajithr] [~tsp] do we have next steps to share with the customer?,"17/Jan/17 3:39 PM;sbadam;Hey Mayank, can we have any update or ETA for this issue? Thanks.",17/Jan/17 5:20 PM;mahuja;[~sbadam] - this issue is not being currently worked on. Can you please share impact and priority on this?,18/Jan/17 11:37 AM;sbadam;[mailto:Mayank Ahuja] - we are just reviewing the backlog and this issue is touched in the process. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster bring up failing and going in cycles for 4-5 hours,ACM-627,48048,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,Ranjana,Ranjana,21/Sep/16 6:00 PM,24/Feb/17 1:10 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cluster id: 18497.

This cluster is not in private VPC and does not have a bastion host. 

Command id: 36479033

The command was submitted at  2016-09-21 18:30:11

This happened for cluster instance 264235. 

I see that there is no cluster bring up time. It is NULL.

Cluster stop time is 2016-09-21 21:59:29

and reason is 

""Hadoop bringup failed. Last error message: Encrypted channel to cluster (qbol_acc5616_cl18497) FAILED: Encrypted channel to cluster (qbol_acc5616_cl18497) FAILED: invalid literal for int() with base 10: '' (error: Traceback (most recent call last):
  File ""/usr/lib/qubole/packages/hive_scripts-37.5.0/hive_scripts/service/tunneling/gen-py/tunnelserver/TunnelService-remote"", line 94, in <module>
    pp.pprint(client.getProxyPort(eval(args[0]),args[1],args[2],args[3],args[4],args[5],))
  File ""/usr/lib/qubole/packages/hive_scripts-37.5.0/hive_scripts/service/tunneling/gen-py/tunnelserver/TunnelService.py"", line 127, in getProxyPort
    return self.recv_getProxyPort()
  File ""/usr/lib/qubole/packages/hive_scripts-37.5.0/hive_scripts/service/tunneling/gen-py/tunnelserver/TunnelService.py"", line 155, in recv_getProxyPort
    raise TApplicationException(TApplicationException.MISSING_RESULT, ""getProxyPort failed: unknown result"");
thrift.Thrift.TApplicationException: getProxyPort failed: unknown result
) 
""

We saw from the logs for 36479033 that the cluster was finally available to the command at  2016-09-21 23:52:37.

2016-09-21 23:52:37,467 >>> Resource manager on master ec2-54-221-77-216.compute-1.amazonaws.com for cluster qbol_acc5616_cl18497 is accessible.

But, the next instance of the cluster, 264367 already started at 2016-09-21 22:07:03, but the logs for your job show that , it was still trying at that time. 

The objective of this jira 

(1) The time out is 10 minutes, why didn't the command report failed within 10 minutes ?
(2) The cluster came up normally at 2016-09-21 22:07:03. Why didn't the command pick this up right then. The logs for 36479033 say that the cluster became available only at 2016-09-21 23:52:37.
(3)The cluster is using stable spot instances for the minimum number of nodes. Is not using stable spot instances the solution ?",,ajayaa,ajayb,Ranjana,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,mistsys,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02zz7:,,,,,,,,,,,,,,,,,,,,,2016-09-22 03:53:54.135,,,"22/Sep/16 3:53 AM;ajayb;[~Ranjana] why is this a blocker jira? Is this currently blocking the customer? I see that currentIy the cluster instance (cluster inst id 264367) is UP. Or are you just looking for investigation of above behavior? I am marking the jira as Major with this assumption.

I looked at the clusters start logs and spot requests for the min nodes got fulfilled within 1.5 minutes. Need to look further on why it took so long for the cluster to be marked down on failure.
{code:java}
ip-10-81-170-175 2016-09-21 18:25:22,232 INFO  [cluster:18497] [cluster_instance:263784]  cluster.py:4175 - _lock_and_start - cluster_id 18497, cluster_inst_id 264235:: extending start loop time by 1260 seconds since it uses stable spot nodes
...
ip-10-81-170-175 2016-09-21 18:26:43,508 INFO  [cluster:18497] [cluster_instance:263784]  cluster.py:2931 - wait_for_spots - UPSCALE: Spot instances granted/requested: 5/5
...
ip-10-81-170-175 2016-09-21 18:27:38,727 INFO  [cluster:18497] [cluster_instance:263784]  cluster.py:3447 - wait_for_hadoop - ==> Waiting for Hadoop to come up ...
...
[Big time gap here that I still need to look into]
...
ip-10-81-170-175 2016-09-21 21:58:57,233 ERROR [cluster:18497] [cluster_instance:263784]  cluster.py:4229 - _lock_and_start - cluster_id 18497, cluster_inst_id 264235: exception in start: Hadoop bringup failed.
{code}
","17/Jan/17 3:34 PM;sbadam;Hey Ajay, can we have any update or ETA for this issue? Thanks.","23/Feb/17 9:46 AM;ajayb;[~ajayaa] based on what little information is here, do you think this has been addressed by the changes you have made? These are your changes for avoiding retries for non-recoverable failures, changes in clusterinfo to avoid races & stepping on toes of other start processes.","24/Feb/17 1:05 AM;ajayaa;We have fixed multiple race conditions over time e.g. ACM-728, ACM-861, ACM-942. I think we should close this. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyze window not handling comments in SQL properly,MW-1064,48047,Bug,Open,MW,Middleware,software,sumitm,,,Major,,aswina,Ranjana,Ranjana,21/Sep/16 4:29 PM,06/Aug/17 11:55 PM,09/Aug/17 6:03 AM,,,,,0,bootcamp,jira_escalated,SOLSUP,,,,,,"Customer affected: Oracle
Problem Description: Two command ids, the text of code is exactly the same in both, the one with sections commented out fails. 
Reported twice by Oracle
# +Zendesk ticket: 9647+
Type of Command: Spark Command, Scala
Look at 36503828 (successful) and 36505051(with rest commented out, unsuccessful). Ideally there should be no difference between these two command ids as both have only the same lines uncommented. 
Ran these 2 command ids again today (in my account as these are just println statements):  65744718(successful) and 65744976(failed with below error)
{code:java}
Qubole > Shell command started on mapper
Qubole > Shell Command failed with exit code: 1
App > script.scala:1: error: expected class or object definition
App > println(""link count by provideR"")
App > ^
App > script.scala:2: error: expected class or object definition
App > println(""test"")
App > ^
App > script.scala:3: error: expected class or object definition
App > println(""qubole"")
App > ^
App > three errors found
Qubole > 2017-04-10 03:19:35,527 ERROR shellcli.py:236 - run - Retrying exception reading mapper output: (22, 'The requested URL returned error: 404 Not Found')
Qubole > 2017-04-10 03:19:35,571 ERROR shellcli.py:267 - run - Retrying exception reading mapper logs: (22, 'The requested URL returned error: 404 Not Found')
{code}

# +Zendesh ticket: 13792+
Type of command: Hive query 
This query was part of a larger workflow(64634578) and this issue led to some confusion. Both me and customer first fished around to see if this was due to some error in the Hive query when I thought of simply re-running explain of this query alone and after removing the commented out portion, which passed successfully. 
Command ids: 64967499 (successful without commented out portion), 64940006(failed with commented portion).
You may re-run just the explain part as you desire, the exception stack trace is same : 65746881
The stack trace below mentions a Parse Exception. 
{code:java}
NoViableAltException(-1@[])
at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1084)
at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:202)
at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:327)
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1164)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1227)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1101)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1091)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:242)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:194)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:343)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:445)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:491)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:843)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:798)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:709)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 4:29 cannot recognize input near '' '' ''
org.apache.hadoop.hive.ql.parse.ParseException: line 4:29 cannot recognize input near '' '' ''
at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)
at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:327)
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1164)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1227)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1101)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1091)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:242)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:194)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:412)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:343)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:445)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:491)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:843)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:798)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:709)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{code}




",,adityak,adubey,aswina,drose@qubole.com,mahuja,Ranjana,sbadam,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,oracle,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02zyz:,,,,,,,,,,,,,,,,,,,2.0,"9647,13792",2017-01-17 15:24:12.645,,,"17/Jan/17 3:24 PM;sbadam;Hey Mayank, can we have any update or ETA for this issue? Thanks.","17/Jan/17 5:22 PM;mahuja;[~sbadam] this is not planned yet.

Internally, this is a candidate for bootcamp JIRA (labelled accordingly)","09/Apr/17 8:36 PM;Ranjana;[~mahuja] This was reported again by Oracle, this time for Analyze -> Hive query. 
Please feel free to move this jira to appropriate project, this is not happening just for Spark command, found this for Hive query too. ","12/Apr/17 8:50 PM;Ranjana;Another command id encountered today : 66349589
Customer: Oracle",17/May/17 8:56 AM;sureshr;[~mahuja] will investigated and move to the right project if this is not spark specific.,24/May/17 8:43 AM;adubey;Any update on this..,07/Jun/17 12:11 AM;mahuja;Moving to UI as the behavior is seen across engines,07/Jun/17 8:57 AM;sureshr;[~aswina]: Could you please investigate?,"08/Jun/17 3:29 AM;aswina;This does not seem like a UI issue because UI passes the query to `/qpal/runquery` API which creates a QH and then DJ picks it up.  Also, looking at the stack traces, it does not seem like a MW issue either.  Tagging [~sumitm] anyway.
","28/Jun/17 8:47 AM;sureshr;Any updates, [~sumitm]?","05/Jul/17 8:56 AM;adityak;[~sumitm] - Any updates ? 
CC : [~sureshr]","26/Jul/17 9:04 AM;sureshr;[~aswina]: Back to you now as lead for Analyst scrum team. :) Could this be a reasonable bootcamp JIRA for [~paritap]?
cc: [~raghunandan]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Downloading the ""All commands"" report gives different results",AD-134,47968,Bug,Open,AD,Admin,software,bmohanam,,,Major,,bmohanam,p.vasa,p.vasa,20/Sep/16 5:30 PM,02/Aug/17 11:18 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Steps to reproduce this issue:

1) Go to -> QDS Usage -> Reports.

2) Select the following properties:
a) Report Type -> All Commands
b) Date range -> Custom
c) Select Range -> September 19 to September 19 (in particular any single day range)
d) User -> All
e) Sort Column -> Created At

3) Select the ""Submit"" button.

4) On the UI page some of the records of the report will be seen and note that these commands will be seen according to the local time. (Example) When submitted this report from Mountain View office, the ""Created On"" and the ""End Time"" on the UI is according to the PST/PCT).

5) Now download the results and view the downloaded .csv file

6) The downloaded .csv file will show all the commands in terms of UTC and not the local time.

So here is the error which is seen->

Normally, UTC is 7 hours ahead of PCT/PST.

Consider the following example->

The customer ran this type of report thrice on different time periods on September 20.
The report was submitted on the following times (on September 20)->
a) 13.20 pm PST (20.20 pm UTC)
b) 15.20 pm PST (22.20 pm UTC)
c) 16.40 pm PST (23.40 pm UTC)

The report was submitted for a single date range: September 19 to September 19.

Hence, all these 3 reports should contain the commands ran for an entire 24 hour cycle for that day, ie, the report should show commands from (September 19, 00.00 am PST to September 19, 23.59 pm PST) (or) from (September 19, 07.00 am UTC to September 20, 06.59 am UTC).

However, the last record in each of the reports has a time which is just before the time when the report was submitted instead of showing the entire 24-hour one day cycle commands.

Please check the last record in all the 3 .csv files attached to view the error case.

a) report_2020.csv -> Report submitted on September 20 (13.20 pm PST/20.20 pm UTC)
b) report_2220.csv -> Report submitted on September 20 (15.20 pm PST/22.20 pm UTC)
c) report_2340.csv -> Report submitted on September 20 (16.40 pm PST/23.40 pm UTC)

All the above 3 reports were run for a custom date range from September 19 to September 19.

Please let me know if you need additional information.",,aswina,p.vasa,rohitk,sbadam,sureshr,tabraiz,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Sep/16 5:30 PM;p.vasa;report_2020.csv;https://qubole.atlassian.net/secure/attachment/32933/report_2020.csv,20/Sep/16 5:30 PM;p.vasa;report_2220.csv;https://qubole.atlassian.net/secure/attachment/32932/report_2220.csv,20/Sep/16 5:30 PM;p.vasa;report_2340.csv;https://qubole.atlassian.net/secure/attachment/32931/report_2340.csv,20/Sep/16 5:30 PM;p.vasa;screenshot_2020.png;https://qubole.atlassian.net/secure/attachment/32930/screenshot_2020.png,20/Sep/16 5:30 PM;p.vasa;screenshot_2220.png;https://qubole.atlassian.net/secure/attachment/32929/screenshot_2220.png,20/Sep/16 5:30 PM;p.vasa;screenshot_2340.png;https://qubole.atlassian.net/secure/attachment/32928/screenshot_2340.png,,,,,,,,,,,,,,umg,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|xzz1no:,,,,,,UI Sprint 16 (28-Sep - 11-Oct),UI Sprint 24 (18-Jan - 31-Jan),UI Sprint 25 (1-Feb - 14-Feb),UI Sprint 26 (15-Feb - 28-Feb),,,,,,,,,,1.0,9388,2016-09-22 23:35:42.494,,,22/Sep/16 11:35 PM;aswina;Assigning this to [~tabraiz],"05/Oct/16 11:01 AM;p.vasa;Hello [~tabraiz] Can we have a roadmap on this one so that we can communicate it with the customer?
Thanks",12/Oct/16 6:01 PM;tabraiz;Hi [~p.vasa] missed this message earlier. This will be in production in next full release - rb39 (Nov 16),12/Oct/16 11:12 PM;p.vasa;Thank you for the information [~tabraiz],06/Dec/16 10:48 AM;p.vasa;Hello [~tabraiz] is this already out in production?,08/Dec/16 2:30 AM;aswina;[~p.vasa] - RB39 is not being released to api.qubole.com because of moratorium.  It'll make it to production with R40 in January.,"08/Dec/16 5:39 PM;p.vasa;Hello [~aswina] thank you for the information.
Will notify the customer accordingly.",13/Jan/17 6:55 PM;aswina;[~tabraiz] - Has this made it into production?,"17/Jan/17 11:33 AM;sbadam;[mailto:Tabraiz Ali], I just looked at release notes of RB40, this issue is not listed there. Could you please give us an update of the fix? ",17/Jan/17 8:16 PM;tabraiz;This was not picked up . i will be picking this up in this sprint,"06/Jun/17 5:40 PM;p.vasa;[~tabraiz] Can you please provide any update regarding this issue?
Please let me know if you need any additional information.

Thank you.","31/Jul/17 1:41 PM;p.vasa;[~tabraiz] Any update on this issue?Is this on our roadmap?
(Please let me know if you need any additional information)",02/Aug/17 11:18 PM;aswina;Assigning this to [~bmohanam] for prioritization.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notification in case Persistent Interpreter is not started,ZEP-536,47957,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,beria,beria,20/Sep/16 11:24 AM,21/Feb/17 9:14 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Following from PR discussions in https://bitbucket.org/qubole/zeppelin/pull-requests/207/bootstrap-notebook-persistent-interpreters/diff#chg-zeppelin-zengine/src/test/java/org/apache/zeppelin/util/TestPersistentIntpsAndBootstrapNotebooks.java : In case a persistent interpreter is not started, a noticifation could be sent to appropriate channel via , say, mail. Or it could be a notification to all users / admin users / owner of interpreters whenever they open the zeppelin interface for the 1st time after such event has happened. [~vipulm] [~karuppayyar]",,beria,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02zir:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Pandas python library unable to set S3 access keys,QBOL-5749,47816,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,sureshr,Ranjana,Ranjana,14/Sep/16 2:43 PM,20/Mar/17 11:27 AM,09/Aug/17 6:03 AM,,,,,0,bugbash-nov16,jira_escalated,,,,,,,"%pyspark

import csv
rdd = sc.textFile(""s3://ms-rentrak1/SESSION_SPARK/Spark_Sessions_0616.zip"")
#rdd = rdd.mapPartitions(lambda x: csv.reader(x))
#rdd.take(5)
print rdd.take(1)

import boto

#s3 = boto.client('s3')
#obj = s3.get_object(Bucket='ms-rentrak1', Key='key')
#df = pd.read_csv(obj['Body'])

from boto.s3.key import Key

import pandas as pd
import numpy as np

conn = boto.connect_s3()

mybucket = conn.get_bucket('ms-rentrak1', validate=False)

k = Key(mybucket)
#k.key = 'data_1.csv'
#k.set_canned_acl('public-read')

#conn = boto.connect_s3()

#mybucket = conn.get_bucket('ms-rentrak1', validate=False)

#mybucket.set_acl('public_read')

#for key in mybucket.list():
#    print key, key.storage_class

#k = mybucket.get_key('SESSION_SPARK/Spark_Sessions_0616.zip')
k.key = 'SESSION_SPARK/Spark_Sessions_0616.zip'
k.set_canned_acl('public-read')
#k = boto.s3.key.Key(mybucket)
#k.key = 'Spark_Sessions_0616.zip'

#k = Key('ms-rentrak1')
#k.key = 'SESSION_SPARK/Spark_Sessions_0616.zip'
#k.set_acl('public-read')

print k, k.storage_class

url = 'https://s3.amazonaws.com/ms-rentrak1/SESSION_SPARK/Spark_Sessions_0616.zip'

raw = pd.read_csv(url, compression='zip',sep=""\t"")

#raw = pd.read_csv(""s3://ms-rentrak1/SESSION_SPARK/Spark_Sessions_0616.zip"",sep=""\t"")

Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark.py"", line 162, in <module>
    eval(compiledCode)
  File ""<string>"", line 12, in <module>
  File ""/usr/lib/virtualenv/python27/local/lib/python2.7/site-packages/boto/s3/key.py"", line 584, in set_canned_acl
    return self.bucket.set_canned_acl(acl_str, self.name, headers)
  File ""/usr/lib/virtualenv/python27/local/lib/python2.7/site-packages/boto/s3/bucket.py"", line 908, in set_canned_acl
    response.status, response.reason, body)
S3ResponseError: S3ResponseError: 403 Forbidden
<?xml version=""1.0"" encoding=""UTF-8""?>
<Error><Code>InvalidAccessKeyId</Code><Message>The AWS Access Key Id you provided does not exist in our records.</Message><AWSAccessKeyId></AWSAccessKeyId><RequestId>44B9598E216C5B33</RequestId><HostId>Ff+ftxeWCd6t9qvYgW+FEIJDAtgitprPWksrK7/iq0A13tw3OI25hG6eS+L/loLdFdOA3eYCQsY=</HostId></Error>
",,Ranjana,sbadam,vagrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02ynf:,,,,,,,,,,,,,,,,,,,,,2016-09-27 03:24:32.614,,,27/Sep/16 3:24 AM;vagrawal;[~Ranjana] - can you add more details here. Also looks like a new feature request. Can you pls remove the jira_escalated tag,"11/Oct/16 6:45 PM;Ranjana;[~vagrawal] [~mahuja]  The requirement is as follows:

Notebook ID is: 13432

Switch to user qubole_modeling@marketshare.com

We need to be able to read a .csv file from an s3 location using the Pandas library. But when we try that, the S3 access keys are not getting picked up. Tried several methods, nothing worked. ",11/Jan/17 4:19 PM;sbadam;Can we have any update or ETA for this issue? Thanks.,"20/Mar/17 11:27 AM;Ranjana;The customer who raised the ticket for this has left his organization. So please do accordingly for this jira. If this is something that should be fixed, please do accordingly. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler dependency not checked for first run,SCHED-120,47722,Bug,Open,SCHED,Scheduler,software,sumitm,,,Major,,ksr,megha,addon_zendesk_for_jira,13/Sep/16 11:29 AM,19/Jun/17 2:16 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"* When I create a scheduler and add a dependency for ex for s3 file..
The first run runs irrespective of dependency being present...

Ex: sched id 11639 for account megha , user: megha@qubole.com",,addon_zendesk_for_jira,gayathrym,megha,rahulg,sam,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gannett,,,,,{},NA,Choose from,,SCHED-149,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02ycj:,,,,,,,,,,,,,,,,,,,1.0,9414,2016-09-28 01:45:48.233,,,28/Sep/16 1:45 AM;gayathrym;Thanks [~rahulg] for updating the ETA label. Appreciate it!!,"08/Dec/16 4:45 PM;megha;[~rahulg]Do we have a roadmap for this yet? 

","09/Dec/16 1:14 AM;rahulg;[~megha]..I will not be able to pick this now!
cc: [~sureshr]
",09/Dec/16 11:25 AM;sureshr;[~megha] What is the priority/urgency of this issue? We have 1 developer on-call to work on production issues and customer bugs.,28/Dec/16 2:22 PM;sam;I reduced the priority based on ZD ticket comment that a workaround has been applied successfully ,"28/Dec/16 2:23 PM;sam;Also, the person that opened the ZD left Gannet... and will be joining Qubole in 4 days ) so he can fix the issue for them )))","02/May/17 5:22 PM;megha;[~sureshr]
Do we have a roadmap for this?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When using iam roles, graphframes package is not found (spark 2.0)",SPAR-1250,47660,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,megha,megha,12/Sep/16 3:44 PM,20/Jul/17 8:58 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"When I tried on my account with IAM keys, import graphframes works..
command id: 33611267
For account with IAM roles, import graphframes says: can't find module graphframes
command id: 35435435

I'm using spark 2.0.0 for both clusters, and command line options:
--packages graphframes:graphframes:0.2.0-spark2.0-s_2.11

",,bharatb,mahuja,megha,swatis,yogeshg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02y4r:,,,,,,,,,,,,,,,,,,,1.0,8995,2016-09-13 18:40:48.481,,,13/Sep/16 8:20 AM;megha;Any suggestions or ideas? ,"13/Sep/16 6:40 PM;mahuja;This is interesting, was not expecting IAM roles to impact dependencies with maven coordinates. Need to check if there are addition repositories added that are S3 specific. cc - [~yogeshg], [~bharatb]","17/Oct/16 3:36 PM;megha;[~yogeshg][~bharatb]
any updates on this? 
",17/Oct/16 6:34 PM;bharatb;@swatis can you coordinate with together on this?,18/Oct/16 5:30 AM;yogeshg;Not sure how s3 access is involved while importing/downloading python dependencies. cc: [~swatis] ,"19/Oct/16 1:12 AM;swatis;[~megha] can you tell what is the email for the user using which you ran the commands 33611267 and 35435435. Looks like you have a lot of users in api.qubole.com.

I can check the rstore DB, but that consumes some time. We will appreciate if you can dump as much information as possible when creating a jira.

user_id, account_id, user_email, cluster_id, spark version, command id link/notebook link for both the customer's run and for your run.","20/Oct/16 3:00 AM;swatis;[~megha] I just figured that this link works https://api.qubole.com/super_admin/query_hists/33611267. All this while I user to first figure out the user email and then masquerade. Therefore, I asked for the email :) I will check now and get back.","20/Oct/16 5:53 AM;swatis;[~megha] I tried on a 2.0 cluster in a role and key based account. 

||Account type||Command||Error||
|Key|https://qa2.qubole.net/v2/analyze?command_id=627267|(/)|
|Role|https://devenv3.qubole.net/v2/analyze?command_id=359348|{code}from graphframes import *
App > zipimport.ZipImportError: can't find module 'graphframes'{code}|
|Role|https://qa3.qubole.net/v2/analyze?command_id=269189|{code}App > : java.lang.NoSuchMethodError: com.amazonaws.AmazonWebServiceRequest.copyPrivateRequestParameters()Ljava/util/Map;
App > at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.invoke(AWSSecurityTokenServiceClient.java:953)
App > at com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient.assumeRole(AWSSecurityTokenServiceClient.java:864)
App > at org.jets3t.service.security.AWSSessionCredentials.getAssumeRoleResult(AWSSessionCredentials.java:101)
App > at org.jets3t.service.security.AWSSessionCredentials.assumeRoleAndGetCredentials(AWSSessionCredentials.java:76)
App > at org.jets3t.service.security.AWSSessionCredentials.(AWSSessionCredentials.java:54){code}|

In second case, we are not able to download the package itself (fails at import itself). Where as in the third one the import at least succeeds.
",20/Oct/16 6:42 AM;yogeshg;[~swatis] does these packages come from paid-qubole bucket ? if yes then customer might not have given permission to that bucket,"20/Oct/16 7:12 AM;swatis;Code Snippet

{code}
from graphframes import *
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
conf = SparkConf()
sc = SparkContext(conf=conf)
sqlContext = SQLContext(sc)
vertices = sqlContext.createDataFrame([
  (""a"", ""Alice"", 34),
  (""b"", ""Bob"", 36),
  (""c"", ""Charlie"", 30),
  (""d"", ""David"", 29),
  (""e"", ""Esther"", 32),
  (""f"", ""Fanny"", 36),
  (""g"", ""Gabby"", 60)], [""id"", ""name"", ""age""])

edges = sqlContext.createDataFrame([
  (""a"", ""b"", ""friend""),
  (""b"", ""c"", ""follow""),
  (""c"", ""b"", ""follow""),
  (""f"", ""c"", ""follow""),
  (""e"", ""f"", ""follow""),
  (""e"", ""d"", ""friend""),
  (""d"", ""a"", ""friend""),
  (""a"", ""e"", ""friend"")
], [""src"", ""dst"", ""relationship""])

g = GraphFrame(vertices, edges)
print g
g.inDegrees.select(""inDegree"").show()
{code}","21/Oct/16 2:13 AM;swatis;Failing on older version as well, tried 1.6.1. On devenv3 I tried bringing up 2.0 clusters in roles and keys account and it fails in both.

On qa2 keys account, this completely works and qa3 roles account at least the import works.

[~megha] how critical is this, debugging this will take some time.","25/Oct/16 4:07 PM;megha;[~swatis]
I checked for paid-qubole bucket permissions already.. They do have it.. 
Also about the criticality, they're currently working around it by using graphX in Scala.. So it can wait a bit...","30/Nov/16 12:11 AM;swatis;Looks like this has got something to do with python version. Fails on clusters with python 2.6.9 for both role and key based clusters and passes on clusters with python 2.7.12 (However, this I verified on only a key based cluster on qa2 - cluster tag spark200)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sqoop fails to create Hive table ,SQOOP-45,47514,Bug,Reopened,SQOOP,SQOOP,software,sumitm,,,Major,,ksr,venkatak,venkatak,07/Sep/16 7:06 AM,10/Jun/17 1:15 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Steps to reproduce the issue in our environment;

Run this command on a hadoop2 cluster:

/usr/lib/qubole/packages/sqoop_h2/sqoop-h2/bin/sqoop import --connect jdbc:mysql://msquarelive.com:3306/msquare_qbltest --table City  --username msquare_qbltest --password msquare_qbltest --hive-import --create-hive-table --hive-overwrite --hive-drop-import-delims --hive-table default.City

Error seen:

{Code}
App >
App > 16/09/07 14:00:41 INFO hive.HiveImport: Exception in thread ""main"" java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: java.lang.RuntimeException: Could not load shims in class org.apache.hadoop.hive.shims.Hadoop23Shims
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:355)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:796)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:719)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.lang.reflect.Method.invoke(Method.java:606)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
App > 16/09/07 14:00:41 INFO hive.HiveImport: Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: java.lang.RuntimeException: Could not load shims in class org.apache.hadoop.hive.shims.Hadoop23Shims
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(HiveUtils.java:368)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:347)
App > 16/09/07 14:00:41 INFO hive.HiveImport: ... 8 more
App > 16/09/07 14:00:41 INFO hive.HiveImport: Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Could not load shims in class org.apache.hadoop.hive.shims.Hadoop23Shims
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(HadoopDefaultAuthenticator.java:53)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(HiveUtils.java:365)
App > 16/09/07 14:00:41 INFO hive.HiveImport: ... 9 more
App > 16/09/07 14:00:41 INFO hive.HiveImport: Caused by: java.lang.RuntimeException: Could not load shims in class org.apache.hadoop.hive.shims.Hadoop23Shims
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:123)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:115)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:80)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(HadoopDefaultAuthenticator.java:51)
App > 16/09/07 14:00:41 INFO hive.HiveImport: ... 12 more
App > 16/09/07 14:00:41 INFO hive.HiveImport: Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.shims.Hadoop23Shims
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.security.AccessController.doPrivileged(Native Method)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.lang.Class.forName0(Native Method)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at java.lang.Class.forName(Class.java:190)
App > 16/09/07 14:00:41 INFO hive.HiveImport: at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:120)
App > 16/09/07 14:00:41 INFO hive.HiveImport: ... 15 more
App > 16/09/07 14:00:41 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Hive exited with status 1
App > at org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:449)
App > at org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:233)
App > at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:520)
App > at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:617)
App > at org.apache.sqoop.Sqoop.run(Sqoop.java:173)
App > at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
App > at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:209)
App > at org.apache.sqoop.Sqoop.runTool(Sqoop.java:248)
App > at org.apache.sqoop.Sqoop.runTool(Sqoop.java:257)
App > at org.apache.sqoop.Sqoop.main(Sqoop.java:266)
App >
Qubole > Shell Command failed with exit code: 1

{Code}

[~beria] If you are not the right person to look at this issue, please reassign accordingly. 

cc [~sureshr] This is a blocker to Cimpress - not being able to sqoop their data.",,abhisheks,ashishs,jellin,ksr,mpatel,raghunandan,sam,sureshr,vagrawal,venkatak,,,,,,,,,,,,,,,,,,,,DOC-520,,,,,,,,,,,,,,,,,,,,,,,cimpress,,,,,"{repository={count=6, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":6,""lastUpdated"":""2016-09-14T21:45:27.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":6,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,abhisheks,#Empty,,1|z02xer:,,,,,,,,,,,,,,,,,,,1.0,9262,2016-09-07 07:26:32.378,,,07/Sep/16 7:26 AM;beria;I am not sure who is the right person for sqoop. I think it is @ksr . [~venkatak] seems you confused in Beria and Kedia? :) ,"07/Sep/16 9:43 PM;sureshr;Yes, [~ksr] is the correct owner for sqoop issues.

[~ksr]: Would appreciate you looking into this blocker issue today. Please connect directly with [~venkatak] if you have any questions. Thanks! ","08/Sep/16 5:40 AM;ksr;[~sureshr] [~venkatak] Yes, I was working on this today. I still need some more time to figure this out.","08/Sep/16 10:41 AM;sureshr;Thanks, Sameer. Please continue to keep this jira ticket updated with your
progress/latest status.
","08/Sep/16 10:11 PM;ksr;I think the issue here is that when we try to run a db import via shell command, it is not able to save the generated hive script on the web node to import data into hive.","12/Sep/16 5:42 AM;sam;Can you guys please give an ETA and/or progress on this issue. Met with Cimpress this morning, and this is a blocker for them. Thanks. ","12/Sep/16 9:11 AM;sureshr;[~ashishs] and [~vagrawal] Could one of you also help [~ksr] with the investigation/debugging?

Are there any workarounds available? Would it be possible to create the Hive table manually before the import?

cc: [~abhisheks] and [~karthikk]: Is this scenario something we verify in our hive or sqoop regression tests? Do we have sqoop regression tests?","12/Sep/16 10:12 AM;abhisheks;[~sureshr] we do have sqoop regression tests (run on pixie-hadoop1 cluster and hadoop2 customer cluster), but we dont have tests specifically for import via shell script, we will have to add these (created test jira to track: TES-1502)
workaround can be to use data import instead of import via shell, if this is ok with the customer (eg normal data import with non existing hive table: https://api.qubole.com/v2/analyze?command_id=35416359)

creating a hive table and running shell command for import gives FileAlreadyExists exception:
https://api.qubole.com/v2/analyze?command_id=35417543
{code}
App > 16/09/12 17:04:42 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://ec2-54-80-107-244.compute-1.amazonaws.com:9000/user/abhisheks/import_test_table already exists

App > at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)

App > at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:579)

App > at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:440)

App > at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1385)

App > at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1382)

App > at java.security.AccessController.doPrivileged(Native Method)

App > at javax.security.auth.Subject.doAs(Subject.java:415)

App > at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635)

App > at org.apache.hadoop.mapreduce.Job.submit(Job.java:1382)

App > at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1403)

App > at org.apache.sqoop.mapreduce.JobBase.runJob(JobBase.java:472)

App > at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:169)

App > at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:273)

App > at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:747)

App > at org.apache.sqoop.manager.MySQLManager.importTable(MySQLManager.java:124)

App > at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:498)

App > at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:617)

App > at org.apache.sqoop.Sqoop.run(Sqoop.java:173)
{code}
cc [~ksr] [~ashishs]

","12/Sep/16 1:50 PM;sam;Importing via data import does not work for them, because they are importing this data from mysql dumps that are placed on S3 by various groups in the org. In other words, they can't creating static data source, because it is dynamic on their end, and a new group may put a dump for the import. Creating a table upfront is not an option for the same reason, as they do not know schema upfront.

If this is a matter of 1-2 weeks for fixing, that may be sufficient","12/Sep/16 9:55 PM;sureshr;[~sam] Yes, we will investigate and fix this issue, but will using DB import or pre-creating the hive table unblock the customer PoC?

[~ashishs] Since Sameer is OOO today, could you please take a look at this?",12/Sep/16 10:09 PM;ashishs;looking and will update.,"12/Sep/16 10:56 PM;ashishs;[~sureshr] I looked into it.. though I am not sure why it is failing.. [~ksr] would have better knowledge of it but I have a workaround. rather this is what we should recommend from now also.

running import/export via shell command was a actually a workaround because sqoop on customer cluster was not working. [~ksr] has done the necessary changes and now customers can run sqoop on their cluster without going via shell command. 
I tried running the same job on my hadoop2 cluster via sqoop import and it succeeded. 
https://api.qubole.com/v2/analyze?command_id=35477566

right now that flag to run sqoop on customer cluster is turned off for others. its turned only for qubole.com domain right now. 
To start with cimpress only, if we enable the flag for them(I am not sure if we have account level setting for this or only one global setting), they should be able to import their data. 
[~raghunandan] can tell if we have one global setting or we have account level setting also.","12/Sep/16 11:28 PM;ashishs;just went through Samuel's comment and realized they can not have a static data source. 
but going through above should unblock their PoC.","13/Sep/16 12:41 AM;sureshr;Thanks, Ashish.
","13/Sep/16 7:41 AM;sureshr;[~raghunandan] Can you please respond to the question about this setting being global or account level? Also, [~ashishs] can you share the exact setting name, please?",13/Sep/16 7:58 AM;raghunandan;`tapp.enable_customer_cluster_dd` is the flag. This flag can be enabled at account level if needed.,13/Sep/16 9:31 AM;sam;Thanks. I will advice the client and follow up to see if that unblocked them,"13/Sep/16 9:52 PM;sureshr;Latest update from [~vagrawal]
{quote}
Thanks Sam. I think I got the requirement now. So to summarise, they want to have following workflow:
  -  Download mysql dump from S3 and setup rds 
  -  make a Db-query to get the list of all tables
  -  Iterate though the list to run sqoop job to import data

We don't support this use-case out of the box. But it can be done via small python script
   1) use qds-sdk  to run shell command for setting up RDS
   2) use qds-sdk to create dbtap for the new RDS
   3) use qds-sdk to run dbtapquery to get the list of tables
   4) write a for-loop to iterate though the list and run the dbtap command (this can happen in parallel as well)

This script can be run from anywhere.
{quote}","13/Sep/16 9:54 PM;sureshr;From [~sam]
bq. Thanks Vikram. They do something similar but use command line submit jobs instead in step 4. I will advice on your recommendation. 
",15/Sep/16 11:14 AM;sureshr;[~sam] Please let us know the latest status on this issue?,"15/Sep/16 12:05 PM;sam;Suresh, I met the client yesterday and gave the recommendation for scripting it.. As per my email, they asked if someone would be able to advice them on the call regarding the details of such script. Would Vikram or someone else be able to join a call with them?","18/Sep/16 9:59 PM;sureshr;Email update from [~sam] on Monday, 19 Sep 1:10 AM IST
bq. Vikram will be on a call with Matt next week,  will update after that","20/Sep/16 10:48 PM;vagrawal;[~sam] and I spoke to Matt. He was okay with the proposed approach. He will get back to us in case he has other issues.

Removing the blocker tag.","20/Sep/16 10:53 PM;vagrawal;I dont think there is anything else to be done here. Closing this ticket. Raised a Doc jira for this issue

[~sureshr] @ [~sam]- shall we write some sample code to handle this use-case and post it in our support forum","17/Oct/16 2:47 PM;sam;That would be great. In fact we suggested to Cimpress that we will help them do this. [~jellin]is going to work on that, so if you are able to support him with an example, we can pull resources to come up with the solution that would work for both Cimpress and other clients, as well. ",20/Oct/16 2:54 PM;jellin;running into an issue implementing this workaround as it seems we provide no way to expose --map-column-hive <mapping>   in dbimport;  They have some blob datatypes that sqoop can't handle without some prodding.,"21/Oct/16 9:05 AM;sam;[~vagrawal], Jeff is working on the script for Cimpress, which we can use to post on the forums, etc.. but he is running into an issue with blobs, so if you could please help him with that. Once we are able to support blobs, that will satisfy Cimpress, and be a good example for other clients that run into the same issue. Thanks ","30/Dec/16 9:11 AM;jellin;This really shouldn’t be closed as its still an issue that we are currently working around.

Reopening and downgrading since there is a workaround.","30/Dec/16 10:58 AM;sam;I believe the current hurdle is in supporting different data types and ability specify mappings through Import/Export job? Jeff, can you confirm.","01/Jan/17 9:15 PM;sureshr;[~jellin], [~sam]: Since we already have SQOOP-85 tracking the issue regarding data types and mapping of the columns, why does this bug need to remain open? Vikram has pointed out earlier (https://qubole.atlassian.net/browse/SQOOP-45?focusedCommentId=82664&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-82664), that we don't directly support the customer requirement and that it can be achieved via a fairly simple script.

If we want to support this particular usecase, I would suggest creating a new enhancement request.

cc: [~sumitm]",02/Jan/17 1:04 PM;jellin;We have many use cases with sqoop that dB import can not address. The work around is often to use standalone sqoop.  However reliability of getting tbis to work had been problematic.  If this function exists in open source we should support it. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JT links throw HTTP 500 error in certain scenarios.,HAD-626,47387,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,hiyer,Kulbir,Kulbir,06/Sep/16 7:01 PM,11/Jul/17 2:59 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Hadoop Team,

Command ID: https://api.qubole.com/v2/analyze?command_id=34756907
JT URL: https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fec2-54-144-245-190.compute-1.amazonaws.com%3A50030%2Fjobdetails.jsp%3Fjobid%3Djob_4113.201609040308_1932&clusterInst=249651

Customer reported an issue whereby JT links were not working and throwing 500 error:
{code}
java.lang.NullPointerException
	at org.apache.hadoop.mapred.qbol.QbolJTProxy.getJobLog(QbolJTProxy.java:1074)
	at org.apache.hadoop.mapred.qbol.QbolJTProxy.getJobLog(QbolJTProxy.java:1053)
	at org.apache.hadoop.mapred.qbol.QbolJTProxy$QbolJTProxyServlet.getHistoryUrl(QbolJTProxy.java:461)
	at org.apache.hadoop.mapred.qbol.QbolJTProxy$QbolJTProxyServlet.doPost(QbolJTProxy.java:586)
	at org.apache.hadoop.mapred.qbol.QbolJTProxy$QbolJTProxyServlet.doGet(QbolJTProxy.java:496)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:728)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}

Review of failing code indicates that the required history files are not present on S3, which was actually verified:
{code}
[ec2-user@ip-10-81-205-9 logs]$ hadoop dfs -ls s3://brp-logs/qubole/logs/account_id/4113/logs/hadoop/201609040308/history
Found 14 items
-rwxrwxrwx  1      18567 2016-09-04 03:32 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0002_br-qubole-ops%40bloomreach.com_%5BE13DBD204F79494B844994334966A506%2FA8107A60CB724F0A.lzo
-rwxrwxrwx  1      85392 2016-09-04 03:32 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0002_conf.xml
-rwxrwxrwx  1      14165 2016-09-04 03:32 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0003_br-qubole-ops%40bloomreach.com_%5B1C3829B2017D423FB30D16D4AB2AC2CF%2F8843137F568A4423.lzo
-rwxrwxrwx  1      85402 2016-09-04 03:32 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0003_conf.xml
-rwxrwxrwx  1      10581 2016-09-04 03:55 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0006_br-qubole-ops%40bloomreach.com_%5B62CBD761C7154E18BEB99E40988B8DE6%2F05AB60D907C64C46.lzo
-rwxrwxrwx  1      85382 2016-09-04 03:55 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0006_conf.xml
-rwxrwxrwx  1      16533 2016-09-04 04:01 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0009_br-qubole-ops%40bloomreach.com_%5B9D856CCF96F542E39C961EC30D1124A8%2F39A5C8EFFCBC476B.lzo
-rwxrwxrwx  1      85426 2016-09-04 04:01 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0009_conf.xml
-rwxrwxrwx  1      12427 2016-09-04 04:00 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0011_br-qubole-ops%40bloomreach.com_%5B4F845EDF3EA14542BD739156C94AD80E%2F61D27FEC69B8408F.lzo
-rwxrwxrwx  1      85328 2016-09-04 04:00 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0011_conf.xml
-rwxrwxrwx  1      13590 2016-09-04 04:00 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0012_br-qubole-ops%40bloomreach.com_%5BA87AAFF59F2A4D34B36E0433075653AB%2FB053B2C106CF4405.lzo
-rwxrwxrwx  1      85344 2016-09-04 04:00 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0012_conf.xml
-rwxrwxrwx  1      8739 2016-09-04 04:01 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0019_br-qubole-ops%40bloomreach.com_%5B851EFE07A1F84C6C95E22DC3E59D05E6%2FCC0CF54A75D849C0.lzo
-rwxrwxrwx  1      85328 2016-09-04 04:01 /qubole/logs/account_id/4113/logs/hadoop/201609040308/history/ec2-54-144-245-190.compute-1.amazonaws.com_0_job_4113.201609040308_0019_conf.xml
{code}

In this case job history files seem to be missing because it's 100% spot cluster (min of 1) and while job was running huge number of nodes got lost causing job to fail.

Regardless of the reason we need to handle NPE better and display a proper message in UI, for e.g indicating we couldn't locate the required job history files on S3. Most likely issue also exists in Hadoop2 as well.

Please address.",,Kulbir,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bloomreach,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02x8r:,,,,,,,,,,,,,,,,,,,1.0,9309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark unable to read from S3. ,ZEP-514,47377,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,beria,mohan,mohan,06/Sep/16 7:58 AM,22/May/17 7:11 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Please see the attached screenshots. 
The same files are read when I use a Spark dataframe. 
[~svstaden] Mentioned this was an issue with a customer-Mix",,adubey,beria,drose@qubole.com,karuppayyar,mahuja,mohan,navdeepp,sbadam,svstaden,xing,,,,,,,,,,,,,,,,,,,,,,,,22/May/17 7:10 PM;sbadam;Not_working_in_Spark_2.1_working_in_Spark_2.0.png;https://qubole.atlassian.net/secure/attachment/42672/Not_working_in_Spark_2.1_working_in_Spark_2.0.png,06/Sep/16 7:55 AM;mohan;Screen Shot 2016-09-06 at 7.03.59 AM.png;https://qubole.atlassian.net/secure/attachment/32521/Screen+Shot+2016-09-06+at+7.03.59+AM.png,06/Sep/16 7:55 AM;mohan;Screen Shot 2016-09-06 at 7.07.40 AM.png;https://qubole.atlassian.net/secure/attachment/32522/Screen+Shot+2016-09-06+at+7.07.40+AM.png,06/Sep/16 7:55 AM;mohan;Screen Shot 2016-09-06 at 7.18.26 AM.png;https://qubole.atlassian.net/secure/attachment/32520/Screen+Shot+2016-09-06+at+7.18.26+AM.png,,,,,,,,,,,,,,,,mistsys,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02x6j:,,,,,,,,,,,,,,,,,,,2.0,"9285,9287",2016-09-06 08:06:36.804,,,"06/Sep/16 8:06 AM;svstaden;Customer is: Mist

They are using IAM Keys

Was able to reproduce this yesterday in my own account (also using IAM Keys).","06/Sep/16 9:35 AM;karuppayyar;http://stackoverflow.com/questions/35803601/reading-a-file-from-a-private-s3-bucket-to-a-pandas-dataframe
Panda is an external lib and we need to make it aware of s3 creds .
After configuring with boto with keys , i was able to read the table. I have not tried it with roles.
cc: [~bharatb] in case I am missing something here.","06/Sep/16 9:39 AM;karuppayyar;check this notebook https://api.qubole.com/v2/notebooks#open?id=17053 in [~mohan]'s acc.
Last 4 paragraphs ","06/Sep/16 9:55 AM;svstaden;What about non-Pandas calls?

I was hitting this with:

text_file = sc.textFile(""s3://qubole-svstaden/test/input/moby_dick.txt"")
words = text_file.flatMap(lambda line: line.split("" "")) \
             .map(lambda word: (removePunctuation(word),1)) \
             .reduceByKey(lambda a, b: a + b)

Customer was hitting this with:

for item in items[:1]:
    filePath = ""s3a://{}/{}"".format(""mist-staging-kafka"",item[""Key""])
    df = file_to_df(sc, spark, filePath)
    df_save_to_table(df, ""published_ts"", topic.replace(""-"", ""_""), hive_store)
    print(""Wrote to Hive:"", item[""Key""], type(item[""Key""]))
    marker_store.write(last_saved_marker, item[""Key""])","06/Sep/16 11:18 AM;karuppayyar;Non-pandas things should work fine. No explicit configuartion is required
The keys are read form the hadoop conf files and used for s3 operation in spark.
cc: [~mahuja]

Edit: [~svstaden] configuration file is at /usr/lib/hadoop2/etc/hadoop/core-site.xml",06/Sep/16 9:48 PM;mahuja;[~svstaden] - can you please give a command id with S3 errors accessed through Spark context. Examples like sc.textFile() should work transparently with S3.,"09/Sep/16 8:49 AM;svstaden;I can't, because it was all done via Notebook. Check out this notebook in my QDS account (5285): https://api.qubole.com/v2/notebooks#open?id=13844

The first block contains the error. Be advised, that this seems to occur intermittently and is based on what happens during the cluster startup.","13/Jan/17 4:26 PM;sbadam;Hey Anirudh, can we have any update or ETA for this issue? Thanks.","31/Jan/17 3:17 AM;beria;[~sbadam] Is this still a issue? The notebook mentioned above is no-more. I checked the 4 other notebooks present in that account (ids: 25758, 25759, 25760, 25761), but couldn't didn't find any reference of S3. As [~karuppayyar] mentioned, panda is external library and needs to be made aware of S3 creds. As for the intermittent issue, not sure why it might have happened. Zeppelin itself does not use/store any keys, it passes all commands to spark, which in turn must be using keys provided by hadoop. I dont see any activity on the related ticket as well. Can you please point how to try and reproduce this issue?

cc [~karuppayyar] [~mahuja] in case I am missing some context here.",01/Feb/17 10:52 AM;sbadam;Requesting [~venkatak] to provide any response.,"22/May/17 2:37 AM;navdeepp;In my account df.write.saveAsTable(""spark_saveAsTable"", mode=""overwrite"") is failing with S3 errors

Notebook:28069 Following code gives error, is there something I am missing.
{code}
%pyspark
from pyspark.sql import HiveContext
print(sc.version)
sqlsc = HiveContext(sc)
df = sqlsc.sql(""select * from default_qubole_airline_origin_destination"")
df.show(2)

df.write.saveAsTable(""spark_saveAsTable"", mode=""overwrite"")
{code}

{code}
Py4JJavaError: An error occurred while calling o221.saveAsTable.
: org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 Error Message. DELETE '/production_ec2-user_hu_6632%2Fwarehouse%2Fspark_saveastable%2F_temporary%2F0%2F_temporary%2Fattempt_20170522073831_0013_m_000000_0%2Fpart-00000-72e469cf-877a-4775-ba92-e08ab41cc717.snappy.parquet' on Host 'prod.qubole.com.s3.amazonaws.com' @ 'Mon, 22 May 2017 09:32:12 GMT' -- ResponseCode: 403, ResponseStatus: Forbidden, XML Error Message: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>5784BE1C46D8E676</RequestId><HostId>QuOZZQRjak9iCCfHLCJUHAgFQ8OiQ9DRvhmiK2iFw9UmJMcPA/TH851o9jUh3Xyd2h/mUFQ1Vuw=</HostId></Error>
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:328)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:250)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at org.apache.hadoop.fs.s3native.$Proxy52.delete(Unknown Source)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(NativeS3FileSystem.java:866)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(NativeS3FileSystem.java:860)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(NativeS3FileSystem.java:860)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(NativeS3FileSystem.java:860)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(NativeS3FileSystem.java:860)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.delete(NativeS3FileSystem.java:860)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:139)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:82)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:227)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:404)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:358)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.jets3t.service.S3ServiceException: S3 Error Message. DELETE '/production_ec2-user_hu_6632%2Fwarehouse%2Fspark_saveastable%2F_temporary%2F0%2F_temporary%2Fattempt_20170522073831_0013_m_000000_0%2Fpart-00000-72e469cf-877a-4775-ba92-e08ab41cc717.snappy.parquet' on Host 'prod.qubole.com.s3.amazonaws.com' @ 'Mon, 22 May 2017 09:32:12 GMT' -- ResponseCode: 403, ResponseStatus: Forbidden, XML Error Message: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>5784BE1C46D8E676</RequestId><HostId>QuOZZQRjak9iCCfHLCJUHAgFQ8OiQ9DRvhmiK2iFw9UmJMcPA/TH851o9jUh3Xyd2h/mUFQ1Vuw=</HostId></Error>
	at org.jets3t.service.S3Service.deleteObject(S3Service.java:2279)
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.delete(Jets3tNativeFileSystemStore.java:323)
	... 52 more
(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o221.saveAsTable.\n', JavaObject id=o222), <traceback object at 0x7f68051814d0>)
{code}",22/May/17 8:11 AM;sbadam;[~navdeepp]- can you please mention Spark version that you tried this on? I have seen similar stack traces(Access Denied) for different scenario in Spark 2.1 while it was working in 2.0. I have attached my scenario to this ticket. Looks like we have some permission issues to Internal Qubole users. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pininterest | reduce got hang on datacore-13 for 3 hours,HAD-625,47372,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,hiyer,biswajit,biswajit,06/Sep/16 5:54 AM,11/Jul/17 2:58 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"https://api.qubole.com/v2/analyze?command_id=34756050

https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fec2-54-237-28-255.compute-1.amazonaws.com%3A50030%2Ftaskdetailshistory.jsp%3Fjobid%3Djob_3508.201607010506_135775%26logFile%3Ds3n%3A%2F%2Fqubole-pinterest%2Fdata-core13%2Flogs%2Fhadoop%2F201607010506%2Fhistory%2Fec2-54-237-28-255.compute-1.amazonaws.com_0_job_3508.201607010506_135775_qubole-prod%252540pinterest.com_SanitizerTopicJob.stingray_actions_taken.dt%25253D2016-0.lzo%26taskid%3Dtask_3508.201607010506_135775_r_001488&clusterInst=202260


reducer was stuck for 3 hrs. not much log to troubleshoot. ",,biswajit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,pinterest,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02x5f:,,,,,,,,,,,,,,,,,,,1.0,9291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues with sessions in qds-sdk ,MW-127,47156,Bug,Reopened,MW,Middleware,software,sumitm,,,Critical,,abhijitj,megha,addon_zendesk_for_jira,31/Aug/16 3:43 PM,16/Jun/17 4:18 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Attaching the code herewith..
User is basically polling continuously until they get a result (success/failure) of the query.. They have added increasing sleep time in between in order to avoid this issue, however are not succesful (timeout start from 1 second to 20 seconds)..

",,abhijitj,addon_zendesk_for_jira,drose@qubole.com,gayathrym,megha,sureshr,tanishg,vagrawal,wvaldez,,,,,,,,,,,,,,,,,,,,,,,,,31/Aug/16 3:58 PM;megha;8992.py;https://qubole.atlassian.net/secure/attachment/32311/8992.py,31/Oct/16 11:08 PM;tanishg;check_session.py;https://qubole.atlassian.net/secure/attachment/34208/check_session.py,,,,,,,,,,,,,,,,,,oracle,,,,,{},NA,Choose from,,QBOL-5183,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02wdn:,,,,,,MW Sprint 14(14 Sep- 27 Sep),MW Sprint 15(28Sep-11Oct),MW Sprint 16(12Oct-25Oct),MW Sprint 17(26Oct-8Nov),MW Sprint 18(9Nov-22Nov),,,,,,,,,1.0,8992,2016-09-07 09:00:17.007,,,07/Sep/16 9:00 AM;drose@qubole.com;[~vagrawal]are we developing on the Python SDK?  Oracle considers this to be a blocker on code development.,07/Sep/16 10:11 AM;vagrawal;This does not look like a python sdk issue but soemthing to do with our backend. We don't have any retry limit as well. ,13/Sep/16 9:34 AM;megha;Any further updates [~vagrawal] ?,13/Sep/16 9:47 AM;vagrawal;[~sumitm] - can you priorities this. This is urgent. ,"22/Sep/16 11:09 AM;megha;[~sumitm][~vagrawal]
Any updates? 

-Megha",22/Sep/16 10:10 PM;vagrawal;Sorry [~megha]. It fell off the radar.  is it okay if i get back to you by early next week.,23/Sep/16 4:53 PM;megha;Sure Thanks [~vagrawal],27/Sep/16 10:58 PM;gayathrym;Hi [~vagrawal] [~sumitm] :Can we please put a target ETA - rbxx-candidate label if this involves a code fix? ,"28/Sep/16 12:57 AM;vagrawal;We are still figuring out the issue. So dont know if there would be a code fix.

[~megha] - I replied directly in the zendesk tickets.  This can be client side issue as well. I asked for more information about the error. Any new details will help us to figure out the problem.

","29/Sep/16 10:04 PM;vagrawal;I spoke directly to Jeffrey via slack. in first glance we couldnt find anything wrong at Qubole's end. 
{code:java}

vagrawal [10:55 PM]  
yeah

[10:56]  
These are from ELB logs where all requests to api.qubole.com is recorded. (edited)

[10:57]  
so its quite possible that your script has errored out without even making the API request

jkamei [11:09 PM]  
Yeah agreed, it seems like the code is failing on the client side. Before it even hits api.qubole.com

jkamei [11:17 PM]  
Have you or Megha tried to run the code I supplied?

[11:17]  
I ask because I'm curious if it's some network setting on our server that is causing the error.


----- Today September 30th, 2016 -----
vagrawal [12:15 AM]  
We haven't but I am gonna do it.  May be next week I will give it a shot.
{code}

His script works completely fine for 10 hrs. Post that he is seeing the issue. We can try to run the script from our end as well to understand the problem.
",30/Sep/16 11:13 AM;drose@qubole.com;Thank you [~vagrawal] This ticket has been escalated from the sponsors at Oracle.  Can we try some of this testing on Monday?,"04/Oct/16 10:15 AM;vagrawal;[~drose@qubole.com] - I spent decent amount of time last week and there doesnt seem to be any issue at our end. It should be something in the script. One way to debug the issue is that we run the script locally and try to reproduce it.
",05/Oct/16 8:41 AM;sureshr;We discussed this in the solutions sync-up call and [~adubey] said he would sync-up with [~megha] on this. Looks like we need to try and reproduce this in-house...,"06/Oct/16 1:00 PM;megha;Thanks for your replies [~sureshr][~vagrawal]
The user has found something in qds sdk that could be causing this issue.. Following is his comment:


{panel:title=Customer Comment}
In qds_sdk/commands.py - Line 71, a Qubole.agent() is created. 
In qds_sdk/qubole.py - Line 63, this agent sets a class variable called cached_agent. This creates a connection in Line 87. 
In qds_sdk/connection.py - Line 43, a session is created for the agent. This is what is causing the problem. This session times out and since the cached_agent variable is a class variable, despite creating new Command objects, this class variable persists.

We set the reuse parameter to False in the Connection constructor in Line 34 of the Connection class and this fixed the problem. However, I'm sure this causes performance hits as instead of using a session, it makes separate requests.
{panel}


",06/Oct/16 11:56 PM;vagrawal;wow thats an awesome debugging. Thats explains the issue. Will soon come up with solution and start a PR. ,12/Oct/16 11:00 PM;vagrawal;I have asked [~tanishg] to help me out on this. ,"31/Oct/16 11:10 PM;tanishg;Findings :
I have attached my script in description which ran for more than 15 hours without any timeout and worked fine. So its not related to timeout as by default, session do not time out unless a timeout value is set explicitly. While setting connection in qds , by default pool size is 10 which means it can connect to 10 different hosts but here in this case its 1 so this might not be the issue too. Now for each of these hosts max number of connections are set as 10 which prevents creating additional connections outside of the limit. But the parameter block which is set as False can create more connections as needed , provided that  connection will not be saved for re-use. May be script might have failed due to increase memory or socket consumption.
Can we tell the user if they are facing this issue again after reassigning the reuse parameter as True ?
Also what was the exact error faced by user when the script was failed ?

For reference 
http://stackoverflow.com/questions/38410661/python3-urllib3-crawler-cant-limit-max-connections-to-aa-single-domain

",29/Nov/16 10:35 PM;tanishg;[~megha] Closing this issue as its not reproducible. If the user is still facing any issue then please send the exact error faced by them.,"23/Jan/17 11:36 AM;megha;[~tanishg] This is the reply from the customer:
It was not so much of a problem with one operation taking a long time but keeping it running. If you take a look at the script that I sent, it occurs when a query runs (opens the connection), then goes to sleep for a long period of time, then reruns the query again. If it does this enough, it will hit the timeout error.","17/May/17 9:13 AM;drose@qubole.com;[~tanishg] do you have the details you need to correct this issue, the customer has asked for an update.? [~wvaldez]
","17/May/17 1:13 PM;tanishg;[~abhijitj] [~sourabhg] Can anyone take a look on this.  I don't have bandwidth right now . And moreover this issue looks like to ponder over it in a thorough manner. I have the gist about it fixing the issue but seems like it will be a workaround and still skeptical about it whether it will work or not. 
Also If possible I will try working on the next sprint.
cc [~sumitm] [~sureshr]","14/Jun/17 2:35 PM;wvaldez;Hi [~tanishg] [~abhijitj] [~sourabhg],

Oracle is asking for an update / ETA on getting this fix.  Is there something I can share?  

Thank you,
Willie
","16/Jun/17 4:18 AM;abhijitj;Will take up the debugging in the next week.

cc: [~karthikk]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler v2: Scheduler Summary and Runs table does not show Time Zone,UI-4256,47138,Bug,Open,UI,UI,software,aswina,,,Major,,tabraiz,nimitk,nimitk,31/Aug/16 8:53 AM,14/Sep/16 1:57 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Login to qa3

Go to scheduler v2
Schedule a job.
Scheduler summary displays ""Next Materialize Time"" but it does not display Time zone selected by user while creating job.

Scheduler v1 had time zone along with time

Please find the attached snapshots for reference

old scheduler:
 !image-2016-08-31-21-21-47-419.png|thumbnail! 


New Scheduler:

 !image-2016-08-31-21-23-17-212.png|thumbnail! 

cc: [~kmayank]
",,aswina,karthikk,nimitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Aug/16 8:51 AM;nimitk;image-2016-08-31-21-21-47-419.png;https://qubole.atlassian.net/secure/attachment/32308/image-2016-08-31-21-21-47-419.png,31/Aug/16 8:53 AM;nimitk;image-2016-08-31-21-23-17-212.png;https://qubole.atlassian.net/secure/attachment/32307/image-2016-08-31-21-23-17-212.png,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,UI-4236,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02w9n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Scheduler v2: ""Nominal time"" and ""Next Materialize Time"" are not in sync",UI-4251,47107,Bug,Open,UI,UI,software,aswina,,,Major,,tabraiz,nimitk,nimitk,31/Aug/16 3:24 AM,04/Jan/17 8:09 PM,09/Aug/17 6:03 AM,,release-branch-37,,,0,jira_escalated,,,,,,,,"Login to qa3
Go to Scheduler v2
Schedule a job with Start date as past date (say 5 days in past)
save it
The instances (Runs table) show ""Nominal time"" and scheduler summary displays ""Next Materialize Time"", these both are not sync.

Example: My Next Materialize Time is 31 Aug 2016 00:58
So my next instance will run on 31 Aug 2016 00:58. 
But when it actually runs, the nominal time displayed is 31 Aug 2016 12:28

Please find the attached snapshot
 !image-2016-08-31-15-53-43-172.png|thumbnail! 

",,aswina,nimitk,,,,,,,,,,,,,,,,,,,,,,,,,,UI-4980,,,,,,31/Aug/16 3:23 AM;nimitk;image-2016-08-31-15-53-43-172.png;https://qubole.atlassian.net/secure/attachment/32302/image-2016-08-31-15-53-43-172.png,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,UI-4236,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02w2r:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UTC TZ is out of order in scheduler UI,UI-4234,47030,Bug,Open,UI,UI,software,aswina,,,Major,,tabraiz,sam,sam,30/Aug/16 10:02 AM,30/Aug/16 10:29 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Clickagy is using v2 scheduler and noted that the UTC TZ is missing from available options. Can it be added back in? Also, do we intend to replace the scheduler in the UI with V2?

https://qubole.zendesk.com/agent/tickets/9086
",,aswina,sam,sureshr,tabraiz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02vrn:,,,,,,,,,,,,,,,,,,,,,2016-08-30 10:06:02.356,,,30/Aug/16 10:06 AM;tabraiz;There is already UTC but  UTC TZ  is out of order . Will fix the order,"30/Aug/16 10:28 AM;sureshr;Thanks for the quick response, [~tabraiz]. Assigning to you and moving to UI project.",30/Aug/16 10:28 AM;sureshr;Please move into the appropriate sprint.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShellCmd failed bringing up cluster,ACM-577,47027,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,adubey,addon_zendesk_for_jira,30/Aug/16 8:26 AM,30/Aug/16 8:26 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,I remember when we run shell cmnd to bring up the cluster it used to do 3 retries. In this 34122576 case i don't see any retries. Anything changed? how can we minimize this failure chances..,,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02vqz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Narrow down java.util.concurrent.ExecutionException: javax.ws.rs.ProcessingException: java.io.EOFException ,SDK-138,46405,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,abhisheks,megha,megha,19/Aug/16 2:17 PM,14/Jun/17 11:43 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Attaching scala and java code..
Questions:
How can we avoid these types of errors..?
And is there a way to add  additional logging in the sdk to narrow down this issue..? 

As mentioned by Abhishek, the issue is in jetty http client: 
https://bugs.eclipse.org/bugs/show_bug.cgi?id=440729

Currently they have added retry logic, but they really want to get to the root of this issue..

",,abhijitj,abhisheks,amoghm,drose@qubole.com,gayathrym,megha,,,,,,,,,,,,,,,,,,,,,,,,SDK-139,,,,19/Aug/16 2:05 PM;megha;error_log.java;https://qubole.atlassian.net/secure/attachment/31626/error_log.java,19/Aug/16 2:05 PM;megha;scala_code.java;https://qubole.atlassian.net/secure/attachment/31625/scala_code.java,,,,,,,,,,,,,,,,,,fanatics,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02tn7:,,,,,,,,,,,,,,,,,,,1.0,8850,2016-08-24 00:50:17.286,,,"23/Aug/16 3:14 PM;megha;[~abhisheks] Any updates on this? 
","24/Aug/16 12:50 AM;abhisheks;Can we ask them to try with these settings:
1) Set less value of async threadpool size. (i.e. 2-3)
config.property(ClientProperties.ASYNC_THREADPOOL_SIZE, 3)

2) If above doesnt work we can ask them to use the jetty client version >9.3.0
We can ask them to use: https://oss.sonatype.org/content/repositories/jetty-snapshots/org/eclipse/jetty/jetty-client/9.3.0-SNAPSHOT/jetty-client-9.3.0-20150613.103950-140.jar

Also can we check if they are running in parallel, if yes then approx how many threads",27/Sep/16 9:51 PM;gayathrym;[~megha] : Any updates on [~abhisheks]'s question above? ,"07/Oct/16 6:01 AM;amoghm;[~megha] Hi Megha, Any updates on this ?",08/Dec/16 2:57 PM;megha;They havnt gotten back to us on this suggestion.. I'll update here if we hear back,14/Jun/17 11:42 PM;abhijitj;[~megha] reducing the priority for this as there has been no updates from the customer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read/write access to s3 fails intermediately for IMS ,SPAR-1212,46383,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,biswajit,biswajit,18/Aug/16 10:36 PM,06/Sep/16 10:22 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Read/write access to s3 fails intermediately for IMS in Notebook. The secret key has forward slash ""/"" but it works some times and sometimes it does not. When it does not works in one cluster, at the same time it does works in another cluster. 

So customer is not ready to accept forward slash may be the issue and hence not ready to re-generate a new key and try it. 

Notebook details :- https://api.qubole.com/v2/notebooks#open?id=18155


Code :-

{code}

set fs.s3n.awsAccessKeyId=xxxx
set fs.s3n.awsSecretAccessKey=xxx
val df = sqlContext.read.parquet(""s3n://imsone-data/IMS-ONE-FACTORY/DDD_FCT_Latest/*.parquet"")

{code}
",,biswajit,mahuja,mohan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,imshealth,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02tib:,,,,,,,,,,,,,,,,,,,,,2016-09-06 22:22:30.727,,,06/Sep/16 10:22 PM;mahuja;[~biswajit] - can we close this based on conversation on ticket.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
._ Does not import classes,ZEP-483,46380,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,Ranjana,Ranjana,18/Aug/16 4:37 PM,06/Sep/16 10:24 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"import org.apache.spark.sql.expressions._

class MyClass(val myVal: MutableAggregationBuffer) {
//class MyClass(val myVal: org.apache.spark.sql.expressions.MutableAggregationBuffer) {

}

import org.apache.spark.sql.expressions._
<console>:9: error: not found: type MutableAggregationBuffer
       class MyClass(val myVal: MutableAggregationBuffer) {
                                ^
ERROR   
",,drose@qubole.com,mahuja,mpatel,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,imshealth,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02thn:,,,,,,,,,,,,,,,,,,,,,2016-09-06 15:17:28.908,,,06/Sep/16 3:17 PM;drose@qubole.com;[~karuppayyar]  do we have any update for this customer?,"06/Sep/16 10:24 PM;mahuja;[~Ranjana] - can you please let us know:

a) Spark version used
b) Is the problem specific to notebooks or also with Analyze",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Errors in AWS action log,ACM-543,46301,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,stagra,stagra,17/Aug/16 2:13 AM,19/Aug/16 10:06 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Copying from PRES-730

Presto cluster 0.142 in private subnet for Activision threw this error in the AWS action log:

{code}
2016-08-15 23:52:30 INFO - DOWNSCALE: Terminating 2 nodes of 2.
2016-08-15 23:52:30 INFO - terminate_instances: [u'i-c7e15553', u'i-cae1555e']
2016-08-15 23:50:19 INFO - DOWNSCALE: Terminating 1 nodes of 1.
2016-08-15 23:50:19 INFO - terminate_instances: [u'i-98e0540c']
2016-08-15 23:00:07 INFO - DOWNSCALE: Terminating 25 nodes of 25.
2016-08-15 23:00:07 INFO - terminate_instances: [u'i-9ae0540e', u'i-c4e15550', u'i-c6e15552', u'i-c1e15555', u'i-c0e15554', u'i-c3e15557', u'i-c2e15556', u'i-cde15559', u'i-cce15558', u'i-cfe1555b', u'i-cee1555a', u'i-c9e1555d', u'i-cbe1555f', u'i-d5e15541', u'i-d4e15540', u'i-d7e15543', u'i-d6e15542', u'i-d1e15545', u'i-d0e15544', u'i-d3e15547', u'i-d2e15546', u'i-dce15548', u'i-dfe1554b', u'i-dee1554a', u'i-d9e1554d']
2016-08-15 22:52:32 ERROR - and submit it to starcluster@mit.edu
2016-08-15 22:52:32 ERROR - Please remove any sensitive data from the crash report
2016-08-15 22:52:32 ERROR - Crash report written to: ./logs/crash-report-10658.txt
2016-08-15 22:52:32 ERROR - Oops! Looks like you've found a bug in StarCluster
2016-08-15 22:51:55 INFO - DOWNSCALE: Terminating 4 nodes of 4.
2016-08-15 22:51:55 INFO - terminate_instances: [u'i-9be0540f', u'i-a5e05431', u'i-a4e05430', u'i-a7e05433']
2016-08-15 22:49:27 ERROR - and submit it to starcluster@mit.edu
2016-08-15 22:49:27 ERROR - Please remove any sensitive data from the crash report
2016-08-15 22:49:27 ERROR - Crash report written to: ./logs/crash-report-10486.txt
2016-08-15 22:49:27 ERROR - Oops! Looks like you've found a bug in StarCluster
2016-08-15 22:48:27 ERROR - and submit it to starcluster@mit.edu
2016-08-15 22:48:27 ERROR - Please remove any sensitive data from the crash report
2016-08-15 22:48:27 ERROR - Crash report written to: ./logs/crash-report-10460.txt
2016-08-15 22:48:27 ERROR - Oops! Looks like you've found a bug in StarCluster
2016-08-15 22:43:43 ERROR - and submit it to starcluster@mit.edu
2016-08-15 22:43:43 ERROR - Please remove any sensitive data from the crash report
2016-08-15 22:43:43 ERROR - Crash report written to: ./logs/crash-report-10211.txt
{code}

The crash report has this content:

{code}
---------- CRASH DETAILS ----------
2016-08-15 22:52:31,413 PID: 10658 config.py:601 - DEBUG - Loading config
2016-08-15 22:52:31,414 PID: 10658 config.py:134 - DEBUG - Loading file: ./config
2016-08-15 22:52:31,544 PID: 10658 cli.py:257 - DEBUG - Command: CmdListClusters, Args: ['qbol_acc5165_cl20331'], Opts: {'json_mode': True, 'show_ssh_status': False, 'show_stable': True}
2016-08-15 22:52:31,544 PID: 10658 awsutils.py:228 - DEBUG - creating self._conn w/ connection_authenticator kwargs = {'path': '/', 'region': RegionInfo:us-west-2, 'port': None, 'is_secure': True}
2016-08-15 22:52:32,551 PID: 10658 cli.py:300 - DEBUG - Traceback (most recent call last):
  File ""/usr/lib/hustler/lib/py/starcluster/cli.py"", line 266, in main
    sc.execute(args)
  File ""/usr/lib/hustler/lib/py/starcluster/commands/listclusters.py"", line 34, in execute
    show_stable=self.opts.show_stable)
  File ""/usr/lib/hustler/lib/py/starcluster/cluster.py"", line 256, in list_clusters
    in cluster_groups]
  File ""/usr/lib/hustler/lib/py/starcluster/cluster.py"", line 220, in get_cluster_security_group
    return self.ec2.get_security_group(gname)
  File ""/usr/lib/hustler/lib/py/starcluster/awsutils.py"", line 496, in get_security_group
    filters={'group-name': groupname})[0]
  File ""/usr/lib/hustler/lib/py/starcluster/awsutils.py"", line 508, in get_security_groups
    return self.conn.get_all_security_groups(filters=filters,
  File ""/usr/lib/hustler/lib/py/starcluster/awsutils.py"", line 231, in conn
    self._conn = self.connection_authenticator(**self._kwargs)
  File ""/usr/local/lib/python2.6/site-packages/boto/__init__.py"", line 171, in connect_ec2
    return EC2Connection(aws_access_key_id, aws_secret_access_key, **kwargs)
  File ""/usr/local/lib/python2.6/site-packages/boto/ec2/connection.py"", line 103, in __init__
    profile_name=profile_name)
  File ""/usr/local/lib/python2.6/site-packages/boto/connection.py"", line 1100, in __init__
    provider=provider)
  File ""/usr/local/lib/python2.6/site-packages/boto/connection.py"", line 569, in __init__
    host, config, self.provider, self._required_auth_capability())
  File ""/usr/local/lib/python2.6/site-packages/boto/auth.py"", line 989, in get_auth_handler
    'Check your credentials' % (len(names), str(names)))
NoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. ['HmacAuthV4Handler'] Check your credentials

---------- SYSTEM INFO ----------
StarCluster: 0.9999
Python: 2.6.9 (unknown, Jul 20 2016, 20:52:42)  [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)]
Platform: Linux-4.4.15-25.57.amzn1.x86_64-x86_64-with-glibc2.2.5
boto: 2.39.0
paramiko: 1.7.7.1 (George)
Crypto: 2.5
jinja2: 2.6
decorator: 3.3.2
{code}


Cross checking these logs with RDS shows that the nodes were terminated fine even with this error. Opening this jira to see if these are a real problem or not",,ajayb,Jove,stagra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02t0b:,,,,,,,,,,,,,,,,,,,,,2016-08-17 15:00:36.586,,,"17/Aug/16 3:00 PM;Jove;Thanks Shubham.

I suppose it's expected behavior if the nodes were downscaled by Presto per the downscaling logic. The only thing that stood out is that error message where it says:

{code}
2016-08-15 22:52:32 ERROR - and submit it to starcluster@mit.edu
2016-08-15 22:52:32 ERROR - Please remove any sensitive data from the crash report
2016-08-15 22:52:32 ERROR - Crash report written to: ./logs/crash-report-10658.txt
2016-08-15 22:52:32 ERROR - Oops! Looks like you've found a bug in StarCluster
{code}

We want to verify through this ticket that there's no other star cluster related issues in this issue Activision has encountered.","18/Aug/16 9:59 AM;ajayb;[~Jove] it looks like a benign failure. Failure while listing nodes of a cluster i.e. while calling ```list_clusters``` are not fatal for the callers like the auto scaling manager of hadoop2 or presto. Searching on the web, it seems [these|http://blog.johnryding.com/post/122337566993/solving-intermittent-noauthhandlerfound-errors-in] are intermittent failures and can happen when frequent authentication calls are made through boto while using IAM roles.

However, based on the log snippet pasted above it looks like it is hit frequently in above cluster. Is the content same in all the crash files listed above (./logs/crash-report-10486.txt, ./logs/crash-report-10460.txt, ./logs/crash-report-10211.txt)? Was the cluster rebooted after which these errors stated showing up?","18/Aug/16 10:12 AM;Jove;I looked at a few of those crash report files and the contents are very similar. THere's no cluster restart in between the times shown in logs above. Eventually, the cluster was shutdown obviously however. 

Do you think this auth issue caused the generation of crash report and the line from starcluster?",19/Aug/16 10:03 AM;ajayb;[~Jove] the auth issue certainly caused the crash report from starcluster. What's not clear is why would we suddenly see this on a cluster where it was previously working. I am assuming they have not seen this since the cluster has been restarted?,"19/Aug/16 10:06 AM;Jove;[~ajayb], you are correct that they have not seen this before. With the fix [~stagra] provided via PRES-730, now the queries can finish successfully and cluster seems to be scaled up and down correctly. However, we are still seeing this error in the auto scaling log... When this auth issue is encountered, do we have a retry which is why the scale up/down actions still finished perhaps?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notebook sync fails if bucket requires encrypted puts,MW-335,46104,Bug,Reopened,MW,Middleware,software,sumitm,,,Critical,,karuppayyar,mpatel,mpatel,15/Aug/16 8:39 AM,02/Jun/17 11:41 AM,09/Aug/17 6:03 AM,,,,,0,blocker,in-rb38-cp12,jira_escalated,notebooks,qubot-master,qubot-release-branch-38,qubot-release-branch-40,qubot-release-branch-40.0,"[~karuppayyar], [~mahuja], [~Jove]

Scripps is trying to use Spark + Spark Notebooks. This is a new use case and they are trying to on board new users...

They are unable to create notebooks, or clone notebooks. The default location bucket requires encrypted puts.

From the sync log I can see:

{code}
+ /usr/bin/s3cmd -c /usr/lib/hustler/s3cfg --no-check-md5 sync /usr/lib/zeppelin/notebook/ s3://sni-dds-qubole/defloc/zeppelin/notebook/
ERROR: S3 error: Access Denied
{code}

Scripps account is 'digital data services', account_id = 5134. Admin User = 	dl-dds-operations@scrippsnetworks.com

There are two properties set which hadoop / web tier use to check for encrypted writes to s3:

Account Limits: `S3 server side encryption`
Account Features: `EncryptResult` 

(I honestly don't know if both of those are required or just one...)
",,aabbas,adubey,ajayb,ekang,gayathrym,hiyer,karthikk,karuppayyar,mahuja,monikak,mpatel,qubot,rgupta,sourabhg,sumitm,sureshr,,,,,,,,,,,,,,SOL-166,HADTWO-598,TES-1754,,,,,,,,,,,,,,,,,,,,,scripps,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=2}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":2,""lastUpdated"":""2017-07-11T14:30:28.000-0700"",""stateCount"":2,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":2,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,No Doc Impact,,,,,,,#Empty,No,,,,,,No,monikak,#Empty,,1|z03l0e:zzzi,,,,38.19.0,,MW Sprint 21(4Jan-17Jan),nb-RB-45,,,,,,,,,,,,2.0,"8796,11883",2016-08-15 09:41:47.163,,,"15/Aug/16 8:39 AM;mpatel;if I hand run it with the --server-side-encryption flag, it works:

{code}
[ec2-user@ip-10-49-229-116 logs]$ /usr/bin/s3cmd -c /usr/lib/hustler/s3cfg --no-check-md5 --server-side-encryption sync /usr/lib/zeppelin/notebook/ s3://sni-dds-qubole/defloc/20574/spark/notebook/
/usr/lib/zeppelin/notebook/ExampleNote/note.json -> s3://sni-dds-qubole/defloc/20574/spark/notebook/ExampleNote/note.json  [1 of 1]
 34308 of 34308   100% in    0s   366.00 kB/s  done
{code}",15/Aug/16 9:41 AM;sureshr;cc: [~bharatb] and [~vipulm],"15/Aug/16 9:59 AM;mpatel;Changing to 'critical' for now.. i can create a notebook from cluster page. However, the notebook will probably not sync to s3 - and will will thus wipe it out after cluster restarts.
","19/Aug/16 4:37 PM;mahuja;Considering we need to support notebooks on multiple clouds, we can change note sync to Hadoop command line. 

Users opting for SSE will/most likely have fs.s3n.sse property set in their Hadoop overrides or we can bucketize these settings under an already available account feature. 

Another thing to explore will be to enable SSE for all note syncs (not an ask here).","24/Aug/16 12:32 PM;mahuja;Summarizing our discussion from SOLSUP meeting:

For now we can install a cron in node bootstrap with SSE option (similar to current put command). We can in parallel work on moving these command lines to hadoop (also needed for multi-cloud). cc - [~ajayb], [~karuppayyar]",31/Aug/16 8:43 AM;sureshr;[~mpatel] Can we remove the SOLSUP tag since [~mahuja] has suggested a workaround?,31/Aug/16 9:26 AM;mpatel;OK.. what are the next steps for a proper fix though?,06/Sep/16 10:34 PM;mahuja;Tagging [~ajayb] - move from s3cmd to hadoop command line required,07/Sep/16 9:26 AM;mpatel;[~ajayb] could you comment on any ETA we can share with the customer? I see this is marked for RB38 - correct?,"07/Sep/16 9:50 AM;mahuja;[~mpatel] - Zeppelin team will doing the change. Current plan is RB38.

Is it a high priority ask (I do see critical priority) but wanted to confirm.",07/Sep/16 11:57 AM;mpatel;I would classify it as high priority. RB 38 should be OK. ,"07/Sep/16 6:16 PM;adubey;[~karuppayyar]
Today [~mahuja] and i looked more into this and it appears this is failing in initial download of notebook only. s3cmd sycn is working fine as you see the S3 files are continuously getting updated. So if we fix the download flow that should unblock these guys. 
i wonder - what is the risk of adding node_bootstrap and copying all the files from S3 note folder to zeppelin/notebook until we have a proper fix after identifying the root cause.
Cluster id = 20563

I applied that 507 patch as well but log files only had 1 line few times ( Downloading ==== )","08/Sep/16 3:40 AM;karuppayyar;This is not related to *ZEP-507*

Checked the node bootstrap.
It has the wrong cluster_id. The cluster_id should be 20563, instead it is 20574(deleted one, or they are reusing the same bootsrap which was created for 20574 with 20563)

{code:java}
if [[ ""$is_master"" == ""1"" ]]; then
        echo ""*/2 * * * * /usr/bin/s3cmd -c /usr/lib/hustler/s3cfg --no-check-md5 --server-side-encryption put /usr/lib/zeppelin/conf/interpreter.json s3://sni-dds-qubole/defloc/20574/spark/conf/"" | tee -a /var/spool/cron/root
        echo ""*/2 * * * * /usr/bin/s3cmd -c /usr/lib/hustler/s3cfg --no-check-md5 --server-side-encryption sync /usr/lib/zeppelin/notebook/ s3://sni-dds-qubole/defloc/20574/spark/notebook/"" | tee -a /var/spool/cron/root
fi
{code}

1. 
We can get the cluster id like below and use it in the node bootstrap
{code:java}
source /usr/lib/hustler/bin/qubole-bash-lib.sh
CLUSTER_ID=`nodeinfo cluster_id`
{code}

2. 
Notebooks are sync-ed to wrong location.
It should be <defloc>/zeppelin/notebook
The one in the bootstrap is not used any more and use for backup.

The workaround here is to move all notebooks from s3://sni-dds-qubole/defloc/20574/spark/notebook/ to s3://sni-dds-qubole/defloc/zeppelin/notebook/

Also, the put does not work with even the serevr-side encryption flag.


","08/Sep/16 7:13 AM;adubey;thanks i will review this - 

the reason we used zep-507 was only for the logging you added as part of this roll out.","09/Sep/16 10:46 PM;adubey;[~karuppayyar] after modifying more i was able to make sync part work - 

relevant steps below :

if [[ ""$is_master"" == ""1"" ]]; then
        echo ""*/2 * * * * /usr/bin/s3cmd -c /usr/lib/hustler/s3cfg --no-check-md5 --server-side-encryption put /usr/lib/zeppelin/conf/interpreter.json s3://sni-dds-qubole/defloc/$CLUSTER_ID/spark/conf/"" | tee -a /var/spool/cron/root
        echo ""*/2 * * * * /usr/bin/s3cmd -c /usr/lib/hustler/s3cfg --no-check-md5 --server-side-encryption sync /usr/lib/zeppelin/notebook/ s3://sni-dds-qubole/defloc/$CLUSTER_ID/spark/notebook/"" | tee -a /var/spool/cron/root


        sed -i ""s/--no-check-md5 sync/--no-check-md5 --server-side-encryption sync /g"" /usr/lib/zeppelin/hustler/sync-notes.sh
fi","27/Sep/16 10:22 PM;gayathrym;RB38 closes in 24 hours from now,  on Sep 28 9pm PST.  RB38-CP1 goes out a week after. ","29/Dec/16 7:43 PM;aabbas;Hi [~mpatel] and [~mahuja]

I noticed that the associated [zendesk|https://qubole.zendesk.com/agent/tickets/8796] is closed for this ticket while it's still marked critical. Should it this Jira be deprioritized (get it out of JT's list of escalated Jiras) or was the Zendesk close prematurely (should be on-hold instead)?

Thanks,
Amer

","03/Jan/17 3:20 PM;mpatel;oh - looks like the zendesk ticket was closed due to inactivity (auto closed)...we had given them a workaround, and they may not be using notebooks much.

I think we still need to fix this, right [~mahuja] [~karuppayyar] ?","03/Jan/17 6:24 PM;karuppayyar;We are moving to hadoop commands in HADTWO-598 .
We should have have this in next release.  cc: [~sourabhg]",06/Jan/17 3:46 PM;mpatel;[~karuppayyar] They are facing issues while creating new notebooks too.  Will that be addressed with HADTWO-598?,"06/Jan/17 4:04 PM;mpatel;Trying to clone a notebook gives the following pop up in the UI:

{code}
Expected(200) <=> Actual(403 Forbidden) excon.error.response :body => ""\nAccessDeniedAccess Denied3B90831F5EDC15ECZHzI4OyNccwNIBWfv58y/v0xnGW8CJbhu6VJmdp6CCwl8c1J0sE1uLJCpv10uTLFHm2JxCcrUXo="" :headers => { ""Connection"" => ""close"" ""Content-Type"" => ""application/xml"" ""Date"" => ""Sat, 07 Jan 2017 00:03:28 GMT"" ""Server"" => ""AmazonS3"" ""x-amz-id-2"" => ""ZHzI4OyNccwNIBWfv58y/v0xnGW8CJbhu6VJmdp6CCwl8c1J0sE1uLJCpv10uTLFHm2JxCcrUXo="" ""x-amz-request-id"" => ""3B90831F5EDC15EC"" } :local_address => ""10.150.151.233"" :local_port => 58364 :reason_phrase => ""Forbidden"" :remote_ip => ""54.231.40.35"" :status => 403 :status_line => ""HTTP/1.1 403 Forbidden\r\n""
{code}

I suspect HADTWO-598 might not address this case as it is coming from web tier...

thoughts?","06/Jan/17 7:53 PM;aabbas;Hi [~mahuja] and [~karuppayyar] - this issue occurred again with scripps today - their boss escalated with us:
_One of my engineers is submitting a support ticket that we need to escalate. It is related to the ability to create a new notebook in our accounts. This is important to us ………….we want make sure we maintain our credibility with our expanding consumer base. I wanted to give you a heads up so that you could look for it._

Would you please escalate and resolve this issue? Scripps are on track to become one of our bigger accounts by quadrupling (or more) their usage.

We considered setting it to blocker but we identified a workaround that will work in the short term but is not pleasing to the Scripps team","06/Jan/17 8:33 PM;karuppayyar;[~sumitm] [~mmajithia] Looks like error is thrown when performing some s3 operation.
Also, I was trying to tail controller log in log server, I dont see  the error there. Not sure if I am checking the right one","07/Jan/17 9:12 AM;karuppayyar;Some more observations
{code:java}
[root@ip-10-49-229-149 ~]# /usr/bin/s3cmd -c /usr/lib/hustler/s3cfg --no-check-md5 put /usr/lib/zeppelin/conf/interpreter.json s3://sni-dds-qubole/defloc/15880/spark/conf/
/usr/lib/zeppelin/conf/interpreter.json -> s3://sni-dds-qubole/defloc/15880/spark/conf/interpreter.json  [1 of 1]
 2302 of 2302   100% in    0s   315.96 kB/s  done
ERROR: S3 error: Access Denied
{code}

I see the above error within cluster also. But the *PUT* is successful.
This problem has been there since some time(/var/spool/mail/root shows this error in cluster 15879 which has been running since 6 months ).

I am not sure how  fog handler handles s3 calls in tapp. Looks like we are depending on the response code from the s3 call. [~sumitm] can you please confirm this.

[~mpatel] [~aabbas] Can we ask them to fix the s3 permissions issue(We should not see *Access Denied*).

[~mpatel] Yes HADTWO-598 will not handle this issue. ",07/Jan/17 9:56 AM;sumitm;[~karuppayyar] not sure whats the reason as of now.. lets debug this on Monday.. ,"09/Jan/17 4:07 AM;aabbas;Than you, please keep us in the loop so we can update the customer.","09/Jan/17 5:00 AM;sumitm;My findings are same of what [~karuppayyar] had found. With the given role we are able to read, but not able to write anything to the bucket. ","09/Jan/17 7:23 AM;ekang;[~karuppayyar] the access denied is occurring because of the lack of encryption. so there shouldn't be anything new to debug. Here's their policy that generates access denied.

{
                ""Sid"": ""DenyUnEncryptedObjectUploads"",
                ""Effect"": ""Deny"",
                ""Principal"": ""*"",
                ""Action"": ""s3:PutObject"",
                ""Resource"": ""arn:aws:s3:::<bucket_name>/*"",
                ""Condition"": {
                        ""Null"": {
                               ""s3:x-amz-server-side-encryption"": true
                        }
               }
           }

I'm getting confused by all the workarounds and references to other jira tickets. Has a long term solution been identified? You said HADTWO-598 will not handle this issue. If not, then what will? Thanks.",09/Jan/17 8:49 AM;sumitm;[~ekang] if a customer is using encryption then there is flag (kms_cmk_id) in db which should be turned on. But I see that its turned off as of now.. [~sourabhg] am I correct?,"09/Jan/17 9:27 AM;karuppayyar;[~ekang]
There are two issues being discussed in this jira
1. Not being able to do  s3 operation with encryption in clusters.(Jira's original purpose)
 This will be fixed with HADTWO-598.(We will start using hadoop commands instead of s3cmd)
2. Not being able to create/clone notebook.
    This is happening in our webapp tier.  
     s3 write operations are not succeeding in our tier. Mostly likely some permission issue.
    If possible we can compare the policies of where notebook creation is succeeding and   scripps policy to see what is missing or in addition.","09/Jan/17 9:37 AM;sumitm;[~karuppayyar] actually both issues are same.. The S3 operations (in tapp) also need to go through hadoop only, as we've not explored its (encryption) feasibility with fog.. 

So there should be a flag in db which tells tapp to use hadoop only for all s3 operations.. and that flag is turned off.. ","09/Jan/17 9:44 AM;ekang;[~sumitm] so you are saying if we set that flag, we can retest to see if it works?",09/Jan/17 9:49 AM;sumitm;[~ekang] yeah my suspicion on that only.. we can try that.. you need to set proper {{kms_cmk_id}} (in accounts table).. ,"09/Jan/17 9:50 AM;sourabhg;[~sumitm]: The column _kms_cmk_id_ is required *only* for _client side encryption_. So _kms_cmk_id_ flag can be ignored here. 

[~ekang] [~karuppayyar]
As far as I understand, the issue here is that any put request to customer's bucket requires server-side-encryption to be turned on. 

I think while creating notebook (from tapp) in customer's s3 bucket, we are (probably) using fog handler and in fog handler we are not passing server side encryption flag resulting in Access denied. 

We can check if the server side encryption is enabled for this account by looking into _account_limits_ table for property name _S3 server side encryption_. Once we put this entry in account_limits table, fog handler should take care of the rest. 
 ",09/Jan/17 9:57 AM;sumitm;[~sourabhg] so its not about {{kms_cmk_id}}? Cause many other customers have server-side-encryption enabled and fog is working fine there.. ,"09/Jan/17 10:11 AM;ekang;Rather than using the client as the guinea pig to test things out, can we apply the policy that I pasted in previous comment to enforce s3 encryption and try running tests internally first to figure out what the correct solution is?",10/Jan/17 3:25 AM;sumitm;[~ekang] [~mpatel] we've found the issue in tapp layer. Will fix this and see how fast we can hotfix this..,"10/Jan/17 4:09 AM;sumitm;Opened a [PR|https://bitbucket.org/qubole/qbol/pull-requests/3766].. [~sureshr], [~gayathrym], [~karthikk] we should hotix this to webapp, as changes are not much and can be verified easily..","10/Jan/17 10:13 AM;sureshr;[~sumitm]: When do you want to do the hotfix? Can this wait till after R40 (planned for Monday, 15 Jan on api.qubole.com) or should we do this sooner?
cc: [~ekang] and [~mpatel]","10/Jan/17 10:16 AM;aabbas;[~sureshr] - The customer seemed agitated that it keeps happening, their boss did escalate it this past Friday. I can surely ask them how urgent this is and get their input. Do you mind giving an estimate of when it can be applied if after R40? I think we may be able to delay it a bit. i will get back to you.","10/Jan/17 10:37 AM;aabbas;[~sureshr], [~sumitm], [~ekang] and [~mpatel].. I checked with Elizabeth from Scripps and she said it is not urgent to get it out right away and work-around is manageable.","11/Jan/17 2:17 AM;qubot;`Sumit Maheshwari <sumitm@qubole.com>` commited to `master in qbol`
 Msg: `fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed` 
 Link: https://bitbucket.org/qubole/qbol/commits/a7e1d089751cd62b958b2eb1482d4a8f85f81f7c","11/Jan/17 10:00 PM;qubot;`Sumit Maheshwari <sumitm@qubole.com>` commited to `master in qbol`
 Msg: `fix: dev: MW-335: Fixing remaining upload_file calls with S3 encryption settings` 
 Link: https://bitbucket.org/qubole/qbol/commits/ca725f2f2b5108e6c17d18699ce488d9e14ab025","11/Jan/17 10:23 PM;gayathrym;Commit to MASTER :
*  a7e1d089751cd62b958b2eb1482d4a8f85f81f7c	Wed Jan 11 10:15:36 2017	hustler	master	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed				
										
*  a7e1d089751cd62b958b2eb1482d4a8f85f81f7c	Wed Jan 11 10:15:36 2017	hive_scripts	master	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed				
										
*  a7e1d089751cd62b958b2eb1482d4a8f85f81f7c	Wed Jan 11 10:15:36 2017	tapp2	master	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed				","12/Jan/17 2:07 AM;sureshr;Thanks, [~aabbas].
[~sumitm]: Please include this into R40-CP1. We could also consider a hotfix after R40.
cc: [~gayathrym]","12/Jan/17 9:21 AM;mpatel;[~sureshr] [~gayathrym] - Scripps is asking if we can hotfix this asap. They enabled foldering, and the original workaround no longer works now. They don't want to roll back foldering to avoid confusing their users...

Is there any chance we can get this in before the weekend?",12/Jan/17 8:04 PM;sureshr;Will check with Ops team and confirm.,"12/Jan/17 9:17 PM;sureshr;Discussed with Ops team. Webapp hotfix approved for today (Friday IST).
[~sumitm]: Please file an RM ticket for the hotfix and I will approve.
[~sajant]: Please coordinate with Sumit on the deployment.
[~monikak], [~karthikk]: Please help with the verification.",12/Jan/17 9:20 PM;karthikk;[~monikak] is prepping qa2 to do a sanity test on this.,"12/Jan/17 11:15 PM;qubot;`Sumit Maheshwari <sumitm@qubole.com>` commited to `master in qbol`
 Msg: `fix: dev: MW-335: S3 sse fix when foldering is being used` 
 Link: https://bitbucket.org/qubole/qbol/commits/81e03f1933d123cbe958fe23ce9505ceb1ec62cb","12/Jan/17 11:30 PM;qubot;`Sumit Maheshwari <sumitm@qubole.com>` commited to `release-branch-38 in qbol`
 Msg: `fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed properly` 
 Link: https://bitbucket.org/qubole/qbol/commits/2266c642d41590e27fb4d6819e8a5c4954ea6c19","12/Jan/17 11:59 PM;qubot;`Sumit Maheshwari <sumitm@qubole.com>` commited to `release-branch-40 in qbol`
 Msg: `fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed properly` 
 Link: https://bitbucket.org/qubole/qbol/commits/db56207e73fbaa8c7380b69d4f1e77a4b74610b3","13/Jan/17 2:43 AM;qubot;`Sumit Maheshwari <sumitm@qubole.com>` commited to `release-branch-38 in qbol`
 Msg: `fix: dev: MW-335: Fix due to code mismatch in rb38` 
 Link: https://bitbucket.org/qubole/qbol/commits/eaccf3d68e163c10f5d369991d44e8635384d409","15/Jan/17 9:35 PM;gayathrym;Commit to R40 :
*  db56207e73fbaa8c7380b69d4f1e77a4b74610b3	Fri Jan 13 07:58:52 2017	hustler	release-branch-40	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed properly			
									
*  db56207e73fbaa8c7380b69d4f1e77a4b74610b3	Fri Jan 13 07:58:52 2017	hive_scripts	release-branch-40	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed properly			
									
*  db56207e73fbaa8c7380b69d4f1e77a4b74610b3	Fri Jan 13 07:58:52 2017	tapp2	release-branch-40	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed properly			","15/Jan/17 10:06 PM;gayathrym;Commit to MASTER :
*  81e03f1933d123cbe958fe23ce9505ceb1ec62cb	Fri Jan 13 07:15:04 2017	hustler	master	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 sse fix when foldering is being used			
									
*  81e03f1933d123cbe958fe23ce9505ceb1ec62cb	Fri Jan 13 07:15:04 2017	hive_scripts	master	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 sse fix when foldering is being used			
									
*  81e03f1933d123cbe958fe23ce9505ceb1ec62cb	Fri Jan 13 07:15:04 2017	tapp2	master	MW-335	sumitm@qubole.com	fix: dev: MW-335: S3 sse fix when foldering is being used			","16/Jan/17 2:15 AM;qubot;`Sumit Maheshwari <sumitm@qubole.com>` commited to `release-branch-40.0 in qbol`
 Msg: `fix: dev: MW-335: S3 put fails if server side encryption deatils are not passed properly` 
 Link: https://bitbucket.org/qubole/qbol/commits/d489a702cfa23195790db45f3b7ff116ba97862c","16/Jan/17 2:18 AM;gayathrym;Hi [~sajant]  : As discussed, [~sumitm] has CPed his hot fix from last Friday to release-branch-40.0.  [~tanishg] has also CPed his missing change from MW-274 onto the same branch. We are good for the package creation now for R40. ","17/May/17 1:19 AM;rgupta;[~karuppayyar] lets see if we can productionize it. 
Does hustler have a flag that can be used to propagate this info to the spark cluster  [~sumitm]  [~mahuja] ?
","19/May/17 2:16 AM;karuppayyar;There is a account_limit flag *S3 server side encryption* to enable server side encryption for all tapp s3 ops.
Do we enable this in tapp for account whose defloc is encrypter on server side?
Also do we add hadoop overrides to all clusters in such account to enforce server side encryption? [~mpatel]

Basically, we want to know if the bucket for a account is server side encrypted.
Which should we depend upon - the account limit or hadoop override (cores-iste.xml)
cc: [~hiyer] [~sumitm] [~rgupta][~sumitm] [~mpatel]
",19/May/17 2:53 AM;hiyer;I would suggest setting the hadoop override if the account setting is set.,"19/May/17 10:39 AM;rgupta;""I would suggest setting the hadoop override if the account setting is set.""

Right now my question is that: Is checking account limit enough to figure out sse or we need to look into hadoop overrides also?

Setting hadoop overrides based on account setting is more smthing hustler team should fix it",19/May/17 9:09 PM;hiyer;The implicit meaning of my statement was that we should take the account setting as the single source of truth :-),"19/May/17 9:22 PM;rgupta;agree....We want to scope this jira to fixing zeppelin. Though karu PR takes care of passing the sse param from rails layer into node_info but cluster team can fix the hadoop overrides stuff.

I guess we can add cluster team in PR [~karuppayyar]"
Pyspark paragraph hangs ,ZEP-466,45908,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,ajithr,ajithr,10/Aug/16 8:39 AM,20/Jul/17 11:45 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"*Description:*

Python paragraph hangs with Anaconda python while trying to import a classifier module. import of the same module in python shell works fine.

*Env Details:*

Notebook: https://api.qubole.com/v2/notebooks#open?id=17498
User: venkatesh@indix.com

*Detailed description:*

* Paragraph is a simple import of the classifier module (s3://indix-classifier-model/module_36628b5573/classifier.zip) installed via node bootstrap.
* Customer is using anaconda python installed in path - /opt/anaconda/envs/pyspark/bin/python
* When we run this paragraph, the zeppelin interpreter logs do not show movement and seems stuck in while initializing for a very long time.
* The code in the paragraph works fine from the python shell on the master or slave nodes. Problem only when run via notebooks
* When I run the same command with a new interpreter with driver memory set to 10 GB, the paragraph again seems to get stuck in the same place
* Tried to run from analyze, but it looks like the dependencies aren't there for the classifier paragraph. But, ran other python commands and it worked without any hassles.
* Wondering if this is to do with the zeppelin server memory. Consulted with [~beria] and he tells me that it isn't the case.
* Need dev help to check this further and advise what could be wrong.",,ajithr,beria,bharatb,karuppayyar,mahuja,nikunjv,rvenkatesh,sbadam,sriramg,,,,,,,,,,,,,,,,,,,,,,,,,11/Aug/16 1:40 AM;karuppayyar;Screen Shot 2016-08-11 at 11.51.23.png;https://qubole.atlassian.net/secure/attachment/31208/Screen+Shot+2016-08-11+at+11.51.23.png,,,,,,,,,,,,,,,,,,,indix,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02ryj:,,,,,,,,,,,,,,,,,,,1.0,8671,2016-08-10 10:10:13.082,,,"10/Aug/16 10:10 AM;karuppayyar;I was checking the notebook.
The paragraph with import from *classifier* module seems to have suceeded.
Or was the paragraph removed?
Also can we check if we can get a clone of  the clsuetr or start the existing cluster, since it seems to be problem coming from their bootstarpped binary.","10/Aug/16 10:42 AM;ajithr;Hey Karu, you can start cluster and run the paragraph. Note that the paragraph is just an import. So, nothing much is expected as output I think. I'm also suspecting something with the binary when run in Zeppelin. Note that the binary runs okay in Python shell.","11/Aug/16 1:44 AM;karuppayyar; !Screen Shot 2016-08-11 at 11.51.23.png!
When i run the paras in above sequence(import classifier, then import the AgeGroupClassifier), the imports go through.
May be python experts will know what scenarios this can happen(added them in watch list)
May be something to do with their python library itself.
 ","11/Aug/16 7:43 AM;ajithr;[~karuppayyar] I'm not sure if I understand if the first paragraph is anyway related to the second one. By these 2 imports, we are basically importing 2 different modules from the package named - ""classifier"".

If you run the below command that the customer is trying in his node bootstrap script:

{noformat}
s3cmd get s3://indix-classifier-model/module_36628b5573/classifier.zip classifier.zip && unzip classifier.zip && cd classifier && /opt/anaconda/envs/pyspark/bin/python setup.py install
{noformat}

you will notice the following lines:

{noformat}
copying build/lib/classifier/AgeGroupClassifier.py -> /usr/local/lib/python2.6/site-packages/classifier
.
.
copying build/lib/classifier/Classifier.py -> /usr/local/lib/python2.6/site-packages/classifier

{noformat}

So, while this may be a data point, I'm not sure if this is some sort of a red herring.",17/Aug/16 12:44 AM;ajithr;[~karuppayyar] Any update on this?,17/Aug/16 12:59 AM;nikunjv;Raising the priority of this. There are new data scientists at Indix working on this and need this fixed for their adoption on Qubole. ,"17/Aug/16 1:50 AM;ajithr;I've conveyed the above observations of [~karuppayyar] to the customer:

{noformat}
%pyspark
from classifier import Classifier
from classifier import AgeGroupClassifier
{noformat}

The above seems to work in customer's account/notebook. Checked with customer if this is a good enough workaround for now while we investigate this further.",17/Aug/16 4:14 AM;ajithr;Customer is satisfied with the workaround to run the 2 imports as mentioned above. Reducing priority to major.,"19/Aug/16 5:27 PM;mahuja;[~ajithr], [~karuppayyar] - can you please post the original notes which are not working. The notebook has since been updated. 

Can we post the snippet of what did not work earlier.",19/Aug/16 5:28 PM;mahuja;cc - [~mohan] for a similar example if possible,"12/Jan/17 5:07 PM;sbadam;Hey Rajendran, can we have any update or ETA for this issue? Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`socket.error: [Errno 104] Connection reset by peer -- Error reading from & writing to s3.,ACM-531,45830,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,laks,addon_zendesk_for_jira,10/Aug/16 3:13 AM,12/Aug/16 12:30 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"A python Spark command (32208977), fails with `socket.error: [Errno 104] Connection reset by peer`
It Errors while reading from & writing to s3.

Looks like there are few known issues w.r.t boto and here is link I referred.
https://github.com/boto/boto/issues/2207

Can someone confirm on this?

Below is the actual actions that customer is performing
```
1. Reads data(queries in a txt file) from s3 
2. Connects to hive_context, retrieve data from table 
3. Save the retrieved data to csv in s3 (using python pandas & boto)
```",,addon_zendesk_for_jira,ajayb,laks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02rwr:,,,,,,,,,,,,,,,,,,,,,2016-08-11 06:27:09.926,,,11/Aug/16 6:27 AM;ajayb;[~laks] not sure what's expected here. The github link you referenced does not have any fix for this issue in boto2. There are some workarounds listed there that the customer can try.,12/Aug/16 12:30 AM;laks;[~ajayb] Looks like it is consistent. Will try the workarounds and update this jira. Keep this open until then. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Create Schema shows error ""The selected cluster is invalid. Please choose a different cluster.""",UI-4405,45565,Bug,Open,UI,UI,software,aswina,,,Major,,sureshr,Ranjana,Ranjana,05/Aug/16 10:40 AM,27/Sep/16 3:26 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Explore -> Amazon S3 -> Select csv file -> Create Schema -> Click through -> Run -> Opens Analyze window -> Shows error ""The selected cluster is invalid. Please choose a different cluster."" The cluster is valid and UP.

If you click run on analyze window, the command goes through and completes. 

",,aswina,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Aug/16 10:41 AM;Ranjana;Screen Shot 2016-08-05 at 10.36.55 AM.png;https://qubole.atlassian.net/secure/attachment/31027/Screen+Shot+2016-08-05+at+10.36.55+AM.png,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02qtn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems with append of parquet file,SPAR-1183,45440,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,venkats,laks,addon_zendesk_for_jira,02/Aug/16 8:10 PM,23/May/17 5:44 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Referred from:https://api.qubole.com/v2/notebooks#open?id=18033

I try to append parquet file with data but it seems it does not work properly. Can you please check it? You can find the code inside Harry_test. 
-----------------------------------
Looks like its an open source bug while checking in google but I'm not really sure though. 
Here is a github url referred. 
https://github.com/apache/spark/pull/9408/files

Can you please confirm on this?",,addon_zendesk_for_jira,laks,mahuja,venkats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02q2b:,,,,,,,,,,,,,,,,,,,1.0,8516,2016-08-04 16:33:20.257,,,04/Aug/16 4:33 PM;venkats;[~laks] Mayank and I took a look at that and couldn't find the notebooks in their account. Can you paste the error? Or Can you paste the notebook?,04/Aug/16 6:24 PM;mahuja;The account was expired and the notebook is in other account. Checking.,"04/Aug/16 7:06 PM;mahuja;[~laks] - I was not able to reproduce the issue. Please take a look at cloned notebook and it shows the desired behavior https://api.qubole.com/v2/notebooks#open?id=18218

I have coped data into s3://ims.data/hcp/qubole.. Please delete it if not required.",04/Aug/16 8:02 PM;laks;The notebooks are part of IMS account. I've replied the customer to check the cloned notebook and shall update the JIRA when customer responds. ,"04/Aug/16 9:59 PM;laks;[~mahuja] Customer tested again using s3://ims.data/hcp/hcp_names5 and able to reproduce the issue. To reproduce the issue run in below sequence.

1. Run all paragraph in notebook https://api.qubole.com/v2/notebooks#open?id=18221, you can change the table name at the top to create new parquet file.
2. Run test case to check https://api.qubole.com/v2/notebooks#open?id=18033","05/Aug/16 7:32 PM;venkats;We tried to repro this and we were able to repro this. This happens only when the first notebook ran and then the second notebook is ran. Now the notebook 2 is not able to append data. 

There is something which is going on in the first notebook which kinds of get cached or something happening there. We need more investigation. Atleast we were able to repro this and got the behavior, how its happening.

Will update more details soon.","06/Sep/16 10:56 PM;mahuja;The ticket is closed as customer found this issue in general with Spark. 

We should revisit it though. Disable parquet cache metadata could be one thing to try out.",06/Sep/16 10:57 PM;mahuja;Removed jira escalated ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster 18960 showed as up even though there were 0 nodes,ACM-514,45439,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,Ranjana,Ranjana,02/Aug/16 6:37 PM,02/Aug/16 6:38 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Termination and start up again solved the problem. 

But this was what I observed:

The cluster is supposed to have 1-3 slave nodes, even autoscaling option is on-demand instances. 

The cluster status was green today evening , but there were zero nodes on the cluster. 

This was reported as the notebook associated with this cluster showed 503 error. ",,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02q23:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port https://issues.apache.org/jira/browse/MAPREDUCE-5875,HADTWO-560,45277,Bug,Reopened,HADTWO,Hadoop2,software,ajayb,,,Minor,,sourabhg,adubey,addon_zendesk_for_jira,29/Jul/16 8:38 PM,11/Jul/17 2:58 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,qubot-hadoop2-qubole-2.6.0,qubot-qubole-2.6.0,,,,,,Clickagy is hitting,,abhishekmodi,addon_zendesk_for_jira,adubey,gayathrym,qubot,sourabhg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,clickagy,,,,,"{repository={count=3, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":3,""lastUpdated"":""2016-09-26T03:16:16.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":3,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,No Doc Impact,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02pgz:,,,,,,,,,,,,,,,,,,,1.0,8417,2016-09-19 03:34:30.496,,,29/Jul/16 8:39 PM;adubey;cc [~sam],30/Jul/16 11:26 AM;adubey;Found a JIRA in Pig project which has a workaround - KB-431,"19/Sep/16 3:34 AM;qubot;`sourabh912 <sourabhg@qubole.com>` commited to `qubole-2.6.0 in hadoop2`
 Msg: `fix: dev: HADTWO-560: Port MAPREDUCE-5875. Make Counter limits consistent across JobClient, MRAppMaster, and YarnChild. (Gera Shegalov via kasha)` 
 Link: https://bitbucket.org/qubole/hadoop2/commits/f9d3b6d9389a871a46022c0cb0a4754e65d1da3f","19/Sep/16 3:34 AM;qubot;`sourabh912 <sourabhg@qubole.com>` commited to `qubole-2.6.0 in hadoop2`
 Msg: `fix: dev: HADTWO-560: Port MAPREDUCE-5875. Make Counter limits consistent across JobClient, MRAppMaster, and YarnChild. (Gera Shegalov via kasha)` 
 Link: https://bitbucket.org/qubole/hadoop2/commits/f9d3b6d9389a871a46022c0cb0a4754e65d1da3f",19/Sep/16 3:37 AM;sourabhg;Merged into master. ,"19/Sep/16 9:58 PM;gayathrym;Commit to MASTER :
*  f9d3b6d9389a871a46022c0cb0a4754e65d1da3f	Mon Sep 19 10:33:42 2016	hadoop2	qubole-2.6.0	HADTWO-560	  sourabhg@qubole.com 	fix: dev: HADTWO-560: Port MAPREDUCE-5875. Make Counter limits consistent across JobClient","26/Sep/16 9:16 AM;qubot;`sourabh912 <sourabhg@qubole.com>` commited to `qubole-2.6.0 in hadoop2`
 Msg: `Revert ""fix: dev: HADTWO-560: Port MAPREDUCE-5875. Make Counter limits consistent across JobClient, MRAppMaster, and YarnChild. (Gera Shegalov via kasha)""

This reverts commit f9d3b6d9389a871a46022c0cb0a4754e65d1da3f.` 
 Link: https://bitbucket.org/qubole/hadoop2/commits/ba79bc856a67b69072fb13f579f1d0d9b52cb8cc","26/Sep/16 9:16 AM;qubot;`sourabh912 <sourabhg@qubole.com>` commited to `qubole-2.6.0 in hadoop2`
 Msg: `Revert ""fix: dev: HADTWO-560: Port MAPREDUCE-5875. Make Counter limits consistent across JobClient, MRAppMaster, and YarnChild. (Gera Shegalov via kasha)""

This reverts commit f9d3b6d9389a871a46022c0cb0a4754e65d1da3f.` 
 Link: https://bitbucket.org/qubole/hadoop2/commits/ba79bc856a67b69072fb13f579f1d0d9b52cb8cc","26/Sep/16 9:19 AM;sourabhg;We are seeing some hive and shell commands failing after this change. 
I have reverted the commit in master and reopened the Jira. ","26/Sep/16 7:48 PM;gayathrym;Revert from MASTER :
*  ba79bc856a67b69072fb13f579f1d0d9b52cb8cc	Mon Sep 26 16:16:16 2016	hadoop2	qubole-2.6.0	HADTWO-560	sourabhg@qubole.com	Revert &quot;fix: dev: HADTWO-560: Port MAPREDUCE-5875. Make Counter limits consistent across JobClient	MRAppMaster","18/Oct/16 3:40 AM;sourabhg;The original changes in [MAPREDUCE-5875|https://issues.apache.org/jira/browse/MAPREDUCE-5875] broke `mapred job -status` i.e [MAPREDUCE-6288|https://issues.apache.org/jira/browse/MAPREDUCE-6288] which has been marked as blocker for 2.8

[This|https://issues.apache.org/jira/browse/MAPREDUCE-6288?focusedCommentId=14387066&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14387066] comment on later Jira suggests the potential problems that can arise as a result of MAPREDUCE-5875. Basically the issue will be resolved only for the user who launched the job and not for other users who only have job list permissions. This would happen if ACL is enabled. 

I think we have disabled ACLs by default and so this fix might just work fine for us. 

[~abhishekmodi]: Should we still merge this?   ","18/Oct/16 3:42 AM;abhishekmodi;As discussed offline, this fix has been reverted from OS. So we should wait for it's proper fix and then we should merge it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive tmp logs cleanup ( not happening in case of MR job re-attempt in yarn ),HIVE-1538,45229,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,adas,adubey,adubey,28/Jul/16 7:55 PM,06/Jul/17 10:26 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"cc [~adas]

Can we check the scenario - i observed if MR job is retried as part of hive query - the older MR jobs leave the HDFS data in locations like /tmp/hive-ec2-user/hive_2016-07-29_00-41-25_841_7551539937530050060-1


",,adubey,asomani,psrinivas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,fanatics,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-13T22:35:02.000-0800"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02p6r:,,,,,,Hive Sprint 21,,,,,,,,,,,,,1.0,9721,2016-10-05 20:58:11.55,,,23/Sep/16 4:51 PM;adubey;hey guys - i saw this today on fanatics cluster where  /tmp/hive-ec2-user/ filled up entire HDFS .. Can we prioritize this now?,"05/Oct/16 8:58 PM;asomani;Hey AD, do you have some more details on this? Is this easily reproducible?","13/Oct/16 1:57 PM;psrinivas;[~adubey]: Do u think, this is still the case? ",13/Oct/16 2:21 PM;adubey;yes i dont think it was fixed right? i observed it twice.,13/Oct/16 2:27 PM;psrinivas;[~adubey]: You think the cleaning is not happening even for the successful jobs or only failed jobs? ,27/Dec/16 9:17 AM;adubey;[~psrinivas] now it was probably for failed only.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unable to bring up Grofers cluster and  fails with ""Hadoop bring up failed""",ACM-501,45181,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,laks,addon_zendesk_for_jira,27/Jul/16 2:39 AM,27/Jul/16 2:39 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Unable to bring up the cluster 17646, after starting the cluster it tries to come up (status per UI) and later terminates with the below error. 

Unable to ssh into the master while the cluster trying to come up. 

[ec2-user@ip-10-102-186-219 ~]$ easy_hustler --cluster-id 17646 sshmaster qbol_acc4848_cl17646
No config file specified - defaulting to hustler/configs/config.default for config file
*** 2016-07-27 09:16:31,958 WARNING - Skipping keypair fingerprint validation...
nc: read failed (0/10): Broken pipe
ssh_exchange_identification: Connection closed by remote host

From debug.log

ip-10-61-203-250 2016-07-27 09:17:03,478 PID: 25659 cluster.py:2734 - DEBUG - Traceback (most recent call last):
ip-10-61-203-250   File ""/usr/lib/qubole/packages/hustler-34.18.0/hustler/lib/py/starcluster/cluster.py"", line 2725, in wait_for_hadoop
ip-10-61-203-250     output = self.access_url(rm_url, 10, (recreate_tunnel_if_required and (i == (tries / 2))))
ip-10-61-203-250   File ""/usr/lib/qubole/packages/hustler-34.18.0/hustler/lib/py/starcluster/cluster.py"", line 2613, in access_url
ip-10-61-203-250     raise IOError('URL %s not accessible (error: %s)' % (url, error))
ip-10-61-203-250 IOError: URL http://ec2-54-169-104-65.ap-southeast-1.compute.amazonaws.com:8088/ not accessible (error: % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
ip-10-61-203-250                                  Dload  Upload   Total   Spent    Left  Speed
ip-10-61-203-250 ^M  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0^M  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to receive SOCKS5 connect request ack.)",,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02ow3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL result display when there are more than 1 statement,SPAR-1162,45062,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,Jove,addon_zendesk_for_jira,22/Jul/16 12:27 PM,20/Jul/17 7:07 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,This is a follow up to SPAR-933. The result display is still not right when there are more than 1 statement.,,addon_zendesk_for_jira,bharatb,Jove,vishalg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02o9f:,,,,,,,,,,,,,,,,,,,,,2016-07-22 20:30:34.691,,,22/Jul/16 8:30 PM;bharatb;[~vishalg] is this similar to hive's behaviour?,"22/Jul/16 10:59 PM;Jove;This behavior is same as running Hive query on spark cluster 30579728. However, running Hive query on hadoop 1 30579820 and hadoop 2 30580156 resulted in a nice formatted table as one would expect.","26/Jul/16 2:05 AM;vishalg;I tested on qa3 :

https://qa3.qubole.net/v2/analyze?command_id=210413 : spark sql result is not formatted
https://qa3.qubole.net/v2/analyze?command_id=210426 : hive command on hadoop2 cluster is not formatted
https://qa3.qubole.net/v2/analyze?command_id=210430 : hive command on hadoop cluster is not formatted",27/Jul/16 10:46 AM;Jove;Hm I don't have an account on QA yet but this seems different from my testing in production... ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skew Join is causing some hive job planning issues,HIVE-1523,45030,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Minor,,nitink,adubey,addon_zendesk_for_jira,21/Jul/16 1:56 PM,25/Jun/17 11:47 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"job id - 30447925 

With map side join this is failing for HDFS file not found because it is expecting some stage to create the dir. Looks to be an OSS issue but we need to check ",,adas,addon_zendesk_for_jira,adubey,drose@qubole.com,nitink,psrinivas,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Aug/16 4:04 AM;nitink;explain_output.rtf;https://qubole.atlassian.net/secure/attachment/32304/explain_output.rtf,,,,,,,,,,,,,,,,,,,activision,,,,,"{repository={count=2, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":2,""lastUpdated"":""2010-08-27T06:19:57.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":2,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02o2b:,,,,,,Hive Sprint 2,Hive Sprint 3,Hive Sprint 4,Hive Sprint 5,Hive Sprint 6,,,,,,,,,1.0,8231,2016-07-27 15:42:24.037,,,26/Jul/16 11:59 AM;adubey;hey [~asomani] - any clues on this?,"27/Jul/16 12:38 PM;adubey;We saw some more issues today -

https://api.qubole.com/v2/analyze?command_id=30967557 it was with map side join as false. I think we need some root cause analysis on this soon",27/Jul/16 3:42 PM;drose@qubole.com;[~asomani][~mahuja] Activision is hitting multiple issues currently that is affecting their ability to test and roll out Qubole as the sole ad-hoc data solution.  Can this be expedited?,"27/Jul/16 3:44 PM;adubey;[~asomani] can you also sync up with [~adas] -

customer is kind of stuck on this use case because tez query is generating wrong dir structure so they can't consume data generated by tez ( QTEZ-74 ) and MR fails for the same reason mentioned above..

cc [~drose@qubole.com] [~sureshr] for help on pri setting.","03/Aug/16 9:57 AM;drose@qubole.com;The customer is confident this is not a data issue, are we able to assist?  We believe this is eroding some confidence that could extend to loss in extended ETL scheduling.",03/Aug/16 10:00 AM;adubey;[~drose@qubole.com] did they report this issue specifically again - or are you referring the one Kurt was raising ( technically that issue is different and hive part works there ) for presto there is an open JIRA -  https://qubole.atlassian.net/browse/PRES-716 ,30/Aug/16 8:14 AM;adubey;any update on this one?,"31/Aug/16 4:10 AM;nitink;[~adubey] [~asomani] [~psrinivas]  This does not seem to be a stage directory creation issue. It seems more like a stage dependency resolution issue where stages are not executed in proper order. 

Here is the stage order given by explain statement ( detailed output is attached)

{code}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-8 depends on stages: Stage-1 , consists of Stage-11, Stage-2
  Stage-11
  Stage-7 depends on stages: Stage-11
  Stage-2 depends on stages: Stage-7, Stage-9
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0
  Stage-5 is a root stage
  Stage-10 depends on stages: Stage-5 , consists of Stage-12, Stage-2
  Stage-12
  Stage-9 depends on stages: Stage-12
{code}

whereas while executing stage 2 is  picked before stage 7/9. From the logs the order seems to be Stage-1 -> Stage-2 

And hence Stage-2 is not getting its desired input directory as its yet to be written
","31/Aug/16 5:43 AM;nitink;[~adubey] They have skewjoin enabled and I guess its the culprint.. Can we ask them to run the query after disabling skewjoin optimization
{code} SET hive.optimize.skewjoin=false;{code}

i tried running it , but the query is very big ( stage 5 had 66k mappers ) , So not sure if I should run it.",01/Sep/16 10:59 AM;adubey;[~nitink] sure - i will ask them.  and i meant the same thing ( not directory issue) - it was expecting directory because it was expecting a particular stage to finish by that time..,14/Sep/16 8:33 PM;nitink;Removing jira_escalated as the above flag is working for customer.,14/Sep/16 8:40 PM;adubey;sure - is this something u found in OSS as well?,"14/Sep/16 9:09 PM;nitink;[~adubey] I tried removing the label , but I guess zendesk is adding it back . Can you remove from Zendesk ? I will get back to you is this issue is there in  OSS as well or not ","15/Sep/16 12:55 PM;adubey;yes it seems to be triggering from the integration apis.
maybe you can close it and create another one with low pri to investigate and link this jira with new one",15/Sep/16 11:49 PM;nitink;[~adubey] Changed the priority to minor.. not opening another jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
explore -> show table rows gives Execution Timed Out,UI-4026,44928,Bug,Reopened,UI,UI,software,aswina,,,Critical,,mmajithia,megha,addon_zendesk_for_jira,20/Jul/16 6:30 PM,03/May/17 11:16 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"On explore tab, when trying to view rows, it's timing out giving error ""Execution Timed Out
""

",,addon_zendesk_for_jira,adubey,aswina,megha,mmajithia,mpatel,psrinivas,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,10/Aug/16 6:07 PM;megha;explore-logs.txt;https://qubole.atlassian.net/secure/attachment/31203/explore-logs.txt,,,,,,,,,,,,,,,,,,,ActivisionBlizzard,,,,,{},NA,No Doc Impact,,,,,,,#Empty,No,,,,,,NA,,#Empty,,1|z02ns3:,,,,,,,,,,,,,,,,,,,,,2016-07-20 22:19:02.899,,,20/Jul/16 10:19 PM;aswina;[~mmajithia] - Can you triage this and get back on this jira with the root cause?,"10/Aug/16 5:08 PM;megha;I am able to explore for myself now (something seems to have changed).
However, this is still a problem for Activision.
Their case is this: 
They are using custom metastore.
Their clusters lie in us-west-2.
When we query from analyze, ""select * from tablename limit 10"", query returns pretty quickly, but times out in explore.
Account 5165, query example: 32290710 - query works but explore fails..

We have modified metastore.prop file for them so that the round-trip time for queries is lesser (reason being they have custom metastore, and are in us-west-2).

","10/Aug/16 6:06 PM;megha;Attaching the logs
",10/Aug/16 10:44 PM;mmajithia;cc: [~psrinivas],"10/Aug/16 11:20 PM;psrinivas;Cross-region queries are very slow, hence, we suggested ""Hive-on-master"" option for normal query execution. 

But, for explore, we cannot have the ""Hive-on-master"" option, so, there is a high chance of explore queries time-out. ",29/Aug/16 1:09 AM;aswina;[~mmajithia] [~psrinivas] - Is there a fix for this or can we close this jira?,"29/Aug/16 10:44 PM;mmajithia;[~megha]: Closing this for now, please reopen if provided work around is not sufficient.","29/Aug/16 11:53 PM;adubey;[~psrinivas] can't we increase the timeout? It is like a broken functionality and we are responsible for it ""as-a-service"" - we need to restore the functionality ( can't just leave on its fate )","01/Sep/16 11:12 AM;adubey;I am reopening this - we can adjust the priority but we should fix this before waiting for it to become a high pri some day - this could be a non-issue if we say ""we do not support custom metastore in non-east regions"" and that is not the case.
Or in case of Dual-IAM roles we already have documented that Explore will not work - but that is not the case here.


",01/Sep/16 6:24 PM;aswina;cc: [~sureshr],"02/Sep/16 12:42 AM;sureshr;Yes, agree with [~adubey] that we either document that cross-region explore is not supported (and provide a better error message) or we make it work properly. Leaving it in a broken state (execution timed out) is not professional or a good user experience IMO.

Having said that :), what are our options to fix this?
* If increasing the time-out is a quick fix, let's do that.
* Longer-term, we are thinking of having separate QDS installations in each AWS region because the cross-region latency impacts the product in other areas as well. There is no definite time-frame for this QDS setup in us-west, but it will not be before early 2017. Once we have a separate QDS environment in us-west2, this issue should be solved.

Thoughts?","04/Sep/16 11:16 PM;psrinivas;Another temporary fix is, we can use hive-on-master for these non-us-east metastores, but explore *will bring up the cluster*.  Is that acceptable? ",05/Sep/16 12:48 AM;adubey;[~psrinivas] is timeout increase not possible. We can definitely justify the delay on UI rendering - but bringing up cluster will not be desired and will be completely opposite of existing design. ,"06/Sep/16 2:50 AM;psrinivas;[~adubey]: Timeout is on UI side only, but that may not be enough always. We can try that I suppose. ","12/Sep/16 10:13 AM;mmajithia;I have discussed with [~vagrawal] about this issue. At present we have 75 seconds of timeout which should not be increased but rather we should move to async solution. 

Backend support for async is already present but we need to add UI feature which may not be straightforward.

[~sureshr]: Could you please suggest the priority for this and we can try to accomodate the change.

Exisiting JIRA for async task UI-3124","12/Sep/16 10:19 AM;sureshr;[~mmajithia] Can you explain why the UI feature is not straightforward? Ideally, there should be no change in the UI at all, right? Explore should just start working properly.","12/Sep/16 10:31 AM;mmajithia;[~sureshr]: 
*Current Implementation:* When user tries to get sample data for a table, we call `/qpal/explorer.json` api with appropriate parameter and we get sample data for the table.

*Async Implementation:* When user tries to get sample data, we call new api and get command id for that. Then we need to keep polling till the time command gets completed. In meanwhile, user can explore other items and it should work seamlessly with proper messages about the progress (mini analyze command run implementation).","13/Sep/16 3:26 AM;sureshr;I see. So, the UI and user interaction will not change, just that we need to add code to poll for the async task to be completed. This is probably a little involved, but should not be super complex (there are patterns for this within our code and out in the field).

I would suggest that we combine this with the Analyze/Explore revamp. ETA would be sometime in 2017.","05/Oct/16 11:30 AM;megha;[~sureshr]
Thank you for the ETA on this.. 
I was wondering if there is another epic jira for Analyze/Explore revamp that I can look at..
",12/Oct/16 5:20 AM;sureshr;[~megha] Please see UI-4017.,"02/May/17 2:27 PM;megha;Hi [~sureshr], [~mmajithia]
Do we have any plans for this issue? ","03/May/17 9:12 AM;sureshr;Given the focus on the features related to insights/recommendations, the UI team does not have the bandwidth to take this up before June 2017. What is the priority of this issue?",03/May/17 11:16 AM;megha;Thanks [~sureshr] I was just looking for an estimate.. That should suffice,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"getting ""SAML validation failed"" trying to sign in to qubole",QBOL-5570,44781,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,sumitm,megha,addon_zendesk_for_jira,19/Jul/16 10:04 AM,29/Nov/16 10:07 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Expedia was getting this error initially. They changed following settings, and it's working for them. 

""In ADFS they are able to set a time off-set which they have set for 1 minute. This is something that fixed the issue for another application that expedia were dealing with issues connecting."" 

But I didnt find anything in controller logs for this.
Is there a way to identify what was going wrong?

Date of issue: july 18th 2016.

h3. Zendesk Attachments


[Attachment 1|https://qubole.zendesk.com/attachments/token/9NtCok3tQp1kCeMcEvyl9thOB/?name=samlvalidationfail.png]
",,addon_zendesk_for_jira,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02n8j:,,,,,,,,,,,,,,,,,,,,,2016-11-29 22:07:13.567,,,"29/Nov/16 10:07 PM;sumitm;[~megha] can u explain a bit more, what was the issue?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ganglia 500,HAD-634,44506,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,hiyer,sam,sam,13/Jul/16 7:05 AM,11/Jul/17 2:57 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"https://qubole.zendesk.com/agent/tickets/7610

I am not certain which project this belongs to.. but the issue is with Ganglia showing 500 error for a client when the cluster is busy. This error also occurs sometimes with an idle cluster. I was able to reproduce this error a couple of times by refreshing the Ganglia page repetitively. 
",,sam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02m0r:,,,,,,,,,,,,,,,,,,,1.0,7610,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Negative values returned from Cluster Metrics API when pulling Aggregate Cluster Metrics,ACM-450,44418,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,Ranjana,Ranjana,12/Jul/16 1:28 PM,17/May/17 8:59 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Note from customer: We are seeing some unexpected results from the Cluster Metrics API when pulling Aggregate Cluster Metrics - specifically mem_report. We often (at least several times per day) see the metric 'User' return negative values! Is this a known issue?

Below is a sample response - and sample API call:

curl -i -X GET -H ""X-AUTH-TOKEN: foo"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -G -d metric=mem_report -d interval=hour ""https://api.qubole.com/api/v1.3/clusters/2573/metrics""

[{""datapoints"":[[50871083008,1468317615],[49121814118,1468317630],[48273210846,1468317645],[58257499204,1468317660],[100600543640,1468317675],[150436128360,1468317690],[184208649420,1468317705],[204306429820,1468317720],[211571449860,1468317735],[210300890860,1468317750],[212734141780,1468317765],[214060657600,1468317780],[219853609370,1468317795],[227702753420,1468317810],[223890924070,1468317825],[253888633510,1468317840],[281021528200,1468317855],[284589034700,1468317870],[278351827490,1468317885],[265915932950,1468317900],[236988280560,1468317915],[203575405500,1468317930],[188596055790,1468317945],[183421298280,1468317960],[173165450040,1468317975],[141177917440,1468317990],[102141101400,1468318005],[85353335194,1468318020],[85793103872,1468318035],[88247151821,1468318050],[92898956902,1468318065],[98448322287,1468318080],[94675397837,1468318095],[90815257805,1468318110],[86011423130,1468318125],[86141490790,1468318140],[86956344115,1468318155],[88253188506,1468318170],[91319961190,1468318185],[113025504870,1468318200],[163406639920,1468318215],[214223210630,1468318230],[287655492270,1468318245],[320216595930,1468318260],[341120708610,1468318275],[350947376330,1468318290],[353721645470,1468318305],[354080231420,1468318320],[356684244310,1468318335],[360764436480,1468318350],[363067400190,1468318365],[298190099800,1468318380],[151480744620,1468318395],[113956970500,1468318410],[116241201830,1468318425],[125640242790,1468318440],[122742360750,1468318455],[119037744470,1468318470],[115779601480,1468318485],[101924037700,1468318500],[37152725948,1468318515],[-55249402812,1468318530],[-104027479790,1468318545],[-122309413000,1468318560],[-99225075712,1468318575],[-63225511390,1468318590],[-2932385655.5,1468318605],[38477434607,1468318620],[65013687910,1468318635],[70252529254,1468318650],[70931675273,1468318665],[72137205350,1468318680],[73076913766,1468318695],[73011367117,1468318710],[73469484237,1468318725],[72745861939,1468318740],[69565704602,1468318755],[66970102170,1468318770],[62638157824,1468318785],[54418605943,1468318800],[49389547520,1468318815],[48658491802,1468318830],[47674730906,1468318845],[47219834061,1468318860],[47993092506,1468318875],[53852193860,1468318890],[57831406797,1468318905],[62011241540,1468318920],[60241255629,1468318935],[56663930743,1468318950],[45305377041,1468318965],[39706519142,1468318980],[29906345984,1468318995],[22892455253,1468319010],[17644084156,1468319025],[21786582494,1468319040],[50139981141,1468319055],[50347243930,1468319070],[48624374033,1468319085],[35605650910,1468319100],[-36293718016,1468319115],[-91327312691,1468319130],[-111890961200,1468319145],[-107691462660,1468319160],[-107131787670,1468319175],[-105335137890,1468319190],[-105480677790,1468319205],[-107051074360,1468319220],[-111037535300,1468319235],[-110204005580,1468319250],[-106401354550,1468319265],[-103083932060,1468319280],[-103362138110,1468319295],[-103696004160,1468319310],[-102336071270,1468319325],[-68442243072,1468319340],[43256185105,1468319355],[114008171860,1468319370],[156234385000,1468319385],[197793025500,1468319400],[208418439170,1468319415],[216892520040,1468319430],[222624789300,1468319445],[225795061350,1468319460],[233005862910,1468319475],[233040956620,1468319490],[168137832860,1468319505],[-74511776154,1468319520],[-128402454390,1468319535],[-1387197235.2,1468319550],[111315215160,1468319565],[161088054070,1468319580],[246820900040,1468319595],[284202565630,1468319610],[300957422390,1468319625],[344445750480,1468319640],[375733636030,1468319655],[396727611390,1468319670],[405887445130,1468319685],[405887445130,1468319700],[386283231640,1468319715],[390606923910,1468319730],[402020949740,1468319745],[414067899320,1468319760],[412108082110,1468319775],[406332305410,1468319790],[400883287930,1468319805],[416916028070,1468319820],[416916028070,1468319835],[451597837110,1468319850],[463493649480,1468319865],[478782338660,1468319880],[483192657780,1468319895],[487483102820,1468319910],[467310285620,1468319925],[424243646740,1468319940],[424243646740,1468319955],[410190868480,1468319970],[416866201600,1468319985],[410083999740,1468320000],[392927813630,1468320015],[386462241860,1468320030],[411991858110,1468320045],[434878824450,1468320060],[477929765820,1468320075],[496797635380,1468320090],[502665854980,1468320105],[497893258580,1468320120],[492346294000,1468320135],[494761143500,1468320150],[487203254270,1468320165],[497205021080,1468320180],[513796334390,1468320195],[529501501300,1468320210],[535842300720,1468320225],[550375605590,1468320240],[556769118890,1468320255],[560199565310,1468320270],[564713026900,1468320285],[567330613930,1468320300],[573227103570,1468320315],[576611900070,1468320330],[576327499780,1468320345],[574362812420,1468320360],[569698607100,1468320375],[559885028010,1468320390],[545243501910,1468320405],[542483154260,1468320420],[549840349590,1468320435],[560355424940,1468320450],[563059701350,1468320465],[565372194820,1468320480],[560105136130,1468320495],[557675408040,1468320510],[561662279680,1468320525],[561788056920,1468320540],[555500812700,1468320555],[548676894720,1468320570],[550889541630,1468320585],[561534459900,1468320600],[567909510350,1468320615],[570500169730,1468320630],[571650711550,1468320645],[568838758400,1468320660],[589964581550,1468320675],[638901099180,1468320690],[639895167800,1468320705],[640809473640,1468320720],[641639368840,1468320735],[644369709740,1468320750],[647822426110,1468320765],[649669854550,1468320780],[651844161540,1468320795],[656258979430,1468320810],[659487476940,1468320825],[663411299120,1468320840],[666727542780,1468320855],[666923483960,1468320870],[664864544090,1468320885],[662111603370,1468320900],[658811378070,1468320915],[655989771470,1468320930],[653189878580,1468320945],[649504547090,1468320960],[647554973150,1468320975],[644017352980,1468320990],[641969705780,1468321005],[641788244790,1468321020],[642810843960,1468321035],[644129981510,1468321050],[644657497430,1468321065],[645698383870,1468321080],[648226487230,1468321095],[660472670070,1468321110],[676692825700,1468321125],[688202259660,1468321140],[699750497210,1468321155],[709865971440,1468321170],[712196855400,1468321185],[""NaN"",1468321200],[""NaN"",1468321215]],""metric"":""Use\\g"",""hostname"":""null"",""interval"":""hour""}",,Ranjana,sam,sbadam,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,mediamath,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02lwr:,,,,,,,,,,,,,,,,,,,1.0,8026,2016-08-23 14:38:17.336,,,"23/Aug/16 2:38 PM;sam;Any status on this ticket, please?","12/Jan/17 2:21 PM;sbadam;Hey Ajay, can we hear any update or ETA for this? Thanks.","17/May/17 8:59 AM;sureshr;Since this not an urgent/immediate issue, removing the SOLSUP label.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cluster not scrubbed - instances were manually shut down by customer on aws,ACM-441,44314,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,megha,addon_zendesk_for_jira,08/Jul/16 10:21 AM,08/Jul/16 11:38 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"The screenshots attached show, last seen time of cluster was yesterday (July 7) , however until now (july 8 10:20 am) cluster has not been cleared up and marked as terminated.

Didnt see scrub logs for this on log server.
Cluster id: 18647

Marking the cluster down from datastore for now.

",,addon_zendesk_for_jira,megha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Jul/16 10:22 AM;megha;Screen Shot 2016-07-08 at 10.17.35 AM.png;https://qubole.atlassian.net/secure/attachment/30104/Screen+Shot+2016-07-08+at+10.17.35+AM.png,08/Jul/16 10:22 AM;megha;Screen Shot 2016-07-08 at 10.18.10 AM.png;https://qubole.atlassian.net/secure/attachment/30105/Screen+Shot+2016-07-08+at+10.18.10+AM.png,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02lgb:,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/16 11:38 AM;megha;One more observation, which we think could be a reason why cluster was not scrubbed.
cc: [~Jove]

The master node ip for this cluster, after it was terminated by customer(returnpath), this ip was assigned to one of the nodes in expedia's cluster. 
It is possible that as we could ssh to this ip, scrub process thought cluster was fine so didnt terminate the cluster.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent versions of Guava jars on spark cluster,SPAR-1148,44238,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,adubey,addon_zendesk_for_jira,07/Jul/16 10:38 PM,16/Jun/17 12:36 PM,09/Aug/17 6:03 AM,,,packaging,,0,jira_escalated,,,,,,,,I found spark tmp-libs has guava 14 but zeppelin has 15. Can we make it consistent,,addon_zendesk_for_jira,bharatb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02lc3:,,,,,,,,,,,,,,,,,,,1.0,7946,2016-07-08 00:06:59.452,,,08/Jul/16 12:06 AM;bharatb;This is a much bigger effort I think. We need to normalise hive/hadoop/spark/zeppelin and also allow user's choice probably. But at least spark and zeppelin should be consistent. cc [~mahuja],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S3 eventual consistency problem in spite of DFOC and s3 end point,SPAR-1142,44027,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,Ranjana,Ranjana,05/Jul/16 5:54 PM,20/Jul/17 8:58 PM,09/Aug/17 6:03 AM,,,,,2,jira_escalated,,,,,,,,,,amoghm,bharatb,drose@qubole.com,gayathrym,gmargabanthu,karthikk,mahuja,mpatel,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,No Doc Impact,,,,,,,#Empty,No,,,,,,No,tsp,#Empty,,1|z02ktn:,,,,,,,,,,,,,,,,,,,1.0,7871,2016-07-05 18:37:08.87,,,05/Jul/16 6:37 PM;bharatb;In a couple of instances I have seen this go away when the data involved was parquet and {{spark.sql.parquet.cacheMetadata}} was set to false (instead of the default true value). Might be good for them to try it out. [~Ranjana],"05/Jul/16 10:50 PM;mpatel;I don't think they are writing Parquet.

The exception they see is:

{code}
16/07/01 14:58:41 pool-26-thread-1 INFO InputPathProcessor: Caught exception java.io.FileNotFoundException: s3://dlx-prod-core-consumer/prod/etl/dlx/match/maas/work/2016.07.01-145705.128-3819502152585787666/transformed/part-00006: No such file or directory.
16/07/01 14:58:41 main INFO InputPathProcessor: Executor shut down
java.io.FileNotFoundException: s3://dlx-prod-core-consumer/prod/etl/dlx/match/maas/work/2016.07.01-145705.128-3819502152585787666/transformed/part-00006: No such file or directory.
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:978)
	at org.apache.hadoop.fs.FileSystem.getFileBlockLocations(FileSystem.java:710)
	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1718)
	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1703)
	at org.apache.hadoop.mapred.InputPathProcessor.perPathComputation(InputPathProcessor.java:238)
	at org.apache.hadoop.mapred.InputPathProcessor.access$000(InputPathProcessor.java:28)
	at org.apache.hadoop.mapred.InputPathProcessor$2.run(InputPathProcessor.java:338)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

It looks like that particular part file is 0 byte file.

Would setting 'fs.inputpathprocessor.ignoreIOException=true' work here? (ref: HAD-425)
","06/Jul/16 12:52 AM;mahuja;[~amoghm] for advice. Will fs.inputpathprocessor.ignoreIOException help in this case?

cc - [~hiyer], [~psrinivas], [~abhishekmodi]","14/Aug/16 2:36 PM;mpatel;This was the latest occurrence of the problem: 31506405

Is there anything we can do here? Maybe some setting to retry on filenotfound exception?",14/Aug/16 10:47 PM;mahuja;[~amoghm] for advice. Will fs.inputpathprocessor.ignoreIOException help in this case?,"15/Aug/16 12:55 PM;amoghm;As far as I can remember setting that flag should fix the issue. As I am on vacation with no access to mail, I cannot look at the code but looking at stack trace I think setting flag may fix the problem","23/Aug/16 11:15 AM;mpatel;[~amoghm] For Spark, how best to set this: fs.inputpathprocessor.ignoreIOException  ? Should it go as a hadoop override?

Also, if that doesn't help - can we consider some very basic retrial mechanism? 
",24/Aug/16 6:41 AM;amoghm;[~mahuja] [~mpatel] I just checked the hadoop2 code ... This flag has not been ported from hadoop1 to hadoop2. We would need to do that before it can be set in hadoop override. Let me talk to [~abhishekmodi] about porting it and raise PR for it.,"24/Aug/16 7:29 AM;mpatel;hi [~amoghm] ok sure. Also, please just double check to ensure this issue would be addressed by that approach.","24/Aug/16 7:43 AM;amoghm;I have raised PR for it: https://bitbucket.org/qubole/hadoop2/pull-requests/208/fix-usr-spark-1142-ignoring-filenotfound/diff

[~mpatel]
Looking at stack trace, this should definitely fix the issue. But to clarify more this would just ignore those files which were not found and continue. But if the files had to be considered for split computation etc, but were not found due to S3 consistency issue then this is not a fix as it would just ignore the files and move on.","24/Aug/16 9:59 PM;mpatel;I see - thats actually bad right? It's probably better to let the job fail and allow the user to re-try than to silently skip files and succeed...

","25/Aug/16 2:08 AM;amoghm;[~mpatel]
The decision whether to continue ignoring the partition or throw an exception is something which client of IPP has to take and set the behaviour appropriately (either through flag or passing value to constructor). For example in case of Hive the partitions sent to IPP for listing during split computation are the one's which have been seen earlier (through alter recover partition) so this S3 consistency problem does not occur. If there exists a partition among the list of partitions sent by Hive to IPP, which is not found on S3 then it just means that partition has been deleted ( as in case documented in HAD-425) and so it expects IPP to skip it. I am expecting something similar with Spark but not sure about it. Will allow [~mahuja] or [~bharatb] to comment on that. In any case IPP does not take care of S3 eventual consistency issues and it is something that needs to be handled by it's client. ","29/Aug/16 1:54 AM;amoghm;I have made submission to qubole-2.6.0 branch: 

{code:java}
commit 5f927c5410b34454f45998f0259921dbf9a2abd2
Author: Amogh Margoor <amoghm@qubole.com>
Date:   Mon Aug 29 04:45:25 2016 -0400

    fix:usr:SPARK-1142 Ignoring FileNotFound Exception thrown when IPP optimization for s3 paths is turned off
{code}


Had chat with [~bharatb] regarding this fix and we came to conclusion that this would fix the issue here safely. Assigning JIRA back to Spark team so that they can change spark config on their level to enable new flag.","29/Aug/16 11:43 PM;gayathrym;Commit to MASTER :
*  5f927c5410b34454f45998f0259921dbf9a2abd2	Mon Aug 29 08:45:25 2016	hadoop2	qubole-2.6.0	UNKNOWN	amoghm@qubole.com	fix:usr:SPARK-1142 Ignoring FileNotFound Exception thrown when IPP optimization for s3 paths is turned off		","14/Sep/16 9:53 PM;karthikk;[~bharatb] this isn rb-37 but still open, can u take care of this ? Cc:[~gayathrym]",15/Sep/16 11:25 PM;bharatb;[~karthikk] basically we need to do more work on this. Basically [~amoghm] has made a change in hadoop code. We need to use that new code path in spark and check if that solves part of the problem we are facing. Overall this JIRA is not yet resolved from spark perspective. Also the real change happened in hadoop code and I think there is no testing needed there as of now.,"15/Sep/16 11:51 PM;gayathrym;[~bharatb] : Removing the in-rb37 label in that case. when the changes are complete wrt to the issue reported on this ticket, we can tag it for the appropriate release and get it tested then. Please let me know in case this is otherwise. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaling thread breaking,ACM-427,43812,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,venkatak,venkatak,01/Jul/16 6:56 PM,12/Jan/17 10:14 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"The logs wont get updated in Autoscaling logs of the cluster, and manually adding the nodes using API revives the thread.

I have seen this issue on 2 occasions atleast now:

https://qubole.zendesk.com/agent/tickets/7818
https://qubole.zendesk.com/agent/tickets/7738

Let me know if there is a known issue. Researching this has been a bit complex. Will use this JIRA to further update details.",,sbadam,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02k8r:,,,,,,,,,,,,,,,,,,,,,2017-01-11 16:28:20.899,,,"11/Jan/17 4:28 PM;sbadam;Hey Ajay, can we have any update or ETA for this issue? Thanks.",12/Jan/17 10:14 AM;sbadam;No Ajay. We are just tracking status of On-hold tickets. This is touched in the process.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster instance set as 'TERMINATING' while the next instance starts,ACM-426,43811,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,venkatak,venkatak,01/Jul/16 6:38 PM,01/Jul/16 6:45 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Latest instance id is set to DOWN
Some of the old instance ids are set to TERMINATING
Resulting the UI showing Orange down arrow showing as if it is terminating. Not sure if this is a UI bug or ACM cc: [~aswina][~jssarma]
Seeing this for the second time in last 10 days. 
Attaching some data.
Also, please see the ACM slack channel on June 20th about my chat on the previous occurrence of the same issue.

update I had to do for the latest and past occurrences:

1. Incident on june 20th:

update cluster_insts
set cluster_state = 'DOWN'
where cluster_id in (17728,18672) and id in (194054, 194044, 194022, 187937, 187929) and cluster_state = 'TERMINATING'
limit 5;

2. Second one:

production> update cluster_insts
-> set cluster_state = 'DOWN'
-> where cluster_id in (18209) and id in (185566,185546,185522,185511,185495) and cluster_state = 'TERMINATING'
-> limit 5;
",,aswina,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Jul/16 6:35 PM;venkatak;clusterterminating.png;https://qubole.atlassian.net/secure/attachment/29611/clusterterminating.png,01/Jul/16 6:37 PM;venkatak;toACM.txt;https://qubole.atlassian.net/secure/attachment/29610/toACM.txt,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02k8j:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to open Application UI (Cluster Proxy) for comcast,ACM-421,43789,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,laks,addon_zendesk_for_jira,01/Jul/16 3:10 AM,01/Jul/16 3:10 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"For the command ids: 14671, 14672 - Application_UI not available. 
Please see attached screenshot and help us with the same.

https://comcast.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-52-91-110-237.compute-1.amazonaws.com%3A19888%2Fjobhistory%2Fjob%2Fjob_1466788121242_0001%2F&clusterInst=756

API URL: https://comcast.qubole.com 

h3. Zendesk Attachments


[Attachment 1|https://qubole.zendesk.com/attachments/token/bLWjg7Y22TD7NIAudSQbSIJZY/?name=image006.jpg]

[Attachment 2|https://qubole.zendesk.com/attachments/token/mSAbTEpmc8JLTc4DlZTqZoujj/?name=image003.jpg]
",,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02k3n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job progress goes beyond 100% and also not very reliable,ZEP-371,43430,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,poojas,adubey,adubey,23/Jun/16 9:16 PM,19/Jul/17 3:00 PM,09/Aug/17 6:03 AM,,,core,,1,jira_escalated,,,,,,,,"Job progress goes beyond 100% and also not very reliable


cc [~mahuja] [~bharatb] ",,adubey,ajithr,bharatb,drose@qubole.com,gayathrym,karuppayyar,kmayank,mahuja,mohan,mpatel,snamburu,,,,,,,,,,,,,,,,,,,ZEP-444,,,,28/Jul/16 5:45 AM;karuppayyar;Screen Shot 2016-07-28 at 6.05.28 pm.png;https://qubole.atlassian.net/secure/attachment/30817/Screen+Shot+2016-07-28+at+6.05.28+pm.png,,,,,,,,,,,,,All,,,,,,autodesk,dataxu,expedia,oracle,salescycle,{},NA,Choose from,,AN-207,,,,,#Empty,No,,,,,,No,,#Empty,,1|z05aot:,,,,,,nb-RB-47,,,,,,,,,,,,,2.0,"7633,15583",2016-06-24 02:38:36.741,,,24/Jun/16 2:38 AM;ajithr;Expedia hit it this morning. Marking it here.,24/Jun/16 5:25 AM;ajithr;Marking it critical based on feedback from Expedia. This issue is causing reliability issues at their end I believe.,28/Jun/16 2:15 PM;drose@qubole.com;Adding some business impact. Both Autodesk and Expedia are reporting this issue.  Expedia's team working on Spark has identified this as their largest notebook concern at the moment and impacts customer confidence. [~mpatel][~ajithr],"29/Jun/16 7:33 PM;mahuja;Feedback from [~karuppayyar] ""one case we found is that when there are failed task, progress shows wrong results"".

[~karuppayyar] - please add if you have more details. Scheduling it for RB36.","29/Jun/16 8:52 PM;karuppayyar;One case that where it was happening was when there were failed tasks in a job, the progress % was shown incorrectly.
But there might be many other cases causing this . Will debug more on this.","28/Jul/16 5:45 AM;karuppayyar;Recording some observations from 
https://qubole.zendesk.com/agent/tickets/8222

Link to spark application UI: https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-54-193-26-95.us-west-1.compute.amazonaws.com%3A18080%2Fhistory%2Fapplication_1468998550316_0087%2Fjobs%2F&clusterInst=213635

I see that the number of succeeded task greater than total.
 !Screen Shot 2016-07-28 at 6.05.28 pm.png! 
Is this expected [~bharatb]","28/Jul/16 7:43 AM;karuppayyar;The succeeded tasks are higher than the total tasks because of task re-execution.
http://stackoverflow.com/questions/31306127/why-are-my-tasks-succeeded-above-tasks-total-in-spark-ui","02/Aug/16 11:27 AM;mahuja;Adding [~bharatb], [~mohan], [~kmayank]. 

- Is there a better way to show progress of Spark job? For example, direct link to Spark UI (DB case)
- Is there a way to make the progress bar based on tasks left, compared to tasks completed","07/Aug/16 7:18 PM;bharatb;We can make the progress bar better by taking stage failures into account. Then it will stay within 100% always.

Reg. showing the progress better -- we can link directly to spark job and we have discussed this previously. But what DB does is much more -- it embeds the entire progressing UI as an overlay over the running para. This will need some investigation. So we can start with providing direct link and then think of better embedding.","07/Aug/16 9:49 PM;karuppayyar;+1 for linking the paragraph to job.
ZEP-444 tracks this. Should we move it to RB-37?","08/Aug/16 11:40 AM;kmayank;That will be really helpful. Show progress and errors right on the paragraph itself. Also, we wills still need a notebook level progress so that user can have 1 view of every job running.","27/Sep/16 10:18 PM;gayathrym;Hi [~karuppayyar] : Can we remove the 'for-rb36' labels and update the revised ETA. As discussed, kindly use the rbxx-candidate label for providing ETAs. ","28/Sep/16 1:05 AM;mahuja;Hi [~gayathrym], I have removed the for-rb36 label, as it is scheduled in rb39 sprint","28/Sep/16 1:17 AM;gayathrym;Hi [~mahuja] : Sure, thank you for updating this. Not sure if you got a chance to see my email from yesterday [Email sub : IMPORTANT : PLEASE READ and TAKE ACTION : PROVIDE ETAs for new features and jira-escalated tickets] yet.   I had sent a note about us starting to use the 'rbxx-candidate' labels, for us to convey the ETA for such tickets. Could you kindly add the 'rb39-candidate' label on this ticket? I would be happy to discuss with you on slack or on a meeting if you have any questions on the  new process. ",28/Sep/16 3:15 AM;mahuja;Hi [~gayathrym] - I have added my thoughts to the email thread. It will be great if we can have dedicated fields for release versions.,"03/Oct/16 10:26 PM;gayathrym;Hi [~mahuja],  As discussed, I have gotten the extra labels removed for SPAR/ZEP tickets, so it should be a little less overloaded now. Could we please add the rbxx-candidate labels? Based on your note above, I guess this will be rb39-candidate? I am adding it here, please feel free to update as required. ",04/Oct/16 12:13 AM;mahuja;Thanks [~gayathrym],"27/Jan/17 1:43 AM;mahuja;cc - [~vipulm]
",19/Jul/17 3:00 PM;snamburu;[~mahuja] [~karuppayyar] Any update on this fix? Can we get an ETA?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The ""spark history "" URL on application UI is not a proxy url - it redirects back to same page instead of going to spark application UI",SPAR-1102,43116,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,megha,megha,20/Jun/16 11:53 AM,14/Jul/17 4:35 PM,09/Aug/17 6:03 AM,,,misc,,0,investigate-20160622,jira_escalated,,,,,,,"-The application is run from another node (not within cluster).
-Spark application is seen in resource manager but not in spark history UI.
-From the application overview, when I click on ""History"" link, it redirects to same page instead of spark history page - i.e cannot reach to Spark UI to view container logs.

There is no command id as this is run from edge node, not qubole.

Following is an example application ui of this:
https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-52-41-117-11.us-west-2.compute.amazonaws.com%3A8088%2Fcluster%2Fapp%2Fapplication_1465996832138_0016&clusterInst=193522

All spark application for this user ""subhashini.vadathe@thomsonreuters.com"" is run in a similar way, hence for all these, no spark UI is found.

This is for completed/failed applications.

",,megha,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,thomsonreuters,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02i63:,,,,,,,,,,,,,,,,,,,1.0,7494,2017-01-11 16:17:33.571,,,"11/Jan/17 4:17 PM;sbadam;Hey Mayank, can we have any update or ETA for this issue? Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Meta AM memory is not properly set when spark command line is multiline,SPAR-1098,42940,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,prakharj,bharatb,bharatb,17/Jun/16 12:58 AM,06/Jul/17 12:20 PM,09/Aug/17 6:03 AM,,,analyze,,1,jira_escalated,,,,,,,,"command id: 27441272
customer: majority

They have a spark-submit command line which is spread over multiple lines using backslash at the end of each line. In tapp layer we assume that the entire command line is in a single line and we look for driver-memory and other params only on one line (the last instance of spark-submit only). In this case, we did not find driver-memory and we did not set the size of shell mapper container properly and the driver ended up crashing.

We need to do a better job of finding driver-memory and other params in spark-submit command lines. 

The workaround is to enter the whole command line in a single line.",,adubey,bharatb,drose@qubole.com,mahuja,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,majority,spacetime,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z00bwa:pzy,,,,,,,,,,,,,,,,,,,2.0,"7546,15643",2016-06-22 19:40:38.894,,,"22/Jun/16 7:40 PM;mahuja;Handle semi colons (different commands), multi line commands (backslash separator).

For now handle multi line commands.","06/Jul/17 12:18 PM;adubey;Customer has hit this issue - example command : https://api.qubole.com/v2/analyze?command_id=82611418

Can we prioritize the fix for this",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saving PipelineModel to s3 does not work,SPAR-1080,42271,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,Ranjana,Ranjana,05/Jun/16 6:18 PM,31/May/17 7:17 PM,09/Aug/17 6:03 AM,,,misc,,0,jira_escalated,,,,,,,,"nb_pipelineModel.write.overwrite().save(""s3n://mix-topic-classifier/NaiveBayesInput_v8.model"")

nb_pipelineModel is an instance of class org.apache.spark.ml.PipelineModel and this class as per following implements implements MLWritable . https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/ml/PipelineModel.html

So this should ideally be a working operation.

I get error

Interpreter JVM has stopped responding. This generally happens if spark driver has run out of memory.
Try rerunning paragraph after increasing value of spark.driver.memory in interpreter settings page.
Drop us a mail at help@qubole.com with notebook link for root cause analysis.

I increased driver memory to 10g. driver overhead memory to 2g. Total memory available on master node is 15g. Nothing helped. 

Also enabled DFOC by adding the following to hadoop overrides. Did not help either. 

mapred.output.committer.class=org.apache.hadoop.mapred.DirectFileOutputCommitter
mapreduce.use.directfileoutputcommitter=true
spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter

For the spark interpreter, added

spark.speculation false

Cluster id: 13863

Last occurrence of this error : Took 43 seconds. Last run at Sun Jun 05 2016 18:08:01 GMT-0700 (PDT)

Notebook link: https://api.qubole.com/v2/notebooks#open?id=15439

The paragraph is annotated ""Failure"". If cluster is stopped, start from paragraph ""Restart from here"".
",,mahuja,Ranjana,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02fdv:,,,,,,,,,,,,,,,,,,,1.0,7019,2016-06-22 20:04:23.902,,,22/Jun/16 8:04 PM;mahuja;[~Ranjana] - ticket seems to be closed. Can you please share what was the change?,"23/Jun/16 5:51 PM;Ranjana;He said he solved it by using Save as Parquet. Increasing number of
executers also helped.

Ranjana Rajendran
Sr Software Developer Support Engineer

Qubole <http://www.qubole.com/> - A cloud based service that makes big data
easy for analysts and data engineers.

Qubole has been named to @CNBC’s ultimate list of disruptive companies! See
the whole list <http://www.cnbc.com/2015/05/12/cnbc-disruptor-50.html>

On Thu, Jun 23, 2016 at 1:01 AM, Bharat Bhushan (JIRA) <

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cluster shut down while write is incomplete,ACM-363,42255,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,megha,addon_zendesk_for_jira,03/Jun/16 10:59 AM,03/Jun/16 10:59 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"at some point all clusters in Qubole are stopped. With cluster #12444 I’ve been writing more 100 GB data in a S3 bucket. 

The attachments show that there were still some temporary files left to be committed to final output while the cluster shutdown.
Customer expected total 503 files - but only 482 files were written.

Looks like this happened while commiting output from temp location to final location. Have recommended user to use DFOC - but ideally the cluster shouldnt shut down due to inactivity while it is actually still writing some stuf.

",,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02fab:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken DAG visualization in Spark UI,SPAR-1069,42115,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,Ranjana,Ranjana,01/Jun/16 9:56 AM,04/Apr/17 2:40 PM,09/Aug/17 6:03 AM,,,analyze,,1,jira_escalated,,,,,,,,"The comment from customer:

""Currently, the DAG visualization for a spark job shows up like this.. you can see those 2 dots. there should have been a lineage of the processes required to finish up the job. This has been gone for the last month or so"".

He says: Its the “spark-dsmodeling” cluster in prod-analytics account. I think the cluster ID is 9061. Customer is Oracle.

This came back as a feedback about after a poor NPS survey. So please fix this ASAP.",,drose@qubole.com,mahuja,Ranjana,rohitk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Jun/16 9:59 AM;Ranjana;Screen Shot 2016-06-01 at 9.57.30 AM.png;https://qubole.atlassian.net/secure/attachment/28107/Screen+Shot+2016-06-01+at+9.57.30+AM.png,04/Apr/17 10:21 AM;drose@qubole.com;Screen Shot 2017-04-04 at 10.21.19 AM.png;https://qubole.atlassian.net/secure/attachment/40909/Screen+Shot+2017-04-04+at+10.21.19+AM.png,04/Apr/17 2:33 PM;mahuja;Screen Shot 2017-04-04 at 2.32.53 PM.png;https://qubole.atlassian.net/secure/attachment/40917/Screen+Shot+2017-04-04+at+2.32.53+PM.png,,,,,,,,,,,,,,,,,clarivate,,,,,{},NA,Choose from,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z00bwa:pzq,,,,,,,,,,,,,,,,,,,,,2016-07-08 11:37:55.213,,,"08/Jul/16 11:37 AM;mahuja;[~Ranjana], [~drose@qubole.com] - a command id will be useful to debug this problem further.",27/Jul/16 4:22 PM;mahuja;Removing from Sprint. We will re-schedule once we hear back from customer.,04/Apr/17 10:21 AM;drose@qubole.com;[~mahuja]  We see this at Clarivate  !Screen Shot 2017-04-04 at 10.21.19 AM.png|thumbnail! ,"04/Apr/17 10:35 AM;mahuja;The original problem got resolved with SPAR-1205. 

[~drose@qubole.com] can you please share the command Id, this seems to be a different problem but we can use this JIRA to track this case.","04/Apr/17 10:47 AM;drose@qubole.com;https://api.qubole.com/v2/notebooks#open?id=4874 
feel free to run it if you want

1st one to load dependency, 2nd to create dataframe definition, 3rd actually run the jo
3rd one might cause the problem.

[~mahuja][~msarkar]","04/Apr/17 2:38 PM;mahuja;The cluster would have recycled and if I now open the job UI, it renders well.  !Screen Shot 2017-04-04 at 2.32.53 PM.png|thumbnail! 

Note that on top, it says history server of 1.5.1 but in the issue screenshot it mentions 1.6.0 (which is most likely because of live Spark UI when the cluster was up - Spark 1.6.0).

[~drose@qubole.com] - can we move them to  Spark1.6.2? We can clone the cluster and try out this job to make sure DAG is fine. Spark 1.6.1 and later had fixes in JavaScript for similar issues. Also 1.6.2 will be more stable that 1.6.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle Empty Parquet Files in Tez,QTEZ-62,41883,Bug,Open,QTEZ,qbol Tez,software,adas,,,Major,,abhisheks,adas,adas,26/May/16 6:41 PM,02/Aug/17 10:24 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,quack,,,,,,,"We should handle empty Parquet file in Tez. This is the error we are getting.

Vertex failed, vertexName=Map 5, vertexId=vertex_1464291968593_0003_1_00, diagnostics=[Task failed, taskId=task_1464291968593_0003_1_00_021908, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: s3n://rnddev/dux/globalvar/date=2016-03-02/_temporary/0/task_201605250736_0002_m_000000_$folder$ is not a Parquet file (too small)


Vertex failed, vertexName=Map 5, vertexId=vertex_1464291968593_0003_1_00, diagnostics=[Task failed, taskId=task_1464291968593_0003_1_00_021908, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: s3n://rnddev/dux/globalvar/date=2016-03-02/_temporary/0/task_201605250736_0002_m_000000_$folder$ is not a Parquet file (too small)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:337)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1635)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: s3n://rnddev/dux/globalvar/date=2016-03-02/_temporary/0/task_201605250736_0002_m_000000_$folder$ is not a Parquet file (too small)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:192)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:131)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:97)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:149)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:80)
	at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:614)
	at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:593)
	at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:141)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:109)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:372)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:129)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)
	... 14 more
Caused by: java.io.IOException: java.lang.RuntimeException: s3n://rnddev/dux/globalvar/date=2016-03-02/_temporary/0/task_201605250736_0002_m_000000_$folder$ is not a Parquet file (too small)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:250)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:189)
	... 25 more
Caused by: java.lang.RuntimeException: s3n://rnddev/dux/globalvar/date=2016-03-02/_temporary/0/task_201605250736_0002_m_000000_$folder$ is not a Parquet file (too small)
	at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:412)
	at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)
	at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)
	at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)
",,adas,asomani,Jove,psrinivas,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-1473,,,,,26/May/16 6:41 PM;adas;log_25649622.txt;https://qubole.atlassian.net/secure/attachment/27927/log_25649622.txt,,,,,,,,,,,,,,,,,,,,,,,,{},NA,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02e4z:,,,,,,,,,,,,,,,,,,,1.0,6888,2016-06-15 04:05:24.198,,,"15/Jun/16 4:05 AM;asomani;[~adas] 
[~rvenkatesh] hit this issue, raised HIVE-1473. Duped that to this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark avro schema evolution not working,SPAR-1058,41875,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,abhisheks,addon_zendesk_for_jira,26/May/16 7:35 AM,23/Jun/17 5:12 AM,09/Aug/17 6:03 AM,,,misc,,0,jira_escalated,quack,,,,,,,"Spark avro schema evolution is not working for the customer. He tried following query:
https://api.qubole.com/v2/analyze?command_id=25513736

Here he is using data sets:
filePaths += ""s3://production-s3-basin/events_v2/2016/5/9/0/*""	
filePaths += ""s3://production-s3-basin/events_v2/2016/5/23/0/*""

The data sets have different schema definition. In second there are some more fields.

Also when run individually with data sets the quereis are successful.

With both data sets It is failing with exception:
App > 16/05/25 05:15:03 dispatcher-event-loop-4 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ip-10-145-50-173.us-west-1.compute.internal:39745 (size: 24.0 KB, free: 3.4 GB)

App > 16/05/25 05:15:04 task-result-getter-1 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4, ip-10-235-32-115.us-west-1.compute.internal): java.lang.ArrayIndexOutOfBoundsException: 25

App > at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:260)

App > at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:250)

App > at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:102)

App > at org.apache.spark.sql.catalyst.CatalystTypeConverters$$anonfun$createToCatalystConverter$2.apply(CatalystTypeConverters.scala:401)

App > at org.apache.spark.sql.execution.RDDConversions$$anonfun$rowToRowRdd$1$$anonfun$apply$2.apply(ExistingRDD.scala:59)

App > at org.apache.spark.sql.execution.RDDConversions$$anonfun$rowToRowRdd$1$$anonfun$apply$2.apply(ExistingRDD.scala:56)

App > at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)

App > at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)",,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02e37:,,,,,,,,,,,,,,,,,,,1.0,6762,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cluster Usage shows actual (UP), Qubole API shows DOWN",ACM-342,41838,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,Ranjana,Ranjana,25/May/16 2:22 PM,14/Jun/16 8:53 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Customer noted this and asked us:

As per DB statistics cluster was started up at 5:32 am PST

start time(UTC)

2016-05-24 12:32:23

and stopped at 8:27 am PST(due to inactivity)

stop time(UTC)
2016-05-24 15:27:06

Cluster Usage agreed with this.

The following Qubole API used by customer showed DOWN

curl -X GET -H ""X-AUTH-TOKEN:$X_AUTH_TOKEN"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" \ 
https://api.qubole.com/api/v1.3/clusters/1710/state

{""state"":""DOWN""}.",,ajayb,Ranjana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02duz:,,,,,,,,,,,,,,,,,,,,,2016-05-26 05:03:27.585,,,"26/May/16 5:03 AM;ajayb;I don't have an explanation about how/why the customer saw that. I checked hustler logs and it matches with what the db shows.

From db, the instance information:
{code:java}
                             id: 180862
                     cluster_id: 15057
              cluster_config_id: 40723
                  cluster_state: DOWN
                       start_at: 2016-05-24 12:32:23
                        down_at: 2016-05-24 15:27:06
               terminate_reason: INACTIVITY
    last_health_check_action_at: NULL
       last_health_check_action: NULL
                     created_at: 2016-05-24 12:27:29
                     updated_at: 2016-05-24 15:27:06
start_cluster_manage_command_id: NULL
{code}

From hustler logs, the instance was marked ""UP"" at 2016-05-24 12:32:23,424 (see last row in below snippet).
{code:java}
...
hustler.log-2016-05-25_1464146341.gz:ip-10-157-136-183 2016-05-24 12:27:29,685 PID: 29986 cluster.py:3118 - DEBUG - cluster_id 15057, cluster_inst_id 180862: taking an exclusive lock
hustler.log-2016-05-25_1464146341.gz:ip-10-157-136-183 2016-05-24 12:27:29,687 PID: 29986 cluster.py:3122 - DEBUG - cluster_id 15057, cluster_inst_id 180862: current cluster status is: INIT
hustler.log-2016-05-25_1464146341.gz:ip-10-157-136-183 2016-05-24 12:27:29,687 PID: 29986 cluster.py:3147 - DEBUG - cluster_id 15057, cluster_inst_id 180862: Updating status to PENDING
hustler.log-2016-05-25_1464146341.gz:ip-10-157-136-183 2016-05-24 12:27:29,703 PID: 29986 cluster.py:3150 - DEBUG - cluster_id 15057, cluster_inst_id 180862: released the exclusive lock
hustler.log-2016-05-25_1464146341.gz:ip-10-157-136-183 2016-05-24 12:27:29,705 PID: 29986 cluster.py:3154 - DEBUG - cluster_id 15057, cluster_inst_id 180862: bring up the cluster
hustler.log-2016-05-25_1464146341.gz:ip-10-157-136-183 2016-05-24 12:32:23,424 PID: 29986 cluster.py:3180 - DEBUG - cluster_id 15057, cluster_inst_id 180862: update status to UP
...
{code}
","26/May/16 5:04 AM;ajayb;[~Ranjana], why is this marked as a blocker? Is the customer consistently seeing this for their clusters?",31/May/16 2:14 PM;Ranjana;Hi [~ajayb]  Customer reported this again. This need to be addressed ASAP. ,"31/May/16 4:11 PM;Ranjana;[~ajayb]

the latest one happened Sat, 28 May 2016 05:57:25 
and the cluster id is 15057.","01/Jun/16 5:19 AM;ajayb;I cannot find any calls for fetching state of cluster-id 15057 in the nginx logs in production on 30th May. There are a bunch of calls on 19th or 20th May for fetching/updating notebooks, but the only other call to fetch cluster state is the last one in below snippet. But even that last one is from control panel code (that's what it looks like). 

{code:java}
[ec2-user@ip-10-65-73-252 nginx]$ zgrep ""/15057"" access.log-2016-05*
access.log-2016-05-20_1463715361.gz:ip-10-99-206-76 78.152.37.198 - - [19/May/2016:12:39:45 +0000] ""GET /v2/notes/15057 HTTP/1.1"" 302 175 ""https://api.qubole.com/v2/notebooks"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"" ""78.152.37.198""
access.log-2016-05-20_1463715361.gz:ip-10-99-206-76 78.152.37.198 - - [19/May/2016:12:50:27 +0000] ""PUT /v2/notes/15057 HTTP/1.1"" 200 146 ""https://api.qubole.com/v2/notebooks"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"" ""78.152.37.198""
access.log-2016-05-20_1463715361.gz:ip-10-65-95-43 78.152.37.198 - - [19/May/2016:12:50:27 +0000] ""GET /v2/notes/15057 HTTP/1.1"" 302 175 ""https://api.qubole.com/v2/notebooks"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"" ""78.152.37.198""
access.log-2016-05-20_1463715361.gz:ip-10-237-211-70 78.152.37.198 - - [19/May/2016:12:50:37 +0000] ""GET /v2/notes/15057 HTTP/1.1"" 302 175 ""https://api.qubole.com/v2/notebooks"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"" ""78.152.37.198""
access.log-2016-05-20_1463715361.gz:ip-10-99-206-76 78.152.37.198 - - [19/May/2016:12:51:06 +0000] ""PUT /v2/notes/15057 HTTP/1.1"" 200 146 ""https://api.qubole.com/v2/notebooks"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"" ""78.152.37.198""
access.log-2016-05-20_1463715361.gz:ip-10-136-100-47 78.152.37.198 - - [19/May/2016:12:51:06 +0000] ""GET /v2/notes/15057 HTTP/1.1"" 302 175 ""https://api.qubole.com/v2/notebooks"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"" ""78.152.37.198""
access.log-2016-05-20_1463715361.gz:ip-10-237-211-70 78.152.37.198 - - [19/May/2016:12:51:35 +0000] ""GET /v2/notes/15057 HTTP/1.1"" 302 175 ""https://api.qubole.com/v2/notebooks"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"" ""78.152.37.198""
access.log-2016-05-25_1464146341.gz:ip-10-157-132-31 50.232.25.10 - - [25/May/2016:01:15:49 +0000] ""GET /api/v1.3/clusters/15057?_=1464139081865 HTTP/1.1"" 200 1290 ""https://api.qubole.com/v2/control-panel"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36"" ""50.232.25.10""
{code}

I tried with one of my clusters and the cluster state was correctly reported by the API within a minute of the cluster being marked as up in the db. And while the cluster was coming up it was reported as pending. This looks as expected.

Few questions:
# Can you confirm the cluster-id with customer?
# Get the exact API call they are making.
","14/Jun/16 8:53 AM;ajayb;[~Ranjana]] do we have any more information or occurrences for this? If not, let's resolve it as not-repro.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient errors about autoscaling after Spark job causing delays,SPAR-1053,41516,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,Ranjana,Ranjana,19/May/16 12:11 PM,21/Jul/17 4:50 AM,09/Aug/17 6:03 AM,,,sql,,0,jira_escalated,,,,,,,,"Customer said there were transient errors after Spark job which  consumes 5-10 mins. 

Customer: fanatics
Command id: 25088671

12.0 (TID 2163) in 6378 ms on ip-172-31-32-226.ec2.internal (199/200)
App > 16/05/19 17:53:55 task-result-getter-3 INFO TaskSetManager: Finished task 74.0 in stage 12.0 (TID 2209) in 6538 ms on ip-172-31-45-101.ec2.internal (200/200)
App > 16/05/19 17:53:55 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 12 (parquet at DataFrameOutputProcessor.scala:17) finished in 6.563 s
App > 16/05/19 17:53:55 task-result-getter-3 INFO YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool
App > 16/05/19 17:53:55 main INFO DAGScheduler: Job 7 finished: parquet at DataFrameOutputProcessor.scala:17, took 30.562876 s
App > 16/05/19 17:56:13 main INFO LocalDirAllocator$AllocatorPerContext$DirSelector: Returning directory: /media/ephemeral0/s3/output-
App > 16/05/19 17:56:13 main ERROR NativeS3FileSystem: md5Hash for product_extract/output/dt=2016-05-13/run_id=201605140001/_SUCCESS is [-44, 29, -116, -39, -113, 0, -78, 4, -23, -128, 9, -104, -20, -8, 66, 126]
App > 16/05/19 17:56:13 main INFO DefaultWriterContainer: Job job_201605191753_0000 committed.
App > 16/05/19 17:56:13 main INFO ParquetRelation: Listing s3://fanatics-moneyball-dev/product_extract/output/dt=2016-05-13/run_id=201605140001 on driver
App > 16/05/19 17:56:13 main INFO ParquetRelation: Listing s3://fanatics-moneyball-dev/product_extract/output/dt=2016-05-13/run_id=201605140001 on driver
App > 16/05/19 17:56:13 main INFO SparkContext: sc.stop called from [SparkSubmit.successfulExitHook[success]]
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution/json,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
App > Exception in thread ""Thread-27"" 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
App > java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext
App > at org.apache.spark.SparkContext.org$apache$spark$SparkContext$$assertNotStopped(SparkContext.scala:110)
App > at org.apache.spark.SparkContext.getExecutorStorageStatus(SparkContext.scala:1611)
App > at org.apache.spark.autoscale.Autoscaler.getExecutorStorageStatus(Autoscaler.scala:370)
App > at org.apache.spark.autoscale.Autoscaler.autoscaleMemoryBased(Autoscaler.scala:327)
App > at org.apache.spark.autoscale.Autoscaler.autoscale(Autoscaler.scala:209)
App > at org.apache.spark.autoscale.Autoscaler.run(Autoscaler.scala:144)
App > at java.lang.Thread.run(Thread.java:745)
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
App > 16/05/19 17:56:13 main INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}",,bharatb,mahuja,Ranjana,venkats,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02cpn:,,,,,,,,,,,,,,,,,,,1.0,6552,2016-05-19 12:47:59.472,,,"19/May/16 12:47 PM;mahuja;[~Ranjana] - the stack/autoscaler might not be the reason for delay. The time before and after the stack does not have much time difference. Although we need to check why a call was made on stopped context.

The command id in description has runtime of ~ 8 minutes. Is it the correct command id? The job links in the zendesk ticket have runtime between 28-35 minutes. ","19/May/16 5:45 PM;mahuja;From the job link customer shared, one of the Qubole command id: 24852539

The following timestamp shows a delay of 12 minutes. Need to figure out the activity during this period.

{code}
App > 16/05/16 20:49:56 main INFO DAGScheduler: Job 7 finished: parquet at DataFrameOutputProcessor.scala:17, took 370.587828 s
App > 16/05/16 21:02:19 main INFO LocalDirAllocator$AllocatorPerContext$DirSelector: Returning directory: /media/ephemeral1/s3/output-
{code}","27/May/16 2:24 PM;venkats;[~Ranjana]
I checked the executor logs and driver logs. It seems like nothing happening there in that time between 20:49 and 21:02. Something is happening after that. It would be better if we can get to see the code of the customer to see what is that they are doing.

Would it be possible to see their code? (PullProducts class)",01/Jun/16 7:10 PM;bharatb;Usually this kind of a silent delay is S3 write operations.,"01/Jun/16 8:17 PM;mahuja;Yeah, we have same suspicion. The next operation in logs after the delay is S3 write. What we could not confirm is what is being written. Or if there is something else between executor completion and job completion (like collect etc)","07/Jun/16 2:02 PM;Ranjana;Putting following in hadoop override solved the ticket.

```
mapred.output.committer.class=org.apache.hadoop.mapred.DirectFileOutputCommitter
mapreduce.use.directfileoutputcommitter=true
spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
```

Not sure if you want to show relevant error message to the customer instructing to add the above in hadoop overrides ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test JIRA for Gayathry's testing - do not delete,QBOL-5320,41031,Bug,Reopened,QBOL,qbol,software,sumitm,,,Major,,gayathrym,venkatak,addon_zendesk_for_jira,08/May/16 9:29 PM,13/May/16 2:50 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,jira_update,,,,,,,test JIRA for Gayathry's testing,,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},NA,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z02aoj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid ec2 credentials label and warning shown for VPC clusters,QBOL-5309,40844,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,yogeshg,karthikk,karthikk,04/May/16 5:18 AM,02/Aug/17 4:48 PM,09/Aug/17 6:03 AM,,release-branch-32,,,0,jira_escalated,,,,,,,,"Steps:
# Login to qa.qubole.net
# https://qa.qubole.net/v2/control-panel#add-cluster
# In EC2 settings choose ""Same As Default Compute""
# In VPC choose vpc-cb9c6fac
# Choose either of the 2 subnets (one is public)
# Save

One can see the label, Invalid EC2 creds and also while starting the cluster we throw a warning window with message compute credentials have not been validated do you want to proceed.

But i have been able to launch such a configured cluster and run a command on it  successfully. I couldn't find much info in the logs to find out what AWS calls were made that caused us to determine Invalid EC2 creds.

Cc:[~sourabhg] [~ajayb] ",qa,karthikk,megha,nimitk,shambhavim,sureshr,yogeshg,,,,,,,,,,,,,,,,,,,,,,MW-369,TES-982,,,,,04/May/16 5:19 AM;karthikk;Screen Shot 2016-05-04 at 5.16.10 PM.png;https://qubole.atlassian.net/secure/attachment/27012/Screen+Shot+2016-05-04+at+5.16.10+PM.png,04/May/16 5:19 AM;karthikk;Screen Shot 2016-05-04 at 5.49.44 PM.png;https://qubole.atlassian.net/secure/attachment/27013/Screen+Shot+2016-05-04+at+5.49.44+PM.png,04/May/16 5:25 AM;karthikk;SuccessfulCommand_On_ClusterWithWarning.png;https://qubole.atlassian.net/secure/attachment/27015/SuccessfulCommand_On_ClusterWithWarning.png,,,,,,,,,,,,,,,,,,,,,,{},NA,,,QBOL-5183,,,,,#Empty,No,,,,,,No,,#Empty,,1|z029zn:,,,,,,,,,,,,,,,,,,,1.0,9819,2016-09-27 15:23:37.084,,,06/May/16 1:54 AM;karthikk;[~ajayb] [~sourabhg],08/May/16 10:12 PM;karthikk;[~karthikb] [~yogeshg] lets add additional logging as needed to get to the root of this problem. There isn't enough logs to determine whats going wrong and we cant add retry logic without knowing what was the request/response in the first try.,"27/Sep/16 3:23 PM;megha;[~shambhavim]
I noticed this with hooklogic clusters today.. 
The cluster config page says ""Compute Credentials not validated"" as well..
Customer uses iam roles, cluster is in vpc..
I also see ""Compute Credentials not validated"" at account level..

Any ideas if this could lead to any problems? Cluster does start successfully..
","27/Sep/16 10:52 PM;shambhavim;[~megha]
Could you please check if the customer's aws_region supports 't1.micro' instance_type, as we perform compute credentials validation using 't1.micro' as the instance_type but when the cluster is brought up the instance_type mentioned by the user is used. This might be the reason why the cluster is started successfully but the compute cred validations fail.","05/Mar/17 11:55 PM;sureshr;Maybe we can add some error handling in our validation code so that if t1.micro fails, we re-try with t2.micro. From the AWS docs, it looks like t1.micro is available in EC2 classic and t2.micro is available in VPC environments.",06/Mar/17 9:10 AM;yogeshg;I was thinking of adding a section in config for test instance details per region and found that [~sumitm] has already resolved the problem in MW-369 :) The change is already in production. If it is okay then I can close the issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FAILED_TO_UNCOMPRESS(5) errors when fetching shuffle data with sort-based shuffle,SPAR-1003,40732,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,drose@qubole.com,drose@qubole.com,01/May/16 8:45 PM,10/Jun/16 5:37 AM,09/Aug/17 6:03 AM,,,misc,,0,jira_escalated,,,,,,,,"Same paragraph completes success in stand-a-lone Spark cluster.  On Qubole Cluster it fails to complete with the above error.

notebook is device counter
datanaught cluster.

Customer is currently trying to define use on Qubole and imperative we show notebook viability rather than lose additional business to their stand-a-lone clusters.

We have permissions to clone clusters and run queries to help resolve the issues.",,ajithr,bharatb,drose@qubole.com,gayathrym,mahuja,,,,,,,,,,,,,,,,,,,,,,,,,SPAR-927,,,,,,,,,,,,,,,,,,,,,,,MobileMajority,,,,,{},NA,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z029df:,,,,,,,,,,,,,,,,,,,,,2016-05-01 23:32:19.861,,,01/May/16 11:32 PM;ajithr;This could be related to https://issues.apache.org/jira/browse/SPARK-4105,"02/May/16 4:43 PM;mahuja;Copying stack trace from ticket

{code}
val count = ids.count()
val distinctCount = ids.distinct().count()
count: Long = 993232941
org.apache.spark.SparkException: Job aborted due to stage failure: Task 170 in stage 6.0 failed 4 times, most recent failure: Lost task 170.3 in stage 6.0 (TID 6504, ip-10-10-0-94.ec2.internal): java.io.IOException: FAILED_TO_UNCOMPRESS(5)
at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:465)
at org.xerial.snappy.Snappy.uncompress(Snappy.java:504)
at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:422)
at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:182)
at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:163)
at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2310)
at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2323)
at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:64)
at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:64)
at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:123)
at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:64)
at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:60)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:58)
at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:83)
at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1458)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1446)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1445)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:826)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:826)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:826)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1626)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1615)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:626)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1943)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1956)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1969)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2040)
at org.apache.spark.rdd.RDD.count(RDD.scala:1143)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:85)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:92)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:94)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:96)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:98)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:100)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:102)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:104)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:106)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:108)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:110)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:112)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:114)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:116)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:118)
at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:120)
at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:122)
at $iwC$$iwC$$iwC$$iwC.<init>(<console>:124)
at $iwC$$iwC$$iwC.<init>(<console>:126)
at $iwC$$iwC.<init>(<console>:128)
at $iwC.<init>(<console>:130)
at <init>(<console>:132)
at .<init>(<console>:136)
at .<clinit>(<console>)
at .<init>(<console>:7)
at .<clinit>(<console>)
at $print(<console>)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
at org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:717)
at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:679)
at org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:672)
at org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)
at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:102)
at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:299)
at org.apache.zeppelin.scheduler.Job.run(Job.java:173)
at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:141)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:465)
at org.xerial.snappy.Snappy.uncompress(Snappy.java:504)
at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:422)
at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:182)
at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:163)
at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2310)
at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2323)
at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:64)
at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:64)
at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:123)
at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:64)
at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:60)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:58)
at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:83)
at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
at org.apache.spark.scheduler.Task.run(Task.scala:89)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
... 3 more
{code}",02/May/16 4:46 PM;mahuja;SPAR-927 is similar. It is also suggests that restart of the cluster helped.,"03/May/16 11:17 AM;mahuja;Using more information in SPAR-927. The same job executed before this event and after a restart of cluster. So I will not doubt the setup and jars present in the cluster. Wild guesses: corrupted cache or corrupted shuffled data etc.? cc - [~bharatb]

As noted in SPARK-4105, it can affect standalone deployment as well. Since after memory tuning the performance is similar for Yarn and standalone deployment, we should continue to pitch that there is no significant difference. Unless there is an example which we should definitely look at.","19/May/16 6:35 AM;bharatb;We have seen the same error happen even when trying to load the spark UI. The spark eventlogs are stored in snappy format and the spark history server runs into a problem when reading this file to render the spark UI. I think this is not restricted to shuffle files but a very generic snappy format reading issue. Most likely because hadoop's snappy lib, hive's snappy lib, spark's snappy lib and god knows which other snappy lib are fighting for top spot in the class path. This is just a hunch though.","10/Jun/16 3:06 AM;gayathrym;[~mahuja] [~bharatb] : Pinging as this is a jira escalated blocker. We have RB33-CP1 in progress now, if you want to merge this fix. ","10/Jun/16 5:29 AM;ajithr;Customer is no longer seeing this issue. Priority has been reduced to ""major"" now. But, this problem may recur. So, let us continue to investigate this.  If it's as per [~bharatb]'s hypothesis then, please let us know if it'd help to have classpaths of various daemons / components on the system.","10/Jun/16 5:37 AM;gayathrym;Thanks [~ajithr]. I think if the customer is no longer seeing this issue, we can probably close this ticket now and Re-open later if the customer faces it again? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Scala code throws Hive-Exception,SPAR-995,40610,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,laks,addon_zendesk_for_jira,27/Apr/16 5:34 AM,20/Jul/17 7:07 PM,09/Aug/17 6:03 AM,,,sql,,0,jira_escalated,,,,,,,,"A script based on an earlier setup, works great until Friday. Since Monday it is failing. 

job: 23073067

```
complains about: Hive version etc.

App > Exception in thread ""main"" java.lang.ClassNotFoundException: java.lang.NoClassDefFoundError: org/apache/hadoop/util/VersionInfo when creating Hive client using classpath: file:/media/ephemeral0/spark/spark-src-04-24-16-1461539572-333cd95b/$(/usr/lib/hadoop2/bin/hadoop%20classpath), file:/usr/lib/spark/lib/hive2/hive-serde-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-shims-0.20-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-hcatalog-server-extensions-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-ant-0.13.1.jar, file:/usr/lib/spark/lib/hive2/libfb303-0.9.0.jar, file:/usr/lib/spark/lib/hive2/hive-contrib-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-qhs-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-metastore-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-shims-common-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-testutils-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-shims-common-secure-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-service-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-shims-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-exec-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-jdbc-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-shims-0.23-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-cli-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-webhcat-java-client-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-beeline-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-hcatalog-streaming-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-common-0.13.1.jar, file:/usr/lib/spark/lib/hive2/mysql-connector-java-5.1.34.jar, file:/usr/lib/spark/lib/hive2/hive-qubole-0.13.1.jar, file:/usr/lib/spark/lib/hive2/csv-serde-1.1.2.jar, file:/usr/lib/spark/lib/hive2/hive-webhcat-0.13.1.jar, file:/usr/lib/spark/lib/hive2/antlr-runtime-3.4.jar, file:/usr/lib/spark/lib/hive2/hive-hcatalog-core-0.13.1.jar, file:/usr/lib/spark/lib/hive2/hive-shims-0.20S-0.13.1.jar, file:/usr/lib/spark/lib/hive2/ST4-4.0.4.jar, file:/usr/lib/spark/lib/hive2/hive-hcatalog-pig-adapter-0.13.1.jar

App > Please make sure that jars for your version of hive and hadoop are included in the paths passed to SQLConfEntry(key = spark.sql.hive.metastore.jars, defaultValue=builtin, doc=

App > Location of the jars that should be used to instantiate the HiveMetastoreClient.

App > This property can be one of three options: ""

App > 1. ""builtin""

App > Use Hive 1.2.1, which is bundled with the Spark assembly jar when

App > -Phive is enabled. When this option is chosen,

App > spark.sql.hive.metastore.version must be either

App > 1.2.1 or not defined.

App > 2. ""maven""

App > Use Hive jars of specified version downloaded from Maven repositories.

App > 3. A classpath in the standard format for both Hive and Hadoop.

App > , isPublic = true).

App > at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:189)

App > at org.apache.spark.sql.hive.client.IsolatedClientLoader.(IsolatedClientLoader.scala:179)

App > at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:264)

App > at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:186)

App > at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:393)

App > at org.apache.spark.sql.SQLContext$$anonfun$5.apply(SQLContext.scala:229)

App > at org.apache.spark.sql.SQLContext$$anonfun$5.apply(SQLContext.scala:228)

App > at scala.collection.Iterator$class.foreach(Iterator.scala:727)

App > at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)

App > at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)

App > at scala.collection.AbstractIterable.foreach(Iterable.scala:54)

App > at org.apache.spark.sql.SQLContext.(SQLContext.scala:228)

App > at org.apache.spark.sql.hive.HiveContext.(HiveContext.scala:72)

App > at ExpressenCase$.main(script.scala:15)

App > at ExpressenCase.main(script.scala)

App > at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

App > at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

App > at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

App > at java.lang.reflect.Method.invoke(Method.java:606)

App > at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:700)

App > at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:183)

App > at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)

App > at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:123)

App > at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

App > 16/04/24 23:13:43 sparkDriver-akka.actor.default-dispatcher-15 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
```

Also tried removing the flags (--conf spark.sql.hive.metastore.version=0.13.1 --conf spark.sql.hive.metastore.jars=""$(/usr/lib/hadoop2/bin/hadoop classpath):/usr/lib/spark/lib/hive2/*"") from Spark Submit Command Line Options but still this fails. 

No changes to the cluster, code etc. Also I believe there was no changes pertaining to last release. ",,addon_zendesk_for_jira,laks,mahuja,vishalg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z028ub:,,,,,,,,,,,,,,,,,,,,,2016-04-27 08:02:16.641,,,27/Apr/16 5:35 AM;laks;[~vishalg] Command ID 21357643 which succeeded on Friday (even with the spark command line options),"27/Apr/16 8:02 AM;vishalg;They seems to be using a very old package of spark.

Is it possible to update their cluster on a newer package/version [ remove bootstrap commands and set spark version in UI ] :
- Recommended version from our side is 1.5.1. 
- They seem to be using 1.5.0. If they do not want to increment 1.5.0, can we move them on the latest 1.5.0 ?
- If this is not possible, should we debug this issue on the custom-spark-package that they are using as of now ? 

----------------------------------------------------------------


The command without arugments seems to be : https://api.qubole.com/v2/analyze?command_id=23287244&qbol_user_id=8768

Also, their cluster 11254 contains bootstrap s3://expressen/scripts/hadoop/node_bootstrap_spark1.5.sh contains :


{code:java}
hdfs dfs -get s3://paid-qubole/spark/scripts/install_spark_1.5.0 && bash install_spark_1.5.0 spark-1.5-rc4.tar.gz
hdfs dfs -get s3://paid-qubole/zeppelin/scripts/install_zeppelin-1.4.0 && bash install_zeppelin-1.4.0 zeppelin-1.5-rc0.tar.gz

{code}


{code:java}

➜  chef-repo git:(release-branch-31) /Users/qubole/Documents/vbox/qubole/qbol/aws-tools/s3cmd-1.0.1/s3cmd -c /Users/qubole/Documents/vbox/qubole/s3cfg ls s3://paid-qubole/spark/packages/spark-1.5         
2015-09-11 02:14 220729900   s3://paid-qubole/spark/packages/spark-1.5-rc3.tar.gz
2015-09-15 15:49 281136504   s3://paid-qubole/spark/packages/spark-1.5-rc4.tar.gz
2015-12-10 08:04 165504294   s3://paid-qubole/spark/packages/spark-1.5.1-scala-2.11-SPAR-476.tar.gz
{code}
","27/Apr/16 1:33 PM;mahuja;The following in command line options requires command substitution and I doubt that is supported.
{code}
spark.sql.hive.metastore.jars=""$(/usr/lib/hadoop2/bin/hadoop classpath):/usr/lib/spark/lib/hive2/*""
{code}

I expanded the value in a new command and it is making progress with command id 23313575 (https://api.qubole.com/v2/analyze?command_id=23313575&qbol_user_id=8768). New value:
{code}
spark.sql.hive.metastore.jars=""/usr/lib/qubole/packages/hadoop2/hadoop2/etc/hadoop:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/common/lib/*:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/common/*:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/hdfs:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/hdfs/lib/*:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/hdfs/*:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/yarn/lib/*:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/yarn/*:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/mapreduce/*:/share/hadoop/tools:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/tools/lib/*:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/tools/*:/share/hadoop/qubole:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/qubole/lib/*:/usr/lib/qubole/packages/hadoop2/hadoop2/share/hadoop/qubole/*:/contrib/capacity-scheduler/*.jar:/usr/lib/spark/lib/hive2/*""
{code}

Still a mystery how earlier command id 21357643 worked (https://api.qubole.com/v2/analyze?command_id=21357643&qbol_user_id=8768)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Tail this file"" link doesn't work and gives HTTP ERROR 403. ",ACM-256,40242,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,laks,addon_zendesk_for_jira,21/Apr/16 3:17 AM,07/Jul/17 2:30 PM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"Most of the links in the log display UI (DFS status) give HTTP ERROR 403.

Follow the below steps to replicate the issue. 
1. Click the DFS Status link in the cluster page.
2. Click on ""Browse the filesystem""
3. Click on ""tmp"" link from the Contents of directory list shown.
4. Now randomly select a file (Ex: 02e73b5a261845b89a74f84d1e50047e) 
5. Scroll down a little and click ""tail this file"" option and it shows HTTP ERROR 404. 

Also other options like ""Go back to DFS Page"", ""view next chunk"" also shows HTTP ERROR 404. Is this expected message?",,addon_zendesk_for_jira,ajayb,aswina,drose@qubole.com,p.vasa,sbadam,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z027o3:,,,,,,,,,,,,,,,,,,,1.0,6026,2016-04-21 06:33:17.986,,,"21/Apr/16 6:33 AM;aswina;Can someone from ACM team look into this?

cc: [~ajayb] [~abhishekmodi] [~prakharj] [~sourabhg]",21/Apr/16 8:33 AM;aswina;Moved it to ACM queue.,17/Aug/16 6:20 PM;p.vasa;Hello [~ajayb] Can we have an ETA on this so that we can go ahead and notify the customer? Thanks.,01/Sep/16 2:38 PM;drose@qubole.com;[~xing] A part of our discussion with Ensighten.,"11/Jan/17 3:49 PM;sbadam;Hey Ajay, can we have any update or ETA for this issue? Thanks.","15/Mar/17 10:17 PM;venkatak;Hi [~ajayb] Any ETA on this issue?
",15/Mar/17 11:32 PM;ajayb;[~venkatak] no one has looked at this issue yet. Is this affecting customers?,"15/Mar/17 11:41 PM;venkatak;Hi [~ajayb] I do not think the customer is blocked, but has been following up for ETA. If we can come up with some tentative ETA - sooner or later might be fine",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig11( hadoop2 ) 'LOAD' Poor Perf due to un-optimized S3 listing,QPIG-52,40095,Bug,Open,QPIG,qbol pig,software,bharatb,,,Critical,,beria,adubey,adubey,15/Apr/16 3:55 PM,22/Jul/17 11:31 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"cc: [~mpatel] [~drose@qubole.com] - AudienceScience reported this. I gave them the workaround but not sure if they have many scripts with * .

+Code:
+
request =
load 's3://49ers.qubole.com/000Adubey/testpigs3/ds=2016-03-01/*'
using PigStorage('\t');
dump request;
--------------

Problem:

Wildcard char in location, without that it works fine in hadoop2

repro:
hadoop2: https://api.qubole.com/v2/analyze?command_id=22417938&qbol_user_id=1395 ( bad command with * ) - check out logs it stays initially for 10+ mins in S3 listing .. ( thread dump attache )

hadoop2: https://api.qubole.com/v2/analyze?command_id=22417948&qbol_user_id=1395 ( without * ) runs very fast - no bottleneck on S3 listing

hadoop1: Same bad script runs fast on hadoop1/pig11: https://api.qubole.com/v2/analyze?command_id=22417931&qbol_user_id=1395

cc: [~jssarma] I am not sure but i remember we did some optimization in our pig8 code especially for these regex related expressions, not sure if some of that can be applied here.",,adubey,amoghm,beria,bharatb,sureshr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/May/16 8:54 PM;adubey;CNBC DISRUPTOR 50 2015 B&W BADGE.PNG;https://qubole.atlassian.net/secure/attachment/27309/CNBC+DISRUPTOR+50+2015+B%26W+BADGE.PNG,15/Apr/16 4:11 PM;adubey;audiencescience_pig;https://qubole.atlassian.net/secure/attachment/26555/audiencescience_pig,,,,,,,,,,,,,,,,,,Oracle,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z026yj:,,,,,,,,,,,,,,,,,,,1.0,11571,2016-04-25 10:39:38.436,,,23/Apr/16 10:37 AM;adubey;[~beria] did you get a chance to test and look into this. ,"25/Apr/16 10:39 AM;beria;No, was mostly looking into QPIG-41 . Need to sync with [~bharatb] and will get back to it.","11/May/16 8:47 AM;sureshr;In the solutions sync-up call tonight, the solutions team wanted to get an ETA on this. [~bharatb] Could you please investigate?","11/May/16 6:16 PM;bharatb;Currently we are not looking into this. [~beria] is on Quack this week and most likely he will not have time to look at it this week. If we take it up next week, I am expecting 2 days to debug and depending on the investigation results we can see how quickly we can fix it. If it requires full IPP integration it will be non-trivial and will take weeks of effort. How important is this? [~adubey]","11/May/16 8:54 PM;adubey;It's fine if you can look next week . It did not come from Customer side -
I noticed and provided them a workaround but it will be good to investigate
and fix

On Wednesday, May 11, 2016, Bharat Bhushan (JIRA) <jira@qubole.atlassian.net>



-- 

Thanks,
Ashish Dubey
Solutions Architect | +1-510-298-9147
Follow us on @Qubole <https://twitter.com/qubole> | Qubole
<http://www.qubole.com/> - The Best Big Data Platform on the Cloud.

Qubole has been named to @CNBC’s ultimate list of disruptive companies! See
the whole list: disruptor50.cnbc.com
","12/Dec/16 8:37 PM;adubey;[~beria] do you know if there is an owner assigned for pig?
I hit this issue again with an important POC","12/Dec/16 8:49 PM;adubey;cc [~amoghm] for a quick review ..

46483195
46486701
This is listing a path s3://bk-pixel-logs/$DATE/*/*.gz

Last time my observation was - s3://49ers.qubole.com/000Adubey/testpigs3/ds=2016-03-01/* was very slow but s3://49ers.qubole.com/000Adubey/testpigs3/ds=2016-03-01/ was quicker .. anything quick you can think why this will be the case.. i see IPP param is being passed..



","12/Dec/16 10:51 PM;beria;[~adubey] Not aware of any pig owner. I could look into it, although I am doing ZEP-654, and not sure how large this effort is. Will need to sync with [~bharatb] / [~amoghm] to start looking into it.
cc [~mahuja]","12/Dec/16 10:53 PM;adubey;Here i am trying to use HCat to force IPP using hive - will continue this tomorrow. In a quick experiment it seemed it worked but will confirm later. 
however we must fix this issue asap now..

","13/Dec/16 2:35 AM;amoghm;Hi [~adubey],  
[~beria] and me synced up on this. I think this has to do with Glob status optimisation which was done in hadoop1 to fb-hadoop-20/src/core/org/apache/hadoop/fs/FileSystem.java, instead of IPP. There seems to be 2 issues here:

1. This was not ported in hadoop2 because there were some significant differences. I suspect that may be one of the reason.
2. flag used in code is seems to be slightly different than that shown in logs of 46483195: 

{code:java}
    if (!conf.getBoolean(""mapred.job.natives3filesystem.globstatus.use"", false)
        || countRegexStar(pathPattern) < S3_MIN_REGEX_STAR) {
      return super.globStatus(patternPath, filter);
    } 
{code}

Logs:
{code:java}
!mapreduce.job.natives3filesystem.globstatus.use=dHJ1ZQ==
{code}

cc [~hiyer] [~abhishekmodi]",13/Dec/16 5:11 PM;adubey;side note here - in this specific case using HCat worked fine for me but again this issue is still a risk for us because in some cases we might not be able to use hive table structure.,14/Dec/16 6:21 AM;beria;[~adubey] [~amoghm] should this Jira be moved to Hadoop team and the feature has not been ported into hadoop2?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pinterest cluster throwing 500 when data is copied using HFTP://,HAD-577,40051,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,hiyer,adubey,adubey,14/Apr/16 6:42 PM,11/Jul/17 2:56 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"cc [~abhishekmodi] [~venkatak] [~mpatel]

i see this 
 hadoop fs -get hftp://ip-10-1-6-62.ec2.internal:50070/tmp/b.txt /tmp/
get: Server returned HTTP response code: 500 for URL: http://ip-10-1-12-94.ec2.internal:50075/streamFile/tmp/b.txt?ugi=ec2-user,ec2-user,wheel&ugi=ec2-user,ec2-user,wheel&candidates=10.1.4.191:50075&nnaddr=qubole-datacore-319be4a9.ec2.pin220.com:9000

I briefly looked into NN logs but did not see anything :

I looked into DN log but nothing pops up in the logs when i hit above command..



=======
In fact if i tried couple of times command got hung and here is the thread dump of it:

I verified socket is established - not sure why DN is responding here


=========
JVM version is 24.65-b04
Deadlock Detection:

No deadlocks found.

Thread 58715: (state = BLOCKED)


Thread 58714: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.ref.ReferenceQueue.remove(long) @bci=44, line=135 (Interpreted frame)
 - java.lang.ref.ReferenceQueue.remove() @bci=2, line=151 (Interpreted frame)
 - java.lang.ref.Finalizer$FinalizerThread.run() @bci=36, line=209 (Interpreted frame)


Thread 58713: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.Object.wait() @bci=2, line=503 (Interpreted frame)
 - java.lang.ref.Reference$ReferenceHandler.run() @bci=46, line=133 (Interpreted frame)


Thread 58688: (state = IN_NATIVE)
 - java.net.SocketInputStream.socketRead0(java.io.FileDescriptor, byte[], int, int, int) @bci=0 (Interpreted frame)
 - java.net.SocketInputStream.read(byte[], int, int, int) @bci=87, line=152 (Interpreted frame)
 - java.net.SocketInputStream.read(byte[], int, int) @bci=11, line=122 (Interpreted frame)
 - java.io.BufferedInputStream.fill() @bci=175, line=235 (Interpreted frame)
 - java.io.BufferedInputStream.read1(byte[], int, int) @bci=44, line=275 (Interpreted frame)
 - java.io.BufferedInputStream.read(byte[], int, int) @bci=49, line=334 (Interpreted frame)
 - sun.net.www.http.HttpClient.parseHTTPHeader(sun.net.www.MessageHeader, sun.net.ProgressSource, sun.net.www.protocol.http.HttpURLConnection) @bci=51, line=687 (Interpreted frame)
 - sun.net.www.http.HttpClient.parseHTTP(sun.net.www.MessageHeader, sun.net.ProgressSource, sun.net.www.protocol.http.HttpURLConnection) @bci=56, line=633 (Interpreted frame)
 - sun.net.www.protocol.http.HttpURLConnection.getInputStream() @bci=327, line=1323 (Interpreted frame)
 - sun.net.www.protocol.http.HttpURLConnection.getHeaderField(java.lang.String) @bci=1, line=2678 (Interpreted frame)
 - org.apache.hadoop.hdfs.HftpFileSystem$HftpInputStream.openStream(java.lang.String, java.lang.String) @bci=23, line=234 (Interpreted frame)
 - org.apache.hadoop.hdfs.HftpFileSystem$HftpInputStream.<init>(org.apache.hadoop.hdfs.HftpFileSystem, org.apache.hadoop.fs.Path) @bci=67, line=160 (Interpreted frame)
 - org.apache.hadoop.hdfs.HftpFileSystem.open(org.apache.hadoop.fs.Path, int) @bci=6, line=139 (Interpreted frame)
 - org.apache.hadoop.fs.FileSystem.open(org.apache.hadoop.fs.Path) @bci=14, line=415 (Interpreted frame)
 - org.apache.hadoop.fs.FileUtil.copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.io.File, boolean, org.apache.hadoop.conf.Configuration) @bci=84, line=349 (Interpreted frame)
 - org.apache.hadoop.fs.FsShell.copyToLocal(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.io.File, boolean) @bci=71, line=276 (Interpreted frame)
 - org.apache.hadoop.fs.FsShell.copyToLocal(java.lang.String[], int) @bci=302, line=227 (Interpreted frame)
 - org.apache.hadoop.fs.FsShell.run(java.lang.String[]) @bci=486, line=1972 (Interpreted frame)
 - org.apache.hadoop.util.ToolRunner.run(org.apache.hadoop.conf.Configuration, org.apache.hadoop.util.Tool, java.lang.String[]) @bci=38, line=65 (Interpreted frame)
 - org.apache.hadoop.util.ToolRunner.run(org.apache.hadoop.util.Tool, java.lang.String[]) @bci=8, line=79 (Interpreted frame)
 - org.apache.hadoop.fs.FsShell.main(java.lang.String[]) @bci=10, line=2118 (Interpreted frame)

",,abhishekmodi,adubey,hiyer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,pinterest,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z026or:,,,,,,,,,,,,,,,,,,,1.0,5881,2016-04-14 22:26:59.328,,,"14/Apr/16 10:26 PM;abhishekmodi;Hari, Could you please take a look at this.",15/Apr/16 7:24 AM;adubey;[~hiyer] did you get time to look into it..,"18/Apr/16 3:43 AM;hiyer;[~adubey] what's the cluster id? hftp seems to be working in general, so need to check if there's something specific to this cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autodesk Command not finishing,SPAR-968,40024,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,vishalg,vishalg,14/Apr/16 1:11 AM,20/Jul/17 8:58 PM,09/Aug/17 6:03 AM,,,sql,,1,jira_escalated,,,,,,,,"Autodesk jobs seems to be running long.. The thread dump for the executor shows timed_waiting for many threads.

Application id: application_1458030112830_11394 , application_1458030112830_11393
Command id : 22267352",,bharatb,drose@qubole.com,laks,mahuja,venkatak,vishalg,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Apr/16 1:17 AM;vishalg;driver-threaddump.txt;https://qubole.atlassian.net/secure/attachment/26530/driver-threaddump.txt,14/Apr/16 1:15 AM;vishalg;executor8-threaddump.txt;https://qubole.atlassian.net/secure/attachment/26528/executor8-threaddump.txt,14/Apr/16 1:16 AM;vishalg;executor9-threaddump.txt;https://qubole.atlassian.net/secure/attachment/26529/executor9-threaddump.txt,,,,,,,,,,,,,,,,,autodesk,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z026j7:,,,,,,,,,,,,,,,,,,,1.0,5864,2016-04-15 03:14:27.797,,,"14/Apr/16 1:12 AM;vishalg;there seems to be a lot of s3 activity :


{code:java}
    ip-10-0-9-146.us-west-1.compute.internal.58100 > s3-us-west-1-w.amazonaws.com.http: Flags [.], cksum 0x36bb (incorrect -> 0x6446), seq 9257, ack 9998, win 428, length 0
08:07:16.537540 IP (tos 0x0, ttl 255, id 2135, offset 0, flags [DF], proto TCP (6), length 752)
    ip-10-0-9-146.us-west-1.compute.internal.58100 > s3-us-west-1-w.amazonaws.com.http: Flags [P.], cksum 0x3983 (incorrect -> 0x73d2), seq 9257:9969, ack 9998, win 428, length 712
08:07:16.555748 IP (tos 0x0, ttl 59, id 18181, offset 0, flags [DF], proto TCP (6), length 385)
    s3-us-west-1-w.amazonaws.com.http > ip-10-0-9-146.us-west-1.compute.internal.58100: Flags [P.], cksum 0x28fa (correct), seq 9998:10343, ack 9969, win 135, length 345
08:07:16.555759 IP (tos 0x0, ttl 59, id 18182, offset 0, flags [DF], proto TCP (6), length 464)
    s3-us-west-1-w.amazonaws.com.http > ip-10-0-9-146.us-west-1.compute.internal.58100: Flags [P.], cksum 0x76fd (correct), seq 10343:10767, ack 9969, win 135, length 424
08:07:16.555763 IP (tos 0x0, ttl 255, id 2136, offset 0, flags [DF], proto TCP (6), length 40)
    ip-10-0-9-146.us-west-1.compute.internal.58100 > s3-us-west-1-w.amazonaws.com.http: Flags [.], cksum 0x36bb (incorrect -> 0x5e6c), seq 9969, ack 10767, win 445, length 0

{code}


Suspicious threads in executors :

{code:java}
Thread 54: MultiThreadedHttpConnectionManager cleanup (WAITING)
java.lang.Object.wait(Native Method)
java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ReferenceQueueThread.run(MultiThreadedHttpConnectionManager.java:1122)
{code}
","14/Apr/16 1:17 AM;vishalg;it is some custom code they are runnin, check with autodesk ?

cc [~laks]",15/Apr/16 3:14 AM;laks;[~vishalg] Customer confirm that those suspicious threads doesn't appear to come from their code ,"20/Apr/16 2:39 AM;vishalg;[~laks]

are there writes pending into s3 ? have all parts of code completed ?

cc [~bharatb]",21/Apr/16 5:24 AM;laks;[~vishalg] Yes the command 22267352 completed. Doesn't look like any writes pending. ,21/Apr/16 8:36 PM;venkatak;[~vishalg]any update on this?,"22/Apr/16 1:31 AM;vishalg;[~venkatak], [~laks] : is it possible to get the custom code ?

Also, since there was a lot of s3 activity, are we sure that s3 operations completed ?",25/Apr/16 8:55 PM;drose@qubole.com;[~vishalg]I requested additional information and they stated that initial custom code is in the ticket. They are concerned that this issue is going to cause many instances of of jobs being extended multiple hour. is there anything your team needs additional to dig further into this issue?,26/Apr/16 1:26 AM;bharatb;[~drose@qubole.com] the ticket does not have the custom code. So we will need that.,"29/Apr/16 2:00 PM;drose@qubole.com;[~mahuja][~bharatb][~adubey] notes from the ticket on impact and issue. ""This is a regular application for us and runs every hour.

The issue is intermittent and is not related to any specific code or data set.

In this case, all the tasks had completed but apparently at the time of ‘cleanup operation’ for the job; spark context got stuck.

May be you can take a look at the executor logs and spark context logs to see what exactly went wrong for this application.

Please let us know what’s the outcome of analysis of executor logs.

In terms of impact, as the job gets stuck, our workflow gets stuck and it does not allow subsequent hourly jobs to run.""

this is currently their top prioirity item.

The code has been requested but from Sarang's notes, do we have what we need?",29/Apr/16 3:25 PM;mahuja;[~bharatb] can you please check if we have the required information.,"12/Jun/16 9:40 PM;drose@qubole.com;[~bharatb][~mahuja] Do we still need copy of the code, for this testing?
",28/Jun/16 10:26 AM;drose@qubole.com;[~bharatb][~mahuja]do we think this is a tuning issue or something we need to test further?,"28/Jun/16 7:28 PM;bharatb;I think it is just a question of enabling DFOC params via cluster config overrides. Basically they are probably writing back to s3 at the end of the job (last line of code) and that can be slow and may not give any indications/logs. From [~vishalg]'s analysis it appears that this is indeed the case.

Can we make sure they have enabled it at the cluster level and that they have enabled all the three params we document? If not, then we should ask them to enable it. If already enabled, then we need to take another look at these jobs.",08/Jul/16 11:30 AM;mahuja;[~drose@qubole.com] - can we take the information above (Bharat's comment) to Autodesk. The ticket seems to have diverged with other issues related to tuning.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Account creation API ( FROM UI ) creates a test ec2 in default VPC ,ACM-234,39842,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,Harsh,adubey,addon_zendesk_for_jira,08/Apr/16 12:59 PM,06/Jul/17 3:47 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"cc [~mahuja]
Since new AWS accounts might not have one so it fails.

i will share more details if needed but  wanted to create this jira quickly",,addon_zendesk_for_jira,Harsh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{repository={count=7, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":7,""lastUpdated"":""2016-04-19T06:25:12.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":7,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z025kb:,,,,,,,,,,,,,,,,,,,1.0,5762,2016-05-13 00:26:33.858,,,"13/May/16 12:26 AM;Harsh;If the creation fails, what happens? Is it the failure message which disturbs user or we are setting incorrect configuration to start with?","14/Dec/16 1:37 PM;Harsh;The issue happened is :
A customer deleted the default VPC from the new AWS account (not EC2 classic). Now in such scenario, if we want to bring up a machine in the account, it would mean we have to specify some VPC id. 
So while validating credentials, there is an assumption that default VPC is around for non EC2-classic accounts. That assumption is broken by few customers account. 

Options to solve:
1. We ask customer to create a default VPC or make a VPC default.
2. In such cases, where we face VPC id not specified error, we attempt to create instance by list all VPC and selecting the first one. In case of error, show it to user what we attempted to do.
3. Open a pop-up where user gets to select a VPC for such accounts. 

3rd is the most ideal option here. But I would suggest to have 2nd option as workaround. In cases of some bring-up failure in the VPC we selected, we can ask them in error message to make/create a default VPC.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to drop external table - after deleting files from s3,HIVE-1348,39821,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Major,,psrinivas,megha,megha,07/Apr/16 5:44 PM,22/Jun/17 1:23 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Cmd id: 21903284 is an example of this.

For following query ids please note, user had renamed table to EMAIL_FACT_CORRUPT after he wasnt able to drop it anymore.

User had gotten rid of the underlying base folder in S3 for this table. Now, any attempt to drop this gets timed out or runs forever. I even tried ""INSERT OVERWRITE but no luck (runs forever). Here are the commandIDs user tried:

21752981 --> DROP command that timed out yesterday 
21815520 --> DROP Aborted after it ran for a while 
21761351 --> INSERT OVERWRITE that I tried yesterday... Ran all day without any result..



",,asomani,gmargabanthu,megha,mpatel,psrinivas,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{repository={count=1, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":1,""lastUpdated"":""2010-06-03T10:15:26.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z025fn:,,,,,,,,,,,,,,,,,,,1.0,5709,2016-04-12 01:04:37.922,,,"07/Apr/16 5:45 PM;megha;logs:

./hive1.2/hive.log:2016-04-07 23:41:55,685 qid21903284 WARN  metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2416)) - Direct SQL failed, falling back to ORM
./hive1.2/hive.log:2016-04-07 23:41:56,411 qid21903284 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(151)) - Error happens in method drop_table_with_environment_context: MetaException(message:Timeout when executing method: drop_table_with_environment_context)
./hive1.2/hive.log:2016-04-07 23:41:56,452 qid21903284 ERROR exec.DDLTask (DDLTask.java:failed(541)) - org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Timeout when executing method: drop_table_with_environment_context)
","12/Apr/16 1:04 AM;asomani;Was this a managed table or an external table? For a managed table the destination folder gets decided automatically and is managed by hive. Only when dropping a managed table should hive try to go to s3 and look at the files there. So I assume this was a managed table for which the user when ahead and deleted the data/directory themselves?

PS: I went ahead and checked this table, it says it is an external table and not a managed table. So there is a deeper issue here.
","03/Oct/16 2:46 PM;mpatel;[~asomani], [~megha] ping on this .. is this still an issue for the customer? ","17/Oct/16 6:16 PM;gmargabanthu;The customer has renamed the table and created a new table - but, we should still fix it, correct? [~psrinivas]","11/Jan/17 3:39 PM;sbadam;Hey Pavan, can we have any update on this issue? Thanks.","19/Jan/17 3:07 PM;psrinivas;[~megha], [~sbadam]: 

This is very old issue. Is this still relevant? 

From the logs above, it seems that it timed-out during the cleanup of the underlying data: 
{code}
Timeout when executing method: drop_table_with_environment_context
{code}

There are other jiras similar: https://qubole.atlassian.net/browse/HIVE-1723. Can we close this as duplicate? 

","30/Jan/17 11:10 AM;megha;[~psrinivas]
In this case, it does seem it timed out during cleaning up, however actual issue was that they cleaned up underlying s3 data, after which drop just kept timing out (there was no data to clean up)..
So it is not related to hive-1723.
This needs to be kept open...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark scala classname auto-detection causing issues for scripts containing keyword: ""object""",SPAR-943,39512,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,megha,megha,31/Mar/16 11:03 AM,20/Jul/17 9:55 PM,09/Aug/17 6:03 AM,,,analyze,,0,jira_escalated,,,,,,,,"Spark scala command fails when the script contains ""object"" keyword, and --class is not specified in command line options.

For script with no explicit class defination, ""--class generated""  needs to be specified to make it run correctly.
failed: 21386256
passed: 21442719

For script with explicit class defination, ""--class <classname>"" needs to be specified to make it run correctly
failed: 21443122
passed: 21443181


",,megha,pseluka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,thomsonreuters,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z024gz:,,,,,,,,,,,,,,,,,,,1.0,5589,2016-07-07 11:05:44.817,,,07/Jul/16 10:30 AM;megha;Do we have an update here? [~pseluka],07/Jul/16 11:05 AM;pseluka;[~megha] I fixed a related JIRA which was more strict about auto-finding class names. It should be fixed. Can you try it now - and let know if there are any issues.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Percentile UDAF exception and Heap space issue,HIVE-1343,39502,Bug,Open,HIVE,qbol hive,software,psrinivas,,,Minor,,nitink,monikak,monikak,31/Mar/16 3:25 AM,01/Jun/17 2:55 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Pinterest is facing a hive issue where one of there query started failing after successful run for more than 6months  :

https://api.qubole.com/v2/analyze?command_id=21305445&qbol_user_id=4693

Ticket : https://qubole.zendesk.com/agent/tickets/5518

Different task urls and there logs:

- https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fip-10-1-10-85.ec2.internal%3A50060%2Ftasklog%3Ftaskid%3Dattempt_247.201602230427_44879_r_000002_1%26all%3Dtrue&clusterInst=136335

Logs: 
{code}

2016-03-29 21:43:45,099 qid21305445 INFO s3native.NativeS3FileSystem:107 (main): setting Progress to null comment clearing progress from taskDone
2016-03-29 21:43:45,102 qid21305445 WARN mapred.Child:246 (main): Error running child
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""_col0"":""8b09e4731753"",""_col1"":""enabled"",""_col2"":""VN"",""_col3"":""android_tablet""},""value"":{""_col0"":14,""_col1"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.5]},""_col2"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.8]},""_col3"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.9]}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:283)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425)
	at org.apache.hadoop.mapred.Child$3.run(Child.java:205)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""_col0"":""8b09e4731753"",""_col1"":""enabled"",""_col2"":""VN"",""_col3"":""android_tablet""},""value"":{""_col0"":14,""_col1"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.5]},""_col2"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.8]},""_col3"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.9]}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:271)
	... 3 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate()  on object org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator@1ebf4391 of class org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator with arguments {} of size 0
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1303)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge$GenericUDAFBridgeEvaluator.terminate(GenericUDAFBridge.java:182)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.evaluate(GenericUDAFEvaluator.java:201)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:1061)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:875)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:737)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:803)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:262)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1279)
	... 10 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.util.TimSort.ensureCapacity(TimSort.java:902)
	at java.util.TimSort.mergeLo(TimSort.java:655)
	at java.util.TimSort.mergeAt(TimSort.java:483)
	at java.util.TimSort.mergeCollapse(TimSort.java:410)
	at java.util.TimSort.sort(TimSort.java:214)
	at java.util.TimSort.sort(TimSort.java:173)
	at java.util.Arrays.sort(Arrays.java:659)
	at java.util.Collections.sort(Collections.java:217)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate(UDAFPercentile.java:201)
	... 14 more
{code}


- https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fip-10-1-11-45.ec2.internal%3A50060%2Ftasklog%3Ftaskid%3Dattempt_247.201602230427_44879_r_000002_0%26all%3Dtrue&clusterInst=136335

Logs:
{code}
2016-03-29 21:34:08,734 -1 FATAL ExecReducer:282 (Task Thread for Task: attempt_247.201602230427_44879_r_000002_0): org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""_col0"":""8b09e4731753"",""_col1"":""enabled"",""_col2"":""VN"",""_col3"":""android_tablet""},""value"":{""_col0"":14,""_col1"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.5]},""_col2"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.8]},""_col3"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.9]}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:271)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425)
	at org.apache.hadoop.mapred.Child$3.run(Child.java:205)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate()  on object org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator@7e4d2f72 of class org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator with arguments {} of size 0
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1303)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge$GenericUDAFBridgeEvaluator.terminate(GenericUDAFBridge.java:182)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.evaluate(GenericUDAFEvaluator.java:201)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:1061)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:875)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:737)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:803)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:262)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1279)
	... 10 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2219)
	at java.util.ArrayList.toArray(ArrayList.java:358)
	at java.util.Collections.sort(Collections.java:216)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate(UDAFPercentile.java:201)
	... 14 more

2016-03-29 21:34:08,735 qid21305445 INFO s3native.NativeS3FileSystem:107 (main): setting Progress to null comment clearing progress from taskDone
2016-03-29 21:34:08,759 qid21305445 WARN mapred.Child:246 (main): Error running child
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""_col0"":""8b09e4731753"",""_col1"":""enabled"",""_col2"":""VN"",""_col3"":""android_tablet""},""value"":{""_col0"":14,""_col1"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.5]},""_col2"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.8]},""_col3"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.9]}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:283)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:509)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:425)
	at org.apache.hadoop.mapred.Child$3.run(Child.java:205)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""_col0"":""8b09e4731753"",""_col1"":""enabled"",""_col2"":""VN"",""_col3"":""android_tablet""},""value"":{""_col0"":14,""_col1"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.5]},""_col2"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.8]},""_col3"":{""counts"":{2751:1,2165:1,2406:1,1614:1,2010:1,949:1,1468:1,1497:1,2490:1,5421:1,1204:1,5515:1,1482:1,3077:1},""percentiles"":[0.9]}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:271)
	... 3 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate()  on object org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator@7e4d2f72 of class org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator with arguments {} of size 0
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1303)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge$GenericUDAFBridgeEvaluator.terminate(GenericUDAFBridge.java:182)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.evaluate(GenericUDAFEvaluator.java:201)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:1061)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:875)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:737)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:803)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:262)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1279)
	... 10 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2219)
	at java.util.ArrayList.toArray(ArrayList.java:358)
	at java.util.Collections.sort(Collections.java:216)
	at org.apache.hadoop.hive.ql.udf.UDAFPercentile$PercentileLongEvaluator.terminate(UDAFPercentile.java:201)
	... 14 more
2016-03-29 21:34:08,763 qid21305445 INFO mapred.Task:1098 (main): Runnning cleanup for the task
2016-03-29 21:34:08,766 qid21305445 INFO mapred.Child:272 (main): Shutting down Child JVM
{code}


 - https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fip-10-1-0-175.ec2.internal%3A50060%2Ftasklog%3Ftaskid%3Dattempt_247.201602230427_44879_r_000003_0%26all%3Dtrue&clusterInst=136335

Logs:
{code}
2016-03-29 21:26:34,390 -1 INFO mapred.Task:741 (communication thread): Communication exception: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
	at sun.nio.cs.StreamDecoder.<init>(StreamDecoder.java:250)
	at sun.nio.cs.StreamDecoder.<init>(StreamDecoder.java:230)
	at sun.nio.cs.StreamDecoder.forInputStreamReader(StreamDecoder.java:69)
	at java.io.InputStreamReader.<init>(InputStreamReader.java:74)
	at java.io.FileReader.<init>(FileReader.java:72)
	at org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:511)
	at org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:180)
	at org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)
	at org.apache.hadoop.mapred.Task.updateResourceCounters(Task.java:806)
	at org.apache.hadoop.mapred.Task.updateCounters(Task.java:912)
	at org.apache.hadoop.mapred.Task.access$600(Task.java:70)
	at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:717)
	at java.lang.Thread.run(Thread.java:745)

2016-03-29 21:28:33,624 qid21305445 INFO s3native.NativeS3FileSystem:107 (main): setting Progress to null comment clearing progress from taskDone
2016-03-29 21:28:37,125 qid21305445 FATAL mapred.Child:262 (main): Error running child : java.lang.OutOfMemoryError: GC overhead limit exceeded
{code}


cc: [~venkatak]",Production,abhisheks,asomani,beria,monikak,mpatel,nitink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,pinterest,,,,,"{repository={count=2, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":2,""lastUpdated"":""2016-05-24T22:00:15.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":2,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z024er:,,,,,,Hive Sprint 1(13-04 to 29-04),,,,,,,,,,,,,1.0,5518,2016-04-06 05:31:17.917,,,"02/Apr/16 5:27 AM;monikak;One of the command id when this query succeeded:

https://api.qubole.com/v2/analyze?command_id=21002262&qbol_user_id=6388","06/Apr/16 5:31 AM;asomani;This seems to be happening because in the percentile function, we try to sort the data and that sorting internally asks for more memory. Now how much memory is asked for here is totally dependent on the data being handled and so it could've asked for a lot of memory leading to the java exception.

To find out the difference between the data sizes being handled between the passed and failed runs, I want to ask the customer to run a two queries, modification of the failed and passed queries provided in this ticket. These queries will do a count of the number of rows going to the percentile function. This will give an idea if the data size changed a lot between queries.
To handle the failure, I want to ask the customer to increase the java heap size for the reducer. This is set to 1 gb by default. Will ask to increase to 2 gb and see if they hit the problem again. This is the configuration :
mapred.reduce.child.java.opts -server -Xmx2048m -Djava.net.preferIPv4Stack=true -Dfile.encoding=UTF8

Here are the queries I want them to run:

EDIT : removed insert overwrite.

1. The failed query, modified:

add archive s3n://qubole-pinterest/staging/yuxiao/e5b9c96b2431ca85eba0c757dfde9f207af29d06/archive.zip;
add file s3n://qubole-pinterest/staging/yuxiao/e5b9c96b2431ca85eba0c757dfde9f207af29d06/py;
set hive.downloaded.resources.dir=s3n://qubole-pinterest/staging/yuxiao/e5b9c96b2431ca85eba0c757dfde9f207af29d06;
set mapred.job.name=yuxiao:CreateSegmentedExperimentTTRFPActionDayJob;
CREATE EXTERNAL TABLE IF NOT EXISTS segmented_experiment_ttrfp_action_dt
(
   experiment_hash STRING,
   experiment_group STRING,
   country STRING,
   app_type STRING,
   num_requests BIGINT,
   ttrfp_50 BIGINT,
   ttrfp_80 BIGINT,
   ttrfp_90 BIGINT
) PARTITIONED BY (action_dt STRING)
STORED AS SEQUENCEFILE
LOCATION 's3n://pinlogs/pinalytics/segmented_experiment_ttrfp_action_dt/';

-- Set split size to a high number to lower the number of mappers/reducers used
SET mapred.min.split.size.per.node=1000000000;
SET mapred.min.split.size.per.rack=1000000000;
SET mapred.max.split.size=1000000000;
SET hive.exec.reducers.bytes.per.reducer=1000000000;

-- Compress Hive table, it will give us ~6x space saving
SET mapred.output.compress=true;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET mapred.output.compression.type=BLOCK;
SET mapreduce.output.fileoutputformat.compress=true;
SET mapreduce.output.fileoutputformat.compress.type=BLOCK;
SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET hive.exec.compress.output=true;
SET hive.exec.compress.intermediate=true;
SET io.seqfile.compression.type=BLOCK;

-- This query JOINs segment information to experiment users who take actions on dt.

SELECT experiment_hash, experiment_group, country, app_type,
COUNT(*) AS num_requests,
COUNT(timing) AS count_of_timing
-- these are the users who are triggered on action_dt
-- exp ttrfp metrics only makes sense for users who were actually triggered on that day
FROM (
  SELECT experiment_hash, experiment_group, userid
  FROM experiment_users_union
  WHERE dt='2016-03-28'
) eu

JOIN
(
  SELECT get_json(json, ""userid"") as userid, ""iphone"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-28' AND get_json(json, ""app"") = 1
        AND (get_json(json, ""fep_type"") = ""ttrfp"" OR get_json(json, ""fep_type"") = ""app_start"")
        AND get_json(json, ""appversion"") <> ""4.3""
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""ipad"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-28' AND get_json(json, ""app"") = 2
        AND (get_json(json, ""fep_type"") = ""ttrfp"" OR get_json(json, ""fep_type"") = ""app_start"")
        AND get_json(json, ""appversion"") <> ""4.3""
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""android"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-28' AND get_json(json, ""app"") = 3
        AND ((get_json(json, ""fep_type"") = ""first_grid_image"" AND get_json(json, ""appversion"") < ""4.4.0"") OR
            (get_json(json, ""fep_type"") = ""ttrfp"" AND get_json(json, ""appversion"") >= ""4.4.0""))
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""android_tablet"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-28' AND get_json(json, ""app"") = 4
        AND ((get_json(json, ""fep_type"") = ""first_grid_image"" AND get_json(json, ""appversion"") < ""4.4.0"") OR
            (get_json(json, ""fep_type"") = ""ttrfp"" AND get_json(json, ""appversion"") >= ""4.4.0""))
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, 'request_info.user_id') as userid, ""web"" as app_type,
         CAST(get_json(json, 'page_load_timing.performanceTiming.requestStart') as BIGINT) as timing,
         get_json(json, 'page_load_timing.country') as country
  FROM webapp_page_load_timing
  WHERE dt = '2016-03-28' AND get_json(json, 'page_load_timing.performanceTiming.requestStart') is not null
        AND get_json(json, 'report_context.current_url') like '%pinterest.com/'
        AND get_json(json, 'page_load_timing.country') IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")
) ua
ON CAST(eu.userid AS BIGINT) = CAST(ua.userid as BIGINT)

GROUP BY experiment_hash, experiment_group, country, app_type;

2. The passed query, modified :

add archive s3n://qubole-pinterest/staging/prod/848e55e2e9aae6dea84daccee591c4d00b5c538b/archive.zip;
add file s3n://qubole-pinterest/staging/prod/848e55e2e9aae6dea84daccee591c4d00b5c538b/py;
set hive.downloaded.resources.dir=s3n://qubole-pinterest/staging/prod/848e55e2e9aae6dea84daccee591c4d00b5c538b;
set mapred.job.name=prod:CreateSegmentedExperimentTTRFPActionDayJob;
 SET mapred.fairscheduler.pool=prod_pool;
CREATE EXTERNAL TABLE IF NOT EXISTS segmented_experiment_ttrfp_action_dt
(
   experiment_hash STRING,
   experiment_group STRING,
   country STRING,
   app_type STRING,
   num_requests BIGINT,
   ttrfp_50 BIGINT,
   ttrfp_80 BIGINT,
   ttrfp_90 BIGINT
) PARTITIONED BY (action_dt STRING)
STORED AS SEQUENCEFILE
LOCATION 's3n://pinlogs/pinalytics/segmented_experiment_ttrfp_action_dt/';

-- Set split size to a high number to lower the number of mappers/reducers used
SET mapred.min.split.size.per.node=1000000000;
SET mapred.min.split.size.per.rack=1000000000;
SET mapred.max.split.size=1000000000;
SET hive.exec.reducers.bytes.per.reducer=1000000000;

-- Compress Hive table, it will give us ~6x space saving
SET mapred.output.compress=true;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET mapred.output.compression.type=BLOCK;
SET mapreduce.output.fileoutputformat.compress=true;
SET mapreduce.output.fileoutputformat.compress.type=BLOCK;
SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET hive.exec.compress.output=true;
SET hive.exec.compress.intermediate=true;
SET io.seqfile.compression.type=BLOCK;

-- This query JOINs segment information to experiment users who take actions on dt.

SELECT experiment_hash, experiment_group, country, app_type,
COUNT(*) AS num_requests,
COUNT(timing) AS count_of_timing
-- these are the users who are activated into experiments within 60 days of action_dt
FROM (
  SELECT all_exp.experiment_hash, experiment_group, userid
  FROM
  (
    SELECT DISTINCT(experiment_hash) as experiment_hash
    FROM experiment_users_union WHERE DATEDIFF('2016-03-24', dt) <= 60
  ) today_exp
  JOIN
  (
    SELECT experiment_hash, experiment_group, userid
    FROM segmented_experiment_users_activation_date
    WHERE experiment_start_date <= '2016-03-24'
  ) all_exp
  ON today_exp.experiment_hash = all_exp.experiment_hash
) eu

JOIN
(
  SELECT get_json(json, ""userid"") as userid, ""iphone"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-24' AND get_json(json, ""app"") = 1
        AND (get_json(json, ""fep_type"") = ""ttrfp"" OR get_json(json, ""fep_type"") = ""app_start"")
        AND get_json(json, ""appversion"") <> ""4.3""
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""ipad"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-24' AND get_json(json, ""app"") = 2
        AND (get_json(json, ""fep_type"") = ""ttrfp"" OR get_json(json, ""fep_type"") = ""app_start"")
        AND get_json(json, ""appversion"") <> ""4.3""
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""android"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-24' AND get_json(json, ""app"") = 3
        AND ((get_json(json, ""fep_type"") = ""first_grid_image"" AND get_json(json, ""appversion"") < ""4.4.0"") OR
            (get_json(json, ""fep_type"") = ""ttrfp"" AND get_json(json, ""appversion"") >= ""4.4.0""))
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""android_tablet"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-24' AND get_json(json, ""app"") = 4
        AND ((get_json(json, ""fep_type"") = ""first_grid_image"" AND get_json(json, ""appversion"") < ""4.4.0"") OR
            (get_json(json, ""fep_type"") = ""ttrfp"" AND get_json(json, ""appversion"") >= ""4.4.0""))
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, 'request_info.user_id') as userid, ""web"" as app_type,
         CAST(get_json(json, 'page_load_timing.performanceTiming.requestStart') as BIGINT) as timing,
         get_json(json, 'page_load_timing.country') as country
  FROM webapp_page_load_timing
  WHERE dt = '2016-03-24' AND get_json(json, 'page_load_timing.performanceTiming.requestStart') is not null
        AND get_json(json, 'report_context.current_url') like '%pinterest.com/'
        AND get_json(json, 'page_load_timing.country') IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")
) ua
ON CAST(eu.userid AS BIGINT) = CAST(ua.userid as BIGINT)

GROUP BY experiment_hash, experiment_group, country, app_type
;","13/Apr/16 9:24 AM;nitink;[~mpatel] [~adubey]

We tried getting sampe data set which could reproduce the issue , but couldnt reproduce the issue. As the query takes lot of time to run, we would like to copy the output of all UNION ALL operators to a temporary table to reduce the time and resource required for repro.

Here is the temp table we want to create, Does it look fine to you ?


{code:java}
Create table temphive_uniontable_1343 as
SELECT experiment_hash, experiment_group, country, app_type, timing
FROM (
  SELECT experiment_hash, experiment_group, userid
  FROM experiment_users_union
  WHERE dt='2016-03-28'
) eu

JOIN
(
  SELECT get_json(json, ""userid"") as userid, ""iphone"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-28' AND get_json(json, ""app"") = 1
        AND (get_json(json, ""fep_type"") = ""ttrfp"" OR get_json(json, ""fep_type"") = ""app_start"")
        AND get_json(json, ""appversion"") <> ""4.3""
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""ipad"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-28' AND get_json(json, ""app"") = 2
        AND (get_json(json, ""fep_type"") = ""ttrfp"" OR get_json(json, ""fep_type"") = ""app_start"")
        AND get_json(json, ""appversion"") <> ""4.3""
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""android"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-28' AND get_json(json, ""app"") = 3
        AND ((get_json(json, ""fep_type"") = ""first_grid_image"" AND get_json(json, ""appversion"") < ""4.4.0"") OR
            (get_json(json, ""fep_type"") = ""ttrfp"" AND get_json(json, ""appversion"") >= ""4.4.0""))
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, ""userid"") as userid, ""android_tablet"" as app_type,
         CAST(get_json(json, ""timing"") as BIGINT) as timing,
         get_json(json, ""geo.country_code"") as country
  FROM raw_mobile_fep
  WHERE dt = '2016-03-28' AND get_json(json, ""app"") = 4
        AND ((get_json(json, ""fep_type"") = ""first_grid_image"" AND get_json(json, ""appversion"") < ""4.4.0"") OR
            (get_json(json, ""fep_type"") = ""ttrfp"" AND get_json(json, ""appversion"") >= ""4.4.0""))
        AND get_json(json, ""geo.country_code"") IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")

  UNION ALL

  SELECT get_json(json, 'request_info.user_id') as userid, ""web"" as app_type,
         CAST(get_json(json, 'page_load_timing.performanceTiming.requestStart') as BIGINT) as timing,
         get_json(json, 'page_load_timing.country') as country
  FROM webapp_page_load_timing
  WHERE dt = '2016-03-28' AND get_json(json, 'page_load_timing.performanceTiming.requestStart') is not null
        AND get_json(json, 'report_context.current_url') like '%pinterest.com/'
        AND get_json(json, 'page_load_timing.country') IN (
            ""BR"", ""DE"", ""JP"", ""GB"", ""FR"", ""US"",
            ""HU"", ""NL"", ""PL"", ""PT"", ""RO"", ""TR"", ""NO"", ""MX"",
            ""DK"", ""ES"", ""GR"", ""ID"", ""IT"", ""SE"", ""TH"", ""VN"")
) ua
ON CAST(eu.userid AS BIGINT) = CAST(ua.userid as BIGINT);

{code}","14/Apr/16 1:48 AM;nitink;[~adubey] [~mpatel] [~psrinivas] [~asomani] 

Allocating 2GB memory to both reduce jvm task as well as  the reduce slot size seems to fix the issue . refer command  - https://api.qubole.com/v2/analyze?command_id=22305630&qbol_user_id=4696

We will investigate internally, about investigating the performance of Percentile function, But as of now Percentile UDAF seems to store data in memory and requires huge amount of space.

Let me know if you have concerns.","12/May/16 11:18 PM;beria;They tried the fix but it was un-successful. https://api.qubole.com/v2/analyze?command_id=24531643 . I have taken permission from them to run the select part of query at least on the ticket on their cluster.
[~nitink]","17/May/16 9:26 AM;abhisheks;[~nitink] any updates on this one ? even after trying the fix the problem persists.
cc [~psrinivas]","23/May/16 3:46 AM;abhisheks;Most recent update from customer:

I tried sampling and seems even with a sampling rate 0.9 the query would finish within about 10 mins. Here are the runs:
rate 0.9: https://api.qubole.com/v2/analyze?command_id=25333137
rate 0.75: https://api.qubole.com/v2/analyze?command_id=25331219
rate 0.5: https://api.qubole.com/v2/analyze?command_id=25330035
rate 0.1: https://api.qubole.com/v2/analyze?command_id=25329366
The most recent failed one: https://api.qubole.com/v2/analyze?command_id=25331816
Could you help us take a look why there is such a big difference when running on full data? Thank you!

Can you please check this ",24/May/16 5:12 AM;abhisheks;[~nitink] pinterest is asking for any analysis ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notebook behavior across cluster restarts,ZEP-272,39368,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,venkatak,venkatak,28/Mar/16 12:10 AM,20/Jun/17 3:25 PM,09/Aug/17 6:03 AM,,,core,,0,for-rb36,jira_escalated,,,,,,,"Customer's comment:

After restarting the cluster the message on notebook is not consistent and not true to the actual error / completion, Might be notebook have lost connection with the cluster once restarted, therefore showed a generic message which needs to be fixed on U/I part.

Questions from my side:

1. How do we link a Notebook status to cluster status? We ofcourse restart the interpreters etc, but what is the expected behavior on the Notebook side, should it work without a problem right away?

2. How do we link the notebook back if lets say the cluster restarts during a command execution? In Analyze, I have seen the command id waiting when there are cluster startup issues and it goes for retries etc and it ultimately runs the command once the cluster is ""UP"".
",,gmargabanthu,karuppayyar,sbadam,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,expedia,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z023p7:,,,,,,,,,,,,,,,,,,,1.0,5225,2016-03-28 01:30:12.851,,,"28/Mar/16 1:30 AM;karuppayyar;The message on the notebook means the message showed after execution of paragraphs?
The error messages/output of each para is stored in a json and the json is synced to s3 every 2 minutes.
if the cluster goes down in between the 2 minute interval period, there is a chance of output not getting synced to s3.

Q1. There are two things here.
a. In FCN UI, the notebooks are shown in left pane with green/red or grey dot.
These icons denote if cluster is up/down. Grey implies that the note is not associated to any cluster.
b. There is dot at the top right corner saying connected /disconnected.
when it is connected, it means the note is able to communicate with the Zeppelin server. And red means that there is no connection to zeppelin server.
In case of read-only mode, we dont show this status at all.

Q2. We store the linking between notebooks and cluster.
When cluster restarts , all the notebooks which were previously available should be present. There is no need for explicit linking again.
The user can go ahead and rerun the para.

In analyze, when we run commands we start the cluster if it is down. In Zeppelin, we need a ACTIVE cluster to execute paragraphs.

",03/Jul/16 1:28 AM;karuppayyar;[~venkatak] shall i resolve this?,11/Jan/17 3:12 PM;sbadam;Can we have any update on this issue? Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Presto Queries failed to start cluster,ACM-208,39144,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,laks,addon_zendesk_for_jira,17/Mar/16 9:29 PM,29/Mar/16 2:23 AM,09/Aug/17 6:03 AM,,,,,1,jira_escalated,,,,,,,,"A regular scheduled ETL Presto query failed to start the cluster. The cluster management report show that it was terminated due to inactivity which is weird. 

Cluster ID: 13532	
Cluster Name: ETLPresto
Account: 3914",,addon_zendesk_for_jira,ajayb,drose@qubole.com,sumitm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,roku,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z022qz:,,,,,,,,,,,,,,,,,,,,,2016-03-24 03:47:40.727,,,"24/Mar/16 3:47 AM;ajayb;Out of the last 20 instances of this cluster, 19 of them were terminated due to inactivity. However, these instances were running for 5-10 hours and so don't seem to have been terminated prematurely.
{code:java}
production-replica> select terminate_reason,start_at,down_at from cluster_insts where cluster_id=13532 order by id desc limit 20;
+------------------+---------------------+---------------------+
| terminate_reason | start_at            | down_at             |
+------------------+---------------------+---------------------+
| INACTIVITY       | 2016-03-24 02:48:29 | 2016-03-24 10:27:01 |
| USER-INITIATED   | 2016-03-23 16:20:56 | 2016-03-24 02:41:30 |
| INACTIVITY       | 2016-03-23 05:34:27 | 2016-03-23 10:25:59 |
| INACTIVITY       | 2016-03-23 02:34:32 | 2016-03-23 05:28:55 |
| INACTIVITY       | 2016-03-22 13:52:08 | 2016-03-23 01:27:58 |
| INACTIVITY       | 2016-03-22 08:04:43 | 2016-03-22 10:54:06 |
| INACTIVITY       | 2016-03-21 19:57:29 | 2016-03-22 07:53:24 |
| INACTIVITY       | 2016-03-21 05:38:56 | 2016-03-21 10:20:38 |
| INACTIVITY       | 2016-03-21 02:34:00 | 2016-03-21 05:26:26 |
| INACTIVITY       | 2016-03-20 05:42:08 | 2016-03-20 10:33:19 |
| INACTIVITY       | 2016-03-20 02:34:27 | 2016-03-20 05:20:54 |
| INACTIVITY       | 2016-03-19 14:52:42 | 2016-03-19 17:41:40 |
| INACTIVITY       | 2016-03-19 06:47:21 | 2016-03-19 10:36:06 |
| INACTIVITY       | 2016-03-19 02:34:13 | 2016-03-19 05:24:44 |
| INACTIVITY       | 2016-03-18 20:49:23 | 2016-03-19 00:37:20 |
| INACTIVITY       | 2016-03-18 06:36:27 | 2016-03-18 10:27:56 |
| INACTIVITY       | 2016-03-18 03:11:18 | 2016-03-18 06:03:00 |
| INACTIVITY       | 2016-03-17 16:42:56 | 2016-03-18 02:30:42 |
| INACTIVITY       | 2016-03-17 05:43:11 | 2016-03-17 10:31:48 |
| INACTIVITY       | 2016-03-16 16:16:44 | 2016-03-17 04:48:25 |
+------------------+---------------------+---------------------+
20 rows in set (0.00 sec)
{code}
","24/Mar/16 5:34 AM;ajayb;The failed commands were 20468313 and 20468319.

{code:java}
production-replica> select * from query_hists where id=20468313\G;    
*************************** 1. row ***************************
                         id: 20468313
               qbol_user_id: 7658
                submit_time: 1458268203
                   end_time: 1458268214
                   progress: 100
                    cube_id: NULL
                 created_at: 2016-03-18 02:30:03
                 updated_at: 2016-03-18 02:30:14
                       path: /tmp/2016-03-18/3914/20468313
                     status: error
                  host_name: ip-10-142-232-226.ec2.internal
                   user_loc: 1
            qbol_session_id: 1137421
                 command_id: 138380
               command_type: PrestoCommand
                       qlog: NULL
            periodic_job_id: NULL
                      wf_id: NULL
             command_source: API
            resolved_macros: NULL
                status_code: 124
                        pid: 12269
             editable_pj_id: NULL
                   template: generic
        command_template_id: NULL
command_template_mutable_id: NULL
                 can_notify: 0
             num_result_dir: 1
                 start_time: 1458268205
                       pool: NULL
                    timeout: NULL
                        tag: ETLPresto
                       name: NULL
     saved_query_mutable_id: NULL
1 row in set (0.00 sec)

ERROR: 
No query specified

production-replica> select * from query_hists where id=20468319\G;
*************************** 1. row ***************************
                         id: 20468319
               qbol_user_id: 7658
                submit_time: 1458268204
                   end_time: 1458268216
                   progress: 100
                    cube_id: NULL
                 created_at: 2016-03-18 02:30:04
                 updated_at: 2016-03-18 02:30:16
                       path: /tmp/2016-03-18/3914/20468319
                     status: error
                  host_name: ip-10-171-78-174.ec2.internal
                   user_loc: 1
            qbol_session_id: 1137421
                 command_id: 138381
               command_type: PrestoCommand
                       qlog: NULL
            periodic_job_id: NULL
                      wf_id: NULL
             command_source: API
            resolved_macros: NULL
                status_code: 124
                        pid: 29828
             editable_pj_id: NULL
                   template: generic
        command_template_id: NULL
command_template_mutable_id: NULL
                 can_notify: 0
             num_result_dir: 1
                 start_time: 1458268206
                       pool: NULL
                    timeout: NULL
                        tag: ETLPresto
                       name: NULL
     saved_query_mutable_id: NULL
1 row in set (0.00 sec)
{code}

They were associated with qbol_session_id 1137421. This session was associated with cluster_id 13532.

{code:java}
production-replica> select * from qbol_sessions where id=1137421\G;
*************************** 1. row ***************************
             id: 1137421
   qbol_user_id: 7658
           data: BAhbAA==

         active: 1
     created_at: 2016-03-18 02:30:04
     updated_at: 2016-03-18 03:37:25
        timeout: 2
     cleaned_up: 0
user_terminated: 0
     account_id: 3914
     cluster_id: 13532
1 row in set (0.00 sec)

production-replica> select tag from cluster_tags where cluster_id=13532;
+-----------+
| tag       |
+-----------+
| ETLPresto |
+-----------+
1 row in set (0.00 sec)
{code}

In the timeperiod encompassing these commands, the presto commands from this qbol_user_id were associated with cluster_instance_id 147546.

{code:java}
production-replica> select distinct(cluster_inst_id) from cluster_nodes cn join  ( select master_dns_name from presto_commands pc join  (select command_id cid from query_hists where qbol_user_id = 7658 and created_at between '2016-03-18 02:00:00' and  '2016-03-18 02:50:00' and tag = 'ETLPresto' and command_type = ""PrestoCommand"") qh  on pc.id = qh.cid) master  on cn.hostname = master.master_dns_name  where cn.role = ""master"";
+-----------------+
| cluster_inst_id |
+-----------------+
|          147546 |
+-----------------+
1 row in set (21.08 sec)

production-replica> select * from cluster_insts where id=147546\G;
*************************** 1. row ***************************
                         id: 147546
                 cluster_id: 13532
          cluster_config_id: 35198
              cluster_state: DOWN
                   start_at: 2016-03-17 16:42:56
                    down_at: 2016-03-18 02:30:42
           terminate_reason: INACTIVITY
last_health_check_action_at: NULL
   last_health_check_action: NULL
                 created_at: 2016-03-17 16:38:53
                 updated_at: 2016-03-18 02:30:42
1 row in set (0.00 sec)
{code}

During the lifetime of this cluster instance, following qbol_sessions were active and associated with this cluster id 13532.
{code:java}
production-replica> select id,qbol_user_id,created_at,updated_at from qbol_sessions where cluster_id=13532 and (created_at between '2016-03-17 16:38:00' and '2016-03-18 02:31:00');
+---------+--------------+---------------------+---------------------+
| id      | qbol_user_id | created_at          | updated_at          |
+---------+--------------+---------------------+---------------------+
| 1135749 |         7563 | 2016-03-17 16:38:51 | 2016-03-17 17:56:20 |
| 1135774 |         7558 | 2016-03-17 16:58:29 | 2016-03-17 17:08:26 |
| 1136002 |         8465 | 2016-03-17 17:55:06 | 2016-03-17 17:55:16 |
| 1136418 |         7563 | 2016-03-17 20:16:36 | 2016-03-17 21:57:45 |
| 1136492 |         7558 | 2016-03-17 20:39:24 | 2016-03-18 00:35:23 |
| 1137039 |         7563 | 2016-03-18 00:10:59 | 2016-03-18 00:29:16 |
| 1137067 |        11576 | 2016-03-18 00:16:29 | 2016-03-18 00:17:15 |
| 1137421 |         7658 | 2016-03-18 02:30:04 | 2016-03-18 03:37:25 |
+---------+--------------+---------------------+---------------------+
8 rows in set (0.01 sec)
{code}

From above list of sessions it seems the penultimate session completed at 03-18 00:17:15. Approx 2hrs 13 mins after that the cluster was detected with no active associated qbol_sessions and so got terminated at 03-18 02:30:42. However close to this moment a new qbol_session got created at 03-18 02:30:04 and associated with this cluster. Looks like we hit a narrow window of race condition.

[~sumitm] can you shed some light on what's expected in such cases?","29/Mar/16 2:23 AM;sumitm;Yeah, it appears to be a race condition. From the data above its clear that old session was expired and being processed by cleanup worker. During that time a new session has been created, and fail to start the cluster, as the cluster wasn't down by that time. 

To prevent such thing we can do something like this:

1. Whenever a session checks for a cluster is running or not, it updates some field (datetime type) in clusters table.
2. Hustler should check for for that flag before start terminating that cluster. 


Adding [~vagrawal] [~mahuja] [~jssarma] for more inputs. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive select query throws array out of bounds error. ,HIVE-1329,39134,Bug,Reopened,HIVE,qbol hive,software,psrinivas,,,Major,,adityak,laks,addon_zendesk_for_jira,17/Mar/16 3:30 AM,27/Jun/17 2:28 AM,09/Aug/17 6:03 AM,,,,,0,hive1.2,jira_escalated,Sustenace,,,,,,"See command id: 20380992 and 20380180

The only difference is that the alias ""b"" columns are at the end of the SELECT clause instead of higher up.

For some reason when I try to place the SITE_ID, SITE_NAME higher in the select I get an array out of bounds error - which makes no sense",,addon_zendesk_for_jira,gmargabanthu,laks,mpatel,p.vasa,psrinivas,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Expedia,,,,,"{pullrequest={dataType=pullrequest, state=DECLINED, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2017-07-10T02:28:36.000-0700"",""stateCount"":1,""state"":""DECLINED"",""open"":false},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z022or:,,,,,,,,,,,,,,,,,,,1.0,5365,2016-03-18 04:40:05.183,,,"18/Mar/16 4:40 AM;psrinivas;[~laks]It is not only with the ""b"" alias, there is also a left outer join with the query that got succeeded.  Need to run couple of experiments to determine the issue. Please let us know when the cluster is up. ","20/Mar/16 7:46 PM;laks;[~psrinivas] Customer confirmed to test the queries. So we can enable the cluster if it is down and try running the queries. 
","19/Aug/16 11:36 AM;p.vasa;Hello [~psrinivas] 
Is there an ETA on this issue so that we can go ahead and notify the customer?

Thank you.",17/Oct/16 6:23 PM;gmargabanthu;[~psrinivas] can you please give an update on this?,26/Jun/17 10:55 AM;venkatak;[~psrinivas] - seems like the customer is not happy with our response when we asked about closing the associated ticket. Any reason why this .JIRA was closed?,"26/Jun/17 11:41 AM;psrinivas;cc: [~rvenkatesh]

[~venkatak]: I believe, we closed it for the reason of inactivity on the jira for a long time. If we have any latest incidents of this, we can take a look. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FAILED_TO_UNCOMPRESS(5),SPAR-927,39105,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,mpatel,mpatel,16/Mar/16 9:34 AM,27/Jun/17 1:33 PM,09/Aug/17 6:03 AM,,,misc,,0,jira_escalated,,,,,,,,"Oracle ran into the issue described here:
https://issues.apache.org/jira/browse/SPARK-4105

They restarted the cluster, and the problem was resolved. Wanted to log here since we had a customer hit it.

Command ID: 20297416

{code}
java.io.IOException: FAILED_TO_UNCOMPRESS(5) 
at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84) 
at org.xerial.snappy.SnappyNative.rawUncompress(Native Method) 
at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444) 
at org.xerial.snappy.Snappy.uncompress(Snappy.java:480) 
at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:135) 
at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:92) 
at org.xerial.snappy.SnappyInputStream.<init>(SnappyInputStream.java:58) 
at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:159) 
at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1179) 
at org.apache.spark.shuffle.hash.HashShuffleReader$$anonfun$3.apply(HashShuffleReader.scala:53) 
at org.apache.spark.shuffle.hash.HashShuffleReader$$anonfun$3.apply(HashShuffleReader.scala:52) 
at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371) 
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) 
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) 
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39) 
at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:132) 
at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:60) 
at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:89) 
at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:90) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297) 
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) 
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297) 
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) 
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297) 
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) 
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297) 
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) 
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) 
at org.apache.spark.scheduler.Task.run(Task.scala:88) 
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
at java.lang.Thread.run(Thread.java:745)
{code}",,drose@qubole.com,gayathrym,mahuja,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Oracle,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z022ij:,,,,,,,,,,,,,,,,,,,1.0,5339,2016-05-22 21:08:29.309,,,22/May/16 9:08 PM;drose@qubole.com;Customer reported they have not encountered this issue again.,"09/Jun/16 7:02 PM;gayathrym;[~mahuja] : These jira_escalated tickets were assigned to Administrator, hence re-assigning to you to re-assign as appropriate for the next steps. ","02/Aug/16 2:25 PM;mahuja;[~mpatel] - if things are stable, can we lower the priority. Thanks ","29/Aug/16 2:37 AM;mahuja;Lowering the priority on this. Please bump it up if we see it again.

From SPARK-4105, possible workaround:

{code}
set 'spark.shuffle.blockTransferService' to 'nio'
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPC timeouts,SPAR-922,38771,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,vishalg,vishalg,09/Mar/16 4:55 AM,09/Jun/16 7:02 PM,09/Aug/17 6:03 AM,,,misc,,0,jira_escalated,,,,,,,,"Query Failure : https://api.qubole.com/v2/analyze?command_id=19787454&qbol_user_id=3761

It seems from the logs that tasks get lost due to no executor heartbeat. So, it tries to kill executors which results into RPC timeout exceptions.

As per Laks, customer increased the timeout to 1000, but that did not seem to help either.",,bharatb,gayathrym,vishalg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,karthikb,#Empty,,1|z021gz:,,,,,,,,,,,,,,,,,,,,,2016-03-09 05:28:19.381,,,09/Mar/16 4:57 AM;vishalg;https://qubole.zendesk.com/agent/tickets/5150,"09/Mar/16 4:59 AM;vishalg;
{code:java}
App > 16/03/08 01:20:27 kill-executor-thread WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(List(1))] in 1 attempts
App > org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [1000 seconds]. This timeout is controlled by spark.rpc.askTimeout
App > 	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
App > 	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
App > 	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
App > 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
App > 	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
App > 	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
App > 	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
App > 	at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.doKillExecutors(YarnSchedulerBackend.scala:67)
App > 	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:513)
App > 	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1567)
App > 	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:203)

{code}
","09/Mar/16 5:28 AM;bharatb;[~vishalg] Some possibilities:-

1. executor logs indicate that the executor is doing GC for a long time and is unable to do any processing (including sending heartbeats)
2. cpu on the nodes is very high indicating executor is stuck in some tight loop doing data decompression or object creation etc
3. yarn tries to allocate containers on a node which is already decommissioned or is a bad node in some way. The container fails to start or has trouble after starting.
4. yarn tries to kill a container which has already crossed memory limits but is unable to do so due to kill command failing (we have seen this a long time ago)
5. there is a network partition where executors can't talk back to driver but driver can talk to executors or generally network failure of some kind

I think any or all of the above can be checked via logs -- check the executor logs and the node manager logs and maybe resource manager logs.","09/Jun/16 7:02 PM;gayathrym;[~mahuja] : These jira_escalated tickets were assigned to Administrator, hence re-assigning to you to re-assign as appropriate for the next steps. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster change notifications sending irrelevant info,ACM-187,38343,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Minor,,somyak,adubey,addon_zendesk_for_jira,27/Feb/16 8:32 AM,18/Jun/17 9:55 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"example -

change in hadoop2 cluster also includes spark, hbase info


You are receiving this email because qubole_sysadmin@marketshare.com changed the following settings of the cluster with id 4989, label(s): DataLoad in your account MarketShare. 
Hadoop max nodes 
Use hbase 
Custom spark config 
Spark version 
Label 
You can view the current settings by accessing https://api.qubole.com/v2/control-panel#edit-cluster-4989 (https://urldefense.proofpoint.com/v2/url?u=https-3A__api.qubole.com_v2_control-2Dpanel-23edit-2Dcluster-2D4989&d=CwQCaQ&c=MOptNlVtIETeDALC_lULrw&r=kNJ3Y28U5G2tui7A0HRMdsLFsOLwgBZWHo7QdqqLDcCXa5irvfi0YuX8zgpxmEbo&m=Ep7DHO88LZdpIJ3mZpd7MW2BFPijWww1ms6i-B2RCqs&s=KqbfPb7c3vljQh0jkiuHshcpJNAIIxUyVqMR5A5Rrao&e=) from your qubole account. 
Please do not reply directly to this e-mail. If you have any questions or comments regarding this email, please contact ",,addon_zendesk_for_jira,adubey,ajayb,sureshr,vagrawal,,,,,,,,,,,,,,,,,,,,,,,ACM-454,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z0200b:,,,,,,,,,,,,,,,,,,,1.0,5062,2016-03-05 01:53:57.502,,,"05/Mar/16 1:53 AM;vagrawal;[~sourabhg], [~prakharj], [~ajayb] - can you please look into this.","23/Feb/17 9:53 AM;ajayb;[~sureshr], [~sumitm] can someone in MW team pick this up? The change is entirely in tapp.",23/Feb/17 10:54 AM;sureshr;[~adubey]: What is the priority of this bug? Is there any customer impact?,23/Feb/17 11:10 AM;adubey;I think it can be downgraded to minor,23/Feb/17 11:11 AM;adubey;User who complained has moved to different company - but this was a general issue and should be a good to fix.. but of course it does not harm the core functionality ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Demo pig script has special char issue ,QPIG-38,37762,Bug,Open,QPIG,qbol pig,software,bharatb,,,Major,,beria,adubey,addon_zendesk_for_jira,10/Feb/16 10:52 AM,09/Jul/17 11:19 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"s3://paid-qubole/PigAPIDemo/scripts/script1-hadoop-s3-small.pig"" does not run. 

feel free to assign it to right owner",,addon_zendesk_for_jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},No,,,,,,,,#Empty,No,,,,,,No,,#Empty,,1|z01xkj:,,,,,,,,,,,,,,,,,,,1.0,4798,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why my para is in pending state. Expose queue info on zep UI,ZEP-214,37220,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,beria,rgupta,rgupta,21/Jan/16 1:53 AM,08/Jul/16 10:57 PM,09/Aug/17 6:03 AM,,,core,,0,autodesk,jira_escalated,sparkcustomer,,,,,,,,rgupta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01uz7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed Spark job has SUCCEEDED status in Resource Manager,SPAR-823,36348,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,kzhang,kzhang,18/Dec/15 3:51 PM,09/Jun/16 7:02 PM,09/Aug/17 6:03 AM,,,misc,,0,jira_escalated,,,,,,,,"Oracle Spark team reported that failed Spark job has SUCCEEDED status in Resource Manager.

CommandID:15536648 
Account:prod-tech-core-consumer
ClusterID:12411

Zendesk#4231",,bharatb,gayathrym,kzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01qpn:,,,,,,,,,,,,,,,,,,,,,2015-12-30 18:26:19.904,,,"18/Dec/15 4:19 PM;kzhang;more feedback from OracleSpark team:

[12/18/15, 4:06:32 PM] Zi Ling(id_graph_spark3): it happened again
[12/18/15, 4:07:03 PM] Kalen Zhang: filed Jira ticket for this
[12/18/15, 4:07:09 PM] Zi Ling(id_graph_spark3): with some data files, that happened consistently
[12/18/15, 4:07:13 PM] Zi Ling(id_graph_spark3): cool, thanks
[12/18/15, 4:07:56 PM] Kalen Zhang: do you find any pattern when it occurs?
[12/18/15, 4:09:33 PM] Zi Ling(id_graph_spark3): when I load more than a certain number of files, it happens
[12/18/15, 4:09:42 PM] Zi Ling(id_graph_spark3): I don’t see any exceptions in the log
[12/18/15, 4:09:56 PM] Zi Ling(id_graph_spark3): it’s pretty weird somehow it ends up as succeeded
[12/18/15, 4:17:47 PM] Kalen Zhang: thank you for sharing this with us


commandID: 15543926",30/Dec/15 6:26 PM;bharatb;This might also be relevant to ThomsonReuters DFOC errors. There successful job is returning FAILED to the caller shell mapper.,"09/Jun/16 7:02 PM;gayathrym;[~mahuja] : These jira_escalated tickets were assigned to Administrator, hence re-assigning to you to re-assign as appropriate for the next steps. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inform user in case EBS volume limits are exceeded,ACM-75,35819,Bug,Reopened,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,Harsh,mahuja,mahuja,07/Dec/15 5:57 PM,05/Jun/17 5:20 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,qubot-master,qubot-qbol-master,,,,,,"Some of the possible investigation points:

* If AWS has APIs to expose the limits. In case there are, what are the IAM permissions required to use them
* While waiting for Hadoop checks to succeed during cluster bring up, if the number of nodes is less than last iteration, then log the reason of termination of those nodes
* Check if really 32G root volume size is used (exception Pinterest search cluster)",,drose@qubole.com,ekang,gayathrym,Harsh,mahuja,mpatel,p.vasa,qubot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,expedia,,,,,"{pullrequest={dataType=pullrequest, state=DECLINED, stateCount=3}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":3,""lastUpdated"":""2017-07-18T10:22:32.000-0700"",""stateCount"":3,""state"":""DECLINED"",""open"":false},""byInstanceType"":{""bitbucket"":{""count"":3,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",,Yes-Internal Release Notes,,ACM-72,,,,,,,,,,,,,anum,,,1|z01oib:,,Improve Cluster nodes logging in hustler.,,,,,,,,,,,,,,,,,2.0,"11859,15012",2016-05-03 17:45:16.119,,,"03/May/16 5:45 PM;Harsh;As per AWS, they do not expose service limits. So when the reservation completes, we need to get the instance id and hold on to it. Ping the AWS with the instance id to get the termination reason. ","23/May/16 1:58 PM;Harsh;As per aws, they have taken this feature request and added them to their pool. 
Couple of options looked into:
1. Before wait_for_instances, do a logging, so that the window is reduced. i.e. while in the initialization state, we have the instances logged into the db. The cons is, we will have to log twice in case there are spot nodes and no-demand nodes requested. Also, there is still no guarantee we will not hit the issue.
2. In case of no master found error describe aws instances to find the terminated instance with master tag. If this was a ebs volume instance issue, we will get the error from there. Cons, works only if master node is not allocated due to volume limit.
3. While fetching nodes from aws, if some of the nodes in current cache are not found, update the nodes into the db in case they do not exist with the termination reason. Cons: If a lot of down scaling happens, there is a lot more db lookup compared to the situation right now.

",31/Aug/16 1:10 PM;Harsh;commit: 3876285dfb4ae72775bb86d2ae1e194977bb7439,"31/Aug/16 1:11 PM;qubot;`harsh <harsh@qubole.com>` commited to `master in qbol`
 Msg: `fix: dev: ACM-75: Improve nodes logging into cluster_nodes table.

Fix an issue where nodes terminated due to EBS volume are logged
into cluster nodes table and improve the termination reason for
the same.

Squashed commit of the following:
commit 8936d9fe4288b89c7668450b58a91e1ae02d5f91
commit be4f1d789dc112ec53f47bce008da16e0e75b1ce
commit 2cf52eb08db7ad396c51c397cd2f654c4c56c216
commit 36867b49861287aa9fed44e30c1f4e50371f09e7
commit 195c9b652432af2a305dfb6df479adec7fb34660
commit cfd38dd33f0c60cda2ec4d9d1549710a17162451` 
 Link: https://bitbucket.org/qubole/qbol/commits/3876285dfb4ae72775bb86d2ae1e194977bb7439","31/Aug/16 11:13 PM;qubot;`harsh <harsh@qubole.com>` commited to `master in qbol`
 Msg: `fix: dev: ACM-75: bug fix from commit-3876285

commit for 3876285 caused a syntax error. Fixing it with this change.` 
 Link: https://bitbucket.org/qubole/qbol/commits/93a8c16a56e3c8a1444bdfc4c6e2774dfbb62060","02/Sep/16 12:32 AM;gayathrym;Commit to MASTER :
*  93a8c16a56e3c8a1444bdfc4c6e2774dfbb62060	Thu Sep 1 06:11:34 2016	hustler	master	ACM-75	harsh@qubole.com	fix: dev: ACM-75: bug fix from commit-3876285	
							
*  93a8c16a56e3c8a1444bdfc4c6e2774dfbb62060	Thu Sep 1 06:11:34 2016	hive_scripts	master	ACM-75	harsh@qubole.com	fix: dev: ACM-75: bug fix from commit-3876285	
							
*  93a8c16a56e3c8a1444bdfc4c6e2774dfbb62060	Thu Sep 1 06:11:34 2016	tapp2	master	ACM-75	harsh@qubole.com	fix: dev: ACM-75: bug fix from commit-3876285	","02/Sep/16 12:51 AM;gayathrym;Some more commits to MASTER :
*  3876285dfb4ae72775bb86d2ae1e194977bb7439	Wed Aug 31 20:05:32 2016	hustler	master	ACM-75	harsh@qubole.com	fix: dev: ACM-75: Improve nodes logging into cluster_nodes table.	
							
*  3876285dfb4ae72775bb86d2ae1e194977bb7439	Wed Aug 31 20:05:32 2016	hive_scripts	master	ACM-75	harsh@qubole.com	fix: dev: ACM-75: Improve nodes logging into cluster_nodes table.	
							
*  3876285dfb4ae72775bb86d2ae1e194977bb7439	Wed Aug 31 20:05:32 2016	tapp2	master	ACM-75	harsh@qubole.com	fix: dev: ACM-75: Improve nodes logging into cluster_nodes table.	","08/Sep/16 10:39 PM;Harsh;I have to repoen to issue due to ACM-603. Now, we have one option available to solve the issue which means that , describe terminated instances from AWS during log instances. 

This is not a favorable situation since we rely on aws feature where they keep terminated instance for the past 10-15 mins.If aws decides to increase this time window, then we need to visit this again. 

A perfect solution would have been AWS providing a filter based on the range of launch time but that is not available as of now. ",20/Dec/16 12:42 PM;drose@qubole.com;[~Harsh][~megha][~xing] How can we in a transparent way show customers they have seen and hit this limit?  Consistently we see customers encounter  this and not understand why their cluster is not starting.,09/Jan/17 11:14 AM;mpatel;[~Harsh] Trying to understand the status of this one. I see there was a feature request with AWS.. is this JIRA something we can address now?,"09/Jan/17 11:51 AM;Harsh;The jira was out for review and lost in time. ACM-770 is being addressed and kind of duplicating this Jira in small manner. I will start to trigger this PR again.
[~ajayaa][~ajayaa]","05/Jun/17 4:45 PM;p.vasa;Just FYI ->
Expedia had their cluster (Cluster ID -> 31974) already UP.
However, there were some Pending Containers and still the nodes were not getting upscaled (due to Volume limits)
When EBS types were changed, the nodes started to upscale.

However, no valid error message was shown either on ""Autoscaling"" (or) ""Cluster Start Up"" logs.

Hence, they raised it as an Enhancement Request to also show messages related to ""Volume Limit Exceeded"" in these cases (autoscaling of nodes)","05/Jun/17 5:20 PM;Harsh;Its been in my bucket for a while. I was talking to mayank and seems like we should use the OPS-API from master to provide a push mechanism (send a list of instance ids), where the up scaling nodes would get recorded into DB and we are in sync.

Each master making a call to API for all upscale can make large number of calls so that is one con.

We can always rely on tags but then its not multi-cloud friendly. So not a long term approach. Also, tags has its own consistency issues.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zeppelin does not honor overridden aws creds,ZEP-143,35506,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,mpatel,mpatel,01/Dec/15 3:19 PM,29/Jul/16 9:56 AM,09/Aug/17 6:03 AM,,,core,,0,jira_escalated,,,,,,,,"[~rgupta], [~bharatb], [~pseluka], [~adubey]

Smaato reported this as a pretty important blocker for them...

They configure the storage settings for the Qubole account with an IAM user which has minimal access. Then, they were hoping to use the ""Override default AWS settings"" feature. This allows a particular Qubole user to override the account's storage keys with his/her own.

This option works fine for Hadoop1, Hadoop2, as well as Spark via Analyze .. it does not work for Zeppelin.

Here's a reproducible case:

- I have created an account called MP-Qubole-2. This account's storage settings are configured with an IAM user which is limited to accessing:  ""s3://mpatel.qubole.com[/*]""
- The account has 2 users: mpatel@qubole.com, and mpatel+1@qubole.com.
- I have overridden the AWS credentials for mpatel+1@qubole.com to give that user more privileges (it has full access to all buckets in dev+external as well as read/get access to s3://paid-qubole)

1) Log in as mpatel@qubole.com and switch to MP-Qubole-2 (mpatel@qubole.com does not have creds overridden, and thus only has access to s3://mpatel.qubole.com/*)
1a) re-run 14694380 .. that should fail. --> good, that's expected

2) Log in as mpatel+1@qubole.com and switch to MP-Qubole-2 (this user has creds overridden, and has access to s3://paid-qubole, and other buckets in dev+external)
2a) re-run 14694380 -> should work
2b) re-run Spark command in Analyze 14694600 -> should work
2c) in the ExampleNote, run ""Top 10 Words In Dataset In S3"" --> This does not work... hangs indefinitely . If you check the Zeppelin logs, you would see:

{code}
WARN [2015-12-01 22:22:56,312] ({GetFileInfo #0} RestStorageService.java[performRequest]:427) - Error Response: HEAD '/default-datasets%2Fgutenberg%2Fpg20417.txt' -- ResponseCode: 403, ResponseStatus: Forbidden,
{code}

",,bharatb,drose@qubole.com,gayathrym,mahuja,mpatel,pseluka,psrinivas,rgupta,vagrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,smaato,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01npf:,,,,,,,,,,,,,,,,,,,,,2015-12-01 21:58:19.194,,,"01/Dec/15 9:58 PM;pseluka;[~rgupta] I was looking at this issue with Minesh. Summary : qbol_config does user_level_override of keys and this is getting used in Analyze

Zeppelin doesnt go through these layers. On a bigger note, think about how zeppelin can use qbol_config in general","02/Dec/15 1:03 AM;rgupta;[~mpatel] zeppelin runs on cluster directly. Zeppelin starts with wat ever creds comes via udf. I think udf always sends account creds becoz cluster is account level entity. Now zeppelin does not have any concept of user id because cluster does not have concept of user id as of now. Even if we add concept of user is in zeppelin, say user A creates a note1 and user B is trying to use note1. Should we figure this out dynamically and switch the creds to user B for note1? If we do this then some stuff in note1 may stop working......

One way to think of this problem is that clusters should support user level overrides by having object level acls and then zeppelin instance running on that cluster will get the creds with that user override. But the question of how users will share notes remain open in such a scenario....

Analyze handles all these things at the web node level and thus things are easier in analyse....

Also I think this user level overrides was designed so that account level creds are more open and user level creds constraint things. These guys are using it other way round....

[~vagrawal] [~sumitm] [~karuppayyar] thoughts on this....

Also how urgent this is...bfr we do anything this needs a discussion on wat needs to be done....i m completely out of ideas","02/Dec/15 8:40 AM;mpatel;It's pretty urgent. However, based on your points, it's evident there's no easy / clean way forward here...

Let me discuss this further with Smaato. The one way we can unblock them is to convince them to create multiple Qubole Accounts for each class of users...

",02/Dec/15 8:42 AM;rgupta;Coding this is not tough....Problem is I am not sure what we should do. I think if possible lets get on a call and see if we can figure out about what needs to be done here,30/Dec/15 6:09 PM;bharatb;Reducing priority since we are not really working on it.,"09/Jun/16 6:55 PM;gayathrym;[~karuppayyar] : Re-assigning to you, please re-assign as appropriate. ","08/Jul/16 11:11 PM;bharatb;With USER level spark applications, this comes into the realm of doable.","29/Jul/16 9:34 AM;mahuja;cc - [~vagrawal], [~karuppayyar], [~bharatb]

Users can override S3 credentials for an account. With per user interpreter, notebooks can have the ability to honor credential override for execution.

Given that notebooks can be shared and there will be Qubole's access control knobs on top, the notebook sync (sync of json) can continue to use account level S3 credentials. On the other hand execution of the notebook can use the S3 credentials of the owner of notebook. The ability to run the notebook by other users can be controlled using access control. 

Thoughts?

","29/Jul/16 9:50 AM;vagrawal;now when we can identify the user who is using the notebook, we should send his over-riden keys to notebook when he opens it.  execution of notebook should use over-ridden storage keys if available or fall back to default keys saved in the cluster.

while sharing, execution of notebook should not use owner's credentials. it would defeat the rationale to have user-level over-rides.","29/Jul/16 9:56 AM;mahuja;I might be wrong, but if the notebook's interpreter is running then it will already be using one set of S3 credentials (let's say owners). If another user starts using this notebook (or runs another para), I am not sure if the S3 credentials can be overriden in this case for that specific para/run. 

If we can override these creds, then what [~vagrawal] said, should be possible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print out directly from DataFrame mangles the string,ZEP-31,35048,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,kzhang,kzhang,17/Nov/15 10:49 AM,08/Jul/16 11:31 PM,09/Aug/17 6:03 AM,,,zep-ui,,0,jira_escalated,,,,,,,,"It looks like print out directly from DataFrame mangles the string somehow. Please see screenshot attached.

If user maps a DataFrame to RDD, or directly convert it to RDD, then print, everything looks correct. But If user directly takes rows from RDD, then print, the second column (a string) and third column appear wrong.

Oracle Spark team reported this Zendesk #3764",,gayathrym,kzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Nov/15 10:47 AM;kzhang;Screen Shot 2015-11-16 at 5.03.52 PM.png;https://qubole.atlassian.net/secure/attachment/23523/Screen+Shot+2015-11-16+at+5.03.52+PM.png,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01lej:,,,,,,,,,,,,,,,,,,,,,2016-06-09 18:57:12.569,,,"17/Nov/15 2:33 PM;kzhang;Ran the same Scala code in Spark-shell on its MN,  did not hit the issue.  So it seems an issue on Zeppelin.

<Scala>
val source = sqlContext.read.parquet(""s3://dlx-prod-core-consumer/prod/data/dlx/idgraph4/device_processing/data_graph/20151115/vertices/device"")
source.registerTempTable(""vert"")
val dev = sqlContext.sql(""select * from vert"")
dev.map(x=>x).take(5).foreach(println)
dev.rdd.take(5).foreach(println)
dev.take(5).foreach(println)
</Scala>

All three print-out results are same as following:

[300000001266279,4FDE2ADF-868B-4FBF-9124-E670A9F1AA37,device,WrappedArray(),Map(3478 -> WrappedArray(20150924)),1,-1,3,-1,WrappedArray(),WrappedArray(),-1,19000101]
[300000001266379,6991A695-CABD-4AD5-BCA1-E9D2A2539008,device,WrappedArray(),Map(3478 -> WrappedArray(20150924)),1,-1,1,-1,WrappedArray(),WrappedArray(),-1,19000101]
[300000001266479,5A1BC5AE-F013-4186-9ACF-3847452BC7DC,device,WrappedArray(),Map(3478 -> WrappedArray(20150924)),1,-1,5,-1,WrappedArray(),WrappedArray(),-1,19000101]
[300000001266579,358E2CCE-1875-42D3-97EC-12C0094EB84F,device,WrappedArray(),Map(3478 -> WrappedArray(20150924)),1,-1,5,-1,WrappedArray(),WrappedArray(),-1,19000101]
[300000001266679,BD0B8D33-B58D-4A7C-891F-39C67FCACD1E,device,WrappedArray(),Map(3478 -> WrappedArray(20150924)),1,-1,1,-1,WrappedArray(),WrappedArray(),-1,19000101]

","09/Jun/16 6:57 PM;gayathrym;[~karuppayyar] : Re-assigning to you, please re-assign as appropriate. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java sdk: jersey read timeout to be increased,SDK-60,35034,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,abhisheks,abhisheks,abhisheks,17/Nov/15 2:26 AM,05/Jun/17 5:55 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Currently it is 30 seconds.
But account api is taking more than 30 seconds and erroring out.
So increasing this to 60 seconds.",,abhisheks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01lbf:,,,,,,,,,,,,,,,,,,,1.0,14713,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test QBOL-4691: Unable to download complete report - gives 10,TES-687,34924,Bug,Open,TES,Tests,software,karthikk,,,Major,,monikak,abhisheks,abhisheks,15/Nov/15 10:41 PM,20/Mar/16 8:50 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,,,abhisheks,mpatel,,,,,,,,,,,,,,,,,,,,,,,QBOL-4691,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01l3n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pasted values show as ""......"" in red on Spark 1.5.1 Notebook",ZEP-25,34911,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,karuppayyar,kzhang,kzhang,13/Nov/15 5:09 PM,08/Jul/16 7:34 PM,09/Aug/17 6:03 AM,,,zep-ui,,0,jira_escalated,,,,,,,,"Oracle Spark user reports that he copied/pasted values from output and put it in the single select sql statements.  The pasted values look funny with those red “……” in there, and they don’t return any rows, even though the values were taken from existing rows.

He also tried manually typing in values also, and with 0 return rows too. 

Please see attached screenshot of ""Screen Shot 2015-11-13 at 4.43.47 PM.png"" ",spark 1.5.1 notebook,bharatb,kzhang,rgupta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Nov/15 5:08 PM;kzhang;Screen Shot 2015-11-13 at 4.43.47 PM.png;https://qubole.atlassian.net/secure/attachment/23440/Screen+Shot+2015-11-13+at+4.43.47+PM.png,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01l0r:,,,,,,,,,,,,,,,,,,,,,2015-11-13 17:58:53.798,,,"13/Nov/15 5:58 PM;bharatb;[~karuppayyar] please take a quick look at this. You might be able to figure out some simple workaround.
cc [~rgupta]","13/Nov/15 11:35 PM;rgupta;can we check wat browser he is using? seems to be working fine in firefox...
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Killed job is still w/ running state in AppMgr,SPAR-772,34867,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,kzhang,kzhang,09/Nov/15 1:12 PM,09/Jun/16 7:02 PM,09/Aug/17 6:03 AM,,,misc,,0,jira_escalated,,,,,,,,"Oracle reported this issue.

On a Spark 1.5.1 cluster (12115).  After user killed job 13654212 in Analyze, the AppMgr still shows this job application_1447080442153_0004 in Running state.  Please refer to attached screenshot of AppMgr.

In the log spark-yarn-org.apache.spark.deploy.history.HistoryServer-1-ip-10-108-49-106.out on the master of ec2-54-88-179-176.compute-1.amazonaws.com:
...
15/11/09 10:38:14 log-replay-executor ERROR FsHistoryProvider: Exception encountered when attempting to load application log hdfs://ec2-54-88-179-176.compute-1.amazonaws.com:9000/spark-history/application_1447080442153_0004.snappy.inprogress
java.io.IOException: failed to read chunk
at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:347)
at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)
at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)
at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)
at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)
at java.io.InputStreamReader.read(InputStreamReader.java:184)
at java.io.BufferedReader.fill(BufferedReader.java:154)
at java.io.BufferedReader.readLine(BufferedReader.java:317)
at java.io.BufferedReader.readLine(BufferedReader.java:382)
at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:67)
at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:55)
at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$replay(FsHistoryProvider.scala:528)
at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$replay(FsHistoryProvider.scala:509)
at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$10.apply(FsHistoryProvider.scala:358)
at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$10.apply(FsHistoryProvider.scala:355)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$mergeApplicationListing(FsHistoryProvider.scala:355)
at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$checkForLogs$1$$anon$2.run(FsHistoryProvider.scala:276)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
...",Spark 1.5.1 ,gayathrym,kzhang,pseluka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Nov/15 1:07 PM;kzhang;Screen Shot 2015-11-09 at 12.57.39 PM.png;https://qubole.atlassian.net/secure/attachment/23436/Screen+Shot+2015-11-09+at+12.57.39+PM.png,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01kqz:,,,,,,,,,,,,,,,,,,,,,2015-11-09 13:37:35.596,,,"09/Nov/15 1:37 PM;pseluka;[~kzhang] - Not sure where you got this stack trace - not relevant to kill issue.

Did you had any chance to see whether the app got killed after sometime (or did you try kill again) ? Is this reproducible - that can help me look at this. (or ping me if this happens in a live cluster)","11/Nov/15 8:21 PM;kzhang;Yes. The user killed the job, and after a period of time the killed job was still in running state.","09/Jun/16 7:02 PM;gayathrym;[~mahuja] : These jira_escalated tickets were assigned to Administrator, hence re-assigning to you to re-assign as appropriate for the next steps. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task took 11 hours to retry,HAD-520,34601,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,hiyer,kzhang,kzhang,29/Oct/15 12:11 AM,11/Jul/17 2:54 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"DemandBase scheduled job of 13084187 possibly hit an edge case on Hadoop cluster 9529.  A map task failed at 2015/10/28 08:22:03, the task retry started after ~11hr at 2015/10/28 18:59:49

jobID=13084187
clusterID=9529, Demandbase Production 
accountID=2588

Zendesk # 3376

https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fec2-23-22-240-73.compute-1.amazonaws.com%3A50030%2Fqboltaskdetailshistory.jsp%3Fjobid%3Djob_2588.201510280707_0003%26taskid%3Dtask_2588.201510280707_0003_m_000000

>>
DN (where task attempt_2588.201510280707_0003_m_000000_0 failed) IP : ec2-54-90-152-132.compute-1.amazonaws.com
TT log file: hadoop-hadoop-tasktracker-ip-10-143-152-54.log.2015-10-28-08
....
2015-10-28 08:22:03,482 ERROR -1 org.apache.hadoop.mapred.TaskTracker (TaskLauncher for MAP tasks): TaskTracker got stuck for localized Task:attempt_2588.201510280707_0003_m_000000_0
2015-10-28 08:22:03,482 INFO -1 org.apache.hadoop.mapred.TaskTracker (TaskLauncher for MAP tasks): Wait the localizeTask thread to finish
2015-10-28 08:22:03,482 WARN -1 org.apache.hadoop.mapred.TaskTracker (Thread-47): Deleting pre-existing jobDir: /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003 when localizeJob for tip org.apache.hadoop.mapred.TaskTracker$TaskInProgress@d37b4af6
2015-10-28 08:22:03,524 WARN -1 org.apache.hadoop.conf.Configuration (Thread-47): /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003/job.xml:a attempt to override final parameter: fs.s3.buffer.dir;  Ignoring.
2015-10-28 08:22:03,525 WARN -1 org.apache.hadoop.conf.Configuration (Thread-47): /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003/job.xml:a attempt to override final parameter: hadoop.rpc.socket.factory.class.default;  Ignoring.
2015-10-28 08:22:03,528 WARN -1 org.apache.hadoop.mapred.TaskTracker (TaskLauncher for MAP tasks): Error initializing attempt_2588.201510280707_0003_m_000000_0:
java.io.IOException: java.util.concurrent.ExecutionException: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003
        at org.apache.hadoop.mapred.TaskTracker.localizeAndLaunchTask(TaskTracker.java:2291)
        at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2221)
        at org.apache.hadoop.mapred.TaskTracker.access$1100(TaskTracker.java:125)
        at org.apache.hadoop.mapred.TaskTracker$TaskLauncher.run(TaskTracker.java:2182)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:202)
        at org.apache.hadoop.mapred.TaskTracker.localizeAndLaunchTask(TaskTracker.java:2276)
        ... 3 more
Caused by: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003
        at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1008)
        at org.apache.hadoop.mapred.TaskTracker.access$1200(TaskTracker.java:125)
        at org.apache.hadoop.mapred.TaskTracker$4.call(TaskTracker.java:2262)
        at org.apache.hadoop.mapred.TaskTracker$4.call(TaskTracker.java:2260)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.lang.Thread.run(Thread.java:745)

2015-10-28 08:22:03,528 INFO -1 org.apache.hadoop.mapred.TaskStatus (TaskLauncher for MAP tasks): Trying to set finish time for task attempt_2588.201510280707_0003_m_000000_0 when no start time is set.
...

>>
NN IP : ec2-23-22-240-73.compute-1.amazonaws.com
JT log file : hadoop-hadoop-jobtracker-ip-10-69-60-139.log.2015-10-28-08
...
2015-10-28 08:22:03,449 INFO -1 org.apache.hadoop.mapred.JobInProgress (IPC Server handler 3 on 9001): Choosing data-local task task_2588.201510280707_0003_m_000000
2015-10-28 08:22:03,450 INFO -1 org.apache.hadoop.mapred.JobTracker (IPC Server handler 3 on 9001): Adding task (MAP) 'attempt_2588.201510280707_0003_m_000000_0' to tip task_2588.201510280707_0003_m_000000, for tracker 'tracker_ip-10-143-152-54.ec2.internal:localhost/127.0.0.1:46335'
2015-10-28 08:22:03,450 INFO -1 org.apache.hadoop.mapred.JobTracker (IPC Server handler 3 on 9001): Adding task (REDUCE) 'attempt_2588.201510280707_0003_r_000000_0' to tip task_2588.201510280707_0003_r_000000, for tracker 'tracker_ip-10-143-152-54.ec2.internal:localhost/127.0.0.1:46335'
2015-10-28 08:22:03,539 INFO -1 org.apache.hadoop.mapred.TaskInProgress (IPC Server handler 11 on 9001): Error from attempt_2588.201510280707_0003_m_000000_0 on tracker_ip-10-143-152-54.ec2.internal:localhost/127.0.0.1:46335 runTime(msec) 0: Error initializing attempt_2588.201510280707_0003_m_000000_0:
java.io.IOException: java.util.concurrent.ExecutionException: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003
        at org.apache.hadoop.mapred.TaskTracker.localizeAndLaunchTask(TaskTracker.java:2291)
        at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2221)
        at org.apache.hadoop.mapred.TaskTracker.access$1100(TaskTracker.java:125)
        at org.apache.hadoop.mapred.TaskTracker$TaskLauncher.run(TaskTracker.java:2182)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:202)
        at org.apache.hadoop.mapred.TaskTracker.localizeAndLaunchTask(TaskTracker.java:2276)
        ... 3 more
Caused by: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003
        at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1008)
        at org.apache.hadoop.mapred.TaskTracker.access$1200(TaskTracker.java:125)
        at org.apache.hadoop.mapred.TaskTracker$4.call(TaskTracker.java:2262)
        at org.apache.hadoop.mapred.TaskTracker$4.call(TaskTracker.java:2260)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.lang.Thread.run(Thread.java:745)

2015-10-28 08:22:03,540 INFO -1 org.apache.hadoop.mapred.TaskErrorCollector (IPC Server handler 11 on 9001): Undefined diagnostic info:Error initializing attempt_2588.201510280707_0003_m_000000_0: java.io.IOException: java.util.concurrent.ExecutionException: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003         at org.apache.hadoop.mapred.TaskTracker.localizeAndLaunchTask(TaskTracker.java:2291)    at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2221)     at org.apache.hadoop.mapred.TaskTracker.access$1100(TaskTracker.java:125)       at org.apache.hadoop.mapred.TaskTracker$TaskLauncher.run(TaskTracker.java:2182) Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003        at java.util.concurrent.FutureTask.report(FutureTask.java:122)  at java.util.concurrent.FutureTask.get(FutureTask.java:202)     at org.apache.hadoop.mapred.TaskTracker.localizeAndLaunchTask(TaskTracker.java:2276)    ... 3 more Caused by: java.io.IOException: Not able to create job directory /media/ephemeral0/mapred/local/taskTracker/jobcache/job_2588.201510280707_0003      at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1008)      at org.apache.hadoop.mapred.TaskTracker.access$1200(TaskTracker.java:125)       at org.apache.hadoop.mapred.TaskTracker$4.call(TaskTracker.java:2262)   at org.apache.hadoop.mapred.TaskTracker$4.call(TaskTracker.java:2260)   at java.util.concurrent.FutureTask.run(FutureTask.java:262)     at java.lang.Thread.run(Thread.java:745)
2015-10-28 08:22:03,540 INFO -1 org.apache.hadoop.mapred.TaskStatus (IPC Server handler 11 on 9001): Trying to set finish time for task attempt_2588.201510280707_0003_m_000000_0 when no start time is set.
2015-10-28 08:22:03,541 INFO -1 Counters (IPC Server handler 11 on 9001): attempt_2588.201510280707_0003_m_000000_0 completion counters {""Map-Reduce Framework"":{""Spilled Records"":0}}
2015-10-28 08:22:03,541 INFO -1 org.apache.hadoop.mapred.JobTracker (IPC Server handler 11 on 9001): Removing task 'attempt_2588.201510280707_0003_m_000000_0'
2015-10-28 08:22:03,929 INFO -1 org.apache.hadoop.mapred.Bouncer (Thread-19): Job ID: job_2588.201510280707_0001 reported no progress even after wait period expired. TotalTasks: 1, FinishedTasks: 0
2015-10-28 08:22:03,929 INFO -1 org.apache.hadoop.mapred.Bouncer (Thread-19): JobID: job_2588.201510280707_0001 CPU time left: 0, avgProgressRate: NaN
2015-10-28 08:22:03,936 INFO -1 org.apache.hadoop.mapred.Bouncer (Thread-19): Request worksheet:, hustlerNodes: 2, jtTaskTrackers: 2, unfinishedMapTasks: 1, clusterMapCapacity: 8, projectedLat
ency: 120000, requestCountByTasks: -2, requestCountByCpu: 0, mapPendingCpuTimeLeft: 960000
...

Attach log files on NN ec2-23-22-240-73.compute-1.amazonaws.com and DN of ec2-54-90-152-132.compute-1.amazonaws.com",,abhishekmodi,kzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Oct/15 12:10 AM;kzhang;demandbase2588Logs.zip;https://qubole.atlassian.net/secure/attachment/23329/demandbase2588Logs.zip,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01jaj:,,,,,,,,,,,,,,,,,,,2.0,"3376,3385",2016-04-11 02:31:59.459,,,"11/Apr/16 2:31 AM;abhishekmodi;to hari for triage
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zeppelin progress bar drops from 98/100 pc to zero percent and says running,ZEP-39,34380,Bug,Open,ZEP,ZEPPELIN,software,rgupta,,,Major,,beria,venkatak,venkatak,22/Oct/15 6:25 AM,08/Dec/16 3:14 PM,09/Aug/17 6:03 AM,,,core,,0,jira_escalated,,,,,,,,"The way I could reproduce the issue:

created a table by name default.temp_default_qubole_memetracker as hive command with just a string column

Went to the Zeppelin notebook and ran this:

sqlContext.sql(""""""select * from default.default_qubole_memetracker"""""").repartition(80).saveAsTable(""default.temp_default_qubole_memetracker"")

Found the progress going upto ~100 pc and dropping back to 0 and still said running

I hope the same issue is what customer faced. Please review the zendesk ticket once as well.

",,megha,p.vasa,Ranjana,rgupta,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Oct/15 6:19 AM;venkatak;image-2015-10-22-18-49-42-213.png;https://qubole.atlassian.net/secure/attachment/23217/image-2015-10-22-18-49-42-213.png,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01ihv:,,,,,,,,,,,,,,,,,,,,,2015-11-25 09:13:01.806,,,"25/Nov/15 9:13 AM;rgupta;Thats becoz its starting 2 jobs. Fix will be to also give info about the job number whose progress we are reporting on UI.
We do plan to fix this in future",22/Feb/16 8:10 PM;venkatak;[~rgupta] Any ETA to this?,"22/Feb/16 11:13 PM;rgupta;No ETA as of now.

Assigning to [~beria]

",29/Apr/16 5:02 PM;Ranjana;[~beria] Please let us know an ETA for this. ,"18/Aug/16 10:10 AM;p.vasa;Hello, 
Is there an ETA for this issue which we can give to the customer?
Thank you.","08/Dec/16 3:14 PM;megha;[~beria]
Any ETA on this? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark DDLs fail when customer uses Thrift server with their Custom Metastore Server,SPAR-706,33813,Bug,Open,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,venkatak,venkatak,28/Sep/15 8:13 PM,20/Jun/17 1:01 PM,09/Aug/17 6:03 AM,,,sql,,0,jira_escalated,,,,,,,,"cc [~mpatel] Is there a way to solve this for Autodesk? They use custom metastore server with Spark thrift server.

Spark SQL cannot run Hive DDLs.

#10829298 seems to hang.

It also needs to work through a JDBC client connecting to the thrift server.

0: jdbc:hive2://ip-10-0-196-81.us-west-1.comp> CREATE EXTERNAL TABLE `ap_pl2_o2_activity_attri_daily_derived`(

. . . . . . . . . . . . . . . . . . . . . . .> `day` string,

. . . . . . . . . . . . . . . . . . . . . . .> `gbq_day` string,

. . . . . . . . . . . . . . . . . . . . . . .> `consumer_src` string,

. . . . . . . . . . . . . . . . . . . . . . .> `product_line_code` string,

. . . . . . . . . . . . . . . . . . . . . . .> `pim` string,

. . . . . . . . . . . . . . . . . . . . . . .> `pigs` string,

. . . . . . . . . . . . . . . . . . . . . . .> `context_user` string,

. . . . . . . . . . . . . . . . . . . . . . .> `eidm_id` string,

. . . . . . . . . . . . . . . . . . . . . . .> `csn` string,

. . . . . . . . . . . . . . . . . . . . . . .> `country` string,

. . . . . . . . . . . . . . . . . . . . . . .> `subscription_flag` string,

. . . . . . . . . . . . . . . . . . . . . . .> `industry_segment` string,

. . . . . . . . . . . . . . . . . . . . . . .> `industry_group` string,

. . . . . . . . . . . . . . . . . . . . . . .> `is_student` string,

. . . . . . . . . . . . . . . . . . . . . . .> `is_external` string,

. . . . . . . . . . . . . . . . . . . . . . .> `activity_count` bigint,

. . . . . . . . . . . . . . . . . . . . . . .> `servicename` string,

. . . . . . . . . . . . . . . . . . . . . . .> `productname` string)

. . . . . . . . . . . . . . . . . . . . . . .> ROW FORMAT DELIMITED

. . . . . . . . . . . . . . . . . . . . . . .> FIELDS TERMINATED BY '\t'

. . . . . . . . . . . . . . . . . . . . . . .> STORED AS INPUTFORMAT

. . . . . . . . . . . . . . . . . . . . . . .> 'org.apache.hadoop.mapred.TextInputFormat'

. . . . . . . . . . . . . . . . . . . . . . .> OUTPUTFORMAT

. . . . . . . . . . . . . . . . . . . . . . .> 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'

. . . . . . . . . . . . . . . . . . . . . . .> LOCATION

. . . . . . . . . . . . . . . . . . . . . . .> 's3://com.autodesk.analytics.bdp.prd/tables/pl2/o2_activity_attri_daily_derived/2-data'

. . . . . . . . . . . . . . . . . . . . . . .> ;

Error: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.IllegalArgumentException: AWS Access Key ID and Secret Access Key must be specified as the username or password (respectively) of a s3n URL, or by setting the fs.s3n.awsAccessKeyId or fs.s3n.awsSecretAccessKey properties (respectively).) (state=,code=0)

0: jdbc:hive2://ip-10-0-196-81.us-west-1.comp>",,bharatb,gayathrym,mpatel,psrinivas,rgupta,rohita,rohitk,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01ftf:,,,,,,,,,,,,,,,,,,,2.0,"2659,3243",2015-09-28 23:35:21.421,,,"28/Sep/15 8:23 PM;venkatak;Basically the AWS keys are not getting passed to the metastore server resulting in this. cc [~psrinivas] - Is there a way to pass the AWS keys to metastore server to avoid this issue. A bit of background:

Autodesk runs Spark Thrift Server on Master and their Metastore server and the database are custom managed.",28/Sep/15 8:35 PM;venkatak;Is this the answer? https://netangels.net/knowledge-base/hive-mapreduce-amazon-s3/,"28/Sep/15 11:35 PM;psrinivas;Metastore operations does not need AWS creds. Can you share the complete stacktrace.  Having said that, yes, these are the properties need to be set as mentioned in the above blog post. 

{code}
<property>
   <name>fs.s3n.awsAccessKeyId</name>
   <value>AccessKey</value>
</property>
<property>
   <name>fs.s3n.awsSecretAccessKey</name>
   <value>SecretKey</value>
</property>
{code}

Make sure both s3n and s3 variations are set. ",21/Oct/15 8:44 AM;venkatak;[~psrinivas] could you please review the log attached to the last comment on the ticket. the full stack trace is attached. This is a bit urgent.,21/Oct/15 10:04 AM;psrinivas;[~venkatak]: The stacktrace also says that the access keys are missing. ,"21/Oct/15 12:42 PM;mpatel;I am seeing something similar using a Qubole managed metastore via Analyze when running ""alter table <table> recover partitions;"" ... See: 12787970.

{code}
App > 15/10/21 19:35:36 main WARN ShellBasedUnixGroupsMapping: got exception trying to get groups for user mpatel
App > org.apache.hadoop.util.Shell$ExitCodeException: id: mpatel: no such user
App >
App > at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)
App > at org.apache.hadoop.util.Shell.run(Shell.java:379)
App > at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
App > at org.apache.hadoop.util.Shell.execCommand(Shell.java:678)
App > at org.apache.hadoop.util.Shell.execCommand(Shell.java:661)
App > at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
App > at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
App > at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
App > at org.apache.hadoop.security.Groups.getGroups(Groups.java:89)
App > at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1352)
App > at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:436)
App > at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.(HiveMetaStoreClient.java:236)
App > at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.(SessionHiveMetaStoreClient.java:74)
{code}

Same thing?
","21/Oct/15 10:01 PM;venkatak;@minesh, I have seen a similar stack above when the user's domain (for example qubole.com) is not in list of allowed domain users in Account settings.",21/Oct/15 10:33 PM;rgupta;looking at zendesk ticket this seems to be a issue with customer query?,"25/Nov/15 8:59 AM;venkatak;Customer's comment:

Do you mean putting these settings in the HiveServer2 (54.153.27.131:10008) or the Metastore server (52.8.218.175:9083)? Either way, both servers require access to multiple S3 buckets, so how do we specify different access keys for different buckets? Furthermore, I would prefer not to have specify access keys since the S3 bucket policy already grants cross-account access.

-have to check this further...","25/Jan/16 4:41 PM;rohita;What's the issue here? I see one of the tickets is closed but one is still open.
cc - [~venkatak], [~bharatb]","25/Jan/16 5:50 PM;bharatb;Last I heard from Autodesk, all their DDL issues are solved. I think we can close this JIRA.
[~venkatak] sounds good?","09/Jun/16 7:02 PM;gayathrym;[~mahuja] : These jira_escalated tickets were assigned to Administrator, hence re-assigning to you to re-assign as appropriate for the next steps. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate permissions attached to compute credentials,ACM-15,33417,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,tsp,tsp,18/Sep/15 7:39 PM,09/Jul/17 3:04 PM,09/Aug/17 6:03 AM,,release-branch-25,,,0,intern,jira_escalated,,,,,,,"We are not effectively cleaning up the creds-validation-instance t2.micros when we test creds of an account. There are a few instances lying around for many weeks now

{noformat}
$ aws ec2 describe-instances --filters ""Name=tag-key,Values=Qubole"" ""Name=tag:Qubole,Values=creds-validation-instance"" ""Name=instance-state-name,Values=running"" --query ""Reservations[*].[Instances[0].InstanceId, Instances[0].LaunchTime, Instances[0].PublicDnsName]"" --output table                                                                                                                                                           
------------------------------------------------------------------------------------------
|                                    DescribeInstances                                   |
+------------+----------------------------+----------------------------------------------+
|  i-371e409f|  2015-07-27T09:58:20.000Z  |  ec2-54-144-221-9.compute-1.amazonaws.com    |
|  i-076a2dac|  2015-08-25T11:27:20.000Z  |  ec2-54-161-248-3.compute-1.amazonaws.com    |
|  i-2bb62888|  2015-09-15T07:16:37.000Z  |  ec2-54-211-175-117.compute-1.amazonaws.com  |
|  i-2ccc718f|  2015-09-10T14:52:15.000Z  |  ec2-54-147-252-200.compute-1.amazonaws.com  |
|  i-8083cc2b|  2015-08-26T11:03:31.000Z  |  ec2-54-157-0-79.compute-1.amazonaws.com     |
|  i-bc1d5014|  2015-07-24T15:26:43.000Z  |  ec2-54-163-141-104.compute-1.amazonaws.com  |
+------------+----------------------------+----------------------------------------------+
{noformat}",,gayathrym,mahuja,p.vasa,prakharj,sureshr,tsp,vagrawal,venkatak,,,,,,,,,,,,,,,,,,,,,,QBOL-5099,,,,,,,,,,,,,,,,,,,,,,,commonfloor,,,,,"{pullrequest={dataType=pullrequest, state=DECLINED, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2016-01-20T18:56:15.000-0800"",""stateCount"":1,""state"":""DECLINED"",""open"":false},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",,,,,,,,,,,,,,,,,,,,1|z01ey3:,,,,,,,,,,,,,,,,,,,1.0,3490,2015-09-18 20:33:22.092,,,18/Sep/15 8:33 PM;mahuja;Oh wow! I assume you meant t1.mciro instance. Are we sure it is via rb-25? Do we have idea which environment generated these instances? - will need this information to look at logs.,20/Sep/15 8:30 PM;tsp;Yes. t1.micros - though I couldn't find any of them in the logs of the qa/qa2/qa3 environments. The most recent one is from Sep 15. The environment is not part of the tag of the creds-validation-instance unfortunately. Should that be added?,"21/Sep/15 11:43 AM;mahuja;@tsp - can we try it once in an environment and confirm if it is leaving behind an instance. If it does, please re-assign to Prakhar",21/Sep/15 8:22 PM;tsp;I'm not sure how this is reproduced and was hoping that the code in this area would provide some insight. Because the logs are pretty bare. Where can I look for the code so I can come up with a more educated guess of reproducible cases?,"23/Sep/15 1:05 AM;mahuja;Makes sense. Whenever one changes region or compute creds or creates a new account, a test instance will be launched.

Let me know if this doesn't reproduce the problem. I can check tomorrow.",03/Nov/15 6:14 AM;venkatak;Same issue - Commonfloor have reported today,03/Nov/15 10:28 AM;mahuja;[~venkatak] - we use t1.micro and the ticket seems to suggest t2.micro. Can you please confirm if this is the case including some tags on the instance?,03/Nov/15 10:32 AM;mahuja;[~prakharj] - let's move the instance checks in tapp to use dry run APIs. Which implies that we will not start a real instance and only check for permissions.,"03/Nov/15 6:10 PM;venkatak;[~mahuja] This JIRA also says t2.micros - may be something in common? I will however check
",03/Nov/15 6:18 PM;mahuja;[~venkatak] - also can you please get the cluster id and the date of start of the validation instance. This will be helpful in looking at logs if it is in 7 days range.,03/Nov/15 7:54 PM;tsp;[~mahuja] I *thought* we moved to t2.micros during the {{eu-central}} effort. I'm wondering if that got merged into master at some point in time. Cc  [~vagrawal],"03/Nov/15 11:53 PM;mahuja;Can confirm with [~vagrawal] but I think the problem was that t2 is available only in VPC, so we use t2 for eu-central-1 and t1 for prod.","03/Nov/15 11:56 PM;vagrawal;yeah we reverted back the change
so we use t2 for eu-central-1 and t1 for prod.",21/Dec/15 3:54 PM;mahuja;[~prakharj] - can you summarize the approach for using simulate policy here,"28/Dec/15 1:00 AM;prakharj;When customer gives us compute creds, we try to validate if Creds have permission to start and terminate the instances. Currently we are bringing up t1.micro instances and then terminating them to verify the same.
In the new approach, the idea is to check for necessary permissions instead of bringing up a real instance. We can't use the dry-run of terminate_instance api as it expects a real ec2 instance as argument.

1) The compute keys provided by the customer should have proper IAM policy that grants permissions to make ""get-user"" and ""simulate-principal-policy"" API calls
something like - 
{code:java}
{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""iam:GetUser"",
                ""iam:SimulatePrincipalPolicy""
            ],
            ""Resource"": ""*""
        }
    ]
}
{code}

2) Get the user_id, account_id using the get-user api - http://docs.aws.amazon.com/cli/latest/reference/iam/get-user.html

3) Once we have user_id, account_id, We can verify creds using the simulate-principal-policy API - http://docs.aws.amazon.com/cli/latest/reference/iam/simulate-principal-policy.html?highlight=simulatepolicy

aws iam simulate-principal-policy --policy-source-arn arn:aws:iam::<ACCOUNT ID> :user/<USER NAME> --action-names ec2:TerminateInstances 
  

More details on AWS support ticket - https://console.aws.amazon.com/support/home?region=us-east-1#/case/?displayId=1550479591&language=en","21/Jan/16 9:11 AM;mahuja;Capturing thoughts from earlier PR

{code}
Can we re-structure these tests into a series of dry run AWS API calls e.g. test for new instance, test for termination, test for tagging, test to describe an instance, test to create a security group, test to make a spot instance request. In case of roles, any specific tests around what roles can be assumed etc. (cc- Vishal Gupta, yogesh_garg). The status of each test needs to be recorded. If any of the test fails, then overall the compute verification fails.

The failure information needs to be shown to the user. Aswin Anand, mmajithia, Vikram Agrawal, Sumit Maheshwari for help on rest of the flow. UI needs to have a button to perform re-verification of compute at any point of time. Once the tests are initiated, the results populated above can be used to display the status to user. For example:
Create Instance ... Passed
Describe Instance ... Passed
Terminate Instance ... Passed
Tag Instance ... Passed
Create Security Group ... Passed
Spot Instance Request ... Passed
Qubole Key pair Imported ... Passed

This is often required in field when dealing with permissions which are very restricted.
{code}",02/Mar/16 8:54 AM;sureshr;[~vagrawal] please check with [~ajayb] if this is what we were discussing for [~surendranm] to work on.,10/Jun/16 12:33 AM;gayathrym;[~vagrawal] : Checking on the latest update on this. ,"07/Jul/17 2:54 PM;p.vasa;[~prakharj] Can we have an update on this?
We are just trying to scrub off some older JIRAs to see if we have any updates on some issues if they are in the future roadmap.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
datalogix: cassandra read is failing from spark: org.apache.thrift.transport.TTransportException: Read a negative frame size (-2113929216)!,SPAR-617,32618,Bug,Reopened,SPAR,Spark,software,mahuja,,https://qubole.atlassian.net/browse/KB-315,Major,,mahuja,mpatel,mpatel,20/Aug/15 9:04 PM,22/May/17 3:33 AM,09/Aug/17 6:03 AM,,,misc,,0,jira_escalated,,,,,,,,"After upgrading to 1.4.0, Datalogix (Powers Foss) is seeing:

org.apache.thrift.transport.TTransportException: Read a negative frame size (-2113929216)!

his note:
{code}
I am running a spark-cassandra-connector job on the hadoop2/spark cluster and it is failing with this error: ""org.apache.thrift.transport.TTransportException: Read a negative frame size (-2113929216)!"" We downgraded the version of the cassandra connector we're using to 1.2.5 which supports the version of cassandra we're using (2.0), but the error remains which make me think it's finding the spark cluster thrift client on the class path first. Perhaps the cluster has a newer version of the thrift client that it's using that is causing the error. Would explain why it's broken after the spark 1.4.0 upgrade. We tried setting --conf spark.executor.userClassPathFirst=true but that caused even more problems.

Do you think you could set the thrift client libraries to 1.2.5 for us on the hadoop2/spark cluster?
{code}

I'm not sure how this error comes up.. any ideas?
",,bharatb,gayathrym,mpatel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,datalogix,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01bgr:,,,,,,,,,,,,,,,,,,,1.0,2388,2015-08-20 21:26:24.205,,,"20/Aug/15 9:26 PM;bharatb;Tested this on vbox. Used spark-1.4.1 + cassandra 2.1.8 + cassandra-connector 1.4.0-M1. This combination works fine.

I will check with downgraded connector next.","20/Aug/15 10:18 PM;bharatb;Tried out 1.2.5 connector. It crashes spark-shell. Will try some other things out.

{code}
./bin/spark-shell --master yarn-client --jars /home/qubole/Documents/software/spark-cassandra-connector_2.10-1.2.5.jar,/home/qubole/Documents/software/cassandra-driver-core-2.1.5.jar,/home/qubole/Documents/software/cassandra-clientutil-2.1.5.jar --driver-library-path /usr/lib/hadoop2/bin/native --conf spark.cassandra.connection.host=127.0.0.1
{code}

This throws an exception:-
{code}
scala> val rdd = sc.cassandraTable(""mykeyspace"", ""users"")
warning: Class org.joda.convert.FromString not found - continuing with a stub.
error: 
     while compiling: <console>
        during phase: erasure
     library version: version 2.10.4
    compiler version: version 2.10.4
  reconstructed args: -classpath .:/home/qubole/Documents/software/spark-cassandra-connector_2.10-1.2.5.jar:/home/qubole/Documents/software/cassandra-driver-core-2.1.5.jar:/home/qubole/Documents/software/cassandra-clientutil-2.1.5.jar:/usr/lib/spark/lib/streaming/spark-streaming-twitter_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-kinesis-assembly_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/streaming/spark-streaming-flume_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/streaming/spark-streaming-flume-sink_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-zeromq_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/streaming/spark-streaming-kinesis-asl_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-twitter_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/streaming/spark-streaming-kafka_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/streaming/spark-streaming-zeromq_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-kinesis-assembly_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-kafka_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-mqtt_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-kafka-assembly_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-mqtt_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/streaming/spark-streaming-flume_2.10-1.4.0-SNAPSHOT.jar:/usr/lib/spark/lib/streaming/spark-streaming-kinesis-asl_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/streaming/spark-streaming-flume-sink_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/streaming/spark-streaming-kafka-assembly_2.10-1.4.0-SNAPSHOT-tests.jar:/usr/lib/spark/lib/hive2/hive-common-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-qubole-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-shims-0.23-0.13.1.jar:/usr/lib/spark/lib/hive2/libfb303-0.9.0.jar:/usr/lib/spark/lib/hive2/hive-ant-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-testutils-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-metastore-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-cli-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-shims-common-secure-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-contrib-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-shims-0.20S-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-shims-common-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-service-0.13.1.jar:/usr/lib/spark/lib/hive2/mysql-connector-java-5.1.35-bin.jar:/usr/lib/spark/lib/hive2/csv-serde-1.1.2.jar:/usr/lib/spark/lib/hive2/hive-shims-0.20-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-serde-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-hbase-handler-0.13.1.jar:/usr/lib/spark/lib/hive2/antlr-runtime-3.4.jar:/usr/lib/spark/lib/hive2/hive-shims-0.13.1.jar:/usr/lib/spark/lib/hive2/hive-exec-0.13.1.jar:/usr/lib/spark/lib/hive2/mysql-connector-java-5.1.34.jar:/usr/lib/spark/lib/hive2/ST4-4.0.4.jar

  last tree to typer: Apply(value $outer)
              symbol: value $outer (flags: <method> <synthetic> <stable> <expandedname> <triedcooking>)
   symbol definition: val $outer(): $iwC.$iwC.type
       symbol owners: value $outer -> class $iwC -> class $iwC -> class $iwC -> class $read -> package $line19
      context owners: value x$6 -> value rdd -> class $iwC -> class $iwC -> class $iwC -> class $iwC -> class $iwC -> class $iwC -> class $iwC -> class $iwC -> class $iwC -> class $iwC -> class $read -> package $line19

== Enclosing template or block ==

Template( // val <local $iwC>: <notype>, tree.tpe=$iwC
  ""java.lang.Object"", ""scala.Serializable"" // parents
  ValDef(
    private
    ""_""
    <tpt>
    <empty>
  )
  // 5 statements
  DefDef( // def <init>(arg$outer: $iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.type): $iwC
    <method> <triedcooking>
    ""<init>""
    []
    // 1 parameter list
    ValDef( // $outer: $iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.type
      <param>
      ""$outer""
      <tpt> // tree.tpe=$iwC
      <empty>
    )
    <tpt> // tree.tpe=$iwC
    Block(
      Apply( // def <init>(): Object in class Object
        $iwC.super.""<init>"" // def <init>(): Object in class Object
        Nil
      )
      ()
    )
  )
  ValDef( // private[this] val rdd: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]
    private <local> <triedcooking>
    ""rdd ""
    <tpt> // tree.tpe=com.datastax.spark.connector.rdd.CassandraTableScanRDD
    Block( // tree.tpe=com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]
      // 8 statements
      ValDef( // val qual$1: com.datastax.spark.connector.SparkContextFunctions
        <triedcooking>
        ""qual$1""
        <tpt> // tree.tpe=com.datastax.spark.connector.SparkContextFunctions
        Apply( // implicit def toSparkContextFunctions(sc: org.apache.spark.SparkContext): com.datastax.spark.connector.SparkContextFunctions in package connector
          ""com"".""datastax"".""spark"".""connector"".""package"".""toSparkContextFunctions"" // implicit def toSparkContextFunctions(sc: org.apache.spark.SparkContext): com.datastax.spark.connector.SparkContextFunctions in package connector
          Apply( // val sc(): org.apache.spark.SparkContext
            $iwC.this.$line19$$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$$outer().$line19$$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$$outer().$line19$$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$$outer().$line19$$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$$outer().$line19$$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$$outer().$line19$$read$$iwC$$iwC$$iwC$$iwC$$iwC$$$outer().$line19$$read$$iwC$$iwC$$iwC$$iwC$$$outer().$line19$$read$$iwC$$iwC$$iwC$$$outer().$line19$$read$$iwC$$iwC$$$outer().$VAL17().$iw().$iw().""sc"" // val sc(): org.apache.spark.SparkContext
            Nil
          )
        )
      )
      ValDef( // val x$1: String
        0
        ""x$1""
        <tpt> // tree.tpe=String
        ""mykeyspace""
      )
      ValDef( // val x$2: String
        0
        ""x$2""
        <tpt> // tree.tpe=String
        ""users""
      )
      ValDef( // val x$3: scala.reflect.ClassTag[com.datastax.spark.connector.CassandraRow] @scala.reflect.internal.annotations.uncheckedBounds
        0
        ""x$3""
        <tpt> // tree.tpe=reflect.ClassTag
        Apply( // def apply[T](runtimeClass1: Class[_]): scala.reflect.ClassTag[T] in object ClassTag, tree.tpe=scala.reflect.ClassTag[com.datastax.spark.connector.CassandraRow]
          TypeApply( // def apply[T](runtimeClass1: Class[_]): scala.reflect.ClassTag[T] in object ClassTag, tree.tpe=(runtimeClass1: Class[_])scala.reflect.ClassTag[com.datastax.spark.connector.CassandraRow]
            ""ClassTag"".""apply"" // def apply[T](runtimeClass1: Class[_]): scala.reflect.ClassTag[T] in object ClassTag
            <tpt> // tree.tpe=com.datastax.spark.connector.CassandraRow
          )
          classOf[com.datastax.spark.connector.CassandraRow]
        )
      )
      ValDef( // val x$4: com.datastax.spark.connector.rdd.reader.RowReaderFactory.GenericRowReader$.type
        0
        ""x$4""
        <tpt> // tree.tpe=com.datastax.spark.connector.rdd.reader.RowReaderFactory.GenericRowReader$.type
        reader.this.""RowReaderFactory"".""GenericRowReader$"" // implicit object GenericRowReader$ in object RowReaderFactory
      )
      ValDef( // val x$5: com.datastax.spark.connector.rdd.ValidRDDType[com.datastax.spark.connector.CassandraRow] @scala.reflect.internal.annotations.uncheckedBounds
        0
        ""x$5""
        <tpt> // tree.tpe=com.datastax.spark.connector.rdd.ValidRDDType
        Apply( // implicit def javaSerializableAsValidRDDType[T <: java.io.Serializable](): com.datastax.spark.connector.rdd.ValidRDDType[T] in object ValidRDDType, tree.tpe=com.datastax.spark.connector.rdd.ValidRDDType[com.datastax.spark.connector.CassandraRow]
          TypeApply( // implicit def javaSerializableAsValidRDDType[T <: java.io.Serializable](): com.datastax.spark.connector.rdd.ValidRDDType[T] in object ValidRDDType, tree.tpe=()com.datastax.spark.connector.rdd.ValidRDDType[com.datastax.spark.connector.CassandraRow]
            rdd.this.""ValidRDDType"".""javaSerializableAsValidRDDType"" // implicit def javaSerializableAsValidRDDType[T <: java.io.Serializable](): com.datastax.spark.connector.rdd.ValidRDDType[T] in object ValidRDDType
            <tpt> // tree.tpe=com.datastax.spark.connector.CassandraRow
          )
          Nil
        )
      )
      ValDef( // val x$6: com.datastax.spark.connector.cql.CassandraConnector
        0
        ""x$6""
        <tpt> // tree.tpe=com.datastax.spark.connector.cql.CassandraConnector
        Apply( // def cassandraTable$default$3[T](keyspace: String,table: String): com.datastax.spark.connector.cql.CassandraConnector @scala.annotation.unchecked.uncheckedVariance in class SparkContextFunctions, tree.tpe=com.datastax.spark.connector.cql.CassandraConnector
          TypeApply( // def cassandraTable$default$3[T](keyspace: String,table: String): com.datastax.spark.connector.cql.CassandraConnector @scala.annotation.unchecked.uncheckedVariance in class SparkContextFunctions, tree.tpe=(keyspace: String, table: String)com.datastax.spark.connector.cql.CassandraConnector
            ""qual$1"".""cassandraTable$default$3"" // def cassandraTable$default$3[T](keyspace: String,table: String): com.datastax.spark.connector.cql.CassandraConnector @scala.annotation.unchecked.uncheckedVariance in class SparkContextFunctions, tree.tpe=[T](keyspace: String, table: String)com.datastax.spark.connector.cql.CassandraConnector
            <tpt> // tree.tpe=com.datastax.spark.connector.CassandraRow
          )
          // 2 arguments
          ""x$1"" // val x$1: String, tree.tpe=String
          ""x$2"" // val x$2: String, tree.tpe=String
        )
      )
      ValDef( // val x$7: com.datastax.spark.connector.rdd.ReadConf
        0
        ""x$7""
        <tpt> // tree.tpe=com.datastax.spark.connector.rdd.ReadConf
        Apply( // def cassandraTable$default$4[T](keyspace: String,table: String): com.datastax.spark.connector.rdd.ReadConf @scala.annotation.unchecked.uncheckedVariance in class SparkContextFunctions, tree.tpe=com.datastax.spark.connector.rdd.ReadConf
          TypeApply( // def cassandraTable$default$4[T](keyspace: String,table: String): com.datastax.spark.connector.rdd.ReadConf @scala.annotation.unchecked.uncheckedVariance in class SparkContextFunctions, tree.tpe=(keyspace: String, table: String)com.datastax.spark.connector.rdd.ReadConf
            ""qual$1"".""cassandraTable$default$4"" // def cassandraTable$default$4[T](keyspace: String,table: String): com.datastax.spark.connector.rdd.ReadConf @scala.annotation.unchecked.uncheckedVariance in class SparkContextFunctions, tree.tpe=[T](keyspace: String, table: String)com.datastax.spark.connector.rdd.ReadConf
            <tpt> // tree.tpe=com.datastax.spark.connector.CassandraRow
          )
          // 2 arguments
          ""x$1"" // val x$1: String, tree.tpe=String
          ""x$2"" // val x$2: String, tree.tpe=String
        )
      )
      Apply( // def cassandraTable[T](keyspace: String,table: String,implicit connector: com.datastax.spark.connector.cql.CassandraConnector,implicit readConf: com.datastax.spark.connector.rdd.ReadConf,implicit ct: scala.reflect.ClassTag[T],implicit rrf: com.datastax.spark.connector.rdd.reader.RowReaderFactory[T],implicit ev: com.datastax.spark.connector.rdd.ValidRDDType[T]): com.datastax.spark.connector.rdd.CassandraTableScanRDD[T] in class SparkContextFunctions, tree.tpe=com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]
        TypeApply( // def cassandraTable[T](keyspace: String,table: String,implicit connector: com.datastax.spark.connector.cql.CassandraConnector,implicit readConf: com.datastax.spark.connector.rdd.ReadConf,implicit ct: scala.reflect.ClassTag[T],implicit rrf: com.datastax.spark.connector.rdd.reader.RowReaderFactory[T],implicit ev: com.datastax.spark.connector.rdd.ValidRDDType[T]): com.datastax.spark.connector.rdd.CassandraTableScanRDD[T] in class SparkContextFunctions, tree.tpe=(keyspace: String, table: String, implicit connector: com.datastax.spark.connector.cql.CassandraConnector, implicit readConf: com.datastax.spark.connector.rdd.ReadConf, implicit ct: scala.reflect.ClassTag[com.datastax.spark.connector.CassandraRow], implicit rrf: com.datastax.spark.connector.rdd.reader.RowReaderFactory[com.datastax.spark.connector.CassandraRow], implicit ev: com.datastax.spark.connector.rdd.ValidRDDType[com.datastax.spark.connector.CassandraRow])com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]
          ""qual$1"".""cassandraTable"" // def cassandraTable[T](keyspace: String,table: String,implicit connector: com.datastax.spark.connector.cql.CassandraConnector,implicit readConf: com.datastax.spark.connector.rdd.ReadConf,implicit ct: scala.reflect.ClassTag[T],implicit rrf: com.datastax.spark.connector.rdd.reader.RowReaderFactory[T],implicit ev: com.datastax.spark.connector.rdd.ValidRDDType[T]): com.datastax.spark.connector.rdd.CassandraTableScanRDD[T] in class SparkContextFunctions, tree.tpe=[T](keyspace: String, table: String, implicit connector: com.datastax.spark.connector.cql.CassandraConnector, implicit readConf: com.datastax.spark.connector.rdd.ReadConf, implicit ct: scala.reflect.ClassTag[T], implicit rrf: com.datastax.spark.connector.rdd.reader.RowReaderFactory[T], implicit ev: com.datastax.spark.connector.rdd.ValidRDDType[T])com.datastax.spark.connector.rdd.CassandraTableScanRDD[T]
          <tpt> // tree.tpe=com.datastax.spark.connector.CassandraRow
        )
        // 7 arguments
        ""x$1"" // val x$1: String, tree.tpe=String
        ""x$2"" // val x$2: String, tree.tpe=String
        ""x$6"" // val x$6: com.datastax.spark.connector.cql.CassandraConnector, tree.tpe=com.datastax.spark.connector.cql.CassandraConnector
        ""x$7"" // val x$7: com.datastax.spark.connector.rdd.ReadConf, tree.tpe=com.datastax.spark.connector.rdd.ReadConf
        ""x$3"" // val x$3: scala.reflect.ClassTag[com.datastax.spark.connector.CassandraRow] @scala.reflect.internal.annotations.uncheckedBounds, tree.tpe=scala.reflect.ClassTag[com.datastax.spark.connector.CassandraRow] @scala.reflect.internal.annotations.uncheckedBounds
        ""x$4"" // val x$4: com.datastax.spark.connector.rdd.reader.RowReaderFactory.GenericRowReader$.type, tree.tpe=com.datastax.spark.connector.rdd.reader.RowReaderFactory.GenericRowReader$.type
        ""x$5"" // val x$5: com.datastax.spark.connector.rdd.ValidRDDType[com.datastax.spark.connector.CassandraRow] @scala.reflect.internal.annotations.uncheckedBounds, tree.tpe=com.datastax.spark.connector.rdd.ValidRDDType[com.datastax.spark.connector.CassandraRow] @scala.reflect.internal.annotations.uncheckedBounds
      )
    )
  )
  DefDef( // val rdd(): com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]
    <method> <stable> <accessor>
    ""rdd""
    []
    List(Nil)
    <tpt> // tree.tpe=com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]
    $iwC.this.""rdd "" // private[this] val rdd: com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow], tree.tpe=com.datastax.spark.connector.rdd.CassandraTableScanRDD[com.datastax.spark.connector.CassandraRow]
  )
  ValDef( // protected val $outer: $iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.type
    protected <synthetic> <paramaccessor> <triedcooking>
    ""$outer ""
    <tpt> // tree.tpe=$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.type
    <empty>
  )
  DefDef( // val $outer(): $iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.type
    <method> <synthetic> <stable> <expandedname> <triedcooking>
    ""$line19$$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$$outer""
    []
    List(Nil)
    <tpt> // tree.tpe=Any
    $iwC.this.""$outer "" // protected val $outer: $iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.type, tree.tpe=$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.$iwC.type
  )
)

uncaught exception during compilation: scala.reflect.internal.Types$TypeError
scala.reflect.internal.Types$TypeError: bad symbolic reference. A signature in CassandraClientProxy.class refers to term thrift
in package org.apache.cassandra which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling CassandraClientProxy.class.
That entry seems to have slain the compiler.  Shall I replay
your session? I can re-run each line except the last one.
[y/n]
Abandoning crashed session.
{code}","20/Aug/15 10:26 PM;bharatb;Looks like all resistance is futile here. Looking at the version compatibility chart here (https://github.com/datastax/spark-cassandra-connector), it appears spark 1.4 and 1.3 don't support the cassandra version used by the customer.

* [~mpatel] can we ask them to upgrade cassandra itself?
* If they cant upgrade, we will need to run spark-1.2 jars on rb24 clusters. But that might be painful since it is untested.",20/Aug/15 10:31 PM;mpatel;Thanks for looking into this - i have asked if they can upgrade Cassandra...,11/Sep/15 7:24 PM;bharatb;Closing this since we can't do much other than waiting for the customer to upgrade cassandra.,"16/Sep/15 2:19 PM;mpatel;Hi [~bharatb]

Powers upgraded to Cassandra 2.1.9. It uses libthrift-0.9.2.jar and thrift-server-0.3.7.jar.

They are still seeing this error... any ideas? See: 10856341","25/Sep/15 9:00 AM;mpatel;[~bharatb], [~rgupta], [~pseluka] any ideas about this one?","28/Sep/15 3:38 AM;bharatb;Just tried out with cassandra 2.1.9 with the following command line and it works:

{code}
/usr/lib/spark/bin/spark-shell --packages 'com.datastax.spark:spark-cassandra-connector_2.10:1.4.0'
{code}

No jars need to be downloaded. Nothing extra is needed. The only assumption here is that the cassandra server is running on the same node where the spark-shell is being started. This assumption can be dropped with appropriate setting for spark.cassandra.connection.host.

Probably one of us can test out on their cluster itself.","09/Jun/16 7:02 PM;gayathrym;[~mahuja] : These jira_escalated tickets were assigned to Administrator, hence re-assigning to you to re-assign as appropriate for the next steps. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump up default value of mapred.task.timeout ?,HAD-480,31825,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,jssarma,adubey,addon_zendesk_for_jira,26/Jul/15 6:56 PM,11/Jul/17 2:53 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"We always get to see these problems where tasks fail because of timeout.
Everytime we recommend them to bump up this value to something higher. I was wondering if there is no disadvantage - can't we just default it to higher value like half hour or so.",,addon_zendesk_for_jira,mahuja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,1|z01717:,,,,,,,,,,,,,,,,,,,1.0,2196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler job time editing - requires better messaging or UI support,UI-3542,31093,Bug,Open,UI,UI,software,aswina,,,Major,,mukundag,adubey,addon_zendesk_for_jira,04/Jul/15 1:57 PM,11/Jul/17 3:48 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"Today if i want to change the scheduled time of my job -i need to pick a start time carefully so that it does not violate the condition of being > previous run time.

we should have a clear option to change the time alone. ( currently time is tied with date and that causes confusion to user )


cc: [~rvenkatesh]",,addon_zendesk_for_jira,aswina,gayathrym,karthikb,mmajithia,mukundag,nimitk,rvenkatesh,sam,svstaden,,,,,,,,,,,,,,,,SCHED-96,,UI-4000,,UI-2369,TES-1124,DOC-320,UI-4235,,,,,,,,,,,,,,,,,,,,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2015-09-23T19:46:44.000-0700"",""stateCount"":1,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",,,,,,,,,,No,,,,,,,,,,1|z013nq:r,,,,,,,,,,,,,,,,,,,1.0,2002,2015-07-14 05:49:29.888,,,14/Jul/15 5:49 AM;rvenkatesh;[~mmajithia] - One question. Can the UI detect that only the time has changed and only send the time part ?,"20/Jul/15 12:33 AM;mmajithia;[~rvenkatesh]: There is no easy way, but we can write code to compare original values and new values to find what has changed. But it can be tricky considering the workflow data.","20/Jul/15 12:55 AM;rvenkatesh;I am asking because if you can send the time only, then we can validate just the time. If you send us date and time, the intent of the user is unknown - did he want to change the date too or did the UI just send it ...","28/Jul/15 11:43 PM;mmajithia;Discussed with [~rvenkatesh], he will add backend support after which we will add UI provision to specify only time.","18/Aug/15 9:37 AM;aswina;[~vagrawal], can [~adeshr] pick up the middleware changes required for this? [~mukundag] can pick up the UI changes.",06/Oct/15 1:37 AM;rvenkatesh;[~adubey][~xing] FYI this is assigned back to me. We dont have any bandwidth right now to take it up. All of us are busy with Nezha stuff. This is going on the back burner for the foreseeable future. ,06/Oct/15 6:51 AM;aswina;[~mukundag] - Would you be able to pick up the ruby changes also?,06/Oct/15 11:45 PM;mukundag;[~aswina] I think [~tanishg] has already worked on the ruby changes and has a PR,"06/Oct/15 11:59 PM;rvenkatesh;No. Its incomplete.

On Wed, Oct 7, 2015 at 12:16 PM, Mukunda Gogoi (JIRA) <




-- 
Rajat Venkatesh | Engg Lead
Qubole Inc | www.qubole.com
","21/Mar/16 2:54 AM;gayathrym;[~rvenkatesh] : Marking this as in-rb31 based on the below commits to MASTER, please advise if this is already gone out to prod in an earlier release and I will update the labels accordingly. 

*  354f6729fadcdba1102317eb244b5b4f8d184574 , Wed Jan 20 11:51:02 2016 , hustler , master, QBOL-4536,  rvenkatesh@qubole.com , new:usr:QBOL-4536:Support edit of time part of start time only 

*  354f6729fadcdba1102317eb244b5b4f8d184574 , Wed Jan 20 11:51:02 2016 , hive_scripts , master, QBOL-4536,  rvenkatesh@qubole.com , new:usr:QBOL-4536:Support edit of time part of start time only 

*  354f6729fadcdba1102317eb244b5b4f8d184574 , Wed Jan 20 11:51:02 2016 , tapp2 , master, QBOL-4536,  rvenkatesh@qubole.com , new:usr:QBOL-4536:Support edit of time part of start time only ","29/Mar/16 6:53 AM;gayathrym;To the Dev engineer who fixed the issue :

* Which repositories was the fix merged to? (in case of fixes spanning multiple repos)
* Which tiers will need to be re-deployed to for the fix to be tested?
* Will your fix need an AMI to be baked?
* Is the fix backward compatible? 
* Is this GATED behind a setting (applicable for Features and improvements). 
* For new features: Has KB and DOC impact been evaluated?
* Specific instructions for testing - 
** log message to watch for
** overridden configurations to apply
** account gates to be toggled
** specific processes to monitor
** Steps needed to reproduce bug (if not filed by QA)
** in case of customer escalated JIRAs - having the zendesk information is useful in reproducing with and without the fix applied
",06/Apr/16 9:37 PM;karthikb;Bug found :SCHED-96,"06/Apr/16 10:48 PM;gayathrym;[~karthikb] : Is SCHED-96 a push blocker for RB31 or can it be handled later? 

CC [~sureshr] and [~tsp].","06/Apr/16 11:18 PM;rvenkatesh;It's not a push blocker. It's a new undocumented feature.
On Thu, 7 Apr 2016 at 11:19, Gayathry (JIRA) <jira@qubole.atlassian.net>

","10/Apr/16 9:42 PM;gayathrym;Fix is not working as expected, so we are removing the in-RB31 tag. We will handle this in a later release. ",11/Apr/16 12:36 AM;aswina;[~mukundag] - Can you pick this up in context of UI-2369? [~rvenkatesh] should be able to help you out,25/Apr/16 11:02 PM;mukundag;[~aswina] Discussed with [~rvenkatesh] He mentioned after SCHED-96 the complexity has escalated so he will be taking care of it.,06/Sep/16 12:19 PM;sam;Any updates on this? Clickagy is asking it.. ,06/Sep/16 8:54 PM;mukundag;[~sam] Pending on SCHED-96,06/Sep/16 10:37 PM;rvenkatesh;FYI - I am not working on SCHED-96. [~sureshr] I have assigned it to you to set priority and an owner. ,"15/Sep/16 6:18 AM;sam;Clickagy has ran into the problem of not being able to change start time, and asked me if we plan addressing this.. Any updates? Overall it is not a critical issue.",15/Sep/16 6:45 AM;mukundag;[~sam] The api fix is being picked up by [~rahulg] next sprint. Once he is done with this fix I will work on the UI mostly minor changes.,19/Jan/17 6:27 AM;svstaden;Fromthebench are hitting the same issue. When is this going to be addressed?,22/Jan/17 10:41 PM;aswina;[~mukundag] [~rahulg] - What exactly are we blocked on for this change?,22/Jan/17 10:47 PM;mukundag;Its blocked by SCHED-96,22/Jan/17 10:50 PM;aswina;[~sumitm] [~rahulg] - When can we pick up SCHED-96?  cc: [~sureshr],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST + SDK: Refresh Table ?,SDK-14,30104,Bug,Open,SDK,SDK,software,abhijitj,,,Major,,vagrawal,mpatel,mpatel,04/Jun/15 9:41 AM,18/Jul/17 1:15 AM,09/Aug/17 6:03 AM,,,,,0,bootcamp,intern,jira_escalated,sdk,,,,,"[~dineshr]

I dont see ""Refresh Table"" available in the python SDK. Is it there in REST?

Need at least rest for now for TubeMogul...",,ksr,mpatel,vagrawal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TubeMogul,,,,,"{pullrequest={dataType=pullrequest, state=DECLINED, stateCount=1}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":1,""lastUpdated"":""2015-06-17T16:39:31.000-0700"",""stateCount"":1,""state"":""DECLINED"",""open"":false},""byInstanceType"":{""bitbucket"":{""count"":1,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",,,,,,,,,,,,,,,,,,,,1|z00ycr:,,,,,,,,,,,,,,,,,,,1.0,15734,2015-07-21 04:22:23.201,,,"21/Jul/15 4:22 AM;ksr;Yes, it is there in REST. The following command can be used:
curl  -i -X POST -H ""X-AUTH-TOKEN: $AUTH_TOKEN"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -d '{ ""db_name"":""DB_NAME"", ""hive_table"":""TABLE_NAME"", ""loader_stable"":""1"", ""loader_stable_mult"":""Minutes"", ""create_loader"":""1"", ""template"":""s3import"", ""label"":""default"" }' ""http://colonelhathi.qubole.com/api/v1.2/commands""

I will also try to implement it in Python SDK","22/Jul/15 3:12 AM;ksr;I have made changes in the SDK to implement refresh table. Waiting for PR to be approved.
This can be used refresh table from SDK:

qds.py --token '$AUTH_TOKEN' --url ""http://api.qubole.com/api"" hivecmd run --template ""s3import"" --db_name ""DB_NAME"" --hive_table ""TABLE_NAME"" --loader_stable ""1"" --loader_stable_mult ""Minutes"" --create_loader ""1"" --cluster-label ""CLUSTER_NAME""","22/Jul/15 6:54 AM;mpatel;Thanks - what is ""template"" for? ","26/Jul/15 10:47 PM;ksr;Because refresh table is also a hive command, it uses a different template (s3import) from the other hive commands.","11/Dec/15 2:29 AM;ksr;[~rohita]
this is pending on you. today is my last day, hence, assigning it to you
https://github.com/qubole/qds-sdk-py/pull/113",22/Feb/16 12:50 AM;vagrawal;[~sourabhg] - can you please look at the pr and approve :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not copy files to s3ncache if its size is bigger than a certain limit,HAD-404,29024,Bug,Open,HAD,Hadoop,software,hiyer,,,Minor,,psrinivas,rjain,rjain,27/Apr/15 3:01 PM,21/Jun/17 9:22 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"s3ncache can easily get filled up if people are large files.

Take for example the query:
https://api.qubole.com/v2/analyze?command_id=5876796&qbol_user_id=5088

which
adds s3://pinlogs/annotations/classifier/tp-test/mallet_source/1M_board_no_repeat_grams.mallet
in the -files option. This file is 6.8GB. Multiple such jobs (with
different files of course) brought down the master node on Pinterest's
cluster. We should prevent copying jars to s3ncache if a file size is
greater than a certain (configurable) limit.
",,psrinivas,rjain,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{repository={count=2, dataType=repository}, json={""cachedValue"":{""errors"":[],""summary"":{""repository"":{""overall"":{""count"":2,""lastUpdated"":""2015-05-06T16:25:11.000-0700""},""byInstanceType"":{""bitbucket"":{""count"":2,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",,,,,,,,,,,,,,,,,,,,1|z00ta3:,,,,,,,,,,,,,,,,,,,1.0,1417,2015-05-06 10:25:46.136,,,"06/May/15 10:25 AM;psrinivas;Did modifications to Cache.java, but does not seem to be real culprit. 

[~rjain]: May need to dig deeper, who actually caches them to s3ncache. ",06/May/15 10:55 AM;rjain;This was primarily a problem for pinterest which was adding huge files to their hadoop jobs via the {{-files}} option. We have added crons to periodically clean up cache so we can reduce the priority of this issue for now.,"06/May/15 10:28 PM;psrinivas;Made some changes to Cache.java. Noting here to keep track of the changes. (Changes are incomplete though)

https://bitbucket.org/qubole/fb-hadoop-20/branch/HAD-404

-- Added checks to verify the size of jar/file. 
-- Questions remaining:
       What should we return in case the jar/file is greater than the threshold. Should we return null or should we return the original fs url? There are lot assumptions made about the returns types across the usages. Need a proper clean-up and set right expectations. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Another Deadlock with Shell Commands and Volatile Nodes,HADTWO-221,27825,Bug,Open,HADTWO,Hadoop2,software,ajayb,,,Minor,,sourabhg,rohita,rohita,17/Mar/15 2:59 PM,04/Jul/17 1:39 AM,09/Aug/17 6:03 AM,,,,,0,hadoop2-fairscheduler,jira_escalated,,,,,,,"As part of HADTWO-200, we introduced {{maxMetaAMShare}} to limit the share of shell command containers. That solves the problem for all on-demand clusters.

Now assume that you have a mixed on-demand and spot cluster.
Let's says there are 4 slave nodes: 1 on-demand and 3 spot. Each node can run 3 containers.
Let's say the {{maxAMShare}} is 0.5 and the {{maxMetaAMShare}} is 0.25. Let's say that 3 shell commands were fired simultaneously and they all get scheduled on the on-demand nodes (this is allowed because 3/12 = 0.25 <= 0.25). Now, the maxAMShare allow 3 more containers to be scheduled. BUT they can't be scheduled because the remaining nodes are volatile and we don't schedule AMs on volatile nodes. Deadlock.",,abhishekmodi,adubey,gmargabanthu,jssarma,mahuja,pseluka,Ranjana,rjain,rohita,sbadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,expedia,,,,,{},,,,,,,,,,,,,,,,,,,,1|z00nhf:,,,,,,,,,,,,,,,,,,,1.0,9960,2016-06-14 10:20:37.985,,,14/Jun/16 10:20 AM;adubey;[~abhishekmodi] is it still open?,"14/Jun/16 10:36 PM;abhishekmodi;Yes, it's still open, we didn't see this in prod yet so never got prioritised.",15/Jun/16 1:20 AM;mahuja;Isn't there a case where AMs can be scheduled on volatile nodes if count of nodes was under a threshold? Cc [~abhishekmodi],15/Jun/16 1:32 AM;abhishekmodi;[~mahuja] that fix is for 100% spot cluster. In that case we allow AMs to be scheduled on spot nodes. I think we can fix it in a proper way by calculating AM share based on stable nodes only.,"15/Jun/16 1:55 AM;mahuja;I some how recollect that there was a plan to allow AMs on volatile if number of nodes are <=5. That would have been applicable to all hybrid clusters as well. But I don't see reference to this in documentation, so it is possible it was only in discussion phase.

For completion,  yarn.scheduler.qubole.am-on-stable.timeout.ms is also available for scheduling AMs on spot after the set time interval.","17/Jan/17 5:06 PM;sbadam;Hey Sourabh, can we have any update or ETA for this issue? Thanks.",17/Jan/17 7:49 PM;abhishekmodi;[~sbadam] Could you please share a command id where it happened? This jira has been kept for tracking purposes and we have never seen it in production.,"18/Jan/17 11:35 AM;sbadam;I am just reviewing the backlog and this issue is touched in the process. Now that I understand the purpose of this ticket, we will report similar issues(along with command id). ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig leaves temp- data in /tmp HDFS location,QPIG-12,27570,Bug,Open,QPIG,qbol pig,software,bharatb,,,Major,Done,beria,venkatak,venkatak,05/Mar/15 9:38 PM,07/Jul/17 2:34 PM,09/Aug/17 6:03 AM,15/Apr/15 5:39 AM,,,,0,jira_escalated,,,,,,,,"Pig leaves data in ""temp-"" folders in /tmp HDFS location ",,abhisheks,ajayb,snamburu,swatis,tanishg,venkatak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{pullrequest={dataType=pullrequest, state=OPEN, stateCount=2}, json={""cachedValue"":{""errors"":[],""summary"":{""pullrequest"":{""overall"":{""count"":2,""lastUpdated"":""2015-04-06T17:04:20.000-0700"",""stateCount"":2,""state"":""OPEN"",""open"":true},""byInstanceType"":{""bitbucket"":{""count"":2,""name"":""Bitbucket Cloud""}}}}},""isStale"":true}}",,,,,,,,,,,,,,,,,,,,1|z00lzv:,,,,,,,,,,,,,,,,,,,1.0,5832,2015-03-19 06:01:53.825,,,"19/Mar/15 5:22 AM;venkatak;To reproduce the issue, launch the pig script in api.qubole.com s3://prod.qubole.com/solutions/scripts/hadoop/pigscript.pig and kill the script before it completes. the issue can be reproduced. See command id 5056187 in production

[ec2-user@ip-10-68-54-69 tmp]$ hadoop dfs -du /tmp/

640999      hdfs://ec2-107-21-179-161.compute-1.amazonaws.com:9000/tmp/e29114c7c31a4465a4aa47febc671945
640999      hdfs://ec2-107-21-179-161.compute-1.amazonaws.com:9000/tmp/f1556751318e4dd89eea1d0fad42e689
11341525    hdfs://ec2-107-21-179-161.compute-1.amazonaws.com:9000/tmp/mapred
0           hdfs://ec2-107-21-179-161.compute-1.amazonaws.com:9000/tmp/temp779316635

But that did not create a temp- folder i dont know how that ""-"" comes in...","19/Mar/15 6:01 AM;swatis;The jobconf has these configurations

{code}
job_3082.201503191244_0001_conf.xml:<property><!--Loaded from /media/ephemeral1/mapred/local/jobTracker/job_3082.201503191244_0001.xml--><name>pig.streaming.task.output.dir</name><value>hdfs://ec2-54-221-155-172.compute-1.amazonaws.com:9000/tmp/temp-1899968498/tmp625305144</value></property>
job_3082.201503191244_0001_conf.xml:<property><!--Loaded from /media/ephemeral1/mapred/local/jobTracker/job_3082.201503191244_0001.xml--><name>mapred.output.dir</name><value>hdfs://ec2-54-221-155-172.compute-1.amazonaws.com:9000/tmp/temp-1899968498/tmp625305144</value></property>
job_3082.201503191244_0001_conf.xml:<property><!--Loaded from /media/ephemeral1/mapred/local/jobTracker/job_3082.201503191244_0001.xml--><name>pig.reduce.output.dirs</name><value>hdfs://ec2-54-221-155-172.compute-1.amazonaws.com:9000/tmp/temp-1899968498/tmp625305144</value></property>
{code}

There are more such configurations. The data gets dumped to these directories, the script that you used, doesn't do it. I will try to repro the issue.
","13/Apr/16 3:18 AM;tanishg;Just come up with this issue again from komli. Are we not deleting temp files when the pig query is failed ?
See commandId=22222934 in production. 
Just now we just manually deleted it from the cluster.

{code:java}
[ec2-user@ip-10-250-5-171 ~]$ hadoop dfs -ls /tmp/temp1625205435/ 
Found 3 items 
drwxr-xr-x - revx.uxrt@revx.io supergroup 0 2016-04-13 03:48 /tmp/temp1625205435/tmp-1275236197 
drwxr-xr-x - revx.uxrt@revx.io supergroup 0 2016-04-13 03:48 /tmp/temp1625205435/tmp-609927664 
drwxr-xr-x - revx.uxrt@revx.io supergroup 0 2016-04-13 03:48 /tmp/temp1625205435/tmp2079196898 
[ec2-user@ip-10-250-5-171 ~]$ hadoop dfs -rmr /tmp/temp1625205435/ 
Deleted hdfs://ec2-54-86-239-241.compute-1.amazonaws.com:9000/tmp/temp1625205435 
{code}","18/May/16 3:16 AM;abhisheks;[~swatis] any update on this, can we give some eta to customer ?","18/May/16 3:20 AM;swatis;There is a hadoop collectfiles sub command that needs to be added in the crontab. If it is already there, I am guessing it has some perm issue and is not able to delete data under /tmp.

Someone from ACM team should be able to fix this. The cron entry should go in the cluster via udf via hustler.",18/May/16 3:24 AM;abhisheks;assigned to [~ajayb],"18/May/16 6:19 AM;ajayb;If its about cleaning up old temp files, then you can refer to [~Harsh]'s change for ACM-270 (commit id: a85b6082600f8b5e5d48ced9a78cedd608380f0e). Assigning to [~beria] for taking care of this.

Generally its preferable for each engine to know & manage their own crontab entries.","24/May/16 3:54 AM;venkatak;I feel this is a core pig bug [~swatis][~beria] cleaning up the /tmp is just a workaround and cleaning it up is just going to delay the problem, but not actually solve the underlying problem",07/Jul/17 2:34 PM;snamburu;[~beria] any update on this issue?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming job with --files option shows task failures with file not found .. ,HAD-371,27324,Bug,Open,HAD,Hadoop,software,hiyer,,,Major,,jssarma,adubey,addon_zendesk_for_jira,28/Feb/15 2:33 PM,17/May/17 9:48 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,Streaming job with --files option shows task failures with file not found .. ,,addon_zendesk_for_jira,adubey,jssarma,mahuja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,pinterest,,,,,{},,,,,,,,,,,,,,,,,,,,1|z00kwr:,,,,,,,,,,,,,,,,,,,1.0,945,2015-03-03 21:50:59.845,,,"28/Feb/15 2:34 PM;adubey;https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fec2-54-166-137-200.compute-1.amazonaws.com%3A50030%2Fjobdetails.jsp%3Fjobid%3Djob_449.201502050239_8974


https://api.qubole.com/qpal/handle_proxy?query=http%3A%2F%2Fec2-54-166-137-200.compute-1.amazonaws.com%3A50030%2Fjobdetails.jsp%3Fjobid%3Djob_449.201502050239_12414
","02/Mar/15 5:53 PM;adubey;[~jssarma] can we get someone to look into this. 

This was very strange issue like out of 1000 reducers only 20 will fail and recover in next attempt. anything we know as a corner case where this might be slipping? i have not got any chance to play with this.

-AD",03/Mar/15 9:50 PM;jssarma;ok - let me take a look.,"04/Mar/15 6:34 AM;jssarma;leaving notes on whatever i am finding on the linked ticket
","04/Mar/15 7:01 AM;jssarma;I think the file is getting deleted from the localized copy before the task runs:

2015-03-04 00:05:36,428 INFO -1 org.apache.hadoop.filecache.DistributedCache (Thread-2534321): Removed cache /media/ephemeral0/mapred/local/taskTracker/archive/5760362795658891660/ec2-54-166-137-200.compute-1.amazonaws.com/d0a06decd955c3da51908dfba86d886e_archive.zip/frequency_file.txt

the task tried to load the file at 00:59 - and didn't find the file. i think this has something to do with space pressure on these nodes. the ephemeral drives are incredibly full.

","04/Mar/15 7:11 AM;jssarma;at the begining of the hour - the node deleted a shit load of stuff frm the cached archives:

[ec2-user@ip-10-182-77-168 logs]$ fgrep 'Deleting local' hadoop-hadoop-tasktracker-ip-10-182-77-168.log.2015-03-04-00 | grep 00:05 | wc -l
652


which would be fine if it was re-cached - but seems like not ..",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hustler won't delete the security group on cluster termination but shows logs to the effect,ACM-1404,25004,Bug,Open,ACM,AWS Cluster Management,software,ajayb,Issues related to cluster provisioning/management,,Major,,ajayb,tsp,tsp,30/Nov/14 10:36 PM,08/Aug/17 4:40 PM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,,,,,,,,"We show the following when terminating a cluster

{noformat}
                 10646 qa_qbol_acc123_cl10646 ec2-54-161-117-29.compute-1.amazonaws.com
2014-12-01 06:10:54,772 PID: 9046 utils.py:644 - DEBUG - Terminated encrypted channel to cluster (qa_qbol_acc123_cl10646)
                 10646 qa_qbol_acc123_cl10646 ec2-54-161-117-29.compute-1.amazonaws.com
2014-12-01 06:10:56,556 PID: 9046 utils.py:670 - DEBUG - Terminated reverse tunnel to cluster (qa_qbol_acc123_cl10646)
2014-12-01 06:11:19,909 PID: 9046 awsutils.py:238 - INFO - Removing @sc-qa_qbol_acc123_cl10646 security group, attempt: 0
2014-12-01 06:11:24,968 PID: 9046 awsutils.py:238 - INFO - Removing @sc-qa_qbol_acc123_cl10646 security group, attempt: 1
2014-12-01 06:11:30,132 PID: 9046 awsutils.py:238 - INFO - Removing @sc-qa_qbol_acc123_cl10646 security group, attempt: 2
2014-12-01 06:29:55,916 PID: 22301 awsutils.py:261 - WARNING - Unable to terminate SG, last error (InvalidGroup.InUse), proceeding anyway
{noformat}

But don't actually remove the security group. This sequence of messages also
has no clear conclusion as it's not clear what happened of the remove
operation. 

The log below
2014-12-01 06:29:55,916 PID: 22301 awsutils.py:261 - WARNING - Unable to terminate SG, last error (InvalidGroup.InUse), proceeding anyway

should also read (delete SG) not terminate SG. 

We do delete the Sec. Group on deletion of cluster however and similarly do not conclude the sequence of 'Remove' messages..
",,ajayb,Harsh,sbadam,tsp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Jul/17 11:27 AM;sbadam;2670_aws_console.png;https://qubole.atlassian.net/secure/attachment/46275/2670_aws_console.png,31/Jul/17 11:27 AM;sbadam;2670_qubole_clusters.png;https://qubole.atlassian.net/secure/attachment/46276/2670_qubole_clusters.png,31/Jul/17 11:27 AM;sbadam;4520_aws_console.png;https://qubole.atlassian.net/secure/attachment/46273/4520_aws_console.png,31/Jul/17 11:27 AM;sbadam;4520_qubole_clusters.png;https://qubole.atlassian.net/secure/attachment/46274/4520_qubole_clusters.png,,,,,,,,,,,,,,,,bloomreach,,,,,{},,,,,,,,,,,,,,,,,,,,1|z009lf:,,,,,,,,,,,,,,,,,,,1.0,16094,2017-07-31 11:30:44.125,,,"31/Jul/17 11:30 AM;sbadam;A user from Bloomreach reported that security groups were not cleared after cluster was terminated. They mentioned they are manually clearing them. In customer words:


{code:java}
For example, for account 2670, we have 7 active clusters, while we have 12 security groups. The temporary SG for some of the inactive clusters are not cleaned up.

The same happens to account 4520, and all other accounts we own.

Right now we are cleaning this up manually, please let us know when will it be resolved.
{code}

I am aware that we encounter ""InvalidGroup.InUse"" when we try to delete cluster security groups during termination logic. Can we re-try multiple times after 30-40 seconds before proceeding away(through gated feature etc)? I am checking with customer if they are ok to have extra cluster termination time in this case. 
",02/Aug/17 10:13 AM;sbadam;Customer(Bloomreach) is ok to retry to delete security groups. He is ok as long as it happens within reasonable time.  ,"02/Aug/17 10:33 AM;ajayb;[~sbadam] while terminating cluster we already do 3 retries if we get errors `InvalidGroup.InUse` or `DependencyViolation` during deletion of security group. These retries work in most cases. Increasing retries doesn't seem like a good idea as it would delay cluster stop and adding a feature flag just for this is too much overhead. Btw whenever a cluster is deleted, then as the description of jira says, the corresponding security group will be deleted and so we shouldn't see security groups lying around for deleted clusters.","02/Aug/17 7:55 PM;sbadam;Thanks [~ajayb] for more info. I have conveyed your inputs to customer. Also, I have enabled Dynamic Tunnel Discovery for 3 of their accounts so that SGs will be limited to our tunnels. I keep you posted as soon as customer comes back. ","03/Aug/17 10:37 PM;sbadam;Thanks [~ajayb] for your inputs. I have informed all these inputs to customer. He asks:


{code:java}
would it be possible to send us some of te following information:

when a security group can't be deleted for any reason, please send us a notification, which SG wasn't deleted, and why it failed.

if you have this for security groups that failed to be deleted over the last few days, that would also be welcome.
{code}

-> Do you think it is feasible to implement it? I think we are coming with cluster termination logs in UI soon in RB46/ 47, maybe that can provide some info? You can confirm on release#.
","06/Aug/17 11:00 PM;ajayb;[~sbadam] these should show up in the cluster terminate command logs. [~Harsh] can you confirm? Also, is `cluster.use_cmc_for_terminate` feature tested & ready to enable for select customers? Then bloomreach may be a candidate due to the above ask. Btw one thing I noticed is that its missing in the file `features_list.yml` of tapp that is used for feature rollouts.","08/Aug/17 4:34 PM;Harsh;Account feature enabled will help in providing backend support for termination logs.  But the jira: UI-6153 is needed to display the logs in the UI.
The feature had faced a bug and we will release it in rb46. The plan was to add it to feature_list once UI is available and till then, enable as per request. Should we change the plan?","08/Aug/17 4:40 PM;Harsh;
They want a notification for such cases: Displaying it in logs should be sufficient. It happens lot of times and not sure if email is the right thing to do.

Having a feature which will wait for a a very long time to make sure we delete the SG: Something we can consider. I was wondering why do they accumulate so many security groups? Do they delete and recreate clusters lots of times?

cc [~sbadam]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DFS clients error out when spot nodes are removed en masse,QBOL-2071,18037,Bug,Open,QBOL,qbol,software,sumitm,,,Major,,hiyer,jssarma,jssarma,01/Nov/13 5:39 AM,11/Jul/17 3:48 AM,09/Aug/17 6:03 AM,,,,,0,future,jira_escalated,,,,,,,"these are probably happening due to node decomissioning ..

Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
java.io.IOException: Bad connect ack with firstBadLink 10.245.182.75:50010
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:4849)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:4750)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2900(DFSClient.java:3765)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:4080)

Job Submission failed with exception 'java.io.IOException(Bad connect ack with firstBadLink 10.245.182.75:50010)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask",,jssarma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,pinterest,,,,,{},,,,,,,,,,,,,,,,,,,,1|xzzhjc:,,,,,,,,,,,,,,,,,,,1.0,10982,,,,"01/Nov/13 6:09 AM;jssarma;this happened when the nodes were lost because of price spike (presumably).

there's a lost task tracker report on the JT for this node:
2013-11-01 12:20:38,661 INFO -1 org.apache.hadoop.mapred.TaskErrorCollector (IPC Server handler 13 on 9001): Undefined diagnostic info:Lost task tracker: tracker_ip-10-245-182-75.ec2.internal:localhost/127.0.0.1:48108
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jets3t warnings,QBOL-1210,15212,Bug,Reopened,QBOL,qbol,software,sumitm,,,Major,,jssarma,rgupta,rgupta,06/Mar/13 11:30 PM,10/Jul/17 6:35 AM,09/Aug/17 6:03 AM,,,,,0,jira_escalated,SPRINT005,,,,,,,"while running db_export commands jets3t seems to be givbving a lot of following warnings: 
2013-03-07 07:25:22,095 INFO commons.py:4 - printStream - sqoop stderr: 13/03/07 07:25:22 INFO s3native.Jets3tNativeFileSystemStore: Jets3tNativeFileSystemStore: multi-part upload set to false
2013-03-07 07:25:24,044 INFO commons.py:4 - printStream - sqoop stderr: 13/03/07 07:25:24 WARN httpclient.RestStorageService: Error Response: HEAD '/dataset%2F12mar2012demo%2F2012-03-12T03-00' -- ResponseCode: 404, ResponseStatus: Not Found, Request Headers: [Content-Type: , x-amz-request-payer: requester, Date: Thu, 07 Mar 2013 07:25:22 GMT, Authorization: AWS AKIAJ4HY7NQ7OKGUIMIQ:1BuGlf0pHaak6yZvv3Acs16iMt4=, User-Agent: AWS-ElasticMapReduce, Host: dev.canopydata.com.s3.amazonaws.com], Response Headers: [x-amz-request-id: 665DA0BB85813A33, x-amz-id-2: r5x7033q2ZTNSBRxHT8X0dYOT6i8s37uEbvG+QmSwPSvtINmJ71qDZfus/L5aMvn, Content-Type: application/xml, Transfer-Encoding: chunked, Date: Thu, 07 Mar 2013 07:25:23 GMT, Server: AmazonS3]
2013-03-07 07:25:24,114 INFO commons.py:4 - printStream - sqoop stderr: 13/03/07 07:25:24 WARN httpclient.RestStorageService: Error Response: HEAD 



Look at cid:17040 on staging jssarma@qubole.com

Would be great it we can figure out y they are happening and remove them from logs that we show to users.",,ekang,jssarma,mahuja,rgupta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,scripps,,,,,{},,,,,,,,,,,,,,,,,,,,1|xzzb2g:,,,,,,,,,,,,,,,,,,,1.0,11620,2013-03-07 00:47:27.608,,,"07/Mar/13 12:47 AM;mahuja;WARN httpclient.RestStorageService: Error Response: HEAD '/dataset%2F12mar2012demo%2F2012-03-12T03-00' -- ResponseCode: 404

The path is URL encoded which looks odd.","07/Mar/13 9:36 AM;jssarma;these are very common and appear all the time. the reason is that the jets3t code - when asked to list a directory for example - tries out a bunch of different heuristics:
- a directory could be named with a name ending in '$folder'
- a common prefix ending in '/' for a bunch of objects

when trying out these heuristics - the underlying REST api layer hits failures and those get logged. there's no easy fixes here since the logs are produced by a different layer than the one trying out the heuristics.

one option is to change the order of these heuristics - it might reduce the log spam - but i am not sure what inadvertent side effect might show up. another option is to change the log level in code for these errors (since an api call that fails and is critical will always result in a higher level exception etc.)",27/May/13 12:03 AM;jssarma;bumped log level for these errors. these lines disappeared in testing at least. unfortunately haven't investigated whether this can be made more optimal.,"15/Dec/16 11:33 AM;ekang;I'm seeing this same warning which translates to a fatal error and is causing the job to fail. https://api.qubole.com/cluster-proxy?encodedUrl=http%3A%2F%2Fec2-54-198-86-94.compute-1.amazonaws.com%3A19888%2Fjobhistory%2Fjob%2Fjob_1469553224919_78947%2F&clusterInst=217415

Caused by: java.io.FileNotFoundException: File s3n://sni-dds-qubole/defloc/warehouse/fw_product_working.db/classify/year=2016/month=12/day=2016-12-01/display=video does not exist.
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.listStatus(NativeS3FileSystem.java:1471)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.cleanS3DynDirectory(FileSinkOperator.java:1178)
	... 19 more

2016-12-15 14:28:04,987 INFO [main] org.apache.hadoop.mapred.Task: Runnning cleanup for the task

I verified that the folder indeed does exist. Is this a latency issue where S3 lags behind the job? Can we add retry logic to wait for S3?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
